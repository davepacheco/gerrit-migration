commit 7ceec7242d0ee35dd8d5f189e63e5d7e0e99b353 (refs/changes/03/3703/1)
Author: John Levon <john.levon@joyent.com>
Date:   2018-03-21T15:02:58+00:00 (1 year, 7 months ago)
    
    OS-6797 bhyve should pin VCPU threads

diff --git a/usr/src/cmd/bhyve/Makefile b/usr/src/cmd/bhyve/Makefile
index 7975589c66..b206cc6911 100644
--- a/usr/src/cmd/bhyve/Makefile
+++ b/usr/src/cmd/bhyve/Makefile
@@ -87,7 +87,8 @@ CPPFLAGS =	-I$(COMPAT)/freebsd -I$(CONTRIB)/freebsd \
 		-I$(SRC)/uts/i86pc \
 		-I$(SRC)/lib/libdladm/common \
 		-DWITHOUT_CAPSICUM
-LDLIBS +=	-lsocket -lnsl -ldlpi -ldladm -lmd -luuid -lvmmapi -lz -lnvpair
+LDLIBS +=	-lsocket -lnsl -ldlpi -ldladm -lmd -luuid -lvmmapi
+LDLIBS +=	-lz -lnvpair -llgrp
 
 POST_PROCESS += ; $(GENSETDEFS) $@
 
diff --git a/usr/src/cmd/bhyve/bhyverun.c b/usr/src/cmd/bhyve/bhyverun.c
index dbc2414e23..6e62c37676 100644
--- a/usr/src/cmd/bhyve/bhyverun.c
+++ b/usr/src/cmd/bhyve/bhyverun.c
@@ -36,7 +36,7 @@
  * http://www.illumos.org/license/CDDL.
  *
  * Copyright 2015 Pluribus Networks Inc.
- * Copyright 2017 Joyent, Inc.
+ * Copyright 2018 Joyent, Inc.
  */
 
 #include <sys/cdefs.h>
@@ -87,6 +87,8 @@ __FBSDID("$FreeBSD$");
 #include "mem.h"
 #ifdef	__FreeBSD__
 #include "mevent.h"
+#else
+#include <sys/lgrp_user.h>
 #endif
 #include "mptbl.h"
 #include "pci_emul.h"
@@ -125,6 +127,7 @@ static char *progname;
 static const int BSP = 0;
 
 #ifndef	__FreeBSD__
+static int pin_vcpus;
 int bcons_wait = 0;
 int bcons_connected = 0;
 pthread_mutex_t bcons_wait_lock = PTHREAD_MUTEX_INITIALIZER;
@@ -168,7 +171,7 @@ usage(int code)
 #ifdef	__FreeBSD__
 		"       %*s [-m memsize[K|k|M|m|G|g|T|t] [-p vcpu:hostcpu] [-s <pci>] [-U uuid] <vm>\n"
 #else
-		"       %*s [-s <pci>] [-U uuid] <vm>\n"
+		"       %*s [-p vcpus|none] [-s <pci>] [-U uuid] <vm>\n"
 #endif
 		"       -a: local apic is in xAPIC mode (deprecated)\n"
 		"       -A: create ACPI tables\n"
@@ -182,6 +185,8 @@ usage(int code)
 		"       -m: memory size\n"
 #ifdef	__FreeBSD__
 		"       -p: pin 'vcpu' to 'hostcpu'\n"
+#else
+		"	-p: automatically pin threads\n"
 #endif
 		"       -P: vmexit from the guest on pause\n"
 		"       -s: <slot,driver,configinfo> PCI slot config\n"
@@ -267,6 +272,20 @@ pincpu_parse(const char *opt)
 	CPU_SET(pcpu, vcpumap[vcpu]);
 	return (0);
 }
+#else
+static int
+pincpu_parse(const char *opt)
+{
+	if (strcmp(opt, "vcpus") == 0) {
+		pin_vcpus = 1;
+		return (0);
+	} else if (strcmp(opt, "none") == 0) {
+		pin_vcpus = 0;
+		return (0);
+	}
+
+	return (-1);
+}
 #endif
 
 void
@@ -325,6 +344,16 @@ fbsdrun_start_thread(void *param)
 	snprintf(tname, sizeof(tname), "vcpu %d", vcpu);
 	pthread_set_name_np(mtp->mt_thr, tname);
 
+#ifndef	__FreeBSD__
+	if (pin_vcpus) {
+		int ret = vm_pin_cpu(mtp->mt_ctx);
+		if (ret == -1) {
+			err(EX_OSERR, "could not pin VCPU%d", vcpu);
+			exit(1);
+		}
+	}
+#endif
+
 	vm_loop(mtp->mt_ctx, vcpu, vmexit[vcpu].rip);
 
 	/* not reached */
@@ -881,11 +910,8 @@ main(int argc, char *argv[])
 	rtc_localtime = 1;
 	memflags = 0;
 
-#ifdef	__FreeBSD__
 	optstr = "abehuwxACHIPSWYp:g:c:s:m:l:B:U:";
-#else
-	optstr = "abehuwxACHIPSWYg:c:s:m:l:B:U:";
-#endif
+
 	while ((c = getopt(argc, argv, optstr)) != -1) {
 		switch (c) {
 		case 'a':
@@ -903,14 +929,12 @@ main(int argc, char *argv[])
 				    "configuration '%s'", optarg);
 			}
 			break;
-#ifdef	__FreeBSD__
 		case 'p':
 			if (pincpu_parse(optarg) != 0) {
 				errx(EX_USAGE, "invalid vcpu pinning "
 				    "configuration '%s'", optarg);
 			}
 			break;
-#endif
                 case 'c':
 			guest_ncpus = atoi(optarg);
 			break;
@@ -1002,6 +1026,24 @@ main(int argc, char *argv[])
 		exit(1);
 	}
 
+#ifndef	__FreeBSD__
+	if (pin_vcpus) {
+		lgrp_id_t home_lgrp = lgrp_home(P_LWPID, P_MYID);
+		if (home_lgrp == -1) {
+			perror("could not get home lgroup");
+			exit(1);
+		}
+
+		err = lgrp_affinity_set(P_PID, P_MYID, home_lgrp,
+		    LGRP_AFF_STRONG);
+
+		if (err != 0) {
+			perror("could not get set lgrp affinity");
+			exit(1);
+		}
+	}
+#endif
+
 	fbsdrun_set_capabilities(ctx, BSP);
 
 	vm_set_memflags(ctx, memflags);
diff --git a/usr/src/lib/libvmmapi/common/mapfile-vers b/usr/src/lib/libvmmapi/common/mapfile-vers
index d8797736ba..580cc4e9fc 100644
--- a/usr/src/lib/libvmmapi/common/mapfile-vers
+++ b/usr/src/lib/libvmmapi/common/mapfile-vers
@@ -11,7 +11,7 @@
 
 #
 # Copyright 2013 Pluribus Networks Inc.
-# Copyright 2017 Joyent, Inc.
+# Copyright 2018 Joyent, Inc.
 #
 
 #
@@ -90,6 +90,7 @@ SYMBOL_VERSION ILLUMOSprivate {
 		vm_mmap_memseg;
 		vm_open;
 		vm_parse_memsize;
+		vm_pin_cpu;
 		vm_reinit;
 		vm_restart_instruction;
 		vm_rtc_gettime;
diff --git a/usr/src/lib/libvmmapi/common/vmmapi.c b/usr/src/lib/libvmmapi/common/vmmapi.c
index 6a19b4d12e..0dd89001d0 100644
--- a/usr/src/lib/libvmmapi/common/vmmapi.c
+++ b/usr/src/lib/libvmmapi/common/vmmapi.c
@@ -1629,5 +1629,13 @@ vm_get_ioctls(size_t *len)
 	*len = nitems(vm_ioctl_cmds);
 	return (NULL);
 }
-#endif /* __FreeBSD__ */
 
+#else
+
+int
+vm_pin_cpu(struct vmctx *ctx)
+{
+	return (ioctl(ctx->fd, VM_PIN_CPU, NULL));
+}
+
+#endif /* __FreeBSD__ */
diff --git a/usr/src/lib/libvmmapi/common/vmmapi.h b/usr/src/lib/libvmmapi/common/vmmapi.h
index 5fc9988b24..77ca57538f 100644
--- a/usr/src/lib/libvmmapi/common/vmmapi.h
+++ b/usr/src/lib/libvmmapi/common/vmmapi.h
@@ -37,6 +37,9 @@
  *
  * Copyright 2015 Pluribus Networks Inc.
  */
+/*
+ * Copyright 2018 Joyent, Inc.
+ */
 
 #ifndef _VMMAPI_H_
 #define	_VMMAPI_H_
@@ -185,6 +188,7 @@ int	vm_setup_pptdev_msix(struct vmctx *ctx, int vcpu, int pptfd,
     int idx, uint64_t addr, uint64_t msg, uint32_t vector_control);
 int	vm_get_pptdev_limits(struct vmctx *ctx, int pptfd, int *msi_limit,
     int *msix_limit);
+int	vm_pin_cpu(struct vmctx *ctx);
 #endif /* __FreeBSD__ */
 
 int	vm_get_intinfo(struct vmctx *ctx, int vcpu, uint64_t *i1, uint64_t *i2);
diff --git a/usr/src/uts/i86pc/io/viona/viona.c b/usr/src/uts/i86pc/io/viona/viona.c
index ee1380ae7d..09a9caa981 100644
--- a/usr/src/uts/i86pc/io/viona/viona.c
+++ b/usr/src/uts/i86pc/io/viona/viona.c
@@ -42,6 +42,7 @@
 #include <sys/stat.h>
 #include <sys/ddi.h>
 #include <sys/disp.h>
+#include <sys/cpuvar.h>
 #include <sys/sunddi.h>
 #include <sys/sunndi.h>
 #include <sys/sysmacros.h>
@@ -1162,7 +1163,18 @@ viona_worker(void *arg)
 {
 	viona_vring_t *ring = (viona_vring_t *)arg;
 	viona_link_t *link = ring->vr_link;
-	proc_t *p = ttoproc(curthread);
+	processorid_t obind;
+	int err;
+
+	/*
+	 * Our parent thread may be a bound VCPU thread, but we don't want to
+	 * inherit such a binding.
+	 */
+	mutex_enter(&cpu_lock);
+	mutex_enter(&curproc->p_lock);
+	(void) cpu_bind_thread(curthread, PBIND_NONE, &obind, &err);
+	mutex_exit(&curproc->p_lock);
+	mutex_exit(&cpu_lock);
 
 	mutex_enter(&ring->vr_lock);
 	VERIFY(ring->vr_state == (VRS_RESET|VRS_SETUP));
@@ -1174,7 +1186,7 @@ viona_worker(void *arg)
 	while (ring->vr_state == VRS_INIT) {
 		(void) cv_wait_sig(&ring->vr_cv, &ring->vr_lock);
 
-		if (VRING_NEED_BAIL(ring, p)) {
+		if (VRING_NEED_BAIL(ring, curproc)) {
 			goto cleanup;
 		}
 	}
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c b/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c
index 602889b779..06bc5f572b 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c
+++ b/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c
@@ -28,6 +28,8 @@
 #include <sys/cpuset.h>
 #include <sys/id_space.h>
 #include <sys/fs/sdev_plugin.h>
+#include <sys/sdt.h>
+#include <sys/cmt.h>
 
 #include <sys/vmm.h>
 #include <sys/vmm_instruction_emul.h>
@@ -65,6 +67,7 @@ static uint_t		vmmdev_inst_count = 0;
 static boolean_t	vmmdev_load_failure;
 static kmutex_t		vmm_mtx;
 static list_t		vmmdev_list;
+static int		vmm_bound_pcpus[NCPU];
 
 static const char *vmmdev_hvm_name = "bhyve";
 
@@ -450,6 +453,196 @@ vcpu_unlock_all(vmm_softc_t *sc)
 		vcpu_unlock_one(sc, vcpu);
 }
 
+#define	CPUSCORE_WORST (SHRT_MAX)
+#define	CPUSCORE_SELF_BOUND (CPUSCORE_WORST - 1)
+#define	CPUSCORE_SELF_SIBLING_BOUND (CPUSCORE_SELF_BOUND - 1)
+#define	CPUSCORE_BOUND(n) \
+	(max(CPUSCORE_SELF_SIBLING_BOUND - 1, MAXCLSYSPRI + (n)))
+
+/*
+ * Penalize a CPU with bound threads, especially if we're bound to the CPU.
+ *
+ * In addition, we walk the PG hierarchy essentially to find our hyper-threaded
+ * sibling (PGHW_IPIPE): we will penalize a CPU based on its sibling, just not
+ * as badly.
+ */
+static short
+vmm_score_pcpu(cpu_t *cp, cpuset_t *avoidset)
+{
+	int count = vmm_bound_pcpus[cp->cpu_id] * 2;
+	group_t *cmt_group = &cp->cpu_pg->cmt_pgs;
+	pg_cmt_t *pg = NULL;
+
+	if (cp == cpu_inmotion)
+		return (CPUSCORE_WORST);
+	if (cpu_in_set(avoidset, cp->cpu_id))
+		return (CPUSCORE_SELF_BOUND);
+
+	for (int level = (int)GROUP_SIZE(cmt_group) - 1; level >= 0; level--) {
+		pg_cmt_t *tpg = GROUP_ACCESS(cmt_group, level);
+
+		if (tpg->cmt_pg.pghw_hw == PGHW_IPIPE &&
+		    GROUP_SIZE(&tpg->cmt_cpus_actv) > 1) {
+			pg = tpg;
+			break;
+		}
+	}
+
+	if (pg != NULL) {
+		for (int i = 0; i < GROUP_SIZE(&pg->cmt_cpus_actv); i++) {
+			cpu_t *sibcp = GROUP_ACCESS(&pg->cmt_cpus_actv, i);
+			int sibcount;
+
+			if (sibcp == cp)
+				continue;
+
+			if (cpu_in_set(avoidset, sibcp->cpu_id))
+				return (CPUSCORE_SELF_SIBLING_BOUND);
+
+			sibcount = vmm_bound_pcpus[sibcp->cpu_id];
+			DTRACE_PROBE3(vmm__sibling__bind__count, cpu_t *,
+			    cp, cpu_t *, sibcp, int, sibcount);
+			count += sibcount;
+		}
+	}
+
+	if (count == 0) {
+		short pri = cp->cpu_dispatch_pri;
+
+		if (cp->cpu_disp->disp_maxrunpri > pri)
+			pri = cp->cpu_disp->disp_maxrunpri;
+		if (cp->cpu_chosen_level > pri)
+			pri = cp->cpu_chosen_level;
+		return (pri);
+	}
+
+	return (CPUSCORE_BOUND(count));
+}
+
+/*
+ * Bind the current thread to a CPU; this has a significant beneficial effect on
+ * networking performance of the VM.  As we only have a static view of CPU
+ * utilization, our main job here is to prevent stupid bindings.  In particular
+ * we *really* don't want a single VM's VCPUs to share a PCPU (avoidset).
+ * We'd also prefer to stay in the same lgroup, and avoid any PCPUs that are
+ * already bound by VMs - especially if it's a hyper-threaded sibling.
+ *
+ * This is clearly not the right place architecturally to be doing this: a
+ * co-operative daemon in userspace would likely be far better.  However, this
+ * is sufficient for now.
+ */
+static cpu_t *
+vmm_choose_pcpu(cpuset_t *avoidset)
+{
+	kthread_t *t = curthread;
+	cpu_t *bestcpu = NULL;
+	klgrpset_t seen_lpls;
+
+	klgrpset_clear(seen_lpls);
+
+	/*
+	 * Walk through all leaf lgroups, starting from curthread.
+	 */
+	for (lpl_t *lpl = t->t_lpl; lpl != NULL; lpl = lpl->lpl_parent) {
+		short bestscore = CPUSCORE_WORST;
+
+		for (size_t i = 0; i < lpl->lpl_nrset; i++) {
+			lpl_t *leaf = lpl->lpl_rset[i];
+			size_t bias;
+			cpu_t *cp;
+
+			DTRACE_PROBE1(vmm__choose__pcpu__lpl, lpl_t *, leaf);
+
+			if (klgrpset_ismember(seen_lpls, leaf->lpl_lgrpid))
+				continue;
+
+			klgrpset_add(seen_lpls, leaf->lpl_lgrpid);
+
+			/*
+			 * Choose a random starting point (in case all CPUs are
+			 * scored equally).
+			 */
+			bias = gethrtime_unscaled() % leaf->lpl_ncpu;
+
+			for (cp = leaf->lpl_cpus; bias != 0; bias--)
+				cp = cp->cpu_next_lpl;
+
+			for (size_t j = 0; j < leaf->lpl_ncpu;
+			    cp = cp->cpu_next_lpl, j++) {
+				short score = vmm_score_pcpu(cp, avoidset);
+
+				if (score < bestscore) {
+					bestscore = score;
+					bestcpu = cp;
+
+					if (score < 0)
+						goto out;
+				}
+			}
+		}
+
+		/*
+		 * If we found a home lgroup CPU that isn't already used by this
+		 * same VM, then we will use it instead of considering remote
+		 * lgroups.
+		 */
+		if (lpl == t->t_lpl && bestscore < CPUSCORE_SELF_BOUND)
+			goto out;
+	}
+
+out:
+	ASSERT((bestcpu->cpu_flags & CPU_QUIESCED) == 0);
+	return (bestcpu);
+}
+
+int
+vmm_pin_cpu(vmm_softc_t *sc, int *rvalp)
+{
+	kthread_t *t = curthread;
+	processorid_t obind;
+	proc_t *p = curproc;
+	cpu_t *chosen;
+	int err;
+
+	if (CLASS_KERNEL(t->t_cid) || !hasprocperm(t->t_cred, CRED()))
+		return (EPERM);
+
+	mutex_enter(&cpu_lock);
+	mutex_enter(&pidlock);
+	mutex_enter(&p->p_lock);
+	mutex_enter(&vmm_mtx);
+
+	ASSERT(t->t_affinitycnt == 0);
+
+	thread_lock(t);
+	chosen = vmm_choose_pcpu(sc->vmm_bound_pcpus);
+	TB_CPU_SOFT_SET(t);
+	thread_unlock(t);
+
+	err = 0;
+	(void) cpu_bind_thread(t, chosen->cpu_id, &obind, &err);
+	if (err != 0)
+		goto out;
+
+	/*
+	 * Mark our bound physical CPU as used for the benefit of other VMs.
+	 */
+	if (!cpu_in_set(sc->vmm_bound_pcpus, chosen->cpu_id)) {
+		cpuset_add(sc->vmm_bound_pcpus, chosen->cpu_id);
+		vmm_bound_pcpus[chosen->cpu_id]++;
+	}
+
+	*rvalp = chosen->cpu_id;
+	err = 0;
+
+out:
+	mutex_exit(&vmm_mtx);
+	mutex_exit(&p->p_lock);
+	mutex_exit(&pidlock);
+	mutex_exit(&cpu_lock);
+	return (err);
+}
+
 static int
 vmmdev_do_ioctl(vmm_softc_t *sc, int cmd, intptr_t arg, int md,
     cred_t *credp, int *rvalp)
@@ -536,6 +729,11 @@ vmmdev_do_ioctl(vmm_softc_t *sc, int cmd, intptr_t arg, int md,
 	}
 
 	switch (cmd) {
+	case VM_PIN_CPU: {
+		error = vmm_pin_cpu(sc, rvalp);
+		break;
+	}
+
 	case VM_RUN: {
 		struct vm_run vmrun;
 
@@ -1299,12 +1497,12 @@ vmmdev_do_vm_create(char *name, cred_t *cr)
 	}
 
 	minor = id_alloc(vmmdev_minors);
-	if (ddi_soft_state_zalloc(vmm_statep, minor) != DDI_SUCCESS) {
-		goto fail;
-	} else if ((sc = ddi_get_soft_state(vmm_statep, minor)) == NULL) {
-		ddi_soft_state_free(vmm_statep, minor);
+	if (ddi_soft_state_zalloc(vmm_statep, minor) != DDI_SUCCESS)
 		goto fail;
-	} else if (ddi_create_minor_node(vmm_dip, name, S_IFCHR, minor,
+
+	sc = ddi_get_soft_state(vmm_statep, minor);
+
+	if (ddi_create_minor_node(vmm_dip, name, S_IFCHR, minor,
 	    DDI_PSEUDO, 0) != DDI_SUCCESS) {
 		goto fail;
 	}
@@ -1320,6 +1518,9 @@ vmmdev_do_vm_create(char *name, cred_t *cr)
 		    offsetof(vmm_hold_t, vmh_node));
 		cv_init(&sc->vmm_cv, NULL, CV_DEFAULT, NULL);
 
+		sc->vmm_bound_pcpus = cpuset_alloc(KM_SLEEP);
+		cpuset_zero(sc->vmm_bound_pcpus);
+
 		sc->vmm_zone = crgetzone(cr);
 		zone_hold(sc->vmm_zone);
 		vmm_zsd_add_vm(sc);
@@ -1574,6 +1775,18 @@ vmm_do_vm_destroy_locked(vmm_softc_t *sc, boolean_t clean_zsd)
 		return (EINTR);
 	}
 
+	/*
+	 * Indicate that we are no longer using our physical CPUs.
+	 */
+	for (size_t i = 0; i < NCPU; i++) {
+		if (cpu_in_set(sc->vmm_bound_pcpus, i)) {
+			vmm_bound_pcpus[i]--;
+			ASSERT(vmm_bound_pcpus[i] >= 0);
+		}
+	}
+
+	cpuset_free(sc->vmm_bound_pcpus);
+
 	/* Clean up devmem entries */
 	vmmdev_devmem_purge(sc);
 
diff --git a/usr/src/uts/i86pc/sys/vmm_dev.h b/usr/src/uts/i86pc/sys/vmm_dev.h
index d9cb23ece9..9e03c524ec 100644
--- a/usr/src/uts/i86pc/sys/vmm_dev.h
+++ b/usr/src/uts/i86pc/sys/vmm_dev.h
@@ -359,6 +359,7 @@ enum {
 #ifndef __FreeBSD__
 	/* illumos-custom ioctls */
 	IOCNUM_DEVMEM_GETOFFSET = 256,
+	IOCNUM_PIN_CPU = 257,
 #endif
 };
 
@@ -462,6 +463,8 @@ enum {
 #ifndef __FreeBSD__
 #define	VM_DEVMEM_GETOFFSET \
 	_IOW('v', IOCNUM_DEVMEM_GETOFFSET, struct vm_devmem_offset)
+#define	VM_PIN_CPU \
+	_IOWR('v', IOCNUM_PIN_CPU, int)
 
 /* ioctls used against ctl device for vm create/destroy */
 #define	VMM_IOC_BASE		(('V' << 16) | ('M' << 8))
diff --git a/usr/src/uts/i86pc/sys/vmm_impl.h b/usr/src/uts/i86pc/sys/vmm_impl.h
index 4d5708e48d..e2a407c2eb 100644
--- a/usr/src/uts/i86pc/sys/vmm_impl.h
+++ b/usr/src/uts/i86pc/sys/vmm_impl.h
@@ -63,6 +63,8 @@ struct vmm_softc {
 	list_t		vmm_holds;
 	kcondvar_t	vmm_cv;
 
+	cpuset_t	*vmm_bound_pcpus;
+
 	/* For zone specific data */
 	list_node_t	vmm_zsd_linkage;
 	zone_t		*vmm_zone;
