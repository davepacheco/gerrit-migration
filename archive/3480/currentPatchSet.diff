commit 7859c1369f34e4ea328514c82b3579e7f3896a57 (refs/changes/80/3480/4)
Author: Patrick Mooney <pmooney@pfmooney.com>
Date:   2018-02-27T01:16:27+00:00 (1 year, 7 months ago)
    
    OS-6672 update vmm to FreeBSD 11.1
    OS-6673 update libvmmapi to FreeBSD 11.1
    OS-6690 want kernel driver API into bhyve
    Reviewed by: Jerry Jelinek <jerry.jelinek@joyent.com>
    Approved by: Jerry Jelinek <jerry.jelinek@joyent.com>

diff --git a/usr/contrib/freebsd/amd64/machine/pmap.h b/usr/contrib/freebsd/amd64/machine/pmap.h
new file mode 100644
index 0000000000..a0b8ee37f2
--- /dev/null
+++ b/usr/contrib/freebsd/amd64/machine/pmap.h
@@ -0,0 +1,455 @@
+/*-
+ * Copyright (c) 2003 Peter Wemm.
+ * Copyright (c) 1991 Regents of the University of California.
+ * All rights reserved.
+ *
+ * This code is derived from software contributed to Berkeley by
+ * the Systems Programming Group of the University of Utah Computer
+ * Science Department and William Jolitz of UUNET Technologies Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 4. Neither the name of the University nor the names of its contributors
+ *    may be used to endorse or promote products derived from this software
+ *    without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * Derived from hp300 version by Mike Hibler, this version by William
+ * Jolitz uses a recursive map [a pde points to the page directory] to
+ * map the page tables using the pagetables themselves. This is done to
+ * reduce the impact on kernel virtual memory for lots of sparse address
+ * space, and to reduce the cost of memory to each process.
+ *
+ *	from: hp300: @(#)pmap.h	7.2 (Berkeley) 12/16/90
+ *	from: @(#)pmap.h	7.4 (Berkeley) 5/12/91
+ * $FreeBSD$
+ */
+
+#ifndef _MACHINE_PMAP_H_
+#define	_MACHINE_PMAP_H_
+
+/*
+ * Page-directory and page-table entries follow this format, with a few
+ * of the fields not present here and there, depending on a lot of things.
+ */
+				/* ---- Intel Nomenclature ---- */
+#define	X86_PG_V	0x001	/* P	Valid			*/
+#define	X86_PG_RW	0x002	/* R/W	Read/Write		*/
+#define	X86_PG_U	0x004	/* U/S  User/Supervisor		*/
+#define	X86_PG_NC_PWT	0x008	/* PWT	Write through		*/
+#define	X86_PG_NC_PCD	0x010	/* PCD	Cache disable		*/
+#define	X86_PG_A	0x020	/* A	Accessed		*/
+#define	X86_PG_M	0x040	/* D	Dirty			*/
+#define	X86_PG_PS	0x080	/* PS	Page size (0=4k,1=2M)	*/
+#define	X86_PG_PTE_PAT	0x080	/* PAT	PAT index		*/
+#define	X86_PG_G	0x100	/* G	Global			*/
+#define	X86_PG_AVAIL1	0x200	/*    /	Available for system	*/
+#define	X86_PG_AVAIL2	0x400	/*   <	programmers use		*/
+#define	X86_PG_AVAIL3	0x800	/*    \				*/
+#define	X86_PG_PDE_PAT	0x1000	/* PAT	PAT index		*/
+#define	X86_PG_NX	(1ul<<63) /* No-execute */
+#define	X86_PG_AVAIL(x)	(1ul << (x))
+
+/* Page level cache control fields used to determine the PAT type */
+#define	X86_PG_PDE_CACHE (X86_PG_PDE_PAT | X86_PG_NC_PWT | X86_PG_NC_PCD)
+#define	X86_PG_PTE_CACHE (X86_PG_PTE_PAT | X86_PG_NC_PWT | X86_PG_NC_PCD)
+
+/*
+ * Intel extended page table (EPT) bit definitions.
+ */
+#define	EPT_PG_READ		0x001	/* R	Read		*/
+#define	EPT_PG_WRITE		0x002	/* W	Write		*/
+#define	EPT_PG_EXECUTE		0x004	/* X	Execute		*/
+#define	EPT_PG_IGNORE_PAT	0x040	/* IPAT	Ignore PAT	*/
+#define	EPT_PG_PS		0x080	/* PS	Page size	*/
+#define	EPT_PG_A		0x100	/* A	Accessed	*/
+#define	EPT_PG_M		0x200	/* D	Dirty		*/
+#define	EPT_PG_MEMORY_TYPE(x)	((x) << 3) /* MT Memory Type	*/
+
+/*
+ * Define the PG_xx macros in terms of the bits on x86 PTEs.
+ */
+#define	PG_V		X86_PG_V
+#define	PG_RW		X86_PG_RW
+#define	PG_U		X86_PG_U
+#define	PG_NC_PWT	X86_PG_NC_PWT
+#define	PG_NC_PCD	X86_PG_NC_PCD
+#define	PG_A		X86_PG_A
+#define	PG_M		X86_PG_M
+#define	PG_PS		X86_PG_PS
+#define	PG_PTE_PAT	X86_PG_PTE_PAT
+#define	PG_G		X86_PG_G
+#define	PG_AVAIL1	X86_PG_AVAIL1
+#define	PG_AVAIL2	X86_PG_AVAIL2
+#define	PG_AVAIL3	X86_PG_AVAIL3
+#define	PG_PDE_PAT	X86_PG_PDE_PAT
+#define	PG_NX		X86_PG_NX
+#define	PG_PDE_CACHE	X86_PG_PDE_CACHE
+#define	PG_PTE_CACHE	X86_PG_PTE_CACHE
+
+/* Our various interpretations of the above */
+#define	PG_W		X86_PG_AVAIL3	/* "Wired" pseudoflag */
+#define	PG_MANAGED	X86_PG_AVAIL2
+#define	EPT_PG_EMUL_V	X86_PG_AVAIL(52)
+#define	EPT_PG_EMUL_RW	X86_PG_AVAIL(53)
+#define	PG_PROMOTED	X86_PG_AVAIL(54)	/* PDE only */
+#define	PG_FRAME	(0x000ffffffffff000ul)
+#define	PG_PS_FRAME	(0x000fffffffe00000ul)
+
+/*
+ * Promotion to a 2MB (PDE) page mapping requires that the corresponding 4KB
+ * (PTE) page mappings have identical settings for the following fields:
+ */
+#define	PG_PTE_PROMOTE	(PG_NX | PG_MANAGED | PG_W | PG_G | PG_PTE_CACHE | \
+	    PG_M | PG_A | PG_U | PG_RW | PG_V)
+
+/*
+ * Page Protection Exception bits
+ */
+
+#define PGEX_P		0x01	/* Protection violation vs. not present */
+#define PGEX_W		0x02	/* during a Write cycle */
+#define PGEX_U		0x04	/* access from User mode (UPL) */
+#define PGEX_RSV	0x08	/* reserved PTE field is non-zero */
+#define PGEX_I		0x10	/* during an instruction fetch */
+
+/* 
+ * undef the PG_xx macros that define bits in the regular x86 PTEs that
+ * have a different position in nested PTEs. This is done when compiling
+ * code that needs to be aware of the differences between regular x86 and
+ * nested PTEs.
+ *
+ * The appropriate bitmask will be calculated at runtime based on the pmap
+ * type.
+ */
+#ifdef AMD64_NPT_AWARE
+#undef PG_AVAIL1		/* X86_PG_AVAIL1 aliases with EPT_PG_M */
+#undef PG_G
+#undef PG_A
+#undef PG_M
+#undef PG_PDE_PAT
+#undef PG_PDE_CACHE
+#undef PG_PTE_PAT
+#undef PG_PTE_CACHE
+#undef PG_RW
+#undef PG_V
+#endif
+
+/*
+ * Pte related macros.  This is complicated by having to deal with
+ * the sign extension of the 48th bit.
+ */
+#define KVADDR(l4, l3, l2, l1) ( \
+	((unsigned long)-1 << 47) | \
+	((unsigned long)(l4) << PML4SHIFT) | \
+	((unsigned long)(l3) << PDPSHIFT) | \
+	((unsigned long)(l2) << PDRSHIFT) | \
+	((unsigned long)(l1) << PAGE_SHIFT))
+
+#define UVADDR(l4, l3, l2, l1) ( \
+	((unsigned long)(l4) << PML4SHIFT) | \
+	((unsigned long)(l3) << PDPSHIFT) | \
+	((unsigned long)(l2) << PDRSHIFT) | \
+	((unsigned long)(l1) << PAGE_SHIFT))
+
+/*
+ * Number of kernel PML4 slots.  Can be anywhere from 1 to 64 or so,
+ * but setting it larger than NDMPML4E makes no sense.
+ *
+ * Each slot provides .5 TB of kernel virtual space.
+ */
+#define NKPML4E		4
+
+#define	NUPML4E		(NPML4EPG/2)	/* number of userland PML4 pages */
+#define	NUPDPE		(NUPML4E*NPDPEPG)/* number of userland PDP pages */
+#define	NUPDE		(NUPDPE*NPDEPG)	/* number of userland PD entries */
+
+/*
+ * NDMPML4E is the maximum number of PML4 entries that will be
+ * used to implement the direct map.  It must be a power of two,
+ * and should generally exceed NKPML4E.  The maximum possible
+ * value is 64; using 128 will make the direct map intrude into
+ * the recursive page table map.
+ */
+#define	NDMPML4E	8
+
+/*
+ * These values control the layout of virtual memory.  The starting address
+ * of the direct map, which is controlled by DMPML4I, must be a multiple of
+ * its size.  (See the PHYS_TO_DMAP() and DMAP_TO_PHYS() macros.)
+ *
+ * Note: KPML4I is the index of the (single) level 4 page that maps
+ * the KVA that holds KERNBASE, while KPML4BASE is the index of the
+ * first level 4 page that maps VM_MIN_KERNEL_ADDRESS.  If NKPML4E
+ * is 1, these are the same, otherwise KPML4BASE < KPML4I and extra
+ * level 4 PDEs are needed to map from VM_MIN_KERNEL_ADDRESS up to
+ * KERNBASE.
+ *
+ * (KPML4I combines with KPDPI to choose where KERNBASE starts.
+ * Or, in other words, KPML4I provides bits 39..47 of KERNBASE,
+ * and KPDPI provides bits 30..38.)
+ */
+#define	PML4PML4I	(NPML4EPG/2)	/* Index of recursive pml4 mapping */
+
+#define	KPML4BASE	(NPML4EPG-NKPML4E) /* KVM at highest addresses */
+#define	DMPML4I		rounddown(KPML4BASE-NDMPML4E, NDMPML4E) /* Below KVM */
+
+#define	KPML4I		(NPML4EPG-1)
+#define	KPDPI		(NPDPEPG-2)	/* kernbase at -2GB */
+
+/*
+ * XXX doesn't really belong here I guess...
+ */
+#define ISA_HOLE_START    0xa0000
+#define ISA_HOLE_LENGTH (0x100000-ISA_HOLE_START)
+
+#define	PMAP_PCID_NONE		0xffffffff
+#define	PMAP_PCID_KERN		0
+#define	PMAP_PCID_OVERMAX	0x1000
+
+#ifndef LOCORE
+
+#include <sys/queue.h>
+#include <sys/_cpuset.h>
+#include <sys/_lock.h>
+#include <sys/_mutex.h>
+
+#include <vm/_vm_radix.h>
+
+typedef u_int64_t pd_entry_t;
+typedef u_int64_t pt_entry_t;
+typedef u_int64_t pdp_entry_t;
+typedef u_int64_t pml4_entry_t;
+
+/*
+ * Address of current address space page table maps and directories.
+ */
+#ifdef _KERNEL
+#define	addr_PTmap	(KVADDR(PML4PML4I, 0, 0, 0))
+#define	addr_PDmap	(KVADDR(PML4PML4I, PML4PML4I, 0, 0))
+#define	addr_PDPmap	(KVADDR(PML4PML4I, PML4PML4I, PML4PML4I, 0))
+#define	addr_PML4map	(KVADDR(PML4PML4I, PML4PML4I, PML4PML4I, PML4PML4I))
+#define	addr_PML4pml4e	(addr_PML4map + (PML4PML4I * sizeof(pml4_entry_t)))
+#define	PTmap		((pt_entry_t *)(addr_PTmap))
+#define	PDmap		((pd_entry_t *)(addr_PDmap))
+#define	PDPmap		((pd_entry_t *)(addr_PDPmap))
+#define	PML4map		((pd_entry_t *)(addr_PML4map))
+#define	PML4pml4e	((pd_entry_t *)(addr_PML4pml4e))
+
+extern int nkpt;		/* Initial number of kernel page tables */
+extern u_int64_t KPDPphys;	/* physical address of kernel level 3 */
+extern u_int64_t KPML4phys;	/* physical address of kernel level 4 */
+
+/*
+ * virtual address to page table entry and
+ * to physical address.
+ * Note: these work recursively, thus vtopte of a pte will give
+ * the corresponding pde that in turn maps it.
+ */
+pt_entry_t *vtopte(vm_offset_t);
+#define	vtophys(va)	pmap_kextract(((vm_offset_t) (va)))
+
+#define	pte_load_store(ptep, pte)	atomic_swap_long(ptep, pte)
+#define	pte_load_clear(ptep)		atomic_swap_long(ptep, 0)
+#define	pte_store(ptep, pte) do { \
+	*(u_long *)(ptep) = (u_long)(pte); \
+} while (0)
+#define	pte_clear(ptep)			pte_store(ptep, 0)
+
+#define	pde_store(pdep, pde)		pte_store(pdep, pde)
+
+extern pt_entry_t pg_nx;
+
+#endif /* _KERNEL */
+
+/*
+ * Pmap stuff
+ */
+struct	pv_entry;
+struct	pv_chunk;
+
+/*
+ * Locks
+ * (p) PV list lock
+ */
+struct md_page {
+	TAILQ_HEAD(, pv_entry)	pv_list;  /* (p) */
+	int			pv_gen;   /* (p) */
+	int			pat_mode;
+};
+
+enum pmap_type {
+	PT_X86,			/* regular x86 page tables */
+	PT_EPT,			/* Intel's nested page tables */
+	PT_RVI,			/* AMD's nested page tables */
+};
+
+struct pmap_pcids {
+	uint32_t	pm_pcid;
+	uint32_t	pm_gen;
+};
+
+/*
+ * The kernel virtual address (KVA) of the level 4 page table page is always
+ * within the direct map (DMAP) region.
+ */
+struct pmap {
+	struct mtx		pm_mtx;
+	pml4_entry_t		*pm_pml4;	/* KVA of level 4 page table */
+	uint64_t		pm_cr3;
+	TAILQ_HEAD(,pv_chunk)	pm_pvchunk;	/* list of mappings in pmap */
+	cpuset_t		pm_active;	/* active on cpus */
+	enum pmap_type		pm_type;	/* regular or nested tables */
+	struct pmap_statistics	pm_stats;	/* pmap statistics */
+	struct vm_radix		pm_root;	/* spare page table pages */
+	long			pm_eptgen;	/* EPT pmap generation id */
+	int			pm_flags;
+	struct pmap_pcids	pm_pcids[MAXCPU];
+};
+
+/* flags */
+#define	PMAP_NESTED_IPIMASK	0xff
+#define	PMAP_PDE_SUPERPAGE	(1 << 8)	/* supports 2MB superpages */
+#define	PMAP_EMULATE_AD_BITS	(1 << 9)	/* needs A/D bits emulation */
+#define	PMAP_SUPPORTS_EXEC_ONLY	(1 << 10)	/* execute only mappings ok */
+
+typedef struct pmap	*pmap_t;
+
+#ifdef _KERNEL
+extern struct pmap	kernel_pmap_store;
+#define kernel_pmap	(&kernel_pmap_store)
+
+#define	PMAP_LOCK(pmap)		mtx_lock(&(pmap)->pm_mtx)
+#define	PMAP_LOCK_ASSERT(pmap, type) \
+				mtx_assert(&(pmap)->pm_mtx, (type))
+#define	PMAP_LOCK_DESTROY(pmap)	mtx_destroy(&(pmap)->pm_mtx)
+#define	PMAP_LOCK_INIT(pmap)	mtx_init(&(pmap)->pm_mtx, "pmap", \
+				    NULL, MTX_DEF | MTX_DUPOK)
+#define	PMAP_LOCKED(pmap)	mtx_owned(&(pmap)->pm_mtx)
+#define	PMAP_MTX(pmap)		(&(pmap)->pm_mtx)
+#define	PMAP_TRYLOCK(pmap)	mtx_trylock(&(pmap)->pm_mtx)
+#define	PMAP_UNLOCK(pmap)	mtx_unlock(&(pmap)->pm_mtx)
+
+int	pmap_pinit_type(pmap_t pmap, enum pmap_type pm_type, int flags);
+int	pmap_emulate_accessed_dirty(pmap_t pmap, vm_offset_t va, int ftype);
+#endif
+
+/*
+ * For each vm_page_t, there is a list of all currently valid virtual
+ * mappings of that page.  An entry is a pv_entry_t, the list is pv_list.
+ */
+typedef struct pv_entry {
+	vm_offset_t	pv_va;		/* virtual address for mapping */
+	TAILQ_ENTRY(pv_entry)	pv_next;
+} *pv_entry_t;
+
+/*
+ * pv_entries are allocated in chunks per-process.  This avoids the
+ * need to track per-pmap assignments.
+ */
+#define	_NPCM	3
+#define	_NPCPV	168
+struct pv_chunk {
+	pmap_t			pc_pmap;
+	TAILQ_ENTRY(pv_chunk)	pc_list;
+	uint64_t		pc_map[_NPCM];	/* bitmap; 1 = free */
+	TAILQ_ENTRY(pv_chunk)	pc_lru;
+	struct pv_entry		pc_pventry[_NPCPV];
+};
+
+#ifdef	_KERNEL
+
+extern caddr_t	CADDR1;
+extern pt_entry_t *CMAP1;
+extern vm_paddr_t phys_avail[];
+extern vm_paddr_t dump_avail[];
+extern vm_offset_t virtual_avail;
+extern vm_offset_t virtual_end;
+extern vm_paddr_t dmaplimit;
+extern int pmap_pcid_enabled;
+extern int invpcid_works;
+
+#define	pmap_page_get_memattr(m)	((vm_memattr_t)(m)->md.pat_mode)
+#define	pmap_page_is_write_mapped(m)	(((m)->aflags & PGA_WRITEABLE) != 0)
+#define	pmap_unmapbios(va, sz)	pmap_unmapdev((va), (sz))
+
+struct thread;
+
+void	pmap_activate_sw(struct thread *);
+void	pmap_bootstrap(vm_paddr_t *);
+int	pmap_cache_bits(pmap_t pmap, int mode, boolean_t is_pde);
+int	pmap_change_attr(vm_offset_t, vm_size_t, int);
+void	pmap_demote_DMAP(vm_paddr_t base, vm_size_t len, boolean_t invalidate);
+void	pmap_init_pat(void);
+void	pmap_kenter(vm_offset_t va, vm_paddr_t pa);
+void	*pmap_kenter_temporary(vm_paddr_t pa, int i);
+vm_paddr_t pmap_kextract(vm_offset_t);
+void	pmap_kremove(vm_offset_t);
+void	*pmap_mapbios(vm_paddr_t, vm_size_t);
+void	*pmap_mapdev(vm_paddr_t, vm_size_t);
+void	*pmap_mapdev_attr(vm_paddr_t, vm_size_t, int);
+boolean_t pmap_page_is_mapped(vm_page_t m);
+void	pmap_page_set_memattr(vm_page_t m, vm_memattr_t ma);
+void	pmap_pinit_pml4(vm_page_t);
+void	pmap_unmapdev(vm_offset_t, vm_size_t);
+void	pmap_invalidate_page(pmap_t, vm_offset_t);
+void	pmap_invalidate_range(pmap_t, vm_offset_t, vm_offset_t);
+void	pmap_invalidate_all(pmap_t);
+void	pmap_invalidate_cache(void);
+void	pmap_invalidate_cache_pages(vm_page_t *pages, int count);
+void	pmap_invalidate_cache_range(vm_offset_t sva, vm_offset_t eva,
+	    boolean_t force);
+void	pmap_get_mapping(pmap_t pmap, vm_offset_t va, uint64_t *ptr, int *num);
+boolean_t pmap_map_io_transient(vm_page_t *, vm_offset_t *, int, boolean_t);
+void	pmap_unmap_io_transient(vm_page_t *, vm_offset_t *, int, boolean_t);
+#endif /* _KERNEL */
+
+/* Return various clipped indexes for a given VA */
+static __inline vm_pindex_t
+pmap_pte_index(vm_offset_t va)
+{
+
+	return ((va >> PAGE_SHIFT) & ((1ul << NPTEPGSHIFT) - 1));
+}
+
+static __inline vm_pindex_t
+pmap_pde_index(vm_offset_t va)
+{
+
+	return ((va >> PDRSHIFT) & ((1ul << NPDEPGSHIFT) - 1));
+}
+
+static __inline vm_pindex_t
+pmap_pdpe_index(vm_offset_t va)
+{
+
+	return ((va >> PDPSHIFT) & ((1ul << NPDPEPGSHIFT) - 1));
+}
+
+static __inline vm_pindex_t
+pmap_pml4e_index(vm_offset_t va)
+{
+
+	return ((va >> PML4SHIFT) & ((1ul << NPML4EPGSHIFT) - 1));
+}
+
+#endif /* !LOCORE */
+
+#endif /* !_MACHINE_PMAP_H_ */
diff --git a/usr/contrib/freebsd/amd64/machine/specialreg.h b/usr/contrib/freebsd/amd64/machine/specialreg.h
deleted file mode 100644
index 41d4125cb9..0000000000
--- a/usr/contrib/freebsd/amd64/machine/specialreg.h
+++ /dev/null
@@ -1,6 +0,0 @@
-/*-
- * This file is in the public domain.
- */
-/* $FreeBSD: head/sys/amd64/include/specialreg.h 233207 2012-03-19 21:34:11Z tijl $ */
-
-#include <x86/specialreg.h>
diff --git a/usr/contrib/freebsd/isa/rtc.h b/usr/contrib/freebsd/isa/rtc.h
new file mode 100644
index 0000000000..bb964ddf6a
--- /dev/null
+++ b/usr/contrib/freebsd/isa/rtc.h
@@ -0,0 +1,125 @@
+/*-
+ * Copyright (c) 1990 The Regents of the University of California.
+ * All rights reserved.
+ *
+ * This code is derived from software contributed to Berkeley by
+ * William Jolitz.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 4. Neither the name of the University nor the names of its contributors
+ *    may be used to endorse or promote products derived from this software
+ *    without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ *	from: @(#)rtc.h	7.1 (Berkeley) 5/12/91
+ * $FreeBSD$
+ */
+
+#ifndef _I386_ISA_RTC_H_
+#define _I386_ISA_RTC_H_ 1
+
+/*
+ * MC146818 RTC Register locations
+ */
+
+#define RTC_SEC		0x00	/* seconds */
+#define RTC_SECALRM	0x01	/* seconds alarm */
+#define RTC_MIN		0x02	/* minutes */
+#define RTC_MINALRM	0x03	/* minutes alarm */
+#define RTC_HRS		0x04	/* hours */
+#define RTC_HRSALRM	0x05	/* hours alarm */
+#define RTC_WDAY	0x06	/* week day */
+#define RTC_DAY		0x07	/* day of month */
+#define RTC_MONTH	0x08	/* month of year */
+#define RTC_YEAR	0x09	/* month of year */
+
+#define RTC_STATUSA	0x0a	/* status register A */
+#define  RTCSA_TUP	 0x80	/* time update, don't look now */
+#define  RTCSA_RESET	 0x70	/* reset divider */
+#define  RTCSA_DIVIDER   0x20   /* divider correct for 32768 Hz */
+#define  RTCSA_8192      0x03	/* 8192 Hz interrupt */
+#define  RTCSA_4096      0x04
+#define  RTCSA_2048      0x05
+#define  RTCSA_1024      0x06	/* default for profiling */
+#define  RTCSA_PROF      RTCSA_1024
+#define  RTC_PROFRATE    1024
+#define  RTCSA_512       0x07
+#define  RTCSA_256       0x08
+#define  RTCSA_128       0x09
+#define  RTCSA_NOPROF	 RTCSA_128
+#define  RTC_NOPROFRATE  128
+#define  RTCSA_64        0x0a
+#define  RTCSA_32        0x0b	/* 32 Hz interrupt */
+
+#define RTC_STATUSB	0x0b	/* status register B */
+#define	 RTCSB_DST	 0x01	/* USA Daylight Savings Time enable */
+#define	 RTCSB_24HR	 0x02	/* 0 = 12 hours, 1 = 24	hours */
+#define	 RTCSB_BCD	 0x04	/* 0 = BCD, 1 =	Binary coded time */
+#define	 RTCSB_SQWE	 0x08	/* 1 = output sqare wave at SQW	pin */
+#define	 RTCSB_UINTR	 0x10	/* 1 = enable update-ended interrupt */
+#define	 RTCSB_AINTR	 0x20	/* 1 = enable alarm interrupt */
+#define	 RTCSB_PINTR	 0x40	/* 1 = enable periodic clock interrupt */
+#define  RTCSB_HALT      0x80	/* stop clock updates */
+
+#define RTC_INTR	0x0c	/* status register C (R) interrupt source */
+#define  RTCIR_UPDATE	 0x10	/* update intr */
+#define  RTCIR_ALARM	 0x20	/* alarm intr */
+#define  RTCIR_PERIOD	 0x40	/* periodic intr */
+#define  RTCIR_INT	 0x80	/* interrupt output signal */
+
+#define RTC_STATUSD	0x0d	/* status register D (R) Lost Power */
+#define  RTCSD_PWR	 0x80	/* clock power OK */
+
+#define RTC_DIAG	0x0e	/* status register E - bios diagnostic */
+#define RTCDG_BITS	"\020\010clock_battery\007ROM_cksum\006config_unit\005memory_size\004fixed_disk\003invalid_time"
+
+#define RTC_RESET	0x0f	/* status register F - reset code byte */
+#define	 RTCRS_RST	 0x00		/* normal reset */
+#define	 RTCRS_LOAD	 0x04		/* load system */
+
+#define RTC_FDISKETTE	0x10	/* diskette drive type in upper/lower nibble */
+#define	 RTCFDT_NONE	 0		/* none present */
+#define	 RTCFDT_360K	 0x10		/* 360K */
+#define	 RTCFDT_12M	 0x20		/* 1.2M */
+#define  RTCFDT_720K     0x30           /* 720K */
+#define	 RTCFDT_144M	 0x40		/* 1.44M */
+#define  RTCFDT_288M_1   0x50		/* 2.88M, some BIOSes */
+#define	 RTCFDT_288M	 0x60		/* 2.88M */
+
+#define RTC_BASELO	0x15	/* low byte of basemem size */
+#define RTC_BASEHI	0x16	/* high byte of basemem size */
+#define RTC_EXTLO	0x17	/* low byte of extended mem size */
+#define RTC_EXTHI	0x18	/* low byte of extended mem size */
+
+#define	RTC_CENTURY	0x32	/* current century */
+
+#ifdef __FreeBSD__
+#ifdef _KERNEL
+extern  struct mtx clock_lock;
+extern	int atrtcclock_disable;
+int	rtcin(int reg);
+void	atrtc_restore(void);
+void	writertc(int reg, u_char val);
+void	atrtc_set(struct timespec *ts);
+#endif
+#endif
+
+#endif /* _I386_ISA_RTC_H_ */
diff --git a/usr/contrib/freebsd/lib/libutil/humanize_number.c b/usr/contrib/freebsd/lib/libutil/humanize_number.c
new file mode 100644
index 0000000000..675a969aaa
--- /dev/null
+++ b/usr/contrib/freebsd/lib/libutil/humanize_number.c
@@ -0,0 +1,179 @@
+/*	$NetBSD: humanize_number.c,v 1.14 2008/04/28 20:22:59 martin Exp $	*/
+
+/*
+ * Copyright (c) 1997, 1998, 1999, 2002 The NetBSD Foundation, Inc.
+ * Copyright 2013 John-Mark Gurney <jmg@FreeBSD.org>
+ * All rights reserved.
+ *
+ * This code is derived from software contributed to The NetBSD Foundation
+ * by Jason R. Thorpe of the Numerical Aerospace Simulation Facility,
+ * NASA Ames Research Center, by Luke Mewburn and by Tomas Svensson.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
+ * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
+ * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
+ * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/types.h>
+#include <assert.h>
+#include <inttypes.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <string.h>
+#include <locale.h>
+#include <libutil.h>
+
+static const int maxscale = 6;
+
+int
+humanize_number(char *buf, size_t len, int64_t quotient,
+    const char *suffix, int scale, int flags)
+{
+	const char *prefixes, *sep;
+	int	i, r, remainder, s1, s2, sign;
+	int	divisordeccut;
+	int64_t	divisor, max;
+	size_t	baselen;
+
+	/* Since so many callers don't check -1, NUL terminate the buffer */
+	if (len > 0)
+		buf[0] = '\0';
+
+	/* validate args */
+	if (buf == NULL || suffix == NULL)
+		return (-1);
+	if (scale < 0)
+		return (-1);
+	else if (scale > maxscale &&
+	    ((scale & ~(HN_AUTOSCALE|HN_GETSCALE)) != 0))
+		return (-1);
+	if ((flags & HN_DIVISOR_1000) && (flags & HN_IEC_PREFIXES))
+		return (-1);
+
+	/* setup parameters */
+	remainder = 0;
+
+	if (flags & HN_IEC_PREFIXES) {
+		baselen = 2;
+		/*
+		 * Use the prefixes for power of two recommended by
+		 * the International Electrotechnical Commission
+		 * (IEC) in IEC 80000-3 (i.e. Ki, Mi, Gi...).
+		 *
+		 * HN_IEC_PREFIXES implies a divisor of 1024 here
+		 * (use of HN_DIVISOR_1000 would have triggered
+		 * an assertion earlier).
+		 */
+		divisor = 1024;
+		divisordeccut = 973;	/* ceil(.95 * 1024) */
+		if (flags & HN_B)
+			prefixes = "B\0\0Ki\0Mi\0Gi\0Ti\0Pi\0Ei";
+		else
+			prefixes = "\0\0\0Ki\0Mi\0Gi\0Ti\0Pi\0Ei";
+	} else {
+		baselen = 1;
+		if (flags & HN_DIVISOR_1000) {
+			divisor = 1000;
+			divisordeccut = 950;
+			if (flags & HN_B)
+				prefixes = "B\0\0k\0\0M\0\0G\0\0T\0\0P\0\0E";
+			else
+				prefixes = "\0\0\0k\0\0M\0\0G\0\0T\0\0P\0\0E";
+		} else {
+			divisor = 1024;
+			divisordeccut = 973;	/* ceil(.95 * 1024) */
+			if (flags & HN_B)
+				prefixes = "B\0\0K\0\0M\0\0G\0\0T\0\0P\0\0E";
+			else
+				prefixes = "\0\0\0K\0\0M\0\0G\0\0T\0\0P\0\0E";
+		}
+	}
+
+#define	SCALE2PREFIX(scale)	(&prefixes[(scale) * 3])
+
+	if (quotient < 0) {
+		sign = -1;
+		quotient = -quotient;
+		baselen += 2;		/* sign, digit */
+	} else {
+		sign = 1;
+		baselen += 1;		/* digit */
+	}
+	if (flags & HN_NOSPACE)
+		sep = "";
+	else {
+		sep = " ";
+		baselen++;
+	}
+	baselen += strlen(suffix);
+
+	/* Check if enough room for `x y' + suffix + `\0' */
+	if (len < baselen + 1)
+		return (-1);
+
+	if (scale & (HN_AUTOSCALE | HN_GETSCALE)) {
+		/* See if there is additional columns can be used. */
+		for (max = 1, i = len - baselen; i-- > 0;)
+			max *= 10;
+
+		/*
+		 * Divide the number until it fits the given column.
+		 * If there will be an overflow by the rounding below,
+		 * divide once more.
+		 */
+		for (i = 0;
+		    (quotient >= max || (quotient == max - 1 &&
+		    remainder >= divisordeccut)) && i < maxscale; i++) {
+			remainder = quotient % divisor;
+			quotient /= divisor;
+		}
+
+		if (scale & HN_GETSCALE)
+			return (i);
+	} else {
+		for (i = 0; i < scale && i < maxscale; i++) {
+			remainder = quotient % divisor;
+			quotient /= divisor;
+		}
+	}
+
+	/* If a value <= 9.9 after rounding and ... */
+	/*
+	 * XXX - should we make sure there is enough space for the decimal
+	 * place and if not, don't do HN_DECIMAL?
+	 */
+	if (((quotient == 9 && remainder < divisordeccut) || quotient < 9) &&
+	    i > 0 && flags & HN_DECIMAL) {
+		s1 = (int)quotient + ((remainder * 10 + divisor / 2) /
+		    divisor / 10);
+		s2 = ((remainder * 10 + divisor / 2) / divisor) % 10;
+		r = snprintf(buf, len, "%d%s%d%s%s%s",
+		    sign * s1, localeconv()->decimal_point, s2,
+		    sep, SCALE2PREFIX(i), suffix);
+	} else
+		r = snprintf(buf, len, "%" PRId64 "%s%s%s",
+		    sign * (quotient + (remainder + divisor / 2) / divisor),
+		    sep, SCALE2PREFIX(i), suffix);
+
+	return (r);
+}
diff --git a/usr/contrib/freebsd/x86/segments.h b/usr/contrib/freebsd/x86/segments.h
new file mode 100644
index 0000000000..1b8c4a3c1c
--- /dev/null
+++ b/usr/contrib/freebsd/x86/segments.h
@@ -0,0 +1,274 @@
+/*-
+ * Copyright (c) 1989, 1990 William F. Jolitz
+ * Copyright (c) 1990 The Regents of the University of California.
+ * All rights reserved.
+ *
+ * This code is derived from software contributed to Berkeley by
+ * William Jolitz.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 3. Neither the name of the University nor the names of its contributors
+ *    may be used to endorse or promote products derived from this software
+ *    without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ *	from: @(#)segments.h	7.1 (Berkeley) 5/9/91
+ * $FreeBSD$
+ */
+
+#ifndef _X86_SEGMENTS_H_
+#define	_X86_SEGMENTS_H_
+
+/*
+ * X86 Segmentation Data Structures and definitions
+ */
+
+/*
+ * Selectors
+ */
+#define	SEL_RPL_MASK	3		/* requester priv level */
+#define	ISPL(s)		((s)&3)		/* priority level of a selector */
+#define	SEL_KPL		0		/* kernel priority level */
+#define	SEL_UPL		3		/* user priority level */
+#define	ISLDT(s)	((s)&SEL_LDT)	/* is it local or global */
+#define	SEL_LDT		4		/* local descriptor table */
+#define	IDXSEL(s)	(((s)>>3) & 0x1fff) /* index of selector */
+#define	LSEL(s,r)	(((s)<<3) | SEL_LDT | r) /* a local selector */
+#define	GSEL(s,r)	(((s)<<3) | r)	/* a global selector */
+
+/*
+ * User segment descriptors (%cs, %ds etc for i386 apps. 64 bit wide)
+ * For long-mode apps, %cs only has the conforming bit in sd_type, the sd_dpl,
+ * sd_p, sd_l and sd_def32 which must be zero).  %ds only has sd_p.
+ */
+struct segment_descriptor {
+	unsigned sd_lolimit:16;		/* segment extent (lsb) */
+	unsigned sd_lobase:24;		/* segment base address (lsb) */
+	unsigned sd_type:5;		/* segment type */
+	unsigned sd_dpl:2;		/* segment descriptor priority level */
+	unsigned sd_p:1;		/* segment descriptor present */
+	unsigned sd_hilimit:4;		/* segment extent (msb) */
+	unsigned sd_xx:2;		/* unused */
+	unsigned sd_def32:1;		/* default 32 vs 16 bit size */
+	unsigned sd_gran:1;		/* limit granularity (byte/page units)*/
+	unsigned sd_hibase:8;		/* segment base address  (msb) */
+} __packed;
+
+struct user_segment_descriptor {
+	unsigned sd_lolimit:16;		/* segment extent (lsb) */
+	unsigned sd_lobase:24;		/* segment base address (lsb) */
+	unsigned sd_type:5;		/* segment type */
+	unsigned sd_dpl:2;		/* segment descriptor priority level */
+	unsigned sd_p:1;		/* segment descriptor present */
+	unsigned sd_hilimit:4;		/* segment extent (msb) */
+	unsigned sd_xx:1;		/* unused */
+	unsigned sd_long:1;		/* long mode (cs only) */
+	unsigned sd_def32:1;		/* default 32 vs 16 bit size */
+	unsigned sd_gran:1;		/* limit granularity (byte/page units)*/
+	unsigned sd_hibase:8;		/* segment base address  (msb) */
+} __packed;
+
+#define	USD_GETBASE(sd)		(((sd)->sd_lobase) | (sd)->sd_hibase << 24)
+#define	USD_SETBASE(sd, b)	(sd)->sd_lobase = (b);	\
+				(sd)->sd_hibase = ((b) >> 24);
+#define	USD_GETLIMIT(sd)	(((sd)->sd_lolimit) | (sd)->sd_hilimit << 16)
+#define	USD_SETLIMIT(sd, l)	(sd)->sd_lolimit = (l);	\
+				(sd)->sd_hilimit = ((l) >> 16);
+
+#ifdef __i386__
+/*
+ * Gate descriptors (e.g. indirect descriptors)
+ */
+struct gate_descriptor {
+	unsigned gd_looffset:16;	/* gate offset (lsb) */
+	unsigned gd_selector:16;	/* gate segment selector */
+	unsigned gd_stkcpy:5;		/* number of stack wds to cpy */
+	unsigned gd_xx:3;		/* unused */
+	unsigned gd_type:5;		/* segment type */
+	unsigned gd_dpl:2;		/* segment descriptor priority level */
+	unsigned gd_p:1;		/* segment descriptor present */
+	unsigned gd_hioffset:16;	/* gate offset (msb) */
+} __packed;
+
+/*
+ * Generic descriptor
+ */
+union descriptor {
+	struct segment_descriptor sd;
+	struct gate_descriptor gd;
+};
+#else
+/*
+ * Gate descriptors (e.g. indirect descriptors, trap, interrupt etc. 128 bit)
+ * Only interrupt and trap gates have gd_ist.
+ */
+struct gate_descriptor {
+	uint64_t gd_looffset:16;	/* gate offset (lsb) */
+	uint64_t gd_selector:16;	/* gate segment selector */
+	uint64_t gd_ist:3;		/* IST table index */
+	uint64_t gd_xx:5;		/* unused */
+	uint64_t gd_type:5;		/* segment type */
+	uint64_t gd_dpl:2;		/* segment descriptor priority level */
+	uint64_t gd_p:1;		/* segment descriptor present */
+	uint64_t gd_hioffset:48;	/* gate offset (msb) */
+	uint64_t sd_xx1:32;
+} __packed;
+
+/*
+ * Generic descriptor
+ */
+union descriptor {
+	struct user_segment_descriptor sd;
+	struct gate_descriptor gd;
+};
+#endif
+
+	/* system segments and gate types */
+#define	SDT_SYSNULL	 0	/* system null */
+#define	SDT_SYS286TSS	 1	/* system 286 TSS available */
+#define	SDT_SYSLDT	 2	/* system local descriptor table */
+#define	SDT_SYS286BSY	 3	/* system 286 TSS busy */
+#define	SDT_SYS286CGT	 4	/* system 286 call gate */
+#define	SDT_SYSTASKGT	 5	/* system task gate */
+#define	SDT_SYS286IGT	 6	/* system 286 interrupt gate */
+#define	SDT_SYS286TGT	 7	/* system 286 trap gate */
+#define	SDT_SYSNULL2	 8	/* system null again */
+#define	SDT_SYS386TSS	 9	/* system 386 TSS available */
+#define	SDT_SYSTSS	 9	/* system available 64 bit TSS */
+#define	SDT_SYSNULL3	10	/* system null again */
+#define	SDT_SYS386BSY	11	/* system 386 TSS busy */
+#define	SDT_SYSBSY	11	/* system busy 64 bit TSS */
+#define	SDT_SYS386CGT	12	/* system 386 call gate */
+#define	SDT_SYSCGT	12	/* system 64 bit call gate */
+#define	SDT_SYSNULL4	13	/* system null again */
+#define	SDT_SYS386IGT	14	/* system 386 interrupt gate */
+#define	SDT_SYSIGT	14	/* system 64 bit interrupt gate */
+#define	SDT_SYS386TGT	15	/* system 386 trap gate */
+#define	SDT_SYSTGT	15	/* system 64 bit trap gate */
+
+	/* memory segment types */
+#define	SDT_MEMRO	16	/* memory read only */
+#define	SDT_MEMROA	17	/* memory read only accessed */
+#define	SDT_MEMRW	18	/* memory read write */
+#define	SDT_MEMRWA	19	/* memory read write accessed */
+#define	SDT_MEMROD	20	/* memory read only expand dwn limit */
+#define	SDT_MEMRODA	21	/* memory read only expand dwn limit accessed */
+#define	SDT_MEMRWD	22	/* memory read write expand dwn limit */
+#define	SDT_MEMRWDA	23	/* memory read write expand dwn limit accessed*/
+#define	SDT_MEME	24	/* memory execute only */
+#define	SDT_MEMEA	25	/* memory execute only accessed */
+#define	SDT_MEMER	26	/* memory execute read */
+#define	SDT_MEMERA	27	/* memory execute read accessed */
+#define	SDT_MEMEC	28	/* memory execute only conforming */
+#define	SDT_MEMEAC	29	/* memory execute only accessed conforming */
+#define	SDT_MEMERC	30	/* memory execute read conforming */
+#define	SDT_MEMERAC	31	/* memory execute read accessed conforming */
+
+/*
+ * Size of IDT table
+ */
+#define	NIDT		256	/* 32 reserved, 0x80 syscall, most are h/w */
+#define	NRSVIDT		32	/* reserved entries for cpu exceptions */
+
+/*
+ * Entries in the Interrupt Descriptor Table (IDT)
+ */
+#define	IDT_DE		0	/* #DE: Divide Error */
+#define	IDT_DB		1	/* #DB: Debug */
+#define	IDT_NMI		2	/* Nonmaskable External Interrupt */
+#define	IDT_BP		3	/* #BP: Breakpoint */
+#define	IDT_OF		4	/* #OF: Overflow */
+#define	IDT_BR		5	/* #BR: Bound Range Exceeded */
+#define	IDT_UD		6	/* #UD: Undefined/Invalid Opcode */
+#define	IDT_NM		7	/* #NM: No Math Coprocessor */
+#define	IDT_DF		8	/* #DF: Double Fault */
+#define	IDT_FPUGP	9	/* Coprocessor Segment Overrun */
+#define	IDT_TS		10	/* #TS: Invalid TSS */
+#define	IDT_NP		11	/* #NP: Segment Not Present */
+#define	IDT_SS		12	/* #SS: Stack Segment Fault */
+#define	IDT_GP		13	/* #GP: General Protection Fault */
+#define	IDT_PF		14	/* #PF: Page Fault */
+#define	IDT_MF		16	/* #MF: FPU Floating-Point Error */
+#define	IDT_AC		17	/* #AC: Alignment Check */
+#define	IDT_MC		18	/* #MC: Machine Check */
+#define	IDT_XF		19	/* #XF: SIMD Floating-Point Exception */
+#define	IDT_IO_INTS	NRSVIDT	/* Base of IDT entries for I/O interrupts. */
+#define	IDT_SYSCALL	0x80	/* System Call Interrupt Vector */
+#define	IDT_DTRACE_RET	0x92	/* DTrace pid provider Interrupt Vector */
+#define	IDT_EVTCHN	0x93	/* Xen HVM Event Channel Interrupt Vector */
+
+#if defined(__i386__)
+/*
+ * Entries in the Global Descriptor Table (GDT)
+ * Note that each 4 entries share a single 32 byte L1 cache line.
+ * Some of the fast syscall instructions require a specific order here.
+ */
+#define	GNULL_SEL	0	/* Null Descriptor */
+#define	GPRIV_SEL	1	/* SMP Per-Processor Private Data */
+#define	GUFS_SEL	2	/* User %fs Descriptor (order critical: 1) */
+#define	GUGS_SEL	3	/* User %gs Descriptor (order critical: 2) */
+#define	GCODE_SEL	4	/* Kernel Code Descriptor (order critical: 1) */
+#define	GDATA_SEL	5	/* Kernel Data Descriptor (order critical: 2) */
+#define	GUCODE_SEL	6	/* User Code Descriptor (order critical: 3) */
+#define	GUDATA_SEL	7	/* User Data Descriptor (order critical: 4) */
+#define	GBIOSLOWMEM_SEL	8	/* BIOS low memory access (must be entry 8) */
+#define	GPROC0_SEL	9	/* Task state process slot zero and up */
+#define	GLDT_SEL	10	/* Default User LDT */
+#define	GUSERLDT_SEL	11	/* User LDT */
+#define	GPANIC_SEL	12	/* Task state to consider panic from */
+#define	GBIOSCODE32_SEL	13	/* BIOS interface (32bit Code) */
+#define	GBIOSCODE16_SEL	14	/* BIOS interface (16bit Code) */
+#define	GBIOSDATA_SEL	15	/* BIOS interface (Data) */
+#define	GBIOSUTIL_SEL	16	/* BIOS interface (Utility) */
+#define	GBIOSARGS_SEL	17	/* BIOS interface (Arguments) */
+#define	GNDIS_SEL	18	/* For the NDIS layer */
+#define	NGDT		19
+
+/*
+ * Entries in the Local Descriptor Table (LDT)
+ */
+#define	LSYS5CALLS_SEL	0	/* forced by intel BCS */
+#define	LSYS5SIGR_SEL	1
+#define	LUCODE_SEL	3
+#define	LUDATA_SEL	5
+#define	NLDT		(LUDATA_SEL + 1)
+
+#else /* !__i386__ */
+/*
+ * Entries in the Global Descriptor Table (GDT)
+ */
+#define	GNULL_SEL	0	/* Null Descriptor */
+#define	GNULL2_SEL	1	/* Null Descriptor */
+#define	GUFS32_SEL	2	/* User 32 bit %fs Descriptor */
+#define	GUGS32_SEL	3	/* User 32 bit %gs Descriptor */
+#define	GCODE_SEL	4	/* Kernel Code Descriptor */
+#define	GDATA_SEL	5	/* Kernel Data Descriptor */
+#define	GUCODE32_SEL	6	/* User 32 bit code Descriptor */
+#define	GUDATA_SEL	7	/* User 32/64 bit Data Descriptor */
+#define	GUCODE_SEL	8	/* User 64 bit Code Descriptor */
+#define	GPROC0_SEL	9	/* TSS for entering kernel etc */
+/* slot 10 is second half of GPROC0_SEL */
+#define	GUSERLDT_SEL	11	/* LDT */
+/* slot 12 is second half of GUSERLDT_SEL */
+#define	NGDT 		13
+#endif /* __i386__ */
+
+#endif /* !_X86_SEGMENTS_H_ */
diff --git a/usr/contrib/freebsd/x86/specialreg.h b/usr/contrib/freebsd/x86/specialreg.h
index bea3122423..10bc4e7bd9 100644
--- a/usr/contrib/freebsd/x86/specialreg.h
+++ b/usr/contrib/freebsd/x86/specialreg.h
@@ -27,7 +27,7 @@
  * SUCH DAMAGE.
  *
  *	from: @(#)specialreg.h	7.1 (Berkeley) 5/9/91
- * $FreeBSD: head/sys/x86/include/specialreg.h 273338 2014-10-20 18:09:33Z neel $
+ * $FreeBSD$
  */
 
 #ifndef _MACHINE_SPECIALREG_H_
@@ -53,6 +53,7 @@
 #define	CR0_CD  0x40000000	/* Cache Disable */
 
 #define	CR3_PCID_SAVE 0x8000000000000000
+#define	CR3_PCID_MASK 0xfff
 
 /*
  * Bits in PPro special registers
@@ -82,6 +83,9 @@
 #define	EFER_LMA 0x000000400	/* Long mode active (R) */
 #define	EFER_NXE 0x000000800	/* PTE No-Execute bit enable (R/W) */
 #define	EFER_SVM 0x000001000	/* SVM enable bit for AMD, reserved for Intel */
+#define	EFER_LMSLE 0x000002000	/* Long Mode Segment Limit Enable */
+#define	EFER_FFXSR 0x000004000	/* Fast FXSAVE/FSRSTOR */
+#define	EFER_TCE   0x000008000	/* Translation Cache Extension */
 
 /*
  * Intel Extended Features registers
@@ -154,6 +158,7 @@
 #define	CPUID2_TM2	0x00000100
 #define	CPUID2_SSSE3	0x00000200
 #define	CPUID2_CNXTID	0x00000400
+#define	CPUID2_SDBG	0x00000800
 #define	CPUID2_FMA	0x00001000
 #define	CPUID2_CX16	0x00002000
 #define	CPUID2_XTPR	0x00004000
@@ -190,7 +195,7 @@
 #define	AMDID_MP	0x00080000
 #define	AMDID_NX	0x00100000
 #define	AMDID_EXT_MMX	0x00400000
-#define	AMDID_FFXSR	0x01000000
+#define	AMDID_FFXSR	0x02000000
 #define	AMDID_PAGE1GB	0x04000000
 #define	AMDID_RDTSCP	0x08000000
 #define	AMDID_LM	0x20000000
@@ -222,6 +227,7 @@
 #define	AMDID2_DBE	0x04000000
 #define	AMDID2_PTSC	0x08000000
 #define	AMDID2_PTSCEL2I	0x10000000
+#define	AMDID2_MWAITX	0x20000000
 
 /*
  * CPUID instruction 1 eax info
@@ -327,25 +333,45 @@
  */
 #define	CPUID_STDEXT_FSGSBASE	0x00000001
 #define	CPUID_STDEXT_TSC_ADJUST	0x00000002
+#define	CPUID_STDEXT_SGX	0x00000004
 #define	CPUID_STDEXT_BMI1	0x00000008
 #define	CPUID_STDEXT_HLE	0x00000010
 #define	CPUID_STDEXT_AVX2	0x00000020
+#define	CPUID_STDEXT_FDP_EXC	0x00000040
 #define	CPUID_STDEXT_SMEP	0x00000080
 #define	CPUID_STDEXT_BMI2	0x00000100
 #define	CPUID_STDEXT_ERMS	0x00000200
 #define	CPUID_STDEXT_INVPCID	0x00000400
 #define	CPUID_STDEXT_RTM	0x00000800
+#define	CPUID_STDEXT_PQM	0x00001000
+#define	CPUID_STDEXT_NFPUSG	0x00002000
 #define	CPUID_STDEXT_MPX	0x00004000
+#define	CPUID_STDEXT_PQE	0x00008000
 #define	CPUID_STDEXT_AVX512F	0x00010000
+#define	CPUID_STDEXT_AVX512DQ	0x00020000
 #define	CPUID_STDEXT_RDSEED	0x00040000
 #define	CPUID_STDEXT_ADX	0x00080000
 #define	CPUID_STDEXT_SMAP	0x00100000
+#define	CPUID_STDEXT_AVX512IFMA	0x00200000
+#define	CPUID_STDEXT_PCOMMIT	0x00400000
 #define	CPUID_STDEXT_CLFLUSHOPT	0x00800000
+#define	CPUID_STDEXT_CLWB	0x01000000
 #define	CPUID_STDEXT_PROCTRACE	0x02000000
 #define	CPUID_STDEXT_AVX512PF	0x04000000
 #define	CPUID_STDEXT_AVX512ER	0x08000000
 #define	CPUID_STDEXT_AVX512CD	0x10000000
 #define	CPUID_STDEXT_SHA	0x20000000
+#define	CPUID_STDEXT_AVX512BW	0x40000000
+
+/*
+ * CPUID instruction 7 Structured Extended Features, leaf 0 ecx info
+ */
+#define	CPUID_STDEXT2_PREFETCHWT1 0x00000001
+#define	CPUID_STDEXT2_UMIP	0x00000004
+#define	CPUID_STDEXT2_PKU	0x00000008
+#define	CPUID_STDEXT2_OSPKE	0x00000010
+#define	CPUID_STDEXT2_RDPID	0x00400000
+#define	CPUID_STDEXT2_SGXLC	0x40000000
 
 /*
  * CPUID manufacturers identifiers
@@ -446,6 +472,7 @@
 #define	MSR_DRAM_ENERGY_STATUS	0x619
 #define	MSR_PP0_ENERGY_STATUS	0x639
 #define	MSR_PP1_ENERGY_STATUS	0x641
+#define	MSR_TSC_DEADLINE	0x6e0	/* Writes are not serializing */
 
 /*
  * VMX MSRs
@@ -467,8 +494,10 @@
 #define	MSR_VMX_TRUE_ENTRY_CTLS	0x490
 
 /*
- * X2APIC MSRs
+ * X2APIC MSRs.
+ * Writes are not serializing.
  */
+#define	MSR_APIC_000		0x800
 #define	MSR_APIC_ID		0x802
 #define	MSR_APIC_VERSION	0x803
 #define	MSR_APIC_TPR		0x808
@@ -515,6 +544,17 @@
 #define	IA32_FEATURE_CONTROL_SMX_EN	0x02	/* enable VMX inside SMX */
 #define	IA32_FEATURE_CONTROL_VMX_EN	0x04	/* enable VMX outside SMX */
 
+/* MSR IA32_MISC_ENABLE */
+#define	IA32_MISC_EN_FASTSTR	0x0000000000000001ULL
+#define	IA32_MISC_EN_ATCCE	0x0000000000000008ULL
+#define	IA32_MISC_EN_PERFMON	0x0000000000000080ULL
+#define	IA32_MISC_EN_PEBSU	0x0000000000001000ULL
+#define	IA32_MISC_EN_ESSTE	0x0000000000010000ULL
+#define	IA32_MISC_EN_MONE	0x0000000000040000ULL
+#define	IA32_MISC_EN_LIMCPUID	0x0000000000400000ULL
+#define	IA32_MISC_EN_xTPRD	0x0000000000800000ULL
+#define	IA32_MISC_EN_XDD	0x0000000400000000ULL
+
 /*
  * PAT modes.
  */
@@ -665,6 +705,22 @@
 #define	MC_MISC_ADDRESS_MODE	0x00000000000001c0	/* If MCG_CAP_SER_P */
 #define	MC_CTL2_THRESHOLD	0x0000000000007fff
 #define	MC_CTL2_CMCI_EN		0x0000000040000000
+#define	MC_AMDNB_BANK		4
+#define	MC_MISC_AMDNB_VAL	0x8000000000000000	/* Counter presence valid */
+#define	MC_MISC_AMDNB_CNTP	0x4000000000000000	/* Counter present */
+#define	MC_MISC_AMDNB_LOCK	0x2000000000000000	/* Register locked */
+#define	MC_MISC_AMDNB_LVT_MASK	0x00f0000000000000	/* Extended LVT offset */
+#define	MC_MISC_AMDNB_LVT_SHIFT	52
+#define	MC_MISC_AMDNB_CNTEN	0x0008000000000000	/* Counter enabled */
+#define	MC_MISC_AMDNB_INT_MASK	0x0006000000000000	/* Interrupt type */
+#define	MC_MISC_AMDNB_INT_LVT	0x0002000000000000	/* Interrupt via Extended LVT */
+#define	MC_MISC_AMDNB_INT_SMI	0x0004000000000000	/* SMI */
+#define	MC_MISC_AMDNB_OVERFLOW	0x0001000000000000	/* Counter overflow */
+#define	MC_MISC_AMDNB_CNT_MASK	0x00000fff00000000	/* Counter value */
+#define	MC_MISC_AMDNB_CNT_SHIFT	32
+#define	MC_MISC_AMDNB_CNT_MAX	0xfff
+#define	MC_MISC_AMDNB_PTR_MASK	0x00000000ff000000	/* Pointer to additional registers */
+#define	MC_MISC_AMDNB_PTR_SHIFT	24
 
 /*
  * The following four 3-byte registers control the non-cacheable regions.
@@ -791,6 +847,7 @@
 #define	MSR_P_STATE_CONFIG(n) (0xc0010064 + (n)) /* P-state Config */
 #define	MSR_SMM_ADDR	0xc0010112	/* SMM TSEG base address */
 #define	MSR_SMM_MASK	0xc0010113	/* SMM TSEG address mask */
+#define	MSR_EXTFEATURES	0xc0011005	/* Extended CPUID Features override */
 #define	MSR_IC_CFG	0xc0011021	/* Instruction Cache Configuration */
 #define	MSR_K8_UCODE_UPDATE	0xc0010020	/* update microcode */
 #define	MSR_MC0_CTL_MASK	0xc0010044
diff --git a/usr/src/cmd/Makefile b/usr/src/cmd/Makefile
index b1fe28447a..d97da99fd7 100644
--- a/usr/src/cmd/Makefile
+++ b/usr/src/cmd/Makefile
@@ -480,6 +480,7 @@ i386_SUBDIRS=		\
 	acpi		\
 	acpihpd		\
 	addbadsec	\
+	bhyvectl	\
 	biosdev		\
 	diskscan	\
 	nvmeadm		\
diff --git a/usr/src/cmd/bhyvectl/Makefile.com b/usr/src/cmd/bhyvectl/Makefile.com
index 03ca34792c..450ba6709e 100644
--- a/usr/src/cmd/bhyvectl/Makefile.com
+++ b/usr/src/cmd/bhyvectl/Makefile.com
@@ -11,12 +11,13 @@
 
 #
 # Copyright 2013 Pluribus Networks Inc.
+# Copyright 2017 Joyent, Inc.
 #
 
 PROG= bhyvectl
 
 SRCS = bhyvectl.c
-OBJS = $(SRCS:.c=.o)
+OBJS = $(SRCS:.c=.o) humanize_number.o
 
 include ../../Makefile.cmd
 
@@ -28,6 +29,8 @@ CPPFLAGS =	-I$(COMPAT)/freebsd -I$(CONTRIB)/freebsd $(CPPFLAGS.master) \
 	-I$(SRC)/uts/i86pc/io/vmm
 LDLIBS +=	-lvmmapi
 
+CERRWARN +=	-_gcc=-Wno-uninitialized
+
 all: $(PROG)
 
 $(PROG): $(OBJS)
@@ -43,6 +46,10 @@ lint:	lint_SRCS
 
 include ../../Makefile.targ
 
+%.o: $(CONTRIB)/freebsd/lib/libutil/%.c
+	$(COMPILE.c) -o $@ $<
+	$(POST_PROCESS_O)
+
 %.o: ../%.c
 	$(COMPILE.c) -I$(SRC)/common $<
 	$(POST_PROCESS_O)
diff --git a/usr/src/cmd/bhyvectl/bhyvectl.c b/usr/src/cmd/bhyvectl/bhyvectl.c
index 319d1995f0..f1657a00aa 100644
--- a/usr/src/cmd/bhyvectl/bhyvectl.c
+++ b/usr/src/cmd/bhyvectl/bhyvectl.c
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/usr.sbin/bhyvectl/bhyvectl.c 273375 2014-10-21 07:10:43Z neel $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -42,27 +42,35 @@
  */
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/usr.sbin/bhyvectl/bhyvectl.c 273375 2014-10-21 07:10:43Z neel $");
+__FBSDID("$FreeBSD$");
 
 #include <sys/param.h>
 #include <sys/types.h>
 #include <sys/sysctl.h>
 #include <sys/errno.h>
 #include <sys/mman.h>
+#include <sys/cpuset.h>
 
 #include <stdio.h>
 #include <stdlib.h>
+#include <stdbool.h>
+#include <string.h>
 #include <unistd.h>
 #include <libgen.h>
 #include <libutil.h>
 #include <fcntl.h>
-#include <string.h>
 #include <getopt.h>
+#include <time.h>
 #include <assert.h>
+#include <libutil.h>
 
+#include <machine/cpufunc.h>
+#include <machine/specialreg.h>
 #include <machine/vmm.h>
+#include <machine/vmm_dev.h>
 #include <vmmapi.h>
 
+#include "amd/vmcb.h"
 #include "intel/vmcs.h"
 
 #define	MB	(1UL << 20)
@@ -75,7 +83,7 @@ __FBSDID("$FreeBSD: head/usr.sbin/bhyvectl/bhyvectl.c 273375 2014-10-21 07:10:43
 static const char *progname;
 
 static void
-usage(void)
+usage(bool cpu_intel)
 {
 
 	(void)fprintf(stderr,
@@ -156,61 +164,98 @@ usage(void)
 	"       [--get-ss]\n"
 	"       [--get-tr]\n"
 	"       [--get-ldtr]\n"
-	"       [--get-vmcs-pinbased-ctls]\n"
-	"       [--get-vmcs-procbased-ctls]\n"
-	"       [--get-vmcs-procbased-ctls2]\n"
-	"       [--get-vmcs-entry-interruption-info]\n"
-	"       [--set-vmcs-entry-interruption-info=<info>]\n"
-	"       [--get-vmcs-eptp]\n"
-	"       [--get-vmcs-guest-physical-address\n"
-	"       [--get-vmcs-guest-linear-address\n"
-	"       [--set-vmcs-exception-bitmap]\n"
-	"       [--get-vmcs-exception-bitmap]\n"
-	"       [--get-vmcs-io-bitmap-address]\n"
-	"       [--get-vmcs-tsc-offset]\n"
-	"       [--get-vmcs-guest-pat]\n"
-	"       [--get-vmcs-host-pat]\n"
-	"       [--get-vmcs-host-cr0]\n"
-	"       [--get-vmcs-host-cr3]\n"
-	"       [--get-vmcs-host-cr4]\n"
-	"       [--get-vmcs-host-rip]\n"
-	"       [--get-vmcs-host-rsp]\n"
-	"       [--get-vmcs-cr0-mask]\n"
-	"       [--get-vmcs-cr0-shadow]\n"
-	"       [--get-vmcs-cr4-mask]\n"
-	"       [--get-vmcs-cr4-shadow]\n"
-	"       [--get-vmcs-cr3-targets]\n"
-	"       [--get-vmcs-apic-access-address]\n"
-	"       [--get-vmcs-virtual-apic-address]\n"
-	"       [--get-vmcs-tpr-threshold]\n"
-	"       [--get-vmcs-msr-bitmap]\n"
-	"       [--get-vmcs-msr-bitmap-address]\n"
-	"       [--get-vmcs-vpid]\n"
-	"       [--get-vmcs-ple-gap]\n"
-	"       [--get-vmcs-ple-window]\n"
-	"       [--get-vmcs-instruction-error]\n"
-	"       [--get-vmcs-exit-ctls]\n"
-	"       [--get-vmcs-entry-ctls]\n"
-	"       [--get-vmcs-guest-sysenter]\n"
-	"       [--get-vmcs-link]\n"
-	"       [--get-vmcs-exit-reason]\n"
-	"       [--get-vmcs-exit-qualification]\n"
-	"       [--get-vmcs-exit-interruption-info]\n"
-	"       [--get-vmcs-exit-interruption-error]\n"
-	"       [--get-vmcs-interruptibility]\n"
 	"       [--set-x2apic-state=<state>]\n"
 	"       [--get-x2apic-state]\n"
 	"       [--unassign-pptdev=<bus/slot/func>]\n"
 	"       [--set-mem=<memory in units of MB>]\n"
 	"       [--get-lowmem]\n"
-	"       [--get-highmem]\n",
+	"       [--get-highmem]\n"
+	"       [--get-gpa-pmap]\n"
+	"       [--assert-lapic-lvt=<pin>]\n"
+	"       [--inject-nmi]\n"
+	"       [--force-reset]\n"
+	"       [--force-poweroff]\n"
+	"       [--get-rtc-time]\n"
+	"       [--set-rtc-time=<secs>]\n"
+	"       [--get-rtc-nvram]\n"
+	"       [--set-rtc-nvram=<val>]\n"
+	"       [--rtc-nvram-offset=<offset>]\n"
+	"       [--get-active-cpus]\n"
+	"       [--get-suspended-cpus]\n"
+	"       [--get-intinfo]\n"
+	"       [--get-eptp]\n"
+	"       [--set-exception-bitmap]\n"
+	"       [--get-exception-bitmap]\n"
+	"       [--get-tsc-offset]\n"
+	"       [--get-guest-pat]\n"
+	"       [--get-io-bitmap-address]\n"
+	"       [--get-msr-bitmap]\n"
+	"       [--get-msr-bitmap-address]\n"
+	"       [--get-guest-sysenter]\n"
+	"       [--get-exit-reason]\n",
 	progname);
+
+	if (cpu_intel) {
+		(void)fprintf(stderr,
+		"       [--get-vmcs-pinbased-ctls]\n"
+		"       [--get-vmcs-procbased-ctls]\n"
+		"       [--get-vmcs-procbased-ctls2]\n"
+		"       [--get-vmcs-entry-interruption-info]\n"
+		"       [--set-vmcs-entry-interruption-info=<info>]\n"
+		"       [--get-vmcs-guest-physical-address\n"
+		"       [--get-vmcs-guest-linear-address\n"
+		"       [--get-vmcs-host-pat]\n"
+		"       [--get-vmcs-host-cr0]\n"
+		"       [--get-vmcs-host-cr3]\n"
+		"       [--get-vmcs-host-cr4]\n"
+		"       [--get-vmcs-host-rip]\n"
+		"       [--get-vmcs-host-rsp]\n"
+		"       [--get-vmcs-cr0-mask]\n"
+		"       [--get-vmcs-cr0-shadow]\n"
+		"       [--get-vmcs-cr4-mask]\n"
+		"       [--get-vmcs-cr4-shadow]\n"
+		"       [--get-vmcs-cr3-targets]\n"
+		"       [--get-vmcs-apic-access-address]\n"
+		"       [--get-vmcs-virtual-apic-address]\n"
+		"       [--get-vmcs-tpr-threshold]\n"
+		"       [--get-vmcs-vpid]\n"
+		"       [--get-vmcs-instruction-error]\n"
+		"       [--get-vmcs-exit-ctls]\n"
+		"       [--get-vmcs-entry-ctls]\n"
+		"       [--get-vmcs-link]\n"
+		"       [--get-vmcs-exit-qualification]\n"
+		"       [--get-vmcs-exit-interruption-info]\n"
+		"       [--get-vmcs-exit-interruption-error]\n"
+		"       [--get-vmcs-interruptibility]\n"
+		);
+	} else {
+		(void)fprintf(stderr,
+		"       [--get-vmcb-intercepts]\n"
+		"       [--get-vmcb-asid]\n"
+		"       [--get-vmcb-exit-details]\n"
+		"       [--get-vmcb-tlb-ctrl]\n"
+		"       [--get-vmcb-virq]\n"
+		"       [--get-avic-apic-bar]\n"
+		"       [--get-avic-backing-page]\n"
+		"       [--get-avic-table]\n"
+		);
+	}
 	exit(1);
 }
 
-static int get_stats, getcap, setcap, capval;
+static int get_rtc_time, set_rtc_time;
+static int get_rtc_nvram, set_rtc_nvram;
+static int rtc_nvram_offset;
+static uint8_t rtc_nvram_value;
+static time_t rtc_secs;
+
+static int get_stats, getcap, setcap, capval, get_gpa_pmap;
+static int inject_nmi, assert_lapic_lvt;
+static int force_reset, force_poweroff;
 static const char *capname;
-static int create, destroy, get_lowmem, get_highmem;
+static int create, destroy, get_memmap, get_memseg;
+static int get_intinfo;
+static int get_active_cpus, get_suspended_cpus;
 static uint64_t memsize;
 static int set_cr0, get_cr0, set_cr3, get_cr3, set_cr4, get_cr4;
 static int set_efer, get_efer;
@@ -236,6 +281,12 @@ enum x2apic_state x2apic_state;
 static int unassign_pptdev, bus, slot, func;
 static int run;
 
+/*
+ * VMCB specific.
+ */
+static int get_vmcb_intercept, get_vmcb_exit_details, get_vmcb_tlb_ctrl;
+static int get_vmcb_virq, get_avic_table;
+
 /*
  * VMCS-specific fields
  */
@@ -251,14 +302,15 @@ static int get_cr4_mask, get_cr4_shadow;
 static int get_cr3_targets;
 static int get_apic_access_addr, get_virtual_apic_addr, get_tpr_threshold;
 static int get_msr_bitmap, get_msr_bitmap_address;
-static int get_vpid, get_ple_gap, get_ple_window;
+static int get_vpid_asid;
 static int get_inst_err, get_exit_ctls, get_entry_ctls;
 static int get_host_cr0, get_host_cr3, get_host_cr4;
 static int get_host_rip, get_host_rsp;
 static int get_guest_pat, get_host_pat;
 static int get_guest_sysenter, get_vmcs_link;
-static int get_vmcs_exit_reason, get_vmcs_exit_qualification;
+static int get_exit_reason, get_vmcs_exit_qualification;
 static int get_vmcs_exit_interruption_info, get_vmcs_exit_interruption_error;
+static int get_vmcs_exit_inst_length;
 
 static uint64_t desc_base;
 static uint32_t desc_limit, desc_access;
@@ -292,29 +344,115 @@ dump_vm_run_exitcode(struct vm_exit *vmexit, int vcpu)
 		printf("\tinst_type\t\t%d\n", vmexit->u.vmx.inst_type);
 		printf("\tinst_error\t\t%d\n", vmexit->u.vmx.inst_error);
 		break;
+	case VM_EXITCODE_SVM:
+		printf("\treason\t\tSVM\n");
+		printf("\texit_reason\t\t%#lx\n", vmexit->u.svm.exitcode);
+		printf("\texitinfo1\t\t%#lx\n", vmexit->u.svm.exitinfo1);
+		printf("\texitinfo2\t\t%#lx\n", vmexit->u.svm.exitinfo2);
+		break;
 	default:
 		printf("*** unknown vm run exitcode %d\n", vmexit->exitcode);
 		break;
 	}
 }
 
-static int
-dump_vmcs_msr_bitmap(int vcpu, u_long addr)
+/* AMD 6th generation and Intel compatible MSRs */
+#define MSR_AMD6TH_START	0xC0000000
+#define MSR_AMD6TH_END		0xC0001FFF
+/* AMD 7th and 8th generation compatible MSRs */
+#define MSR_AMD7TH_START	0xC0010000
+#define MSR_AMD7TH_END		0xC0011FFF
+
+static const char *
+msr_name(uint32_t msr)
 {
-	int error, fd, byte, bit, readable, writeable;
-	u_int msr;
-	const char *bitmap;
+	static char buf[32];
+
+	switch(msr) {
+	case MSR_TSC:
+		return ("MSR_TSC");
+	case MSR_EFER:
+		return ("MSR_EFER");
+	case MSR_STAR:
+		return ("MSR_STAR");
+	case MSR_LSTAR:	
+		return ("MSR_LSTAR");
+	case MSR_CSTAR:
+		return ("MSR_CSTAR");
+	case MSR_SF_MASK:
+		return ("MSR_SF_MASK");
+	case MSR_FSBASE:
+		return ("MSR_FSBASE");
+	case MSR_GSBASE:
+		return ("MSR_GSBASE");
+	case MSR_KGSBASE:
+		return ("MSR_KGSBASE");
+	case MSR_SYSENTER_CS_MSR:
+		return ("MSR_SYSENTER_CS_MSR");
+	case MSR_SYSENTER_ESP_MSR:
+		return ("MSR_SYSENTER_ESP_MSR");
+	case MSR_SYSENTER_EIP_MSR:
+		return ("MSR_SYSENTER_EIP_MSR");
+	case MSR_PAT:
+		return ("MSR_PAT");
+	}
+	snprintf(buf, sizeof(buf), "MSR       %#08x", msr);
+
+	return (buf);
+}
 
-	error = -1;
-	bitmap = MAP_FAILED;
+static inline void
+print_msr_pm(uint64_t msr, int vcpu, int readable, int writeable)
+{
 
-	fd = open("/dev/mem", O_RDONLY, 0);
-	if (fd < 0)
-		goto done;
+	if (readable || writeable) {
+		printf("%-20s[%d]\t\t%c%c\n", msr_name(msr), vcpu,
+			readable ? 'R' : '-', writeable ? 'W' : '-');
+	}
+}
 
-	bitmap = mmap(NULL, PAGE_SIZE, PROT_READ, 0, fd, addr);
-	if (bitmap == MAP_FAILED)
-		goto done;
+/*
+ * Reference APM vol2, section 15.11 MSR Intercepts.
+ */
+static void
+dump_amd_msr_pm(const char *bitmap, int vcpu)
+{
+	int byte, bit, readable, writeable;
+	uint32_t msr;
+
+	for (msr = 0; msr < 0x2000; msr++) {
+		byte = msr / 4;
+		bit = (msr % 4) * 2;
+
+		/* Look at MSRs in the range 0x00000000 to 0x00001FFF */
+		readable = (bitmap[byte] & (1 << bit)) ? 0 : 1;
+		writeable = (bitmap[byte] & (2 << bit)) ?  0 : 1;
+		print_msr_pm(msr, vcpu, readable, writeable);
+
+		/* Look at MSRs in the range 0xC0000000 to 0xC0001FFF */
+		byte += 2048;
+		readable = (bitmap[byte] & (1 << bit)) ? 0 : 1;
+		writeable = (bitmap[byte] & (2 << bit)) ?  0 : 1;
+		print_msr_pm(msr + MSR_AMD6TH_START, vcpu, readable,
+				writeable);
+		
+		/* MSR 0xC0010000 to 0xC0011FF is only for AMD */
+		byte += 4096;
+		readable = (bitmap[byte] & (1 << bit)) ? 0 : 1;
+		writeable = (bitmap[byte] & (2 << bit)) ?  0 : 1;
+		print_msr_pm(msr + MSR_AMD7TH_START, vcpu, readable,
+				writeable);
+	}
+}
+
+/*
+ * Reference Intel SDM Vol3 Section 24.6.9 MSR-Bitmap Address
+ */
+static void
+dump_intel_msr_pm(const char *bitmap, int vcpu)
+{
+	int byte, bit, readable, writeable;
+	uint32_t msr;
 
 	for (msr = 0; msr < 0x2000; msr++) {
 		byte = msr / 8;
@@ -322,31 +460,56 @@ dump_vmcs_msr_bitmap(int vcpu, u_long addr)
 
 		/* Look at MSRs in the range 0x00000000 to 0x00001FFF */
 		readable = (bitmap[byte] & (1 << bit)) ? 0 : 1;
-		writeable = (bitmap[2048 + byte] & (1 << bit)) ? 0 : 1;
-		if (readable || writeable) {
-			printf("msr 0x%08x[%d]\t\t%c%c\n", msr, vcpu,
-				readable ? 'R' : '-',
-				writeable ? 'W' : '-');
-		}
+		writeable = (bitmap[2048 + byte] & (1 << bit)) ?  0 : 1;
+		print_msr_pm(msr, vcpu, readable, writeable);
 
 		/* Look at MSRs in the range 0xC0000000 to 0xC0001FFF */
 		byte += 1024;
 		readable = (bitmap[byte] & (1 << bit)) ? 0 : 1;
-		writeable = (bitmap[2048 + byte] & (1 << bit)) ? 0 : 1;
-		if (readable || writeable) {
-			printf("msr 0x%08x[%d]\t\t%c%c\n",
-				0xc0000000 + msr, vcpu,
-				readable ? 'R' : '-',
-				writeable ? 'W' : '-');
-		}
+		writeable = (bitmap[2048 + byte] & (1 << bit)) ?  0 : 1;
+		print_msr_pm(msr + MSR_AMD6TH_START, vcpu, readable,
+				writeable);
+	}
+}
+
+static int
+dump_msr_bitmap(int vcpu, uint64_t addr, bool cpu_intel)
+{
+	int error, fd, map_size;
+	const char *bitmap;
+
+	error = -1;
+	bitmap = MAP_FAILED;
+
+	fd = open("/dev/mem", O_RDONLY, 0);
+	if (fd < 0) {
+		perror("Couldn't open /dev/mem");
+		goto done;
 	}
 
+	if (cpu_intel)
+		map_size = PAGE_SIZE;
+	else
+		map_size = 2 * PAGE_SIZE;
+
+	bitmap = mmap(NULL, map_size, PROT_READ, MAP_SHARED, fd, addr);
+	if (bitmap == MAP_FAILED) {
+		perror("mmap failed");
+		goto done;
+	}
+	
+	if (cpu_intel)
+		dump_intel_msr_pm(bitmap, vcpu);
+	else	
+		dump_amd_msr_pm(bitmap, vcpu);
+
 	error = 0;
 done:
 	if (bitmap != MAP_FAILED)
-		munmap((void *)bitmap, PAGE_SIZE);
+		munmap((void *)bitmap, map_size);
 	if (fd >= 0)
 		close(fd);
+
 	return (error);
 }
 
@@ -364,6 +527,22 @@ vm_set_vmcs_field(struct vmctx *ctx, int vcpu, int field, uint64_t val)
 	return (vm_set_register(ctx, vcpu, VMCS_IDENT(field), val));
 }
 
+static int
+vm_get_vmcb_field(struct vmctx *ctx, int vcpu, int off, int bytes,
+	uint64_t *ret_val)
+{
+
+	return (vm_get_register(ctx, vcpu, VMCB_ACCESS(off, bytes), ret_val));
+}
+
+static int
+vm_set_vmcb_field(struct vmctx *ctx, int vcpu, int off, int bytes,
+	uint64_t val)
+{
+	
+	return (vm_set_register(ctx, vcpu, VMCB_ACCESS(off, bytes), val));
+}
+
 enum {
 	VMNAME = 1000,	/* avoid collision with return values from getopt */
 	VCPU,
@@ -389,481 +568,110 @@ enum {
 	SET_TR,
 	SET_LDTR,
 	SET_X2APIC_STATE,
-	SET_VMCS_EXCEPTION_BITMAP,
+	SET_EXCEPTION_BITMAP,
 	SET_VMCS_ENTRY_INTERRUPTION_INFO,
 	SET_CAP,
 	CAPNAME,
 	UNASSIGN_PPTDEV,
+	GET_GPA_PMAP,
+	ASSERT_LAPIC_LVT,
+	SET_RTC_TIME,
+	SET_RTC_NVRAM,
+	RTC_NVRAM_OFFSET,
 };
 
-int
-main(int argc, char *argv[])
+static void
+print_cpus(const char *banner, const cpuset_t *cpus)
 {
-	char *vmname = NULL;
-	int error, ch, vcpu;
-	vm_paddr_t gpa;
-	size_t len;
-	struct vm_exit vmexit;
-	uint64_t ctl, eptp, bm, addr, u64;
-	struct vmctx *ctx;
-	int wired;
+	int i;
+	int first;
+
+	first = 1;
+	printf("%s:\t", banner);
+	if (!CPU_EMPTY(cpus)) {
+		for (i = 0; i < CPU_SETSIZE; i++) {
+			if (CPU_ISSET(i, cpus)) {
+				printf("%s%d", first ? " " : ", ", i);
+				first = 0;
+			}
+		}
+	} else
+		printf(" (none)");
+	printf("\n");
+}
+
+static void
+print_intinfo(const char *banner, uint64_t info)
+{
+	int type;
+
+	printf("%s:\t", banner);
+	if (info & VM_INTINFO_VALID) {
+		type = info & VM_INTINFO_TYPE;
+		switch (type) {
+		case VM_INTINFO_HWINTR:
+			printf("extint");
+			break;
+		case VM_INTINFO_NMI:
+			printf("nmi");
+			break;
+		case VM_INTINFO_SWINTR:
+			printf("swint");
+			break;
+		default:
+			printf("exception");
+			break;
+		}
+		printf(" vector %d", (int)VM_INTINFO_VECTOR(info));
+		if (info & VM_INTINFO_DEL_ERRCODE)
+			printf(" errcode %#x", (u_int)(info >> 32));
+	} else {
+		printf("n/a");
+	}
+	printf("\n");
+}
+
+static bool
+cpu_vendor_intel(void)
+{
+	u_int regs[4];
+	char cpu_vendor[13];
+
+	do_cpuid(0, regs);
+	((u_int *)&cpu_vendor)[0] = regs[1];
+	((u_int *)&cpu_vendor)[1] = regs[3];
+	((u_int *)&cpu_vendor)[2] = regs[2];
+	cpu_vendor[12] = '\0';
+
+	if (strcmp(cpu_vendor, "AuthenticAMD") == 0) {
+		return (false);
+	} else if (strcmp(cpu_vendor, "GenuineIntel") == 0) {
+		return (true);
+	} else {
+		fprintf(stderr, "Unknown cpu vendor \"%s\"\n", cpu_vendor);
+		exit(1);
+	}
+}
 
-	uint64_t cr0, cr3, cr4, dr7, rsp, rip, rflags, efer, pat;
+static int
+get_all_registers(struct vmctx *ctx, int vcpu)
+{
+	uint64_t cr0, cr3, cr4, dr7, rsp, rip, rflags, efer;
 	uint64_t rax, rbx, rcx, rdx, rsi, rdi, rbp;
 	uint64_t r8, r9, r10, r11, r12, r13, r14, r15;
-	uint64_t cs, ds, es, fs, gs, ss, tr, ldtr;
+	int error = 0;
 
-	struct option opts[] = {
-		{ "vm",		REQ_ARG,	0,	VMNAME },
-		{ "cpu",	REQ_ARG,	0,	VCPU },
-		{ "set-mem",	REQ_ARG,	0,	SET_MEM },
-		{ "set-efer",	REQ_ARG,	0,	SET_EFER },
-		{ "set-cr0",	REQ_ARG,	0,	SET_CR0 },
-		{ "set-cr3",	REQ_ARG,	0,	SET_CR3 },
-		{ "set-cr4",	REQ_ARG,	0,	SET_CR4 },
-		{ "set-dr7",	REQ_ARG,	0,	SET_DR7 },
-		{ "set-rsp",	REQ_ARG,	0,	SET_RSP },
-		{ "set-rip",	REQ_ARG,	0,	SET_RIP },
-		{ "set-rax",	REQ_ARG,	0,	SET_RAX },
-		{ "set-rflags",	REQ_ARG,	0,	SET_RFLAGS },
-		{ "desc-base",	REQ_ARG,	0,	DESC_BASE },
-		{ "desc-limit",	REQ_ARG,	0,	DESC_LIMIT },
-		{ "desc-access",REQ_ARG,	0,	DESC_ACCESS },
-		{ "set-cs",	REQ_ARG,	0,	SET_CS },
-		{ "set-ds",	REQ_ARG,	0,	SET_DS },
-		{ "set-es",	REQ_ARG,	0,	SET_ES },
-		{ "set-fs",	REQ_ARG,	0,	SET_FS },
-		{ "set-gs",	REQ_ARG,	0,	SET_GS },
-		{ "set-ss",	REQ_ARG,	0,	SET_SS },
-		{ "set-tr",	REQ_ARG,	0,	SET_TR },
-		{ "set-ldtr",	REQ_ARG,	0,	SET_LDTR },
-		{ "set-x2apic-state",REQ_ARG,	0,	SET_X2APIC_STATE },
-		{ "set-vmcs-exception-bitmap",
-				REQ_ARG,	0, SET_VMCS_EXCEPTION_BITMAP },
-		{ "set-vmcs-entry-interruption-info",
-				REQ_ARG, 0, SET_VMCS_ENTRY_INTERRUPTION_INFO },
-		{ "capname",	REQ_ARG,	0,	CAPNAME },
-		{ "unassign-pptdev", REQ_ARG,	0,	UNASSIGN_PPTDEV },
-		{ "setcap",	REQ_ARG,	0,	SET_CAP },
-		{ "getcap",	NO_ARG,		&getcap,	1 },
-		{ "get-stats",	NO_ARG,		&get_stats,	1 },
-		{ "get-desc-ds",NO_ARG,		&get_desc_ds,	1 },
-		{ "set-desc-ds",NO_ARG,		&set_desc_ds,	1 },
-		{ "get-desc-es",NO_ARG,		&get_desc_es,	1 },
-		{ "set-desc-es",NO_ARG,		&set_desc_es,	1 },
-		{ "get-desc-ss",NO_ARG,		&get_desc_ss,	1 },
-		{ "set-desc-ss",NO_ARG,		&set_desc_ss,	1 },
-		{ "get-desc-cs",NO_ARG,		&get_desc_cs,	1 },
-		{ "set-desc-cs",NO_ARG,		&set_desc_cs,	1 },
-		{ "get-desc-fs",NO_ARG,		&get_desc_fs,	1 },
-		{ "set-desc-fs",NO_ARG,		&set_desc_fs,	1 },
-		{ "get-desc-gs",NO_ARG,		&get_desc_gs,	1 },
-		{ "set-desc-gs",NO_ARG,		&set_desc_gs,	1 },
-		{ "get-desc-tr",NO_ARG,		&get_desc_tr,	1 },
-		{ "set-desc-tr",NO_ARG,		&set_desc_tr,	1 },
-		{ "set-desc-ldtr", NO_ARG,	&set_desc_ldtr,	1 },
-		{ "get-desc-ldtr", NO_ARG,	&get_desc_ldtr,	1 },
-		{ "set-desc-gdtr", NO_ARG,	&set_desc_gdtr, 1 },
-		{ "get-desc-gdtr", NO_ARG,	&get_desc_gdtr, 1 },
-		{ "set-desc-idtr", NO_ARG,	&set_desc_idtr, 1 },
-		{ "get-desc-idtr", NO_ARG,	&get_desc_idtr, 1 },
-		{ "get-lowmem", NO_ARG,		&get_lowmem,	1 },
-		{ "get-highmem",NO_ARG,		&get_highmem,	1 },
-		{ "get-efer",	NO_ARG,		&get_efer,	1 },
-		{ "get-cr0",	NO_ARG,		&get_cr0,	1 },
-		{ "get-cr3",	NO_ARG,		&get_cr3,	1 },
-		{ "get-cr4",	NO_ARG,		&get_cr4,	1 },
-		{ "get-dr7",	NO_ARG,		&get_dr7,	1 },
-		{ "get-rsp",	NO_ARG,		&get_rsp,	1 },
-		{ "get-rip",	NO_ARG,		&get_rip,	1 },
-		{ "get-rax",	NO_ARG,		&get_rax,	1 },
-		{ "get-rbx",	NO_ARG,		&get_rbx,	1 },
-		{ "get-rcx",	NO_ARG,		&get_rcx,	1 },
-		{ "get-rdx",	NO_ARG,		&get_rdx,	1 },
-		{ "get-rsi",	NO_ARG,		&get_rsi,	1 },
-		{ "get-rdi",	NO_ARG,		&get_rdi,	1 },
-		{ "get-rbp",	NO_ARG,		&get_rbp,	1 },
-		{ "get-r8",	NO_ARG,		&get_r8,	1 },
-		{ "get-r9",	NO_ARG,		&get_r9,	1 },
-		{ "get-r10",	NO_ARG,		&get_r10,	1 },
-		{ "get-r11",	NO_ARG,		&get_r11,	1 },
-		{ "get-r12",	NO_ARG,		&get_r12,	1 },
-		{ "get-r13",	NO_ARG,		&get_r13,	1 },
-		{ "get-r14",	NO_ARG,		&get_r14,	1 },
-		{ "get-r15",	NO_ARG,		&get_r15,	1 },
-		{ "get-rflags",	NO_ARG,		&get_rflags,	1 },
-		{ "get-cs",	NO_ARG,		&get_cs,	1 },
-		{ "get-ds",	NO_ARG,		&get_ds,	1 },
-		{ "get-es",	NO_ARG,		&get_es,	1 },
-		{ "get-fs",	NO_ARG,		&get_fs,	1 },
-		{ "get-gs",	NO_ARG,		&get_gs,	1 },
-		{ "get-ss",	NO_ARG,		&get_ss,	1 },
-		{ "get-tr",	NO_ARG,		&get_tr,	1 },
-		{ "get-ldtr",	NO_ARG,		&get_ldtr,	1 },
-		{ "get-vmcs-pinbased-ctls",
-				NO_ARG,		&get_pinbased_ctls, 1 },
-		{ "get-vmcs-procbased-ctls",
-				NO_ARG,		&get_procbased_ctls, 1 },
-		{ "get-vmcs-procbased-ctls2",
-				NO_ARG,		&get_procbased_ctls2, 1 },
-		{ "get-vmcs-guest-linear-address",
-				NO_ARG,		&get_vmcs_gla,	1 },
-		{ "get-vmcs-guest-physical-address",
-				NO_ARG,		&get_vmcs_gpa,	1 },
-		{ "get-vmcs-entry-interruption-info",
-				NO_ARG, &get_vmcs_entry_interruption_info, 1},
-		{ "get-vmcs-eptp", NO_ARG,	&get_eptp,	1 },
-		{ "get-vmcs-exception-bitmap",
-				NO_ARG,		&get_exception_bitmap, 1 },
-		{ "get-vmcs-io-bitmap-address",
-				NO_ARG,		&get_io_bitmap,	1 },
-		{ "get-vmcs-tsc-offset", NO_ARG,&get_tsc_offset, 1 },
-		{ "get-vmcs-cr0-mask", NO_ARG,	&get_cr0_mask,	1 },
-		{ "get-vmcs-cr0-shadow", NO_ARG,&get_cr0_shadow, 1 },
-		{ "get-vmcs-cr4-mask", NO_ARG,	&get_cr4_mask,	1 },
-		{ "get-vmcs-cr4-shadow", NO_ARG,&get_cr4_shadow, 1 },
-		{ "get-vmcs-cr3-targets", NO_ARG, &get_cr3_targets, 1},
-		{ "get-vmcs-apic-access-address",
-				NO_ARG,		&get_apic_access_addr, 1},
-		{ "get-vmcs-virtual-apic-address",
-				NO_ARG,		&get_virtual_apic_addr, 1},
-		{ "get-vmcs-tpr-threshold",
-				NO_ARG,		&get_tpr_threshold, 1 },
-		{ "get-vmcs-msr-bitmap",
-				NO_ARG,		&get_msr_bitmap, 1 },
-		{ "get-vmcs-msr-bitmap-address",
-				NO_ARG,		&get_msr_bitmap_address, 1 },
-		{ "get-vmcs-vpid", NO_ARG,	&get_vpid,	1 },
-		{ "get-vmcs-ple-gap", NO_ARG,	&get_ple_gap,	1 },
-		{ "get-vmcs-ple-window", NO_ARG,&get_ple_window,1 },
-		{ "get-vmcs-instruction-error",
-				NO_ARG,		&get_inst_err,	1 },
-		{ "get-vmcs-exit-ctls", NO_ARG,	&get_exit_ctls,	1 },
-		{ "get-vmcs-entry-ctls",
-					NO_ARG,	&get_entry_ctls, 1 },
-		{ "get-vmcs-guest-pat",	NO_ARG,	&get_guest_pat,	1 },
-		{ "get-vmcs-host-pat",	NO_ARG,	&get_host_pat,	1 },
-		{ "get-vmcs-host-cr0",
-				NO_ARG,		&get_host_cr0,	1 },
-		{ "get-vmcs-host-cr3",
-				NO_ARG,		&get_host_cr3,	1 },
-		{ "get-vmcs-host-cr4",
-				NO_ARG,		&get_host_cr4,	1 },
-		{ "get-vmcs-host-rip",
-				NO_ARG,		&get_host_rip,	1 },
-		{ "get-vmcs-host-rsp",
-				NO_ARG,		&get_host_rsp,	1 },
-		{ "get-vmcs-guest-sysenter",
-				NO_ARG,		&get_guest_sysenter, 1 },
-		{ "get-vmcs-link", NO_ARG,	&get_vmcs_link, 1 },
-		{ "get-vmcs-exit-reason",
-				NO_ARG,		&get_vmcs_exit_reason, 1 },
-		{ "get-vmcs-exit-qualification",
-			NO_ARG,		&get_vmcs_exit_qualification, 1 },
-		{ "get-vmcs-exit-interruption-info",
-				NO_ARG,	&get_vmcs_exit_interruption_info, 1},
-		{ "get-vmcs-exit-interruption-error",
-				NO_ARG,	&get_vmcs_exit_interruption_error, 1},
-		{ "get-vmcs-interruptibility",
-				NO_ARG, &get_vmcs_interruptibility, 1 },
-		{ "get-x2apic-state",NO_ARG,	&get_x2apic_state, 1 },
-		{ "get-all",	NO_ARG,		&get_all,	1 },
-		{ "run",	NO_ARG,		&run,		1 },
-		{ "create",	NO_ARG,		&create,	1 },
-		{ "destroy",	NO_ARG,		&destroy,	1 },
-		{ NULL,		0,		NULL,		0 }
-	};
+	if (!error && (get_efer || get_all)) {
+		error = vm_get_register(ctx, vcpu, VM_REG_GUEST_EFER, &efer);
+		if (error == 0)
+			printf("efer[%d]\t\t0x%016lx\n", vcpu, efer);
+	}
 
-	vcpu = 0;
-	progname = basename(argv[0]);
-
-	while ((ch = getopt_long(argc, argv, "", opts, NULL)) != -1) {
-		switch (ch) {
-		case 0:
-			break;
-		case VMNAME:
-			vmname = optarg;
-			break;
-		case VCPU:
-			vcpu = atoi(optarg);
-			break;
-		case SET_MEM:
-			memsize = atoi(optarg) * MB;
-			memsize = roundup(memsize, 2 * MB);
-			break;
-		case SET_EFER:
-			efer = strtoul(optarg, NULL, 0);
-			set_efer = 1;
-			break;
-		case SET_CR0:
-			cr0 = strtoul(optarg, NULL, 0);
-			set_cr0 = 1;
-			break;
-		case SET_CR3:
-			cr3 = strtoul(optarg, NULL, 0);
-			set_cr3 = 1;
-			break;
-		case SET_CR4:
-			cr4 = strtoul(optarg, NULL, 0);
-			set_cr4 = 1;
-			break;
-		case SET_DR7:
-			dr7 = strtoul(optarg, NULL, 0);
-			set_dr7 = 1;
-			break;
-		case SET_RSP:
-			rsp = strtoul(optarg, NULL, 0);
-			set_rsp = 1;
-			break;
-		case SET_RIP:
-			rip = strtoul(optarg, NULL, 0);
-			set_rip = 1;
-			break;
-		case SET_RAX:
-			rax = strtoul(optarg, NULL, 0);
-			set_rax = 1;
-			break;
-		case SET_RFLAGS:
-			rflags = strtoul(optarg, NULL, 0);
-			set_rflags = 1;
-			break;
-		case DESC_BASE:
-			desc_base = strtoul(optarg, NULL, 0);
-			break;
-		case DESC_LIMIT:
-			desc_limit = strtoul(optarg, NULL, 0);
-			break;
-		case DESC_ACCESS:
-			desc_access = strtoul(optarg, NULL, 0);
-			break;
-		case SET_CS:
-			cs = strtoul(optarg, NULL, 0);
-			set_cs = 1;
-			break;
-		case SET_DS:
-			ds = strtoul(optarg, NULL, 0);
-			set_ds = 1;
-			break;
-		case SET_ES:
-			es = strtoul(optarg, NULL, 0);
-			set_es = 1;
-			break;
-		case SET_FS:
-			fs = strtoul(optarg, NULL, 0);
-			set_fs = 1;
-			break;
-		case SET_GS:
-			gs = strtoul(optarg, NULL, 0);
-			set_gs = 1;
-			break;
-		case SET_SS:
-			ss = strtoul(optarg, NULL, 0);
-			set_ss = 1;
-			break;
-		case SET_TR:
-			tr = strtoul(optarg, NULL, 0);
-			set_tr = 1;
-			break;
-		case SET_LDTR:
-			ldtr = strtoul(optarg, NULL, 0);
-			set_ldtr = 1;
-			break;
-		case SET_X2APIC_STATE:
-			x2apic_state = strtol(optarg, NULL, 0);
-			set_x2apic_state = 1;
-			break;
-		case SET_VMCS_EXCEPTION_BITMAP:
-			exception_bitmap = strtoul(optarg, NULL, 0);
-			set_exception_bitmap = 1;
-			break;
-		case SET_VMCS_ENTRY_INTERRUPTION_INFO:
-			vmcs_entry_interruption_info = strtoul(optarg, NULL, 0);
-			set_vmcs_entry_interruption_info = 1;
-			break;
-		case SET_CAP:
-			capval = strtoul(optarg, NULL, 0);
-			setcap = 1;
-			break;
-		case CAPNAME:
-			capname = optarg;
-			break;
-		case UNASSIGN_PPTDEV:
-			unassign_pptdev = 1;
-			if (sscanf(optarg, "%d/%d/%d", &bus, &slot, &func) != 3)
-				usage();
-			break;
-		default:
-			usage();
-		}
-	}
-	argc -= optind;
-	argv += optind;
-
-	if (vmname == NULL)
-		usage();
-
-	error = 0;
-
-	if (!error && create)
-		error = vm_create(vmname);
-
-	if (!error) {
-		ctx = vm_open(vmname);
-		if (ctx == NULL)
-			error = -1;
-	}
-
-	if (!error && memsize)
-		error = vm_setup_memory(ctx, memsize, VM_MMAP_NONE);
-
-	if (!error && set_efer)
-		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_EFER, efer);
-
-	if (!error && set_cr0)
-		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_CR0, cr0);
-
-	if (!error && set_cr3)
-		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_CR3, cr3);
-
-	if (!error && set_cr4)
-		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_CR4, cr4);
-
-	if (!error && set_dr7)
-		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_DR7, dr7);
-
-	if (!error && set_rsp)
-		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_RSP, rsp);
-
-	if (!error && set_rip)
-		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_RIP, rip);
-
-	if (!error && set_rax)
-		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_RAX, rax);
-
-	if (!error && set_rflags) {
-		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_RFLAGS,
-					rflags);
-	}
-
-	if (!error && set_desc_ds) {
-		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_DS,
-				    desc_base, desc_limit, desc_access);
-	}
-
-	if (!error && set_desc_es) {
-		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_ES,
-				    desc_base, desc_limit, desc_access);
-	}
-
-	if (!error && set_desc_ss) {
-		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_SS,
-				    desc_base, desc_limit, desc_access);
-	}
-
-	if (!error && set_desc_cs) {
-		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_CS,
-				    desc_base, desc_limit, desc_access);
-	}
-
-	if (!error && set_desc_fs) {
-		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_FS,
-				    desc_base, desc_limit, desc_access);
-	}
-
-	if (!error && set_desc_gs) {
-		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_GS,
-				    desc_base, desc_limit, desc_access);
-	}
-
-	if (!error && set_desc_tr) {
-		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_TR,
-				    desc_base, desc_limit, desc_access);
-	}
-
-	if (!error && set_desc_ldtr) {
-		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_LDTR,
-				    desc_base, desc_limit, desc_access);
-	}
-
-	if (!error && set_desc_gdtr) {
-		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_GDTR,
-				    desc_base, desc_limit, 0);
-	}
-
-	if (!error && set_desc_idtr) {
-		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_IDTR,
-				    desc_base, desc_limit, 0);
-	}
-
-	if (!error && set_cs)
-		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_CS, cs);
-
-	if (!error && set_ds)
-		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_DS, ds);
-
-	if (!error && set_es)
-		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_ES, es);
-
-	if (!error && set_fs)
-		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_FS, fs);
-
-	if (!error && set_gs)
-		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_GS, gs);
-
-	if (!error && set_ss)
-		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_SS, ss);
-
-	if (!error && set_tr)
-		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_TR, tr);
-
-	if (!error && set_ldtr)
-		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_LDTR, ldtr);
-
-	if (!error && set_x2apic_state)
-		error = vm_set_x2apic_state(ctx, vcpu, x2apic_state);
-
-#ifdef	__FreeBSD__
-	if (!error && unassign_pptdev)
-		error = vm_unassign_pptdev(ctx, bus, slot, func);
-#endif
-
-	if (!error && set_exception_bitmap) {
-		error = vm_set_vmcs_field(ctx, vcpu, VMCS_EXCEPTION_BITMAP,
-					  exception_bitmap);
-	}
-
-	if (!error && set_vmcs_entry_interruption_info) {
-		error = vm_set_vmcs_field(ctx, vcpu, VMCS_ENTRY_INTR_INFO,
-					  vmcs_entry_interruption_info);
-	}
-
-	if (!error && (get_lowmem || get_all)) {
-		gpa = 0;
-		error = vm_get_memory_seg(ctx, gpa, &len, &wired);
-		if (error == 0)
-			printf("lowmem\t\t0x%016lx/%ld%s\n", gpa, len,
-			    wired ? " wired" : "");
-	}
-
-	if (!error && (get_highmem || get_all)) {
-		gpa = 4 * GB;
-		error = vm_get_memory_seg(ctx, gpa, &len, &wired);
-		if (error == 0)
-			printf("highmem\t\t0x%016lx/%ld%s\n", gpa, len,
-			    wired ? " wired" : "");
-	}
-
-	if (!error && (get_efer || get_all)) {
-		error = vm_get_register(ctx, vcpu, VM_REG_GUEST_EFER, &efer);
-		if (error == 0)
-			printf("efer[%d]\t\t0x%016lx\n", vcpu, efer);
-	}
-
-	if (!error && (get_cr0 || get_all)) {
-		error = vm_get_register(ctx, vcpu, VM_REG_GUEST_CR0, &cr0);
-		if (error == 0)
-			printf("cr0[%d]\t\t0x%016lx\n", vcpu, cr0);
-	}
+	if (!error && (get_cr0 || get_all)) {
+		error = vm_get_register(ctx, vcpu, VM_REG_GUEST_CR0, &cr0);
+		if (error == 0)
+			printf("cr0[%d]\t\t0x%016lx\n", vcpu, cr0);
+	}
 
 	if (!error && (get_cr3 || get_all)) {
 		error = vm_get_register(ctx, vcpu, VM_REG_GUEST_CR3, &cr3);
@@ -991,31 +799,22 @@ main(int argc, char *argv[])
 		if (error == 0)
 			printf("rflags[%d]\t0x%016lx\n", vcpu, rflags);
 	}
+	
+	return (error);
+}
 
-#ifdef	__FreeBSD__
-	if (!error && (get_stats || get_all)) {
-		int i, num_stats;
-		uint64_t *stats;
-		struct timeval tv;
-		const char *desc;
-
-		stats = vm_get_stats(ctx, vcpu, &tv, &num_stats);
-		if (stats != NULL) {
-			printf("vcpu%d\n", vcpu);
-			for (i = 0; i < num_stats; i++) {
-				desc = vm_get_stat_desc(ctx, i);
-				printf("%-40s\t%ld\n", desc, stats[i]);
-			}
-		}
-	}
-#endif
+static int
+get_all_segments(struct vmctx *ctx, int vcpu)
+{
+	uint64_t cs, ds, es, fs, gs, ss, tr, ldtr;
+	int error = 0;
 
 	if (!error && (get_desc_ds || get_all)) {
 		error = vm_get_desc(ctx, vcpu, VM_REG_GUEST_DS,
-				    &desc_base, &desc_limit, &desc_access);
+				   &desc_base, &desc_limit, &desc_access);
 		if (error == 0) {
 			printf("ds desc[%d]\t0x%016lx/0x%08x/0x%08x\n",
-			       vcpu, desc_base, desc_limit, desc_access);	
+			      vcpu, desc_base, desc_limit, desc_access);
 		}
 	}
 
@@ -1024,7 +823,7 @@ main(int argc, char *argv[])
 				    &desc_base, &desc_limit, &desc_access);
 		if (error == 0) {
 			printf("es desc[%d]\t0x%016lx/0x%08x/0x%08x\n",
-			       vcpu, desc_base, desc_limit, desc_access);	
+			       vcpu, desc_base, desc_limit, desc_access);
 		}
 	}
 
@@ -1033,7 +832,7 @@ main(int argc, char *argv[])
 				    &desc_base, &desc_limit, &desc_access);
 		if (error == 0) {
 			printf("fs desc[%d]\t0x%016lx/0x%08x/0x%08x\n",
-			       vcpu, desc_base, desc_limit, desc_access);	
+			       vcpu, desc_base, desc_limit, desc_access);
 		}
 	}
 
@@ -1042,7 +841,7 @@ main(int argc, char *argv[])
 				    &desc_base, &desc_limit, &desc_access);
 		if (error == 0) {
 			printf("gs desc[%d]\t0x%016lx/0x%08x/0x%08x\n",
-			       vcpu, desc_base, desc_limit, desc_access);	
+			       vcpu, desc_base, desc_limit, desc_access);
 		}
 	}
 
@@ -1051,7 +850,7 @@ main(int argc, char *argv[])
 				    &desc_base, &desc_limit, &desc_access);
 		if (error == 0) {
 			printf("ss desc[%d]\t0x%016lx/0x%08x/0x%08x\n",
-			       vcpu, desc_base, desc_limit, desc_access);	
+			       vcpu, desc_base, desc_limit, desc_access);
 		}
 	}
 
@@ -1060,7 +859,7 @@ main(int argc, char *argv[])
 				    &desc_base, &desc_limit, &desc_access);
 		if (error == 0) {
 			printf("cs desc[%d]\t0x%016lx/0x%08x/0x%08x\n",
-			       vcpu, desc_base, desc_limit, desc_access);	
+			       vcpu, desc_base, desc_limit, desc_access);
 		}
 	}
 
@@ -1069,7 +868,7 @@ main(int argc, char *argv[])
 				    &desc_base, &desc_limit, &desc_access);
 		if (error == 0) {
 			printf("tr desc[%d]\t0x%016lx/0x%08x/0x%08x\n",
-			       vcpu, desc_base, desc_limit, desc_access);	
+			       vcpu, desc_base, desc_limit, desc_access);
 		}
 	}
 
@@ -1078,7 +877,7 @@ main(int argc, char *argv[])
 				    &desc_base, &desc_limit, &desc_access);
 		if (error == 0) {
 			printf("ldtr desc[%d]\t0x%016lx/0x%08x/0x%08x\n",
-			       vcpu, desc_base, desc_limit, desc_access);	
+			       vcpu, desc_base, desc_limit, desc_access);
 		}
 	}
 
@@ -1087,7 +886,7 @@ main(int argc, char *argv[])
 				    &desc_base, &desc_limit, &desc_access);
 		if (error == 0) {
 			printf("gdtr[%d]\t\t0x%016lx/0x%08x\n",
-			       vcpu, desc_base, desc_limit);	
+			       vcpu, desc_base, desc_limit);
 		}
 	}
 
@@ -1096,7 +895,7 @@ main(int argc, char *argv[])
 				    &desc_base, &desc_limit, &desc_access);
 		if (error == 0) {
 			printf("idtr[%d]\t\t0x%016lx/0x%08x\n",
-			       vcpu, desc_base, desc_limit);	
+			       vcpu, desc_base, desc_limit);
 		}
 	}
 
@@ -1148,82 +947,14 @@ main(int argc, char *argv[])
 			printf("ldtr[%d]\t\t0x%04lx\n", vcpu, ldtr);
 	}
 
-	if (!error && (get_x2apic_state || get_all)) {
-		error = vm_get_x2apic_state(ctx, vcpu, &x2apic_state);
-		if (error == 0)
-			printf("x2apic_state[%d]\t%d\n", vcpu, x2apic_state);
-	}
-
-	if (!error && (get_pinbased_ctls || get_all)) {
-		error = vm_get_vmcs_field(ctx, vcpu, VMCS_PIN_BASED_CTLS, &ctl);
-		if (error == 0)
-			printf("pinbased_ctls[%d]\t0x%08lx\n", vcpu, ctl);
-	}
-
-	if (!error && (get_procbased_ctls || get_all)) {
-		error = vm_get_vmcs_field(ctx, vcpu,
-					  VMCS_PRI_PROC_BASED_CTLS, &ctl);
-		if (error == 0)
-			printf("procbased_ctls[%d]\t0x%08lx\n", vcpu, ctl);
-	}
-
-	if (!error && (get_procbased_ctls2 || get_all)) {
-		error = vm_get_vmcs_field(ctx, vcpu,
-					  VMCS_SEC_PROC_BASED_CTLS, &ctl);
-		if (error == 0)
-			printf("procbased_ctls2[%d]\t0x%08lx\n", vcpu, ctl);
-	}
-
-	if (!error && (get_vmcs_gla || get_all)) {
-		error = vm_get_vmcs_field(ctx, vcpu,
-					  VMCS_GUEST_LINEAR_ADDRESS, &u64);
-		if (error == 0)
-			printf("gla[%d]\t\t0x%016lx\n", vcpu, u64);
-	}
-
-	if (!error && (get_vmcs_gpa || get_all)) {
-		error = vm_get_vmcs_field(ctx, vcpu,
-					  VMCS_GUEST_PHYSICAL_ADDRESS, &u64);
-		if (error == 0)
-			printf("gpa[%d]\t\t0x%016lx\n", vcpu, u64);
-	}
-
-	if (!error && (get_vmcs_entry_interruption_info || get_all)) {
-		error = vm_get_vmcs_field(ctx, vcpu, VMCS_ENTRY_INTR_INFO,&u64);
-		if (error == 0) {
-			printf("entry_interruption_info[%d]\t0x%08lx\n",
-				vcpu, u64);
-		}
-	}
-
-	if (!error && (get_eptp || get_all)) {
-		error = vm_get_vmcs_field(ctx, vcpu, VMCS_EPTP, &eptp);
-		if (error == 0)
-			printf("eptp[%d]\t\t0x%016lx\n", vcpu, eptp);
-	}
-
-	if (!error && (get_exception_bitmap || get_all)) {
-		error = vm_get_vmcs_field(ctx, vcpu, VMCS_EXCEPTION_BITMAP,
-					  &bm);
-		if (error == 0)
-			printf("exception_bitmap[%d]\t0x%08lx\n", vcpu, bm);
-	}
-
-	if (!error && (get_io_bitmap || get_all)) {
-		error = vm_get_vmcs_field(ctx, vcpu, VMCS_IO_BITMAP_A, &bm);
-		if (error == 0)
-			printf("io_bitmap_a[%d]\t0x%08lx\n", vcpu, bm);
-		error = vm_get_vmcs_field(ctx, vcpu, VMCS_IO_BITMAP_B, &bm);
-		if (error == 0)
-			printf("io_bitmap_b[%d]\t0x%08lx\n", vcpu, bm);
-	}
+	return (error);
+}
 
-	if (!error && (get_tsc_offset || get_all)) {
-		uint64_t tscoff;
-		error = vm_get_vmcs_field(ctx, vcpu, VMCS_TSC_OFFSET, &tscoff);
-		if (error == 0)
-			printf("tsc_offset[%d]\t0x%016lx\n", vcpu, tscoff);
-	}
+static int
+get_misc_vmcs(struct vmctx *ctx, int vcpu)
+{
+	uint64_t ctl, cr0, cr3, cr4, rsp, rip, pat, addr, u64;
+	int error = 0;
 
 	if (!error && (get_cr0_mask || get_all)) {
 		uint64_t cr0mask;
@@ -1260,7 +991,7 @@ main(int argc, char *argv[])
 		error = vm_get_vmcs_field(ctx, vcpu, VMCS_CR3_TARGET_COUNT,
 					  &target_count);
 		if (error == 0) {
-			printf("cr3_target_count[%d]\t0x%08lx\n",
+			printf("cr3_target_count[%d]\t0x%016lx\n",
 				vcpu, target_count);
 		}
 
@@ -1293,57 +1024,55 @@ main(int argc, char *argv[])
 		}
 	}
 
-	if (!error && (get_apic_access_addr || get_all)) {
-		error = vm_get_vmcs_field(ctx, vcpu, VMCS_APIC_ACCESS, &addr);
+	if (!error && (get_pinbased_ctls || get_all)) {
+		error = vm_get_vmcs_field(ctx, vcpu, VMCS_PIN_BASED_CTLS, &ctl);
 		if (error == 0)
-			printf("apic_access_addr[%d]\t0x%016lx\n", vcpu, addr);
+			printf("pinbased_ctls[%d]\t0x%016lx\n", vcpu, ctl);
 	}
 
-	if (!error && (get_virtual_apic_addr || get_all)) {
-		error = vm_get_vmcs_field(ctx, vcpu, VMCS_VIRTUAL_APIC, &addr);
+	if (!error && (get_procbased_ctls || get_all)) {
+		error = vm_get_vmcs_field(ctx, vcpu,
+					  VMCS_PRI_PROC_BASED_CTLS, &ctl);
 		if (error == 0)
-			printf("virtual_apic_addr[%d]\t0x%016lx\n", vcpu, addr);
+			printf("procbased_ctls[%d]\t0x%016lx\n", vcpu, ctl);
 	}
 
-	if (!error && (get_tpr_threshold || get_all)) {
-		uint64_t threshold;
-		error = vm_get_vmcs_field(ctx, vcpu, VMCS_TPR_THRESHOLD,
-					  &threshold);
+	if (!error && (get_procbased_ctls2 || get_all)) {
+		error = vm_get_vmcs_field(ctx, vcpu,
+					  VMCS_SEC_PROC_BASED_CTLS, &ctl);
 		if (error == 0)
-			printf("tpr_threshold[%d]\t0x%08lx\n", vcpu, threshold);
+			printf("procbased_ctls2[%d]\t0x%016lx\n", vcpu, ctl);
 	}
 
-	if (!error && (get_msr_bitmap_address || get_all)) {
-		error = vm_get_vmcs_field(ctx, vcpu, VMCS_MSR_BITMAP, &addr);
+	if (!error && (get_vmcs_gla || get_all)) {
+		error = vm_get_vmcs_field(ctx, vcpu,
+					  VMCS_GUEST_LINEAR_ADDRESS, &u64);
 		if (error == 0)
-			printf("msr_bitmap[%d]\t\t0x%016lx\n", vcpu, addr);
+			printf("gla[%d]\t\t0x%016lx\n", vcpu, u64);
 	}
 
-	if (!error && (get_msr_bitmap || get_all)) {
-		error = vm_get_vmcs_field(ctx, vcpu, VMCS_MSR_BITMAP, &addr);
+	if (!error && (get_vmcs_gpa || get_all)) {
+		error = vm_get_vmcs_field(ctx, vcpu,
+					  VMCS_GUEST_PHYSICAL_ADDRESS, &u64);
 		if (error == 0)
-			error = dump_vmcs_msr_bitmap(vcpu, addr);
+			printf("gpa[%d]\t\t0x%016lx\n", vcpu, u64);
 	}
 
-	if (!error && (get_vpid || get_all)) {
-		uint64_t vpid;
-		error = vm_get_vmcs_field(ctx, vcpu, VMCS_VPID, &vpid);
-		if (error == 0)
-			printf("vpid[%d]\t\t0x%04lx\n", vcpu, vpid);
+	if (!error && (get_vmcs_entry_interruption_info || 
+		get_all)) {
+		error = vm_get_vmcs_field(ctx, vcpu, VMCS_ENTRY_INTR_INFO,&u64);
+		if (error == 0) {
+			printf("entry_interruption_info[%d]\t0x%016lx\n",
+				vcpu, u64);
+		}
 	}
 	
-	if (!error && (get_ple_window || get_all)) {
-		uint64_t window;
-		error = vm_get_vmcs_field(ctx, vcpu, VMCS_PLE_WINDOW, &window);
-		if (error == 0)
-			printf("ple_window[%d]\t\t0x%08lx\n", vcpu, window);
-	}
-
-	if (!error && (get_ple_gap || get_all)) {
-		uint64_t gap;
-		error = vm_get_vmcs_field(ctx, vcpu, VMCS_PLE_GAP, &gap);
+	if (!error && (get_tpr_threshold || get_all)) {
+		uint64_t threshold;
+		error = vm_get_vmcs_field(ctx, vcpu, VMCS_TPR_THRESHOLD,
+					  &threshold);
 		if (error == 0)
-			printf("ple_gap[%d]\t\t0x%08lx\n", vcpu, gap);
+			printf("tpr_threshold[%d]\t0x%016lx\n", vcpu, threshold);
 	}
 
 	if (!error && (get_inst_err || get_all)) {
@@ -1351,21 +1080,21 @@ main(int argc, char *argv[])
 		error = vm_get_vmcs_field(ctx, vcpu, VMCS_INSTRUCTION_ERROR,
 					  &insterr);
 		if (error == 0) {
-			printf("instruction_error[%d]\t0x%08lx\n",
+			printf("instruction_error[%d]\t0x%016lx\n",
 				vcpu, insterr);
 		}
 	}
-
+	
 	if (!error && (get_exit_ctls || get_all)) {
 		error = vm_get_vmcs_field(ctx, vcpu, VMCS_EXIT_CTLS, &ctl);
 		if (error == 0)
-			printf("exit_ctls[%d]\t\t0x%08lx\n", vcpu, ctl);
+			printf("exit_ctls[%d]\t\t0x%016lx\n", vcpu, ctl);
 	}
 
 	if (!error && (get_entry_ctls || get_all)) {
 		error = vm_get_vmcs_field(ctx, vcpu, VMCS_ENTRY_CTLS, &ctl);
 		if (error == 0)
-			printf("entry_ctls[%d]\t\t0x%08lx\n", vcpu, ctl);
+			printf("entry_ctls[%d]\t\t0x%016lx\n", vcpu, ctl);
 	}
 
 	if (!error && (get_host_pat || get_all)) {
@@ -1374,12 +1103,6 @@ main(int argc, char *argv[])
 			printf("host_pat[%d]\t\t0x%016lx\n", vcpu, pat);
 	}
 
-	if (!error && (get_guest_pat || get_all)) {
-		error = vm_get_vmcs_field(ctx, vcpu, VMCS_GUEST_IA32_PAT, &pat);
-		if (error == 0)
-			printf("guest_pat[%d]\t\t0x%016lx\n", vcpu, pat);
-	}
-
 	if (!error && (get_host_cr0 || get_all)) {
 		error = vm_get_vmcs_field(ctx, vcpu, VMCS_HOST_CR0, &cr0);
 		if (error == 0)
@@ -1409,56 +1132,26 @@ main(int argc, char *argv[])
 		if (error == 0)
 			printf("host_rsp[%d]\t\t0x%016lx\n", vcpu, rsp);
 	}
-
-	if (!error && (get_guest_sysenter || get_all)) {
-		error = vm_get_vmcs_field(ctx, vcpu,
-					  VMCS_GUEST_IA32_SYSENTER_CS, &cs);
-		if (error == 0)
-			printf("guest_sysenter_cs[%d]\t0x%08lx\n", vcpu, cs);
-
-		error = vm_get_vmcs_field(ctx, vcpu,
-					  VMCS_GUEST_IA32_SYSENTER_ESP, &rsp);
-		if (error == 0)
-			printf("guest_sysenter_sp[%d]\t0x%016lx\n", vcpu, rsp);
-		error = vm_get_vmcs_field(ctx, vcpu,
-					  VMCS_GUEST_IA32_SYSENTER_EIP, &rip);
-		if (error == 0)
-			printf("guest_sysenter_ip[%d]\t0x%016lx\n", vcpu, rip);
-	}
-
+	
 	if (!error && (get_vmcs_link || get_all)) {
 		error = vm_get_vmcs_field(ctx, vcpu, VMCS_LINK_POINTER, &addr);
 		if (error == 0)
 			printf("vmcs_pointer[%d]\t0x%016lx\n", vcpu, addr);
 	}
 
-	if (!error && (get_vmcs_exit_reason || get_all)) {
-		error = vm_get_vmcs_field(ctx, vcpu, VMCS_EXIT_REASON, &u64);
-		if (error == 0)
-			printf("vmcs_exit_reason[%d]\t0x%016lx\n", vcpu, u64);
-	}
-
-	if (!error && (get_vmcs_exit_qualification || get_all)) {
-		error = vm_get_vmcs_field(ctx, vcpu, VMCS_EXIT_QUALIFICATION,
-					  &u64);
-		if (error == 0)
-			printf("vmcs_exit_qualification[%d]\t0x%016lx\n",
-				vcpu, u64);
-	}
-
 	if (!error && (get_vmcs_exit_interruption_info || get_all)) {
 		error = vm_get_vmcs_field(ctx, vcpu, VMCS_EXIT_INTR_INFO, &u64);
 		if (error == 0) {
-			printf("vmcs_exit_interruption_info[%d]\t0x%08lx\n",
+			printf("vmcs_exit_interruption_info[%d]\t0x%016lx\n",
 				vcpu, u64);
 		}
 	}
 
 	if (!error && (get_vmcs_exit_interruption_error || get_all)) {
 		error = vm_get_vmcs_field(ctx, vcpu, VMCS_EXIT_INTR_ERRCODE,
-		    &u64);
+		    			  &u64);
 		if (error == 0) {
-			printf("vmcs_exit_interruption_error[%d]\t0x%08lx\n",
+			printf("vmcs_exit_interruption_error[%d]\t0x%016lx\n",
 				vcpu, u64);
 		}
 	}
@@ -1467,58 +1160,1076 @@ main(int argc, char *argv[])
 		error = vm_get_vmcs_field(ctx, vcpu,
 					  VMCS_GUEST_INTERRUPTIBILITY, &u64);
 		if (error == 0) {
-			printf("vmcs_guest_interruptibility[%d]\t0x%08lx\n",
+			printf("vmcs_guest_interruptibility[%d]\t0x%016lx\n",
 				vcpu, u64);
 		}
 	}
 
-	if (!error && setcap) {
-		int captype;
-		captype = vm_capability_name2type(capname);
-		error = vm_set_capability(ctx, vcpu, captype, capval);
-		if (error != 0 && errno == ENOENT)
-			printf("Capability \"%s\" is not available\n", capname);
+	if (!error && (get_vmcs_exit_inst_length || get_all)) {
+		error = vm_get_vmcs_field(ctx, vcpu,
+		    VMCS_EXIT_INSTRUCTION_LENGTH, &u64);
+		if (error == 0)
+			printf("vmcs_exit_inst_length[%d]\t0x%08x\n", vcpu,
+			    (uint32_t)u64);
 	}
 
-	if (!error && (getcap || get_all)) {
-		int captype, val, getcaptype;
+	if (!error && (get_vmcs_exit_qualification || get_all)) {
+		error = vm_get_vmcs_field(ctx, vcpu, VMCS_EXIT_QUALIFICATION,
+					  &u64);
+		if (error == 0)
+			printf("vmcs_exit_qualification[%d]\t0x%016lx\n",
+				vcpu, u64);
+	}
+	
+	return (error);
+}
 
-		if (getcap && capname)
-			getcaptype = vm_capability_name2type(capname);
-		else
-			getcaptype = -1;
+static int
+get_misc_vmcb(struct vmctx *ctx, int vcpu)
+{
+	uint64_t ctl, addr;
+	int error = 0;
 
-		for (captype = 0; captype < VM_CAP_MAX; captype++) {
-			if (getcaptype >= 0 && captype != getcaptype)
-				continue;
-			error = vm_get_capability(ctx, vcpu, captype, &val);
-			if (error == 0) {
-				printf("Capability \"%s\" is %s on vcpu %d\n",
-					vm_capability_type2name(captype),
-					val ? "set" : "not set", vcpu);
-			} else if (errno == ENOENT) {
-				error = 0;
-				printf("Capability \"%s\" is not available\n",
-					vm_capability_type2name(captype));
-			} else {
-				break;
-			}
-		}
+	if (!error && (get_vmcb_intercept || get_all)) {
+		error = vm_get_vmcb_field(ctx, vcpu, VMCB_OFF_CR_INTERCEPT, 4,
+		    &ctl);
+		if (error == 0)
+			printf("cr_intercept[%d]\t0x%08x\n", vcpu, (int)ctl);
+
+		error = vm_get_vmcb_field(ctx, vcpu, VMCB_OFF_DR_INTERCEPT, 4,
+		    &ctl);
+		if (error == 0)
+			printf("dr_intercept[%d]\t0x%08x\n", vcpu, (int)ctl);
+
+		error = vm_get_vmcb_field(ctx, vcpu, VMCB_OFF_EXC_INTERCEPT, 4,
+		    &ctl);
+		if (error == 0)
+			printf("exc_intercept[%d]\t0x%08x\n", vcpu, (int)ctl);
+
+		error = vm_get_vmcb_field(ctx, vcpu, VMCB_OFF_INST1_INTERCEPT,
+		    4, &ctl);
+		if (error == 0)
+			printf("inst1_intercept[%d]\t0x%08x\n", vcpu, (int)ctl);
+
+		error = vm_get_vmcb_field(ctx, vcpu, VMCB_OFF_INST2_INTERCEPT,
+		    4, &ctl);
+		if (error == 0)
+			printf("inst2_intercept[%d]\t0x%08x\n", vcpu, (int)ctl);
 	}
 
-	if (!error && run) {
-		error = vm_run(ctx, vcpu, &vmexit);
+	if (!error && (get_vmcb_tlb_ctrl || get_all)) {
+		error = vm_get_vmcb_field(ctx, vcpu, VMCB_OFF_TLB_CTRL,
+					  4, &ctl);
 		if (error == 0)
-			dump_vm_run_exitcode(&vmexit, vcpu);
-		else
-			printf("vm_run error %d\n", error);
+			printf("TLB ctrl[%d]\t0x%016lx\n", vcpu, ctl);
+	}
+
+	if (!error && (get_vmcb_exit_details || get_all)) {
+		error = vm_get_vmcb_field(ctx, vcpu, VMCB_OFF_EXITINFO1,
+					  8, &ctl);
+		if (error == 0)
+			printf("exitinfo1[%d]\t0x%016lx\n", vcpu, ctl);
+		error = vm_get_vmcb_field(ctx, vcpu, VMCB_OFF_EXITINFO2,
+					  8, &ctl);
+		if (error == 0)
+			printf("exitinfo2[%d]\t0x%016lx\n", vcpu, ctl);
+		error = vm_get_vmcb_field(ctx, vcpu, VMCB_OFF_EXITINTINFO,
+					  8, &ctl);
+		if (error == 0)
+			printf("exitintinfo[%d]\t0x%016lx\n", vcpu, ctl);
+	}
+
+	if (!error && (get_vmcb_virq || get_all)) {
+		error = vm_get_vmcb_field(ctx, vcpu, VMCB_OFF_VIRQ,
+					  8, &ctl);
+		if (error == 0)
+			printf("v_irq/tpr[%d]\t0x%016lx\n", vcpu, ctl);
+	}
+
+	if (!error && (get_apic_access_addr || get_all)) {
+		error = vm_get_vmcb_field(ctx, vcpu, VMCB_OFF_AVIC_BAR, 8,
+					  &addr);
+		if (error == 0)
+			printf("AVIC apic_bar[%d]\t0x%016lx\n", vcpu, addr);
+	}
+
+	if (!error && (get_virtual_apic_addr || get_all)) {
+		error = vm_get_vmcb_field(ctx, vcpu, VMCB_OFF_AVIC_PAGE, 8,
+					  &addr);
+		if (error == 0)
+			printf("AVIC backing page[%d]\t0x%016lx\n", vcpu, addr);
+	}
+
+	if (!error && (get_avic_table || get_all)) {
+		error = vm_get_vmcb_field(ctx, vcpu, VMCB_OFF_AVIC_LT, 8,
+					  &addr);
+		if (error == 0)
+			printf("AVIC logical table[%d]\t0x%016lx\n",
+				vcpu, addr);
+		error = vm_get_vmcb_field(ctx, vcpu, VMCB_OFF_AVIC_PT, 8,
+					  &addr);
+		if (error == 0)
+			printf("AVIC physical table[%d]\t0x%016lx\n",
+				vcpu, addr);
 	}
 
+	return (error);
+}
+
+static struct option *
+setup_options(bool cpu_intel)
+{
+	const struct option common_opts[] = {
+		{ "vm",		REQ_ARG,	0,	VMNAME },
+		{ "cpu",	REQ_ARG,	0,	VCPU },
+		{ "set-mem",	REQ_ARG,	0,	SET_MEM },
+		{ "set-efer",	REQ_ARG,	0,	SET_EFER },
+		{ "set-cr0",	REQ_ARG,	0,	SET_CR0 },
+		{ "set-cr3",	REQ_ARG,	0,	SET_CR3 },
+		{ "set-cr4",	REQ_ARG,	0,	SET_CR4 },
+		{ "set-dr7",	REQ_ARG,	0,	SET_DR7 },
+		{ "set-rsp",	REQ_ARG,	0,	SET_RSP },
+		{ "set-rip",	REQ_ARG,	0,	SET_RIP },
+		{ "set-rax",	REQ_ARG,	0,	SET_RAX },
+		{ "set-rflags",	REQ_ARG,	0,	SET_RFLAGS },
+		{ "desc-base",	REQ_ARG,	0,	DESC_BASE },
+		{ "desc-limit",	REQ_ARG,	0,	DESC_LIMIT },
+		{ "desc-access",REQ_ARG,	0,	DESC_ACCESS },
+		{ "set-cs",	REQ_ARG,	0,	SET_CS },
+		{ "set-ds",	REQ_ARG,	0,	SET_DS },
+		{ "set-es",	REQ_ARG,	0,	SET_ES },
+		{ "set-fs",	REQ_ARG,	0,	SET_FS },
+		{ "set-gs",	REQ_ARG,	0,	SET_GS },
+		{ "set-ss",	REQ_ARG,	0,	SET_SS },
+		{ "set-tr",	REQ_ARG,	0,	SET_TR },
+		{ "set-ldtr",	REQ_ARG,	0,	SET_LDTR },
+		{ "set-x2apic-state",REQ_ARG,	0,	SET_X2APIC_STATE },
+		{ "set-exception-bitmap",
+				REQ_ARG,	0, SET_EXCEPTION_BITMAP },
+		{ "capname",	REQ_ARG,	0,	CAPNAME },
+		{ "unassign-pptdev", REQ_ARG,	0,	UNASSIGN_PPTDEV },
+		{ "setcap",	REQ_ARG,	0,	SET_CAP },
+		{ "get-gpa-pmap", REQ_ARG,	0,	GET_GPA_PMAP },
+		{ "assert-lapic-lvt", REQ_ARG,	0,	ASSERT_LAPIC_LVT },
+		{ "get-rtc-time", NO_ARG,	&get_rtc_time,	1 },
+		{ "set-rtc-time", REQ_ARG,	0,	SET_RTC_TIME },
+		{ "rtc-nvram-offset", REQ_ARG,	0,	RTC_NVRAM_OFFSET },
+		{ "get-rtc-nvram", NO_ARG,	&get_rtc_nvram,	1 },
+		{ "set-rtc-nvram", REQ_ARG,	0,	SET_RTC_NVRAM },
+		{ "getcap",	NO_ARG,		&getcap,	1 },
+		{ "get-stats",	NO_ARG,		&get_stats,	1 },
+		{ "get-desc-ds",NO_ARG,		&get_desc_ds,	1 },
+		{ "set-desc-ds",NO_ARG,		&set_desc_ds,	1 },
+		{ "get-desc-es",NO_ARG,		&get_desc_es,	1 },
+		{ "set-desc-es",NO_ARG,		&set_desc_es,	1 },
+		{ "get-desc-ss",NO_ARG,		&get_desc_ss,	1 },
+		{ "set-desc-ss",NO_ARG,		&set_desc_ss,	1 },
+		{ "get-desc-cs",NO_ARG,		&get_desc_cs,	1 },
+		{ "set-desc-cs",NO_ARG,		&set_desc_cs,	1 },
+		{ "get-desc-fs",NO_ARG,		&get_desc_fs,	1 },
+		{ "set-desc-fs",NO_ARG,		&set_desc_fs,	1 },
+		{ "get-desc-gs",NO_ARG,		&get_desc_gs,	1 },
+		{ "set-desc-gs",NO_ARG,		&set_desc_gs,	1 },
+		{ "get-desc-tr",NO_ARG,		&get_desc_tr,	1 },
+		{ "set-desc-tr",NO_ARG,		&set_desc_tr,	1 },
+		{ "set-desc-ldtr", NO_ARG,	&set_desc_ldtr,	1 },
+		{ "get-desc-ldtr", NO_ARG,	&get_desc_ldtr,	1 },
+		{ "set-desc-gdtr", NO_ARG,	&set_desc_gdtr, 1 },
+		{ "get-desc-gdtr", NO_ARG,	&get_desc_gdtr, 1 },
+		{ "set-desc-idtr", NO_ARG,	&set_desc_idtr, 1 },
+		{ "get-desc-idtr", NO_ARG,	&get_desc_idtr, 1 },
+		{ "get-memmap",	NO_ARG,		&get_memmap,	1 },
+		{ "get-memseg", NO_ARG,		&get_memseg,	1 },
+		{ "get-efer",	NO_ARG,		&get_efer,	1 },
+		{ "get-cr0",	NO_ARG,		&get_cr0,	1 },
+		{ "get-cr3",	NO_ARG,		&get_cr3,	1 },
+		{ "get-cr4",	NO_ARG,		&get_cr4,	1 },
+		{ "get-dr7",	NO_ARG,		&get_dr7,	1 },
+		{ "get-rsp",	NO_ARG,		&get_rsp,	1 },
+		{ "get-rip",	NO_ARG,		&get_rip,	1 },
+		{ "get-rax",	NO_ARG,		&get_rax,	1 },
+		{ "get-rbx",	NO_ARG,		&get_rbx,	1 },
+		{ "get-rcx",	NO_ARG,		&get_rcx,	1 },
+		{ "get-rdx",	NO_ARG,		&get_rdx,	1 },
+		{ "get-rsi",	NO_ARG,		&get_rsi,	1 },
+		{ "get-rdi",	NO_ARG,		&get_rdi,	1 },
+		{ "get-rbp",	NO_ARG,		&get_rbp,	1 },
+		{ "get-r8",	NO_ARG,		&get_r8,	1 },
+		{ "get-r9",	NO_ARG,		&get_r9,	1 },
+		{ "get-r10",	NO_ARG,		&get_r10,	1 },
+		{ "get-r11",	NO_ARG,		&get_r11,	1 },
+		{ "get-r12",	NO_ARG,		&get_r12,	1 },
+		{ "get-r13",	NO_ARG,		&get_r13,	1 },
+		{ "get-r14",	NO_ARG,		&get_r14,	1 },
+		{ "get-r15",	NO_ARG,		&get_r15,	1 },
+		{ "get-rflags",	NO_ARG,		&get_rflags,	1 },
+		{ "get-cs",	NO_ARG,		&get_cs,	1 },
+		{ "get-ds",	NO_ARG,		&get_ds,	1 },
+		{ "get-es",	NO_ARG,		&get_es,	1 },
+		{ "get-fs",	NO_ARG,		&get_fs,	1 },
+		{ "get-gs",	NO_ARG,		&get_gs,	1 },
+		{ "get-ss",	NO_ARG,		&get_ss,	1 },
+		{ "get-tr",	NO_ARG,		&get_tr,	1 },
+		{ "get-ldtr",	NO_ARG,		&get_ldtr,	1 },
+		{ "get-eptp", 	NO_ARG,		&get_eptp,	1 },
+		{ "get-exception-bitmap",
+					NO_ARG,	&get_exception_bitmap,  1 },
+		{ "get-io-bitmap-address",
+					NO_ARG,	&get_io_bitmap,		1 },
+		{ "get-tsc-offset", 	NO_ARG, &get_tsc_offset, 	1 },
+		{ "get-msr-bitmap",
+					NO_ARG,	&get_msr_bitmap, 	1 },
+		{ "get-msr-bitmap-address",
+					NO_ARG,	&get_msr_bitmap_address, 1 },
+		{ "get-guest-pat",	NO_ARG,	&get_guest_pat,		1 },
+		{ "get-guest-sysenter",
+					NO_ARG,	&get_guest_sysenter, 	1 },
+		{ "get-exit-reason",
+					NO_ARG,	&get_exit_reason, 	1 },
+		{ "get-x2apic-state",	NO_ARG,	&get_x2apic_state, 	1 },
+		{ "get-all",		NO_ARG,	&get_all,		1 },
+		{ "run",		NO_ARG,	&run,			1 },
+		{ "create",		NO_ARG,	&create,		1 },
+		{ "destroy",		NO_ARG,	&destroy,		1 },
+		{ "inject-nmi",		NO_ARG,	&inject_nmi,		1 },
+		{ "force-reset",	NO_ARG,	&force_reset,		1 },
+		{ "force-poweroff", 	NO_ARG,	&force_poweroff, 	1 },
+		{ "get-active-cpus", 	NO_ARG,	&get_active_cpus, 	1 },
+		{ "get-suspended-cpus", NO_ARG,	&get_suspended_cpus, 	1 },
+		{ "get-intinfo", 	NO_ARG,	&get_intinfo,		1 },
+	};
+
+	const struct option intel_opts[] = {
+		{ "get-vmcs-pinbased-ctls",
+				NO_ARG,		&get_pinbased_ctls, 1 },
+		{ "get-vmcs-procbased-ctls",
+				NO_ARG,		&get_procbased_ctls, 1 },
+		{ "get-vmcs-procbased-ctls2",
+				NO_ARG,		&get_procbased_ctls2, 1 },
+		{ "get-vmcs-guest-linear-address",
+				NO_ARG,		&get_vmcs_gla,	1 },
+		{ "get-vmcs-guest-physical-address",
+				NO_ARG,		&get_vmcs_gpa,	1 },
+		{ "get-vmcs-entry-interruption-info",
+				NO_ARG, &get_vmcs_entry_interruption_info, 1},
+		{ "get-vmcs-cr0-mask", NO_ARG,	&get_cr0_mask,	1 },
+		{ "get-vmcs-cr0-shadow", NO_ARG,&get_cr0_shadow, 1 },
+		{ "get-vmcs-cr4-mask", 		NO_ARG,	&get_cr4_mask,	  1 },
+		{ "get-vmcs-cr4-shadow", 	NO_ARG, &get_cr4_shadow,  1 },
+		{ "get-vmcs-cr3-targets", 	NO_ARG, &get_cr3_targets, 1 },
+		{ "get-vmcs-tpr-threshold",
+					NO_ARG,	&get_tpr_threshold, 1 },
+		{ "get-vmcs-vpid", 	NO_ARG,	&get_vpid_asid,	    1 },
+		{ "get-vmcs-exit-ctls", NO_ARG,	&get_exit_ctls,	    1 },
+		{ "get-vmcs-entry-ctls",
+					NO_ARG,	&get_entry_ctls, 1 },
+		{ "get-vmcs-instruction-error",
+					NO_ARG,	&get_inst_err,	1 },
+		{ "get-vmcs-host-pat",	NO_ARG,	&get_host_pat,	1 },
+		{ "get-vmcs-host-cr0",
+					NO_ARG,	&get_host_cr0,	1 },
+		{ "set-vmcs-entry-interruption-info",
+				REQ_ARG, 0, SET_VMCS_ENTRY_INTERRUPTION_INFO },
+		{ "get-vmcs-exit-qualification",
+				NO_ARG,	&get_vmcs_exit_qualification, 1 },
+		{ "get-vmcs-exit-inst-length",
+				NO_ARG,	&get_vmcs_exit_inst_length, 1 },
+		{ "get-vmcs-interruptibility",
+				NO_ARG, &get_vmcs_interruptibility, 1 },
+		{ "get-vmcs-exit-interruption-error",
+				NO_ARG,	&get_vmcs_exit_interruption_error, 1 },
+		{ "get-vmcs-exit-interruption-info",
+				NO_ARG,	&get_vmcs_exit_interruption_info, 1 },
+		{ "get-vmcs-link", 	NO_ARG,		&get_vmcs_link, 1 },
+		{ "get-vmcs-host-cr3",
+					NO_ARG,		&get_host_cr3,	1 },
+		{ "get-vmcs-host-cr4",
+				NO_ARG,		&get_host_cr4,	1 },
+		{ "get-vmcs-host-rip",
+				NO_ARG,		&get_host_rip,	1 },
+		{ "get-vmcs-host-rsp",
+				NO_ARG,		&get_host_rsp,	1 },
+		{ "get-apic-access-address",
+				NO_ARG,		&get_apic_access_addr, 1},
+		{ "get-virtual-apic-address",
+				NO_ARG,		&get_virtual_apic_addr, 1}
+	};
+
+	const struct option amd_opts[] = {
+		{ "get-vmcb-intercepts",
+				NO_ARG,	&get_vmcb_intercept, 	1 },
+		{ "get-vmcb-asid", 
+				NO_ARG,	&get_vpid_asid,	     	1 },
+		{ "get-vmcb-exit-details",
+				NO_ARG, &get_vmcb_exit_details,	1 },
+		{ "get-vmcb-tlb-ctrl",
+				NO_ARG, &get_vmcb_tlb_ctrl, 	1 },
+		{ "get-vmcb-virq",
+				NO_ARG, &get_vmcb_virq, 	1 },
+		{ "get-avic-apic-bar",
+				NO_ARG,	&get_apic_access_addr, 	1 },
+		{ "get-avic-backing-page",
+				NO_ARG,	&get_virtual_apic_addr, 1 },
+		{ "get-avic-table",
+				NO_ARG,	&get_avic_table, 	1 }
+	};
+
+	const struct option null_opt = {
+		NULL, 0, NULL, 0
+	};
+
+	struct option *all_opts;
+	char *cp;
+	int optlen;
+
+	optlen = sizeof(common_opts);
+
+	if (cpu_intel)
+		optlen += sizeof(intel_opts);
+	else
+		optlen += sizeof(amd_opts);
+
+	optlen += sizeof(null_opt);
+
+	all_opts = malloc(optlen);
+
+	cp = (char *)all_opts;
+	memcpy(cp, common_opts, sizeof(common_opts));
+	cp += sizeof(common_opts);
+
+	if (cpu_intel) {
+		memcpy(cp, intel_opts, sizeof(intel_opts));
+		cp += sizeof(intel_opts);
+	} else {
+		memcpy(cp, amd_opts, sizeof(amd_opts));
+		cp += sizeof(amd_opts);
+	}
+
+	memcpy(cp, &null_opt, sizeof(null_opt));
+	cp += sizeof(null_opt);
+
+	return (all_opts);
+}
+
+static const char *
+wday_str(int idx)
+{
+	static const char *weekdays[] = {
+		"Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"
+	};
+
+	if (idx >= 0 && idx < 7)
+		return (weekdays[idx]);
+	else
+		return ("UNK");
+}
+
+static const char *
+mon_str(int idx)
+{
+	static const char *months[] = {
+		"Jan", "Feb", "Mar", "Apr", "May", "Jun",
+		"Jul", "Aug", "Sep", "Oct", "Nov", "Dec"
+	};
+
+	if (idx >= 0 && idx < 12)
+		return (months[idx]);
+	else
+		return ("UNK");
+}
+
+static int
+show_memmap(struct vmctx *ctx)
+{
+	char name[SPECNAMELEN + 1], numbuf[8];
+	vm_ooffset_t segoff;
+	vm_paddr_t gpa;
+	size_t maplen, seglen;
+	int error, flags, prot, segid, delim;
+
+	printf("Address     Length      Segment     Offset      ");
+	printf("Prot  Flags\n");
+
+	gpa = 0;
+	while (1) {
+		error = vm_mmap_getnext(ctx, &gpa, &segid, &segoff, &maplen,
+		    &prot, &flags);
+		if (error)
+			return (errno == ENOENT ? 0 : error);
+
+		error = vm_get_memseg(ctx, segid, &seglen, name, sizeof(name));
+		if (error)
+			return (error);
+
+		printf("%-12lX", gpa);
+		humanize_number(numbuf, sizeof(numbuf), maplen, "B",
+		    HN_AUTOSCALE, HN_NOSPACE);
+		printf("%-12s", numbuf);
+
+		printf("%-12s", name[0] ? name : "sysmem");
+		printf("%-12lX", segoff);
+		printf("%c%c%c   ", prot & PROT_READ ? 'R' : '-',
+		    prot & PROT_WRITE ? 'W' : '-',
+		    prot & PROT_EXEC ? 'X' : '-');
+
+		delim = '\0';
+		if (flags & VM_MEMMAP_F_WIRED) {
+			printf("%cwired", delim);
+			delim = '/';
+		}
+		if (flags & VM_MEMMAP_F_IOMMU) {
+			printf("%ciommu", delim);
+			delim = '/';
+		}
+		printf("\n");
+
+		gpa += maplen;
+	}
+}
+
+static int
+show_memseg(struct vmctx *ctx)
+{
+	char name[SPECNAMELEN + 1], numbuf[8];
+	size_t seglen;
+	int error, segid;
+
+	printf("ID  Length      Name\n");
+
+	segid = 0;
+	while (1) {
+		error = vm_get_memseg(ctx, segid, &seglen, name, sizeof(name));
+		if (error)
+			return (errno == EINVAL ? 0 : error);
+
+		if (seglen) {
+			printf("%-4d", segid);
+			humanize_number(numbuf, sizeof(numbuf), seglen, "B",
+			    HN_AUTOSCALE, HN_NOSPACE);
+			printf("%-12s", numbuf);
+			printf("%s", name[0] ? name : "sysmem");
+			printf("\n");
+		}
+		segid++;
+	}
+}
+
+int
+main(int argc, char *argv[])
+{
+	char *vmname;
+	int error, ch, vcpu, ptenum;
+	vm_paddr_t gpa_pmap;
+	struct vm_exit vmexit;
+	uint64_t rax, cr0, cr3, cr4, dr7, rsp, rip, rflags, efer, pat;
+	uint64_t eptp, bm, addr, u64, pteval[4], *pte, info[2];
+	struct vmctx *ctx;
+	cpuset_t cpus;
+	bool cpu_intel;
+	uint64_t cs, ds, es, fs, gs, ss, tr, ldtr;
+	struct tm tm;
+	struct option *opts;
+
+	cpu_intel = cpu_vendor_intel();
+	opts = setup_options(cpu_intel);
+
+	vcpu = 0;
+	vmname = NULL;
+	assert_lapic_lvt = -1;
+	progname = basename(argv[0]);
+
+	while ((ch = getopt_long(argc, argv, "", opts, NULL)) != -1) {
+		switch (ch) {
+		case 0:
+			break;
+		case VMNAME:
+			vmname = optarg;
+			break;
+		case VCPU:
+			vcpu = atoi(optarg);
+			break;
+		case SET_MEM:
+			memsize = atoi(optarg) * MB;
+			memsize = roundup(memsize, 2 * MB);
+			break;
+		case SET_EFER:
+			efer = strtoul(optarg, NULL, 0);
+			set_efer = 1;
+			break;
+		case SET_CR0:
+			cr0 = strtoul(optarg, NULL, 0);
+			set_cr0 = 1;
+			break;
+		case SET_CR3:
+			cr3 = strtoul(optarg, NULL, 0);
+			set_cr3 = 1;
+			break;
+		case SET_CR4:
+			cr4 = strtoul(optarg, NULL, 0);
+			set_cr4 = 1;
+			break;
+		case SET_DR7:
+			dr7 = strtoul(optarg, NULL, 0);
+			set_dr7 = 1;
+			break;
+		case SET_RSP:
+			rsp = strtoul(optarg, NULL, 0);
+			set_rsp = 1;
+			break;
+		case SET_RIP:
+			rip = strtoul(optarg, NULL, 0);
+			set_rip = 1;
+			break;
+		case SET_RAX:
+			rax = strtoul(optarg, NULL, 0);
+			set_rax = 1;
+			break;
+		case SET_RFLAGS:
+			rflags = strtoul(optarg, NULL, 0);
+			set_rflags = 1;
+			break;
+		case DESC_BASE:
+			desc_base = strtoul(optarg, NULL, 0);
+			break;
+		case DESC_LIMIT:
+			desc_limit = strtoul(optarg, NULL, 0);
+			break;
+		case DESC_ACCESS:
+			desc_access = strtoul(optarg, NULL, 0);
+			break;
+		case SET_CS:
+			cs = strtoul(optarg, NULL, 0);
+			set_cs = 1;
+			break;
+		case SET_DS:
+			ds = strtoul(optarg, NULL, 0);
+			set_ds = 1;
+			break;
+		case SET_ES:
+			es = strtoul(optarg, NULL, 0);
+			set_es = 1;
+			break;
+		case SET_FS:
+			fs = strtoul(optarg, NULL, 0);
+			set_fs = 1;
+			break;
+		case SET_GS:
+			gs = strtoul(optarg, NULL, 0);
+			set_gs = 1;
+			break;
+		case SET_SS:
+			ss = strtoul(optarg, NULL, 0);
+			set_ss = 1;
+			break;
+		case SET_TR:
+			tr = strtoul(optarg, NULL, 0);
+			set_tr = 1;
+			break;
+		case SET_LDTR:
+			ldtr = strtoul(optarg, NULL, 0);
+			set_ldtr = 1;
+			break;
+		case SET_X2APIC_STATE:
+			x2apic_state = strtol(optarg, NULL, 0);
+			set_x2apic_state = 1;
+			break;
+		case SET_EXCEPTION_BITMAP:
+			exception_bitmap = strtoul(optarg, NULL, 0);
+			set_exception_bitmap = 1;
+			break;
+		case SET_VMCS_ENTRY_INTERRUPTION_INFO:
+			vmcs_entry_interruption_info = strtoul(optarg, NULL, 0);
+			set_vmcs_entry_interruption_info = 1;
+			break;
+		case SET_CAP:
+			capval = strtoul(optarg, NULL, 0);
+			setcap = 1;
+			break;
+		case SET_RTC_TIME:
+			rtc_secs = strtoul(optarg, NULL, 0);
+			set_rtc_time = 1;
+			break;
+		case SET_RTC_NVRAM:
+			rtc_nvram_value = (uint8_t)strtoul(optarg, NULL, 0);
+			set_rtc_nvram = 1;
+			break;
+		case RTC_NVRAM_OFFSET:
+			rtc_nvram_offset = strtoul(optarg, NULL, 0);
+			break;
+		case GET_GPA_PMAP:
+			gpa_pmap = strtoul(optarg, NULL, 0);
+			get_gpa_pmap = 1;
+			break;
+		case CAPNAME:
+			capname = optarg;
+			break;
+		case UNASSIGN_PPTDEV:
+			unassign_pptdev = 1;
+			if (sscanf(optarg, "%d/%d/%d", &bus, &slot, &func) != 3)
+				usage(cpu_intel);
+			break;
+		case ASSERT_LAPIC_LVT:
+			assert_lapic_lvt = atoi(optarg);
+			break;
+		default:
+			usage(cpu_intel);
+		}
+	}
+	argc -= optind;
+	argv += optind;
+
+	if (vmname == NULL)
+		usage(cpu_intel);
+
+	error = 0;
+
+	if (!error && create)
+		error = vm_create(vmname);
+
+	if (!error) {
+		ctx = vm_open(vmname);
+		if (ctx == NULL) {
+			printf("VM:%s is not created.\n", vmname);
+			exit (1);
+		}
+	}
+
+	if (!error && memsize)
+		error = vm_setup_memory(ctx, memsize, VM_MMAP_ALL);
+
+	if (!error && set_efer)
+		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_EFER, efer);
+
+	if (!error && set_cr0)
+		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_CR0, cr0);
+
+	if (!error && set_cr3)
+		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_CR3, cr3);
+
+	if (!error && set_cr4)
+		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_CR4, cr4);
+
+	if (!error && set_dr7)
+		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_DR7, dr7);
+
+	if (!error && set_rsp)
+		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_RSP, rsp);
+
+	if (!error && set_rip)
+		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_RIP, rip);
+
+	if (!error && set_rax)
+		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_RAX, rax);
+
+	if (!error && set_rflags) {
+		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_RFLAGS,
+					rflags);
+	}
+
+	if (!error && set_desc_ds) {
+		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_DS,
+				    desc_base, desc_limit, desc_access);
+	}
+
+	if (!error && set_desc_es) {
+		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_ES,
+				    desc_base, desc_limit, desc_access);
+	}
+
+	if (!error && set_desc_ss) {
+		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_SS,
+				    desc_base, desc_limit, desc_access);
+	}
+
+	if (!error && set_desc_cs) {
+		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_CS,
+				    desc_base, desc_limit, desc_access);
+	}
+
+	if (!error && set_desc_fs) {
+		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_FS,
+				    desc_base, desc_limit, desc_access);
+	}
+
+	if (!error && set_desc_gs) {
+		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_GS,
+				    desc_base, desc_limit, desc_access);
+	}
+
+	if (!error && set_desc_tr) {
+		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_TR,
+				    desc_base, desc_limit, desc_access);
+	}
+
+	if (!error && set_desc_ldtr) {
+		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_LDTR,
+				    desc_base, desc_limit, desc_access);
+	}
+
+	if (!error && set_desc_gdtr) {
+		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_GDTR,
+				    desc_base, desc_limit, 0);
+	}
+
+	if (!error && set_desc_idtr) {
+		error = vm_set_desc(ctx, vcpu, VM_REG_GUEST_IDTR,
+				    desc_base, desc_limit, 0);
+	}
+
+	if (!error && set_cs)
+		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_CS, cs);
+
+	if (!error && set_ds)
+		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_DS, ds);
+
+	if (!error && set_es)
+		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_ES, es);
+
+	if (!error && set_fs)
+		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_FS, fs);
+
+	if (!error && set_gs)
+		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_GS, gs);
+
+	if (!error && set_ss)
+		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_SS, ss);
+
+	if (!error && set_tr)
+		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_TR, tr);
+
+	if (!error && set_ldtr)
+		error = vm_set_register(ctx, vcpu, VM_REG_GUEST_LDTR, ldtr);
+
+	if (!error && set_x2apic_state)
+		error = vm_set_x2apic_state(ctx, vcpu, x2apic_state);
+
+	if (!error && unassign_pptdev)
+		error = vm_unassign_pptdev(ctx, bus, slot, func);
+
+	if (!error && set_exception_bitmap) {
+		if (cpu_intel)
+			error = vm_set_vmcs_field(ctx, vcpu,
+						  VMCS_EXCEPTION_BITMAP,
+						  exception_bitmap);
+		else
+			error = vm_set_vmcb_field(ctx, vcpu,
+						  VMCB_OFF_EXC_INTERCEPT,
+						  4, exception_bitmap);
+	}
+
+	if (!error && cpu_intel && set_vmcs_entry_interruption_info) {
+		error = vm_set_vmcs_field(ctx, vcpu, VMCS_ENTRY_INTR_INFO,
+					  vmcs_entry_interruption_info);
+	}
+
+	if (!error && inject_nmi) {
+		error = vm_inject_nmi(ctx, vcpu);
+	}
+
+	if (!error && assert_lapic_lvt != -1) {
+		error = vm_lapic_local_irq(ctx, vcpu, assert_lapic_lvt);
+	}
+
+	if (!error && (get_memseg || get_all))
+		error = show_memseg(ctx);
+
+	if (!error && (get_memmap || get_all))
+		error = show_memmap(ctx);
+
+	if (!error)
+		error = get_all_registers(ctx, vcpu);
+
+	if (!error)
+		error = get_all_segments(ctx, vcpu);
+
+	if (!error) {
+		if (cpu_intel)
+			error = get_misc_vmcs(ctx, vcpu);
+		else
+			error = get_misc_vmcb(ctx, vcpu);
+	}
+	
+	if (!error && (get_x2apic_state || get_all)) {
+		error = vm_get_x2apic_state(ctx, vcpu, &x2apic_state);
+		if (error == 0)
+			printf("x2apic_state[%d]\t%d\n", vcpu, x2apic_state);
+	}
+
+	if (!error && (get_eptp || get_all)) {
+		if (cpu_intel)
+			error = vm_get_vmcs_field(ctx, vcpu, VMCS_EPTP, &eptp);
+		else
+			error = vm_get_vmcb_field(ctx, vcpu, VMCB_OFF_NPT_BASE,
+						   8, &eptp);
+		if (error == 0)
+			printf("%s[%d]\t\t0x%016lx\n",
+				cpu_intel ? "eptp" : "rvi/npt", vcpu, eptp);
+	}
+
+	if (!error && (get_exception_bitmap || get_all)) {
+		if(cpu_intel)
+			error = vm_get_vmcs_field(ctx, vcpu,
+						VMCS_EXCEPTION_BITMAP, &bm);
+		else
+			error = vm_get_vmcb_field(ctx, vcpu,
+						  VMCB_OFF_EXC_INTERCEPT,
+						  4, &bm);
+		if (error == 0)
+			printf("exception_bitmap[%d]\t%#lx\n", vcpu, bm);
+	}
+
+	if (!error && (get_io_bitmap || get_all)) {
+		if (cpu_intel) {
+			error = vm_get_vmcs_field(ctx, vcpu, VMCS_IO_BITMAP_A,
+						  &bm);
+			if (error == 0)
+				printf("io_bitmap_a[%d]\t%#lx\n", vcpu, bm);
+			error = vm_get_vmcs_field(ctx, vcpu, VMCS_IO_BITMAP_B,
+						  &bm);
+			if (error == 0)
+				printf("io_bitmap_b[%d]\t%#lx\n", vcpu, bm);
+		} else {
+			error = vm_get_vmcb_field(ctx, vcpu,
+						  VMCB_OFF_IO_PERM, 8, &bm);
+			if (error == 0)
+				printf("io_bitmap[%d]\t%#lx\n", vcpu, bm);
+		}
+	}
+
+	if (!error && (get_tsc_offset || get_all)) {
+		uint64_t tscoff;
+		if (cpu_intel)
+			error = vm_get_vmcs_field(ctx, vcpu, VMCS_TSC_OFFSET,
+						  &tscoff);
+		else
+			error = vm_get_vmcb_field(ctx, vcpu,
+						  VMCB_OFF_TSC_OFFSET, 
+						  8, &tscoff);
+		if (error == 0)
+			printf("tsc_offset[%d]\t0x%016lx\n", vcpu, tscoff);
+	}
+
+	if (!error && (get_msr_bitmap_address || get_all)) {
+		if (cpu_intel)
+			error = vm_get_vmcs_field(ctx, vcpu, VMCS_MSR_BITMAP, 
+						  &addr);
+		else
+			error = vm_get_vmcb_field(ctx, vcpu,
+						  VMCB_OFF_MSR_PERM, 8, &addr);
+		if (error == 0)
+			printf("msr_bitmap[%d]\t\t%#lx\n", vcpu, addr);
+	}
+
+	if (!error && (get_msr_bitmap || get_all)) {
+		if (cpu_intel) {
+			error = vm_get_vmcs_field(ctx, vcpu, 
+						  VMCS_MSR_BITMAP, &addr);
+		} else {
+			error = vm_get_vmcb_field(ctx, vcpu,
+						  VMCB_OFF_MSR_PERM, 8,
+						  &addr);
+		}
+
+		if (error == 0)
+			error = dump_msr_bitmap(vcpu, addr, cpu_intel);
+	}
+
+	if (!error && (get_vpid_asid || get_all)) {
+		uint64_t vpid;
+		if (cpu_intel)
+			error = vm_get_vmcs_field(ctx, vcpu, VMCS_VPID, &vpid);
+		else
+			error = vm_get_vmcb_field(ctx, vcpu, VMCB_OFF_ASID, 
+						  4, &vpid);
+		if (error == 0)
+			printf("%s[%d]\t\t0x%04lx\n", 
+				cpu_intel ? "vpid" : "asid", vcpu, vpid);
+	}
+
+	if (!error && (get_guest_pat || get_all)) {
+		if (cpu_intel)
+			error = vm_get_vmcs_field(ctx, vcpu,
+						  VMCS_GUEST_IA32_PAT, &pat);
+		else
+			error = vm_get_vmcb_field(ctx, vcpu,
+						  VMCB_OFF_GUEST_PAT, 8, &pat);
+		if (error == 0)
+			printf("guest_pat[%d]\t\t0x%016lx\n", vcpu, pat);
+	}
+
+	if (!error && (get_guest_sysenter || get_all)) {
+		if (cpu_intel)
+			error = vm_get_vmcs_field(ctx, vcpu,
+						  VMCS_GUEST_IA32_SYSENTER_CS,
+						  &cs);
+		else
+			error = vm_get_vmcb_field(ctx, vcpu,
+						  VMCB_OFF_SYSENTER_CS, 8,
+						  &cs);
+
+		if (error == 0)
+			printf("guest_sysenter_cs[%d]\t%#lx\n", vcpu, cs);
+		if (cpu_intel)
+			error = vm_get_vmcs_field(ctx, vcpu,
+						  VMCS_GUEST_IA32_SYSENTER_ESP,
+						  &rsp);
+		else
+			error = vm_get_vmcb_field(ctx, vcpu,
+						  VMCB_OFF_SYSENTER_ESP, 8,
+						  &rsp);
+
+		if (error == 0)
+			printf("guest_sysenter_sp[%d]\t%#lx\n", vcpu, rsp);
+		if (cpu_intel)
+			error = vm_get_vmcs_field(ctx, vcpu,
+						  VMCS_GUEST_IA32_SYSENTER_EIP,
+						  &rip);
+		else
+			error = vm_get_vmcb_field(ctx, vcpu,
+						  VMCB_OFF_SYSENTER_EIP, 8, 
+						  &rip);
+		if (error == 0)
+			printf("guest_sysenter_ip[%d]\t%#lx\n", vcpu, rip);
+	}
+
+	if (!error && (get_exit_reason || get_all)) {
+		if (cpu_intel)
+			error = vm_get_vmcs_field(ctx, vcpu, VMCS_EXIT_REASON,
+						  &u64);
+		else	
+			error = vm_get_vmcb_field(ctx, vcpu,
+						  VMCB_OFF_EXIT_REASON, 8,
+						  &u64);
+		if (error == 0)
+			printf("exit_reason[%d]\t%#lx\n", vcpu, u64);
+	}
+
+	if (!error && setcap) {
+		int captype;
+		captype = vm_capability_name2type(capname);
+		error = vm_set_capability(ctx, vcpu, captype, capval);
+		if (error != 0 && errno == ENOENT)
+			printf("Capability \"%s\" is not available\n", capname);
+	}
+
+	if (!error && get_gpa_pmap) {
+		error = vm_get_gpa_pmap(ctx, gpa_pmap, pteval, &ptenum);
+		if (error == 0) {
+			printf("gpa %#lx:", gpa_pmap);
+			pte = &pteval[0];
+			while (ptenum-- > 0)
+				printf(" %#lx", *pte++);
+			printf("\n");
+		}
+	}
+
+	if (!error && set_rtc_nvram)
+		error = vm_rtc_write(ctx, rtc_nvram_offset, rtc_nvram_value);
+
+	if (!error && (get_rtc_nvram || get_all)) {
+		error = vm_rtc_read(ctx, rtc_nvram_offset, &rtc_nvram_value);
+		if (error == 0) {
+			printf("rtc nvram[%03d]: 0x%02x\n", rtc_nvram_offset,
+			    rtc_nvram_value);
+		}
+	}
+
+	if (!error && set_rtc_time)
+		error = vm_rtc_settime(ctx, rtc_secs);
+
+	if (!error && (get_rtc_time || get_all)) {
+		error = vm_rtc_gettime(ctx, &rtc_secs);
+		if (error == 0) {
+			gmtime_r(&rtc_secs, &tm);
+			printf("rtc time %#lx: %s %s %02d %02d:%02d:%02d %d\n",
+			    rtc_secs, wday_str(tm.tm_wday), mon_str(tm.tm_mon),
+			    tm.tm_mday, tm.tm_hour, tm.tm_min, tm.tm_sec,
+			    1900 + tm.tm_year);
+		}
+	}
+
+	if (!error && (getcap || get_all)) {
+		int captype, val, getcaptype;
+
+		if (getcap && capname)
+			getcaptype = vm_capability_name2type(capname);
+		else
+			getcaptype = -1;
+
+		for (captype = 0; captype < VM_CAP_MAX; captype++) {
+			if (getcaptype >= 0 && captype != getcaptype)
+				continue;
+			error = vm_get_capability(ctx, vcpu, captype, &val);
+			if (error == 0) {
+				printf("Capability \"%s\" is %s on vcpu %d\n",
+					vm_capability_type2name(captype),
+					val ? "set" : "not set", vcpu);
+			} else if (errno == ENOENT) {
+				error = 0;
+				printf("Capability \"%s\" is not available\n",
+					vm_capability_type2name(captype));
+			} else {
+				break;
+			}
+		}
+	}
+
+	if (!error && (get_active_cpus || get_all)) {
+		error = vm_active_cpus(ctx, &cpus);
+		if (!error)
+			print_cpus("active cpus", &cpus);
+	}
+
+	if (!error && (get_suspended_cpus || get_all)) {
+		error = vm_suspended_cpus(ctx, &cpus);
+		if (!error)
+			print_cpus("suspended cpus", &cpus);
+	}
+
+	if (!error && (get_intinfo || get_all)) {
+		error = vm_get_intinfo(ctx, vcpu, &info[0], &info[1]);
+		if (!error) {
+			print_intinfo("pending", info[0]);
+			print_intinfo("current", info[1]);
+		}
+	}
+
+	if (!error && (get_stats || get_all)) {
+		int i, num_stats;
+		uint64_t *stats;
+		struct timeval tv;
+		const char *desc;
+
+		stats = vm_get_stats(ctx, vcpu, &tv, &num_stats);
+		if (stats != NULL) {
+			printf("vcpu%d stats:\n", vcpu);
+			for (i = 0; i < num_stats; i++) {
+				desc = vm_get_stat_desc(ctx, i);
+				printf("%-40s\t%ld\n", desc, stats[i]);
+			}
+		}
+	}
+
+	if (!error && run) {
+		error = vm_run(ctx, vcpu, &vmexit);
+		if (error == 0)
+			dump_vm_run_exitcode(&vmexit, vcpu);
+		else
+			printf("vm_run error %d\n", error);
+	}
+
+	if (!error && force_reset)
+		error = vm_suspend(ctx, VM_SUSPEND_RESET);
+
+	if (!error && force_poweroff)
+		error = vm_suspend(ctx, VM_SUSPEND_POWEROFF);
+
 	if (error)
 		printf("errno = %d\n", errno);
 
 	if (!error && destroy)
-		error = vm_destroy(ctx);
+		vm_destroy(ctx);
 
+	free (opts);
 	exit(error);
 }
diff --git a/usr/src/compat/freebsd/amd64/machine/asmacros.h b/usr/src/compat/freebsd/amd64/machine/asmacros.h
index fcf35a7b78..1f6955130b 100644
--- a/usr/src/compat/freebsd/amd64/machine/asmacros.h
+++ b/usr/src/compat/freebsd/amd64/machine/asmacros.h
@@ -25,4 +25,7 @@ x:
 #define	END(x) \
 	.size x, [.-x]
 
+#define	ALIGN_TEXT \
+	.p2align 4,0x90; /* 16-byte alignment, nop filled */
+
 #endif	/* _COMPAT_FREEBSD_AMD64_MACHINE_ASMACROS_H_ */
diff --git a/usr/src/compat/freebsd/amd64/machine/cpufunc.h b/usr/src/compat/freebsd/amd64/machine/cpufunc.h
index cf485e947c..88b4bca04f 100644
--- a/usr/src/compat/freebsd/amd64/machine/cpufunc.h
+++ b/usr/src/compat/freebsd/amd64/machine/cpufunc.h
@@ -16,6 +16,8 @@
 #ifndef _COMPAT_FREEBSD_AMD64_MACHINE_CPUFUNC_H_
 #define	_COMPAT_FREEBSD_AMD64_MACHINE_CPUFUNC_H_
 
+#include <sys/types.h>
+
 static __inline u_long
 bsfq(u_long mask)
 {
@@ -65,6 +67,12 @@ cpuid_count(u_int ax, u_int cx, u_int *p)
 			 :  "0" (ax), "c" (cx));
 }
 
+static __inline void
+disable_intr(void)
+{
+	__asm __volatile("cli");
+}
+
 static __inline void
 enable_intr(void)
 {
@@ -95,6 +103,15 @@ flsll(long long mask)
 	return (flsl((long)mask));
 }
 
+static __inline u_long
+read_rflags(void)
+{
+	u_long  rf;
+
+	__asm __volatile("pushfq; popq %0" : "=r" (rf));
+	return (rf);
+}
+
 static __inline uint64_t
 rdmsr(u_int msr)
 {
@@ -107,10 +124,10 @@ rdmsr(u_int msr)
 static __inline uint64_t
 rdtsc(void)
 {
-	uint32_t low, high;
- 
-	__asm __volatile("rdtsc" : "=a" (low), "=d" (high));
-	return (low | ((uint64_t)high << 32));
+	extern hrtime_t tsc_gethrtimeunscaled_delta(void);
+
+	/* Get the TSC reading with any needed synch offset applied */
+	return ((uint64_t)tsc_gethrtimeunscaled_delta());
 }
 
 static __inline void
@@ -162,4 +179,23 @@ rcr4(void)
 	return (data);
 }
 
+static __inline u_long
+rxcr(u_int reg)
+{
+	u_int low, high;
+
+	__asm __volatile("xgetbv" : "=a" (low), "=d" (high) : "c" (reg));
+	return (low | ((uint64_t)high << 32));
+}
+
+static __inline void
+load_xcr(u_int reg, u_long val)
+{
+	u_int low, high;
+
+	low = val;
+	high = val >> 32;
+	__asm __volatile("xsetbv" : : "c" (reg), "a" (low), "d" (high));
+}
+
 #endif	/* _COMPAT_FREEBSD_AMD64_MACHINE_CPUFUNC_H_ */
diff --git a/usr/src/compat/freebsd/amd64/machine/md_var.h b/usr/src/compat/freebsd/amd64/machine/md_var.h
index 60fdd566e5..ed57a8bebc 100644
--- a/usr/src/compat/freebsd/amd64/machine/md_var.h
+++ b/usr/src/compat/freebsd/amd64/machine/md_var.h
@@ -21,4 +21,8 @@ extern	u_int	cpu_exthigh;		/* Highest arg to extended CPUID */
 extern	u_int	cpu_id;			/* Stepping ID */
 extern	char	cpu_vendor[];		/* CPU Origin code */
 
+#include <sys/systm.h>
+
+#define	Maxmem	(physmax + 1)
+
 #endif	/* _COMPAT_FREEBSD_AMD64_MACHINE_MD_VAR_H_ */
diff --git a/usr/src/compat/freebsd/amd64/machine/pmap.h b/usr/src/compat/freebsd/amd64/machine/pmap.h
index d0303bdd56..ce3185629b 100644
--- a/usr/src/compat/freebsd/amd64/machine/pmap.h
+++ b/usr/src/compat/freebsd/amd64/machine/pmap.h
@@ -1,3 +1,54 @@
+/*
+ * All rights reserved. This copyright notice is Copyright Management
+ * Information under 17 USC 1202 and is included to protect this work and
+ * deter copyright infringement.  Removal or alteration of this Copyright
+ * Management Information without the express written permission from
+ * Pluribus Networks Inc is prohibited, and any such unauthorized removal
+ * or alteration will be a violation of federal law.
+ *
+ * Copyright (c) 2003 Peter Wemm.
+ * Copyright (c) 1991 Regents of the University of California.
+ * All rights reserved.
+ *
+ * This code is derived from software contributed to Berkeley by
+ * the Systems Programming Group of the University of Utah Computer
+ * Science Department and William Jolitz of UUNET Technologies Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 4. Neither the name of the University nor the names of its contributors
+ *    may be used to endorse or promote products derived from this software
+ *    without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * Derived from hp300 version by Mike Hibler, this version by William
+ * Jolitz uses a recursive map [a pde points to the page directory] to
+ * map the page tables using the pagetables themselves. This is done to
+ * reduce the impact on kernel virtual memory for lots of sparse address
+ * space, and to reduce the cost of memory to each process.
+ *
+ *	from: hp300: @(#)pmap.h	7.2 (Berkeley) 12/16/90
+ *	from: @(#)pmap.h	7.4 (Berkeley) 5/12/91
+ * $FreeBSD$
+ */
+
 /*
  * This file and its contents are supplied under the terms of the
  * Common Development and Distribution License ("CDDL"), version 1.0.
@@ -13,32 +64,426 @@
  * Copyright 2014 Pluribus Networks Inc.
  */
 
+
 #ifndef _COMPAT_FREEBSD_AMD64_MACHINE_PMAP_H_
 #define	_COMPAT_FREEBSD_AMD64_MACHINE_PMAP_H_
 
+/*
+ * Page-directory and page-table entries follow this format, with a few
+ * of the fields not present here and there, depending on a lot of things.
+ */
 				/* ---- Intel Nomenclature ---- */
-#define	PG_V		0x001	/* P	Valid			*/
-#define	PG_RW		0x002	/* R/W	Read/Write		*/
-#define	PG_U		0x004	/* U/S	User/Supervisor 	*/
-#define	PG_A		0x020	/* A	Accessed		*/
-#define	PG_M		0x040	/* D	Dirty			*/
-#define	PG_PS		0x080	/* PS	Page size (0=4k,1=2M)	*/
+#define	X86_PG_V	0x001	/* P	Valid			*/
+#define	X86_PG_RW	0x002	/* R/W	Read/Write		*/
+#define	X86_PG_U	0x004	/* U/S  User/Supervisor		*/
+#define	X86_PG_NC_PWT	0x008	/* PWT	Write through		*/
+#define	X86_PG_NC_PCD	0x010	/* PCD	Cache disable		*/
+#define	X86_PG_A	0x020	/* A	Accessed		*/
+#define	X86_PG_M	0x040	/* D	Dirty			*/
+#define	X86_PG_PS	0x080	/* PS	Page size (0=4k,1=2M)	*/
+#define	X86_PG_PTE_PAT	0x080	/* PAT	PAT index		*/
+#define	X86_PG_G	0x100	/* G	Global			*/
+#define	X86_PG_AVAIL1	0x200	/*    /	Available for system	*/
+#define	X86_PG_AVAIL2	0x400	/*   <	programmers use		*/
+#define	X86_PG_AVAIL3	0x800	/*    \				*/
+#define	X86_PG_PDE_PAT	0x1000	/* PAT	PAT index		*/
+#define	X86_PG_NX	(1ul<<63) /* No-execute */
+#define	X86_PG_AVAIL(x)	(1ul << (x))
+
+/* Page level cache control fields used to determine the PAT type */
+#define	X86_PG_PDE_CACHE (X86_PG_PDE_PAT | X86_PG_NC_PWT | X86_PG_NC_PCD)
+#define	X86_PG_PTE_CACHE (X86_PG_PTE_PAT | X86_PG_NC_PWT | X86_PG_NC_PCD)
+
+/*
+ * Intel extended page table (EPT) bit definitions.
+ */
+#define	EPT_PG_READ		0x001	/* R	Read		*/
+#define	EPT_PG_WRITE		0x002	/* W	Write		*/
+#define	EPT_PG_EXECUTE		0x004	/* X	Execute		*/
+#define	EPT_PG_IGNORE_PAT	0x040	/* IPAT	Ignore PAT	*/
+#define	EPT_PG_PS		0x080	/* PS	Page size	*/
+#define	EPT_PG_A		0x100	/* A	Accessed	*/
+#define	EPT_PG_M		0x200	/* D	Dirty		*/
+#define	EPT_PG_MEMORY_TYPE(x)	((x) << 3) /* MT Memory Type	*/
+
+/*
+ * Define the PG_xx macros in terms of the bits on x86 PTEs.
+ */
+#define	PG_V		X86_PG_V
+#define	PG_RW		X86_PG_RW
+#define	PG_U		X86_PG_U
+#define	PG_NC_PWT	X86_PG_NC_PWT
+#define	PG_NC_PCD	X86_PG_NC_PCD
+#define	PG_A		X86_PG_A
+#define	PG_M		X86_PG_M
+#define	PG_PS		X86_PG_PS
+#define	PG_PTE_PAT	X86_PG_PTE_PAT
+#define	PG_G		X86_PG_G
+#define	PG_AVAIL1	X86_PG_AVAIL1
+#define	PG_AVAIL2	X86_PG_AVAIL2
+#define	PG_AVAIL3	X86_PG_AVAIL3
+#define	PG_PDE_PAT	X86_PG_PDE_PAT
+#define	PG_NX		X86_PG_NX
+#define	PG_PDE_CACHE	X86_PG_PDE_CACHE
+#define	PG_PTE_CACHE	X86_PG_PTE_CACHE
+
+/* Our various interpretations of the above */
+#define	PG_W		X86_PG_AVAIL3	/* "Wired" pseudoflag */
+#define	PG_MANAGED	X86_PG_AVAIL2
+#define	EPT_PG_EMUL_V	X86_PG_AVAIL(52)
+#define	EPT_PG_EMUL_RW	X86_PG_AVAIL(53)
+#define	PG_PROMOTED	X86_PG_AVAIL(54)	/* PDE only */
+#define	PG_FRAME	(0x000ffffffffff000ul)
+#define	PG_PS_FRAME	(0x000fffffffe00000ul)
+
+/*
+ * Promotion to a 2MB (PDE) page mapping requires that the corresponding 4KB
+ * (PTE) page mappings have identical settings for the following fields:
+ */
+#define	PG_PTE_PROMOTE	(PG_NX | PG_MANAGED | PG_W | PG_G | PG_PTE_CACHE | \
+	    PG_M | PG_A | PG_U | PG_RW | PG_V)
 
 /*
  * Page Protection Exception bits
  */
+
 #define PGEX_P		0x01	/* Protection violation vs. not present */
 #define PGEX_W		0x02	/* during a Write cycle */
 #define PGEX_U		0x04	/* access from User mode (UPL) */
 #define PGEX_RSV	0x08	/* reserved PTE field is non-zero */
 #define PGEX_I		0x10	/* during an instruction fetch */
 
+/* 
+ * undef the PG_xx macros that define bits in the regular x86 PTEs that
+ * have a different position in nested PTEs. This is done when compiling
+ * code that needs to be aware of the differences between regular x86 and
+ * nested PTEs.
+ *
+ * The appropriate bitmask will be calculated at runtime based on the pmap
+ * type.
+ */
+#ifdef AMD64_NPT_AWARE
+#undef PG_AVAIL1		/* X86_PG_AVAIL1 aliases with EPT_PG_M */
+#undef PG_G
+#undef PG_A
+#undef PG_M
+#undef PG_PDE_PAT
+#undef PG_PDE_CACHE
+#undef PG_PTE_PAT
+#undef PG_PTE_CACHE
+#undef PG_RW
+#undef PG_V
+#endif
+
+/*
+ * Pte related macros.  This is complicated by having to deal with
+ * the sign extension of the 48th bit.
+ */
+#define KVADDR(l4, l3, l2, l1) ( \
+	((unsigned long)-1 << 47) | \
+	((unsigned long)(l4) << PML4SHIFT) | \
+	((unsigned long)(l3) << PDPSHIFT) | \
+	((unsigned long)(l2) << PDRSHIFT) | \
+	((unsigned long)(l1) << PAGE_SHIFT))
+
+#define UVADDR(l4, l3, l2, l1) ( \
+	((unsigned long)(l4) << PML4SHIFT) | \
+	((unsigned long)(l3) << PDPSHIFT) | \
+	((unsigned long)(l2) << PDRSHIFT) | \
+	((unsigned long)(l1) << PAGE_SHIFT))
+
+/*
+ * Number of kernel PML4 slots.  Can be anywhere from 1 to 64 or so,
+ * but setting it larger than NDMPML4E makes no sense.
+ *
+ * Each slot provides .5 TB of kernel virtual space.
+ */
+#define NKPML4E		4
+
+#define	NUPML4E		(NPML4EPG/2)	/* number of userland PML4 pages */
+#define	NUPDPE		(NUPML4E*NPDPEPG)/* number of userland PDP pages */
+#define	NUPDE		(NUPDPE*NPDEPG)	/* number of userland PD entries */
+
+/*
+ * NDMPML4E is the maximum number of PML4 entries that will be
+ * used to implement the direct map.  It must be a power of two,
+ * and should generally exceed NKPML4E.  The maximum possible
+ * value is 64; using 128 will make the direct map intrude into
+ * the recursive page table map.
+ */
+#define	NDMPML4E	8
+
+/*
+ * These values control the layout of virtual memory.  The starting address
+ * of the direct map, which is controlled by DMPML4I, must be a multiple of
+ * its size.  (See the PHYS_TO_DMAP() and DMAP_TO_PHYS() macros.)
+ *
+ * Note: KPML4I is the index of the (single) level 4 page that maps
+ * the KVA that holds KERNBASE, while KPML4BASE is the index of the
+ * first level 4 page that maps VM_MIN_KERNEL_ADDRESS.  If NKPML4E
+ * is 1, these are the same, otherwise KPML4BASE < KPML4I and extra
+ * level 4 PDEs are needed to map from VM_MIN_KERNEL_ADDRESS up to
+ * KERNBASE.
+ *
+ * (KPML4I combines with KPDPI to choose where KERNBASE starts.
+ * Or, in other words, KPML4I provides bits 39..47 of KERNBASE,
+ * and KPDPI provides bits 30..38.)
+ */
+#define	PML4PML4I	(NPML4EPG/2)	/* Index of recursive pml4 mapping */
+
+#define	KPML4BASE	(NPML4EPG-NKPML4E) /* KVM at highest addresses */
+#define	DMPML4I		rounddown(KPML4BASE-NDMPML4E, NDMPML4E) /* Below KVM */
+
+#define	KPML4I		(NPML4EPG-1)
+#define	KPDPI		(NPDPEPG-2)	/* kernbase at -2GB */
+
+/*
+ * XXX doesn't really belong here I guess...
+ */
+#define ISA_HOLE_START    0xa0000
+#define ISA_HOLE_LENGTH (0x100000-ISA_HOLE_START)
+
+#define	PMAP_PCID_NONE		0xffffffff
+#define	PMAP_PCID_KERN		0
+#define	PMAP_PCID_OVERMAX	0x1000
+
+#ifndef LOCORE
+
+#ifdef __FreeBSD__
+#include <sys/queue.h>
+#include <sys/_cpuset.h>
+#include <sys/_lock.h>
+#include <sys/_mutex.h>
+
+#include <vm/_vm_radix.h>
+#endif /* __FreeBSD__ */
+
 typedef u_int64_t pd_entry_t;
 typedef u_int64_t pt_entry_t;
 typedef u_int64_t pdp_entry_t;
 typedef u_int64_t pml4_entry_t;
 
+/*
+ * Address of current address space page table maps and directories.
+ */
+#ifdef _KERNEL
+#define	addr_PTmap	(KVADDR(PML4PML4I, 0, 0, 0))
+#define	addr_PDmap	(KVADDR(PML4PML4I, PML4PML4I, 0, 0))
+#define	addr_PDPmap	(KVADDR(PML4PML4I, PML4PML4I, PML4PML4I, 0))
+#define	addr_PML4map	(KVADDR(PML4PML4I, PML4PML4I, PML4PML4I, PML4PML4I))
+#define	addr_PML4pml4e	(addr_PML4map + (PML4PML4I * sizeof(pml4_entry_t)))
+#define	PTmap		((pt_entry_t *)(addr_PTmap))
+#define	PDmap		((pd_entry_t *)(addr_PDmap))
+#define	PDPmap		((pd_entry_t *)(addr_PDPmap))
+#define	PML4map		((pd_entry_t *)(addr_PML4map))
+#define	PML4pml4e	((pd_entry_t *)(addr_PML4pml4e))
+
+extern int nkpt;		/* Initial number of kernel page tables */
+extern u_int64_t KPDPphys;	/* physical address of kernel level 3 */
+extern u_int64_t KPML4phys;	/* physical address of kernel level 4 */
+
+/*
+ * virtual address to page table entry and
+ * to physical address.
+ * Note: these work recursively, thus vtopte of a pte will give
+ * the corresponding pde that in turn maps it.
+ */
+pt_entry_t *vtopte(vm_offset_t);
 #define	vtophys(va)	pmap_kextract(((vm_offset_t) (va)))
-vm_paddr_t pmap_kextract(vm_offset_t va);
+#ifndef __FreeBSD__
+extern vm_paddr_t pmap_kextract(vm_offset_t);
+#endif
+
+#define	pte_load_store(ptep, pte)	atomic_swap_long(ptep, pte)
+#define	pte_load_clear(ptep)		atomic_swap_long(ptep, 0)
+#define	pte_store(ptep, pte) do { \
+	*(u_long *)(ptep) = (u_long)(pte); \
+} while (0)
+#define	pte_clear(ptep)			pte_store(ptep, 0)
+
+#define	pde_store(pdep, pde)		pte_store(pdep, pde)
+
+extern pt_entry_t pg_nx;
+
+#endif /* _KERNEL */
+
+#ifdef __FreeBSD__
+/*
+ * Pmap stuff
+ */
+struct	pv_entry;
+struct	pv_chunk;
+
+/*
+ * Locks
+ * (p) PV list lock
+ */
+struct md_page {
+	TAILQ_HEAD(, pv_entry)	pv_list;  /* (p) */
+	int			pv_gen;   /* (p) */
+	int			pat_mode;
+};
+#endif /* __FreeBSD__ */
+
+enum pmap_type {
+	PT_X86,			/* regular x86 page tables */
+	PT_EPT,			/* Intel's nested page tables */
+	PT_RVI,			/* AMD's nested page tables */
+};
+
+#ifdef __FreeBSD__
+struct pmap_pcids {
+	uint32_t	pm_pcid;
+	uint32_t	pm_gen;
+};
+
+/*
+ * The kernel virtual address (KVA) of the level 4 page table page is always
+ * within the direct map (DMAP) region.
+ */
+struct pmap {
+	struct mtx		pm_mtx;
+	pml4_entry_t		*pm_pml4;	/* KVA of level 4 page table */
+	uint64_t		pm_cr3;
+	TAILQ_HEAD(,pv_chunk)	pm_pvchunk;	/* list of mappings in pmap */
+	cpuset_t		pm_active;	/* active on cpus */
+	enum pmap_type		pm_type;	/* regular or nested tables */
+	struct pmap_statistics	pm_stats;	/* pmap statistics */
+	struct vm_radix		pm_root;	/* spare page table pages */
+	long			pm_eptgen;	/* EPT pmap generation id */
+	int			pm_flags;
+	struct pmap_pcids	pm_pcids[MAXCPU];
+};
+#endif /* __FreeBSD__ */
+
+/* flags */
+#define	PMAP_NESTED_IPIMASK	0xff
+#define	PMAP_PDE_SUPERPAGE	(1 << 8)	/* supports 2MB superpages */
+#define	PMAP_EMULATE_AD_BITS	(1 << 9)	/* needs A/D bits emulation */
+#define	PMAP_SUPPORTS_EXEC_ONLY	(1 << 10)	/* execute only mappings ok */
+
+typedef struct pmap	*pmap_t;
+
+#ifdef _KERNEL
+extern struct pmap	kernel_pmap_store;
+#define kernel_pmap	(&kernel_pmap_store)
+
+#define	PMAP_LOCK(pmap)		mtx_lock(&(pmap)->pm_mtx)
+#define	PMAP_LOCK_ASSERT(pmap, type) \
+				mtx_assert(&(pmap)->pm_mtx, (type))
+#define	PMAP_LOCK_DESTROY(pmap)	mtx_destroy(&(pmap)->pm_mtx)
+#define	PMAP_LOCK_INIT(pmap)	mtx_init(&(pmap)->pm_mtx, "pmap", \
+				    NULL, MTX_DEF | MTX_DUPOK)
+#define	PMAP_LOCKED(pmap)	mtx_owned(&(pmap)->pm_mtx)
+#define	PMAP_MTX(pmap)		(&(pmap)->pm_mtx)
+#define	PMAP_TRYLOCK(pmap)	mtx_trylock(&(pmap)->pm_mtx)
+#define	PMAP_UNLOCK(pmap)	mtx_unlock(&(pmap)->pm_mtx)
+
+int	pmap_pinit_type(pmap_t pmap, enum pmap_type pm_type, int flags);
+int	pmap_emulate_accessed_dirty(pmap_t pmap, vm_offset_t va, int ftype);
+#endif
+
+#ifdef	__FreeBSD__
+/*
+ * For each vm_page_t, there is a list of all currently valid virtual
+ * mappings of that page.  An entry is a pv_entry_t, the list is pv_list.
+ */
+typedef struct pv_entry {
+	vm_offset_t	pv_va;		/* virtual address for mapping */
+	TAILQ_ENTRY(pv_entry)	pv_next;
+} *pv_entry_t;
+
+/*
+ * pv_entries are allocated in chunks per-process.  This avoids the
+ * need to track per-pmap assignments.
+ */
+#define	_NPCM	3
+#define	_NPCPV	168
+struct pv_chunk {
+	pmap_t			pc_pmap;
+	TAILQ_ENTRY(pv_chunk)	pc_list;
+	uint64_t		pc_map[_NPCM];	/* bitmap; 1 = free */
+	TAILQ_ENTRY(pv_chunk)	pc_lru;
+	struct pv_entry		pc_pventry[_NPCPV];
+};
+
+#ifdef	_KERNEL
+
+extern caddr_t	CADDR1;
+extern pt_entry_t *CMAP1;
+extern vm_paddr_t phys_avail[];
+extern vm_paddr_t dump_avail[];
+extern vm_offset_t virtual_avail;
+extern vm_offset_t virtual_end;
+extern vm_paddr_t dmaplimit;
+extern int pmap_pcid_enabled;
+extern int invpcid_works;
+
+#define	pmap_page_get_memattr(m)	((vm_memattr_t)(m)->md.pat_mode)
+#define	pmap_page_is_write_mapped(m)	(((m)->aflags & PGA_WRITEABLE) != 0)
+#define	pmap_unmapbios(va, sz)	pmap_unmapdev((va), (sz))
+
+struct thread;
+
+void	pmap_activate_sw(struct thread *);
+void	pmap_bootstrap(vm_paddr_t *);
+int	pmap_cache_bits(pmap_t pmap, int mode, boolean_t is_pde);
+int	pmap_change_attr(vm_offset_t, vm_size_t, int);
+void	pmap_demote_DMAP(vm_paddr_t base, vm_size_t len, boolean_t invalidate);
+void	pmap_init_pat(void);
+void	pmap_kenter(vm_offset_t va, vm_paddr_t pa);
+void	*pmap_kenter_temporary(vm_paddr_t pa, int i);
+vm_paddr_t pmap_kextract(vm_offset_t);
+void	pmap_kremove(vm_offset_t);
+void	*pmap_mapbios(vm_paddr_t, vm_size_t);
+void	*pmap_mapdev(vm_paddr_t, vm_size_t);
+void	*pmap_mapdev_attr(vm_paddr_t, vm_size_t, int);
+boolean_t pmap_page_is_mapped(vm_page_t m);
+void	pmap_page_set_memattr(vm_page_t m, vm_memattr_t ma);
+void	pmap_pinit_pml4(vm_page_t);
+void	pmap_unmapdev(vm_offset_t, vm_size_t);
+void	pmap_invalidate_page(pmap_t, vm_offset_t);
+void	pmap_invalidate_range(pmap_t, vm_offset_t, vm_offset_t);
+void	pmap_invalidate_all(pmap_t);
+void	pmap_invalidate_cache(void);
+void	pmap_invalidate_cache_pages(vm_page_t *pages, int count);
+void	pmap_invalidate_cache_range(vm_offset_t sva, vm_offset_t eva,
+	    boolean_t force);
+void	pmap_get_mapping(pmap_t pmap, vm_offset_t va, uint64_t *ptr, int *num);
+boolean_t pmap_map_io_transient(vm_page_t *, vm_offset_t *, int, boolean_t);
+void	pmap_unmap_io_transient(vm_page_t *, vm_offset_t *, int, boolean_t);
+#endif /* _KERNEL */
+
+/* Return various clipped indexes for a given VA */
+static __inline vm_pindex_t
+pmap_pte_index(vm_offset_t va)
+{
+
+	return ((va >> PAGE_SHIFT) & ((1ul << NPTEPGSHIFT) - 1));
+}
+
+static __inline vm_pindex_t
+pmap_pde_index(vm_offset_t va)
+{
+
+	return ((va >> PDRSHIFT) & ((1ul << NPDEPGSHIFT) - 1));
+}
+
+static __inline vm_pindex_t
+pmap_pdpe_index(vm_offset_t va)
+{
+
+	return ((va >> PDPSHIFT) & ((1ul << NPDPEPGSHIFT) - 1));
+}
+
+static __inline vm_pindex_t
+pmap_pml4e_index(vm_offset_t va)
+{
+
+	return ((va >> PML4SHIFT) & ((1ul << NPML4EPGSHIFT) - 1));
+}
+
+#endif /* __FreeBSD__ */
+#endif /* !LOCORE */
 
-#endif	/* _COMPAT_FREEBSD_AMD64_MACHINE_PMAP_H_ */
+#endif /* !_COMPAT_FREEBSD_AMD64_MACHINE_PMAP_H_ */
diff --git a/usr/src/compat/freebsd/amd64/machine/smp.h b/usr/src/compat/freebsd/amd64/machine/smp.h
index ef719b9684..4c4007bf9d 100644
--- a/usr/src/compat/freebsd/amd64/machine/smp.h
+++ b/usr/src/compat/freebsd/amd64/machine/smp.h
@@ -16,4 +16,23 @@
 #ifndef _COMPAT_FREEBSD_AMD64_MACHINE_SMP_H_
 #define	_COMPAT_FREEBSD_AMD64_MACHINE_SMP_H_
 
+#ifdef _KERNEL
+
+/*
+ * APIC-related definitions would normally be stored in x86/include/apicvar.h,
+ * accessed here via x86/include/x86_smp.h.  Until it becomes necessary to
+ * implment that whole chain of includes, those definitions are short-circuited
+ * into this file.
+ */
+
+#define	IDTVEC(name)	idtvec_ ## name
+
+extern int idtvec_justreturn;
+
+extern int lapic_ipi_alloc(int *);
+extern void lapic_ipi_free(int vec);
+
+
+#endif /* _KERNEL */
+
 #endif	/* _COMPAT_FREEBSD_AMD64_MACHINE_SMP_H_ */
diff --git a/usr/src/compat/freebsd/amd64/machine/specialreg.h b/usr/src/compat/freebsd/amd64/machine/specialreg.h
new file mode 100644
index 0000000000..12bcbfd0a8
--- /dev/null
+++ b/usr/src/compat/freebsd/amd64/machine/specialreg.h
@@ -0,0 +1,43 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2017 Joyent, Inc.
+ */
+
+#ifndef _COMPAT_FREEBSD_AMD64_MACHINE_SPECIALREG_H_
+#define	_COMPAT_FREEBSD_AMD64_MACHINE_SPECIALREG_H_
+
+#ifdef _SYS_X86_ARCHEXT_H
+/* Our x86_archext conflicts with BSD header for the XFEATURE_ defines */
+#undef	XFEATURE_AVX
+#undef	XFEATURE_MPX
+#undef	XFEATURE_AVX512
+#endif
+
+#ifdef _SYS_CONTROLREGS_H
+/* Our CR4 defines conflict with BSD header */
+#undef	CR4_VME
+#undef	CR4_PVI
+#undef	CR4_TSD
+#undef	CR4_DE
+#undef	CR4_PSE
+#undef	CR4_PAE
+#undef	CR4_MCE
+#undef	CR4_PGE
+#undef	CR4_PCE
+#undef	CR4_VMXE
+#undef	CR4_SMEP
+#undef	CR4_PCIDE
+#endif /* _SYS_CONTROLREGS_H */
+
+#include <x86/specialreg.h>
+#endif /* _COMPAT_FREEBSD_AMD64_MACHINE_SPECIALREG_H_ */
diff --git a/usr/src/compat/freebsd/amd64/machine/vmm.h b/usr/src/compat/freebsd/amd64/machine/vmm.h
index 79c3ec959e..1c54c0830d 100644
--- a/usr/src/compat/freebsd/amd64/machine/vmm.h
+++ b/usr/src/compat/freebsd/amd64/machine/vmm.h
@@ -11,11 +11,14 @@
 
 /*
  * Copyright 2013 Pluribus Networks Inc.
+ * Copyright 2017 Joyent, Inc.
  */
 
 #ifndef _COMPAT_FREEBSD_AMD64_MACHINE_VMM_H_
 #define	_COMPAT_FREEBSD_AMD64_MACHINE_VMM_H_
 
+#include <sys/_cpuset.h>
+
 #include <sys/vmm.h>
 
 #endif	/* _COMPAT_FREEBSD_AMD64_MACHINE_VMM_H_ */
diff --git a/usr/src/compat/freebsd/amd64/machine/vmparam.h b/usr/src/compat/freebsd/amd64/machine/vmparam.h
index c80c2af545..167aa1f459 100644
--- a/usr/src/compat/freebsd/amd64/machine/vmparam.h
+++ b/usr/src/compat/freebsd/amd64/machine/vmparam.h
@@ -11,9 +11,23 @@
 
 /*
  * Copyright 2013 Pluribus Networks Inc.
+ * Copyright 2017 Joyent, Inc.
  */
 
 #ifndef _COMPAT_FREEBSD_AMD64_MACHINE_VMPARAM_H_
 #define	_COMPAT_FREEBSD_AMD64_MACHINE_VMPARAM_H_
 
+extern caddr_t kpm_vbase;
+extern size_t kpm_size;
+
+#define	PHYS_TO_DMAP(x)	({ 			\
+	ASSERT((uintptr_t)(x) < kpm_size);	\
+	(uintptr_t)(x) | (uintptr_t)kpm_vbase; })
+
+#define	DMAP_TO_PHYS(x)	({				\
+	ASSERT((uintptr_t)(x) >= (uintptr_t)kpm_vbase);		\
+	ASSERT((uintptr_t)(x) < ((uintptr_t)kpm_vbase + kpm_size));	\
+	(uintptr_t)(x) & ~(uintptr_t)kpm_vbase; })	\
+
+
 #endif	/* _COMPAT_FREEBSD_AMD64_MACHINE_VMPARAM_H_ */
diff --git a/usr/src/compat/freebsd/libutil.h b/usr/src/compat/freebsd/libutil.h
index e22ffc0551..f899d4425e 100644
--- a/usr/src/compat/freebsd/libutil.h
+++ b/usr/src/compat/freebsd/libutil.h
@@ -17,5 +17,19 @@
 #define	_COMPAT_FREEBSD_LIBUTIL_H_
 
 int	expand_number(const char *_buf, uint64_t *_num);
+int	humanize_number(char *_buf, size_t _len, int64_t _number,
+    const char *_suffix, int _scale, int _flags);
+
+/* Values for humanize_number(3)'s flags parameter. */
+#define HN_DECIMAL      0x01
+#define HN_NOSPACE      0x02
+#define HN_B            0x04
+#define HN_DIVISOR_1000     0x08
+#define HN_IEC_PREFIXES     0x10
+
+/* Values for humanize_number(3)'s scale parameter. */
+#define HN_GETSCALE     0x10
+#define HN_AUTOSCALE        0x20
+
 
 #endif	/* _COMPAT_FREEBSD_LIBUTIL_H_ */
diff --git a/usr/src/compat/freebsd/sys/_cpuset.h b/usr/src/compat/freebsd/sys/_cpuset.h
new file mode 100644
index 0000000000..286d26fc00
--- /dev/null
+++ b/usr/src/compat/freebsd/sys/_cpuset.h
@@ -0,0 +1,33 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2017 Joyent, Inc.
+ */
+
+#ifndef _COMPAT_FREEBSD_SYS__CPUSET_H_
+#define	_COMPAT_FREEBSD_SYS__CPUSET_H_
+
+#ifdef _KERNEL
+/*
+ * The sys/_cpuset.h header is used to communicate the layout of cpuset_t while
+ * sys/cpuset.h contains the manipulation routines.
+ *
+ * The explicit guard definition below is necessary as other contrib headers
+ * change their behavior based on its presence.
+ */
+#define	_SYS__CPUSET_H_
+
+#include <sys/cpuvar.h>
+
+#endif /* _KERNEL */
+
+#endif	/* _COMPAT_FREEBSD_SYS__CPUSET_H_ */
diff --git a/usr/src/compat/freebsd/sys/clock.h b/usr/src/compat/freebsd/sys/clock.h
new file mode 100644
index 0000000000..ebf7f171a3
--- /dev/null
+++ b/usr/src/compat/freebsd/sys/clock.h
@@ -0,0 +1,110 @@
+/*-
+ * Copyright (c) 1996 The NetBSD Foundation, Inc.
+ * All rights reserved.
+ *
+ * This code is derived from software contributed to The NetBSD Foundation
+ * by Gordon W. Ross
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
+ * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
+ * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
+ * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
+ * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
+ * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
+ * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+ * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
+ * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
+ * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+ * POSSIBILITY OF SUCH DAMAGE.
+ *
+ *	$NetBSD: clock_subr.h,v 1.7 2000/10/03 13:41:07 tsutsui Exp $
+ *
+ *
+ * This file is the central clearing-house for calendrical issues.
+ *
+ * In general the kernel does not know about minutes, hours, days, timezones,
+ * daylight savings time, leap-years and such.  All that is theoretically a
+ * matter for userland only.
+ *
+ * Parts of kernel code does however care: badly designed filesystems store
+ * timestamps in local time and RTC chips sometimes track time in a local
+ * timezone instead of UTC and so on.
+ *
+ * All that code should go here for service.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _COMPAT_FREEBSD_SYS_CLOCK_H_
+#define	_COMPAT_FREEBSD_SYS_CLOCK_H_
+
+#include_next <sys/clock.h>
+
+#ifdef _KERNEL		/* No user serviceable parts */
+
+#ifdef __FreeBSD__
+/*
+ * Timezone info from settimeofday(2), usually not used
+ */
+extern int tz_minuteswest;
+extern int tz_dsttime;
+extern struct mtx resettodr_lock;
+
+int utc_offset(void);
+#endif /* __FreeBSD__ */
+
+/*
+ * Structure to hold the values typically reported by time-of-day clocks.
+ * This can be passed to the generic conversion functions to be converted
+ * to a struct timespec.
+ */
+struct clocktime {
+	int	year;			/* year (4 digit year) */
+	int	mon;			/* month (1 - 12) */
+	int	day;			/* day (1 - 31) */
+	int	hour;			/* hour (0 - 23) */
+	int	min;			/* minute (0 - 59) */
+	int	sec;			/* second (0 - 59) */
+	int	dow;			/* day of week (0 - 6; 0 = Sunday) */
+	long	nsec;			/* nano seconds */
+};
+
+int clock_ct_to_ts(struct clocktime *, struct timespec *);
+void clock_ts_to_ct(struct timespec *, struct clocktime *);
+#ifdef __FreeBSD__
+void clock_register(device_t, long);
+#endif
+
+#ifndef __FreeBSD__
+extern u_char const bin2bcd_data[];
+#define	bin2bcd(x)	(bin2bcd_data[bin])
+#endif
+
+/*
+ * BCD to decimal and decimal to BCD.
+ */
+#define	FROMBCD(x)	bcd2bin(x)
+#define	TOBCD(x)	bin2bcd(x)
+
+/* Some handy constants. */
+#define SECDAY		(24 * 60 * 60)
+#define SECYR		(SECDAY * 365)
+
+/* Traditional POSIX base year */
+#define	POSIX_BASE_YEAR	1970
+
+void timespec2fattime(struct timespec *tsp, int utc, u_int16_t *ddp, u_int16_t *dtp, u_int8_t *dhp);
+void fattime2timespec(unsigned dd, unsigned dt, unsigned dh, int utc, struct timespec *tsp);
+
+#endif /* _KERNEL */
+
+#endif	/* _COMPAT_FREEBSD_SYS_CLOCK_H_ */
diff --git a/usr/src/compat/freebsd/sys/cpuset.h b/usr/src/compat/freebsd/sys/cpuset.h
index bfaaaa6b6a..4328ebcc46 100644
--- a/usr/src/compat/freebsd/sys/cpuset.h
+++ b/usr/src/compat/freebsd/sys/cpuset.h
@@ -20,26 +20,65 @@
 #define	NOCPU			-1
 
 #ifdef	_KERNEL
-#define	CPU_SET(cpu, set)		CPUSET_ADD(*(set), cpu)
-#define	CPU_SETOF(cpu, set)		CPUSET_ONLY(*(set), cpu)
-#define	CPU_ZERO(set)			CPUSET_ZERO(*(set))
-#define	CPU_CLR(cpu, set)		CPUSET_DEL(*(set), cpu)
+
+#include <sys/_cpuset.h>
+
+#define	CPU_SET(cpu, set)		cpuset_add((set), (cpu))
+#define	CPU_SETOF(cpu, set)		cpuset_only((set), (cpu))
+#define	CPU_ZERO(set)			cpuset_zero((cpuset_t *)(set))
+#define	CPU_CLR(cpu, set)		cpuset_del((set), (cpu))
 #define	CPU_FFS(set)			cpusetobj_ffs(set)
-#define	CPU_ISSET(cpu, set)		CPU_IN_SET(*(set), cpu)
-#define	CPU_CMP(set1, set2)		CPUSET_ISEQUAL(*(set1), *(set2))
-#define	CPU_SET_ATOMIC(cpu, set)	CPUSET_ATOMIC_ADD(*(set), cpu)
+#define	CPU_ISSET(cpu, set)		cpu_in_set((cpuset_t *)(set), (cpu))
+#define	CPU_AND(dst, src)		cpuset_and(			\
+						(cpuset_t *)(dst),	\
+						(cpuset_t *)(src))
+#define	CPU_CMP(set1, set2)		(cpuset_isequal(		\
+						(cpuset_t *)(set1),	\
+						(cpuset_t *)(set2)) == 0)
+#define	CPU_SET_ATOMIC(cpu, set)	cpuset_atomic_add(		\
+						(cpuset_t *)(set),	\
+						(cpu))
+#define	CPU_CLR_ATOMIC(cpu, set)	cpuset_atomic_del(		\
+						(cpuset_t *)(set),	\
+						(cpu))
+
+/* XXXJOY: The _ACQ variants appear to imply a membar too. Is that an issue? */
+#define	CPU_SET_ATOMIC_ACQ(cpu, set)	cpuset_atomic_add((set), (cpu))
 
-#include <sys/cpuvar.h>
 
 int	cpusetobj_ffs(const cpuset_t *set);
+
 #else
+
+#include <sys/bitmap.h>
 #include <machine/atomic.h>
 
-typedef int cpuset_t;
+/* For now, assume NCPU of 256 */
+#define	CPU_SETSIZE			(256)
+
+typedef struct {
+	ulong_t _bits[BT_BITOUL(CPU_SETSIZE)];
+} cpuset_t;
+
+static __inline int cpuset_empty(const cpuset_t *set)
+{
+	uint_t i;
+
+	for (i = 0; i < BT_BITOUL(CPU_SETSIZE); i++) {
+		if (set->_bits[i] != 0)
+			return (0);
+	}
+	return (1);
+}
+
 
-#define	CPUSET(cpu)			(1UL << (cpu))
+#define	CPU_ISSET(cpu, setp)		BT_TEST((setp)->_bits, cpu)
+#define	CPU_EMPTY(setp)			cpuset_empty((setp))
+#define	CPU_SET_ATOMIC(cpu, setp)	\
+	atomic_set_long(&(BT_WIM((setp)->_bits, cpu)), BT_BIW(cpu))
+#define	CPU_CLR_ATOMIC(cpu, setp)	\
+	atomic_clear_long(&(BT_WIM((setp)->_bits, cpu)), BT_BIW(cpu))
 
-#define	CPU_SET_ATOMIC(cpu, set)	atomic_set_int((u_int *)(set), CPUSET(cpu))
 #endif
 
 #endif	/* _COMPAT_FREEBSD_SYS_CPUSET_H_ */
diff --git a/usr/src/compat/freebsd/sys/kernel.h b/usr/src/compat/freebsd/sys/kernel.h
index b1c07674e4..1de3418efa 100644
--- a/usr/src/compat/freebsd/sys/kernel.h
+++ b/usr/src/compat/freebsd/sys/kernel.h
@@ -11,12 +11,14 @@
 
 /*
  * Copyright 2013 Pluribus Networks Inc.
+ * Copyright 2017 Joyent, Inc.
  */
 
 #ifndef _COMPAT_FREEBSD_SYS_KERNEL_H_
 #define	_COMPAT_FREEBSD_SYS_KERNEL_H_
 
 #define	SYSINIT(uniquifier, subsystem, order, func, ident)
+#define	TUNABLE_INT_FETCH(path, var)
 
 #include <sys/linker_set.h>
 
diff --git a/usr/src/compat/freebsd/sys/lock.h b/usr/src/compat/freebsd/sys/lock.h
new file mode 100644
index 0000000000..fd6021a87e
--- /dev/null
+++ b/usr/src/compat/freebsd/sys/lock.h
@@ -0,0 +1,23 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2017 Joyent, Inc.
+ */
+
+#ifndef _COMPAT_FREEBSD_SYS_LOCK_H_
+#define	_COMPAT_FREEBSD_SYS_LOCK_H_
+
+#include_next <sys/lock.h>
+
+#define	WITNESS_WARN(...)
+
+#endif	/* _COMPAT_FREEBSD_SYS_LOCK_H_ */
diff --git a/usr/src/compat/freebsd/sys/malloc.h b/usr/src/compat/freebsd/sys/malloc.h
index 579df44533..341d57b807 100644
--- a/usr/src/compat/freebsd/sys/malloc.h
+++ b/usr/src/compat/freebsd/sys/malloc.h
@@ -39,6 +39,11 @@ struct malloc_type {
 void	free(void *addr, struct malloc_type *type);
 void	*malloc(unsigned long size, struct malloc_type *type, int flags);
 void	*old_malloc(unsigned long size, struct malloc_type *type , int flags);
+void	*contigmalloc(unsigned long, struct malloc_type *, int, vm_paddr_t,
+    vm_paddr_t, unsigned long, vm_paddr_t);
+void	contigfree(void *, unsigned long, struct malloc_type *);
+
+
 #endif	/* _KERNEL */
 
 #endif	/* _COMPAT_FREEBSD_SYS_MALLOC_H_ */
diff --git a/usr/src/compat/freebsd/sys/mutex.h b/usr/src/compat/freebsd/sys/mutex.h
index b99884b652..57ebfee901 100644
--- a/usr/src/compat/freebsd/sys/mutex.h
+++ b/usr/src/compat/freebsd/sys/mutex.h
@@ -28,9 +28,6 @@ struct mtx;
 void mtx_init(struct mtx *, char *name, const char *type_name, int opts);
 void mtx_destroy(struct mtx *);
 
-int mtx_sleep(void *chan, struct mtx *mtx, int priority, const char *wmesg,
-    int timo);
-
 #endif	/* KERNEL */
 #include_next <sys/mutex.h>
 #ifdef	_KERNEL
diff --git a/usr/src/compat/freebsd/sys/param.h b/usr/src/compat/freebsd/sys/param.h
index f09e9183f6..eade1caf5b 100644
--- a/usr/src/compat/freebsd/sys/param.h
+++ b/usr/src/compat/freebsd/sys/param.h
@@ -11,6 +11,7 @@
 
 /*
  * Copyright 2014 Pluribus Networks Inc.
+ * Copyright 2017 Joyent, Inc.
  */
 
 #ifndef _COMPAT_FREEBSD_SYS_PARAM_H_
@@ -20,6 +21,7 @@
 #define	MAXCOMLEN	16
 #endif
 #define	MAXHOSTNAMELEN	256
+#define	SPECNAMELEN	63
 
 #ifdef	_KERNEL
 #include <sys/time.h>
@@ -38,11 +40,15 @@
 #define	rounddown(x,y)	(((x)/(y))*(y))
 #define	roundup(x, y)	((((x)+((y)-1))/(y))*(y))  /* to any y */
 #define	roundup2(x,y)	(((x)+((y)-1))&(~((y)-1))) /* if y is powers of two */
+#define	powerof2(x)	((((x)-1)&(x))==0)
 
 /* Macros for min/max. */
 #define	MIN(a,b) (((a)<(b))?(a):(b))
 #define	MAX(a,b) (((a)>(b))?(a):(b))
 
+#define	trunc_page(x)	((unsigned long)(x) & ~(PAGE_MASK))
+#define	ptoa(x)		((unsigned long)(x) << PAGE_SHIFT)
+
 #include_next <sys/param.h>
 
 #endif	/* _COMPAT_FREEBSD_SYS_PARAM_H_ */
diff --git a/usr/src/compat/freebsd/sys/sglist.h b/usr/src/compat/freebsd/sys/sglist.h
new file mode 100644
index 0000000000..519c67915f
--- /dev/null
+++ b/usr/src/compat/freebsd/sys/sglist.h
@@ -0,0 +1,29 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2017 Joyent, Inc.
+ */
+
+#ifndef _COMPAT_FREEBSD_SYS_SGLIST_H_
+#define	_COMPAT_FREEBSD_SYS_SGLIST_H_
+
+#ifdef _KERNEL
+
+struct sglist;
+
+struct sglist *sglist_alloc(int, int);
+void sglist_free(struct sglist *);
+int sglist_append_phys(struct sglist *, vm_paddr_t, size_t);
+
+#endif /* _KERNEL */
+
+#endif	/* _COMPAT_FREEBSD_SYS_SGLIST_H_ */
diff --git a/usr/src/compat/freebsd/sys/smp.h b/usr/src/compat/freebsd/sys/smp.h
index 46183e8677..8a91c6c8d7 100644
--- a/usr/src/compat/freebsd/sys/smp.h
+++ b/usr/src/compat/freebsd/sys/smp.h
@@ -11,6 +11,7 @@
 
 /*
  * Copyright 2014 Pluribus Networks Inc.
+ * Copyright 2017 Joyent, Inc.
  */
 
 #ifndef _COMPAT_FREEBSD_SYS_SMP_H_
@@ -23,6 +24,8 @@ void	smp_rendezvous(void (*)(void *),
 		       void (*)(void *),
 		       void *arg);
 
+#define	IPI_AST	0
+
 void	ipi_cpu(int cpu, u_int ipi);
 
 #endif	/* _COMPAT_FREEBSD_SYS_SMP_H_ */
diff --git a/usr/src/compat/freebsd/sys/systm.h b/usr/src/compat/freebsd/sys/systm.h
index 613e97371c..43fa16d450 100644
--- a/usr/src/compat/freebsd/sys/systm.h
+++ b/usr/src/compat/freebsd/sys/systm.h
@@ -31,11 +31,6 @@ struct mtx;
 void	critical_enter(void);
 void	critical_exit(void);
 
-int	msleep_spin(void *chan, struct mtx *mutex, const char *wmesg,
-    int ticks);
-void	wakeup(void *chan);
-void	wakeup_one(void *chan);
-
 struct unrhdr *new_unrhdr(int low, int high, struct mtx *mutex);
 void delete_unrhdr(struct unrhdr *uh);
 int alloc_unr(struct unrhdr *uh);
diff --git a/usr/src/compat/freebsd/sys/time.h b/usr/src/compat/freebsd/sys/time.h
index f8f9da5cdf..b63fd7fa4a 100644
--- a/usr/src/compat/freebsd/sys/time.h
+++ b/usr/src/compat/freebsd/sys/time.h
@@ -50,7 +50,13 @@ binuptime(struct bintime *bt)
 	    ((a)->frac cmp (b)->frac) :					\
 	    ((a)->sec cmp (b)->sec))
 
-#define	SBT_1US	(1000)
+#define SBT_1S  ((sbintime_t)1 << 32)
+#define SBT_1M  (SBT_1S * 60)
+#define SBT_1MS (SBT_1S / 1000)
+#define SBT_1US (SBT_1S / 1000000)
+#define SBT_1NS (SBT_1S / 1000000000)
+#define SBT_MAX 0x7fffffffffffffffLL
+
 
 static __inline void
 bintime_add(struct bintime *bt, const struct bintime *bt2)
@@ -91,14 +97,18 @@ bintime_mul(struct bintime *bt, u_int x)
 static __inline sbintime_t
 bttosbt(const struct bintime bt)
 {
-	return ((bt.sec * 1000000000) +
-	    (((uint64_t)1000000000 * (uint32_t)(bt.frac >> 32)) >> 32));
+	return (((sbintime_t)bt.sec << 32) + (bt.frac >> 32));
 }
 
 static __inline sbintime_t
 sbinuptime(void)
 {
-	return (gethrtime());
+	hrtime_t hrt = gethrtime();
+	uint64_t sec = hrt / NANOSEC;
+	uint64_t nsec = hrt % NANOSEC;
+
+	return (((sbintime_t)sec << 32) +
+	    (nsec * (((uint64_t)1 << 63) / 500000000) >> 32));
 }
 
 #endif	/* _COMPAT_FREEBSD_SYS_TIME_H_ */
diff --git a/usr/src/compat/freebsd/sys/types.h b/usr/src/compat/freebsd/sys/types.h
index 6fc8179f2e..922dd83629 100644
--- a/usr/src/compat/freebsd/sys/types.h
+++ b/usr/src/compat/freebsd/sys/types.h
@@ -53,6 +53,16 @@ typedef __vm_ooffset_t	vm_ooffset_t;
 typedef __vm_paddr_t	vm_paddr_t;
 #endif
 
+#ifndef	__VM_PINDEX_T_DEFINED
+#define	__VM_PINDEX_T_DEFINED
+typedef __uint64_t	vm_pindex_t;
+#endif
+
+#ifndef	__VM_SIZE_T_DEFINED
+#define	__VM_SIZE_T_DEFINED
+typedef __vm_size_t	vm_size_t;
+#endif
+
 #ifndef	__VM_MEMATTR_T_DEFINED
 #define	__VM_MEMATTR_T_DEFINED
 typedef char		vm_memattr_t;
diff --git a/usr/src/compat/freebsd/vm/vm.h b/usr/src/compat/freebsd/vm/vm.h
index 7da22099b6..f5bb7b6eb8 100644
--- a/usr/src/compat/freebsd/vm/vm.h
+++ b/usr/src/compat/freebsd/vm/vm.h
@@ -11,23 +11,48 @@
 
 /*
  * Copyright 2014 Pluribus Networks Inc.
+ * Copyright 2017 Joyent, Inc.
  */
 
 #ifndef _FREEBSD_VM_VM_H_
 #define	_FREEBSD_VM_VM_H_
 
 #include <machine/vm.h>
+#include <sys/mman.h>
 
 typedef u_char vm_prot_t;
 
+/*
+ * Even though the FreeBSD VM_PROT defines happen to match illumos, this
+ * references the native values directly so there's no risk of breakage.
+ */
 #define	VM_PROT_NONE		((vm_prot_t) 0x00)
-#define	VM_PROT_READ		((vm_prot_t) 0x01)
-#define	VM_PROT_WRITE		((vm_prot_t) 0x02)
-#define	VM_PROT_EXECUTE		((vm_prot_t) 0x04)
+#define	VM_PROT_READ		((vm_prot_t) PROT_READ)
+#define	VM_PROT_WRITE		((vm_prot_t) PROT_WRITE)
+#define	VM_PROT_EXECUTE		((vm_prot_t) PROT_EXEC)
 
 #define	VM_PROT_ALL		(VM_PROT_READ|VM_PROT_WRITE|VM_PROT_EXECUTE)
 #define	VM_PROT_RW		(VM_PROT_READ|VM_PROT_WRITE)
 
+struct vm_page;
+typedef struct vm_page *vm_page_t;
+
+enum obj_type { OBJT_DEFAULT, OBJT_SWAP, OBJT_VNODE, OBJT_DEVICE, OBJT_PHYS,
+    OBJT_DEAD, OBJT_SG, OBJT_MGTDEVICE };
+typedef u_char objtype_t;
+
+union vm_map_object;
+typedef union vm_map_object vm_map_object_t;
+
+struct vm_map_entry;
+typedef struct vm_map_entry *vm_map_entry_t;
+
+struct vm_map;
+typedef struct vm_map *vm_map_t;
+
+struct vm_object;
+typedef struct vm_object *vm_object_t;
+
 /*
  * <sys/promif.h> contains a troublesome preprocessor define for BYTE.
  * Do this ugly workaround to avoid it.
diff --git a/usr/src/compat/freebsd/vm/vm_param.h b/usr/src/compat/freebsd/vm/vm_param.h
new file mode 100644
index 0000000000..8affac9d7e
--- /dev/null
+++ b/usr/src/compat/freebsd/vm/vm_param.h
@@ -0,0 +1,18 @@
+#ifndef _COMPAT_FREEBSD_VM_VM_PARAM_H_
+#define	_COMPAT_FREEBSD_VM_VM_PARAM_H_
+
+#include <machine/vmparam.h>
+
+#define	KERN_SUCCESS		0
+
+/*
+ * The VM_MAXUSER_ADDRESS is used to determine the upper limit size limit of a
+ * vmspace, their 'struct as' equivalent.  The compat value is sized well below
+ * our native userlimit, even halving the available space below the VA hole.
+ * This is to avoid Intel EPT limits and leave room available in the usabe VA
+ * range for other mmap tricks.
+ */
+#define	VM_MAXUSER_ADDRESS	0x00003ffffffffffful
+
+
+#endif	/* _COMPAT_FREEBSD_VM_VM_PARAM_H_ */
diff --git a/usr/src/compat/freebsd/x86/_types.h b/usr/src/compat/freebsd/x86/_types.h
index a07fc017ad..8bbae549d8 100644
--- a/usr/src/compat/freebsd/x86/_types.h
+++ b/usr/src/compat/freebsd/x86/_types.h
@@ -41,9 +41,11 @@ typedef __int64_t	__register_t;
 typedef __uint64_t	__vm_offset_t;
 typedef __uint64_t	__vm_paddr_t;
 typedef __int64_t	__vm_ooffset_t;
+typedef __uint64_t	__vm_size_t;
 #else
 typedef __int32_t	__register_t;
 typedef __uint32_t	__vm_paddr_t;
+typedef __uint32_t	__vm_size_t;
 #endif
 
 #endif	/* _FREEBSD_X86__TYPES_H_ */
diff --git a/usr/src/compat/freebsd/x86/segments.h b/usr/src/compat/freebsd/x86/segments.h
index bc6ba976b8..11edc582b5 100644
--- a/usr/src/compat/freebsd/x86/segments.h
+++ b/usr/src/compat/freebsd/x86/segments.h
@@ -11,18 +11,19 @@
 
 /*
  * Copyright 2015 Pluribus Networks Inc.
+ * Copyright 2017 Joyent, Inc.
  */
 
-#ifndef _COMPAT_FREEBSD_X86_SEGMENTS_H_
-#define	_COMPAT_FREEBSD_X86_SEGMENTS_H_
+#ifndef _COMPAT_FREEBSD_X86_SEGMENTS_H
+#define	_COMPAT_FREEBSD_X86_SEGMENTS_H
 
-/*
- * Entries in the Interrupt Descriptor Table (IDT)
- */
-#define	IDT_BP		3	/* #BP: Breakpoint */
+#if defined(_COMPAT_FREEBSD_AMD64_MACHINE_VMM_H_) || defined(_KERNEL)
 #define	IDT_UD		6	/* #UD: Undefined/Invalid Opcode */
 #define	IDT_SS		12	/* #SS: Stack Segment Fault */
 #define	IDT_GP		13	/* #GP: General Protection Fault */
 #define	IDT_AC		17	/* #AC: Alignment Check */
+#else
+#include_next <x86/segments.h>
+#endif
 
-#endif	/* _COMPAT_FREEBSD_AMD64_MACHINE_SEGMENTS_H_ */
+#endif /* _COMPAT_FREEBSD_X86_SEGMENTS_H */
diff --git a/usr/src/lib/Makefile b/usr/src/lib/Makefile
index 878ffa9f04..0054c41e6d 100644
--- a/usr/src/lib/Makefile
+++ b/usr/src/lib/Makefile
@@ -22,7 +22,7 @@
 #
 # Copyright (c) 1989, 2010, Oracle and/or its affiliates. All rights reserved.
 # Copyright (c) 2012 by Delphix. All rights reserved.
-# Copyright 2015, Joyent, Inc.
+# Copyright 2017 Joyent, Inc.
 # Copyright (c) 2013 Gary Mills
 # Copyright 2014 Garrett D'Amore <garrett@damore.org>
 # Copyright (c) 2015 Gary Mills
@@ -278,7 +278,8 @@ SUBDIRS +=				\
 
 i386_SUBDIRS=		\
 	libfdisk	\
-	libsaveargs
+	libsaveargs	\
+	libvmmapi
 
 sparc_SUBDIRS=		\
 	efcode		\
@@ -507,7 +508,8 @@ HDRSUBDIRS=				\
 
 i386_HDRSUBDIRS=	\
 	libfdisk	\
-	libsaveargs
+	libsaveargs	\
+	libvmmapi
 
 sparc_HDRSUBDIRS=	\
 	libds		\
diff --git a/usr/src/lib/libvmmapi/common/mapfile-vers b/usr/src/lib/libvmmapi/common/mapfile-vers
index 07286f6907..f69738f50a 100644
--- a/usr/src/lib/libvmmapi/common/mapfile-vers
+++ b/usr/src/lib/libvmmapi/common/mapfile-vers
@@ -33,27 +33,41 @@ $mapfile_version 2
 SYMBOL_VERSION ILLUMOSprivate {
 	global:
 		vcpu_reset;
+		vm_active_cpus;
 		vm_activate_cpu;
+		vm_active_cpus;
 		vm_apicid2vcpu;
+		vm_assign_pptdev;
 		vm_capability_name2type;
 		vm_capability_type2name;
 		vm_copy_setup;
 		vm_copy_teardown;
 		vm_copyin;
 		vm_copyout;
+		vm_create_devmem;
 		vm_create;
+		vm_create_devmem;
+		vm_destroy;
 		vm_destroy;
 		vm_get_capability;
 		vm_get_desc;
+		vm_get_device_fd;
+		vm_get_gpa_pmap;
+		vm_get_hpet_capabilities;
 		vm_get_highmem_size;
+		vm_get_intinfo;
 		vm_get_lowmem_limit;
 		vm_get_lowmem_size;
-		vm_get_memory_seg;
+		vm_get_memflags;
+		vm_get_memseg;
 		vm_get_register;
 		vm_get_seg_desc;
+		vm_get_stat_desc;
+		vm_get_stats;
 		vm_get_x2apic_state;
 		vm_gla2gpa;
 		vm_inject_exception;
+		vm_inject_nmi;
 		vm_isa_assert_irq;
 		vm_isa_deassert_irq;
 		vm_isa_pulse_irq;
@@ -62,19 +76,39 @@ SYMBOL_VERSION ILLUMOSprivate {
 		vm_ioapic_deassert_irq;
 		vm_ioapic_pincount;
 		vm_ioapic_pulse_irq;
+		vm_isa_assert_irq;
+		vm_isa_deassert_irq;
+		vm_isa_pulse_irq;
+		vm_isa_set_irq_trigger;
 		vm_lapic_irq;
+		vm_lapic_local_irq;
 		vm_lapic_msi;
 		vm_map_gpa;
+		vm_map_pptdev_mmio;
+		vm_mmap_getnext;
+		vm_mmap_memseg;
 		vm_open;
 		vm_parse_memsize;
+		vm_reinit;
 		vm_restart_instruction;
+		vm_rtc_gettime;
+		vm_rtc_read;
+		vm_rtc_settime;
+		vm_rtc_write;
 		vm_run;
 		vm_set_capability;
 		vm_set_desc;
+		vm_set_intinfo;
+		vm_set_memflags;
 		vm_set_register;
 		vm_set_x2apic_state;
 		vm_setup_memory;
-		vm_setup_rom;
+		vm_setup_pptdev_msi;
+		vm_setup_pptdev_msix;
+		vm_suspend;
+		vm_suspended_cpus;
+		vm_unassign_pptdev;
+
 	local:
 		*;
 };
diff --git a/usr/src/lib/libvmmapi/common/vmmapi.c b/usr/src/lib/libvmmapi/common/vmmapi.c
index a1b3ef4ae4..f37af36d1e 100644
--- a/usr/src/lib/libvmmapi/common/vmmapi.c
+++ b/usr/src/lib/libvmmapi/common/vmmapi.c
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/lib/libvmmapi/vmmapi.c 280929 2015-04-01 00:15:31Z tychon $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -39,7 +39,7 @@
  */
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/lib/libvmmapi/vmmapi.c 280929 2015-04-01 00:15:31Z tychon $");
+__FBSDID("$FreeBSD$");
 
 #include <sys/param.h>
 #include <sys/sysctl.h>
@@ -48,11 +48,10 @@ __FBSDID("$FreeBSD: head/lib/libvmmapi/vmmapi.c 280929 2015-04-01 00:15:31Z tych
 #include <sys/_iovec.h>
 #include <sys/cpuset.h>
 
+#include <x86/segments.h>
 #include <machine/specialreg.h>
 
-#ifndef	__FreeBSD__
 #include <errno.h>
-#endif
 #include <stdio.h>
 #include <stdlib.h>
 #include <assert.h>
@@ -70,23 +69,32 @@ __FBSDID("$FreeBSD: head/lib/libvmmapi/vmmapi.c 280929 2015-04-01 00:15:31Z tych
 
 #include "vmmapi.h"
 
-#define	KB	(1024UL)
 #define	MB	(1024 * 1024UL)
 #define	GB	(1024 * 1024 * 1024UL)
 
+#ifndef __FreeBSD__
+/* shim to no-op for now */
+#define MAP_NOCORE 0
+#define MAP_ALIGNED_SUPER 0
+#endif
+
+/*
+ * Size of the guard region before and after the virtual address space
+ * mapping the guest physical memory. This must be a multiple of the
+ * superpage size for performance reasons.
+ */
+#define	VM_MMAP_GUARD_SIZE	(4 * MB)
+
+#define	PROT_RW		(PROT_READ | PROT_WRITE)
+#define	PROT_ALL	(PROT_READ | PROT_WRITE | PROT_EXEC)
+
 struct vmctx {
 	int	fd;
 	uint32_t lowmem_limit;
-	enum vm_mmap_style vms;
-	char	*lowermem_addr;
-	char	*biosmem_addr;
+	int	memflags;
 	size_t	lowmem;
-	char	*lowmem_addr;
 	size_t	highmem;
-	char	*highmem_addr;
-	uint64_t rombase;
-	uint64_t romlimit;
-	char	*rom_addr;
+	char	*baseaddr;
 	char	*name;
 };
 
@@ -94,8 +102,32 @@ struct vmctx {
 #define	CREATE(x)  sysctlbyname("hw.vmm.create", NULL, NULL, (x), strlen((x)))
 #define	DESTROY(x) sysctlbyname("hw.vmm.destroy", NULL, NULL, (x), strlen((x)))
 #else
-#define	CREATE(x)	vmm_vm_create(x)
-#define	DESTROY(x)	vmm_vm_destroy(x)
+#define	CREATE(x)	vm_do_ctl(VMM_CREATE_VM, (x))
+#define	DESTROY(x)	vm_do_ctl(VMM_DESTROY_VM, (x))
+
+static int
+vm_do_ctl(int cmd, const char *name)
+{
+	const char vmm_ctl[] = "/devices/pseudo/vmm@0:ctl";
+	int ctl_fd;
+
+	ctl_fd = open(vmm_ctl, O_EXCL | O_RDWR);
+	if (ctl_fd < 0) {
+		return (-1);
+	}
+
+	if (ioctl(ctl_fd, cmd, name) == -1) {
+		int err = errno;
+
+		/* Do not lose ioctl errno through the close(2) */
+		(void) close(ctl_fd);
+		errno = err;
+		return (-1);
+	}
+	(void) close(ctl_fd);
+
+	return (0);
+}
 #endif
 
 static int
@@ -124,39 +156,6 @@ vm_device_open(const char *name)
         return (fd);
 }
 
-#ifndef	__FreeBSD__
-static int
-vmm_vm_create(const char *name)
-{
-	const char vmm_ctl[] = "/devices/pseudo/vmm@0:ctl";
-	struct vmm_ioctl vi;
-	int err = 0;
-	int ctl_fd;
-
-	(void) strlcpy(vi.vmm_name, name, sizeof (vi.vmm_name) - 1);
-
-	ctl_fd = open(vmm_ctl, O_EXCL | O_RDWR);
-	if (ctl_fd == -1) {
-		err = errno;
-		if ((errno == EPERM) || (errno == EACCES)) {
-			fprintf(stderr, "you do not have permission to "
-				"perform that operation.\n");
-		} else {
-			fprintf(stderr, "open: %s: %s\n", vmm_ctl,
-				strerror(errno));
-		}
-		return (err);
-	}
-	if (ioctl(ctl_fd, VMM_CREATE_VM, &vi) == -1) {
-		err = errno;
-		fprintf(stderr, "couldn't create vm \"%s\"", name);
-	}
-	close (ctl_fd);
-
-	return (err);
-}
-#endif
-
 int
 vm_create(const char *name)
 {
@@ -173,6 +172,7 @@ vm_open(const char *name)
 	assert(vm != NULL);
 
 	vm->fd = -1;
+	vm->memflags = 0;
 	vm->lowmem_limit = 3 * GB;
 	vm->name = (char *)(vm + 1);
 	strcpy(vm->name, name);
@@ -182,54 +182,20 @@ vm_open(const char *name)
 
 	return (vm);
 err:
-	(void) vm_destroy(vm);
+	vm_destroy(vm);
 	return (NULL);
 }
 
-#ifndef	__FreeBSD__
-static int
-vmm_vm_destroy(const char *name)
-{
-	const char vmm_ctl[] = "/devices/pseudo/vmm@0:ctl";
-	struct vmm_ioctl vi;	
-	int ctl_fd;
-	int err = 0;
-
-	(void) strlcpy(vi.vmm_name, name, sizeof (vi.vmm_name) - 1);
-
-	ctl_fd = open(vmm_ctl, O_EXCL | O_RDWR);
-	if (ctl_fd == -1) {
-		err = errno;
-		if ((errno == EPERM) || (errno == EACCES)) {
-			fprintf(stderr, "you do not have permission to "
-				"perform that operation.\n");
-		} else {
-			fprintf(stderr, "open: %s: %s\n", vmm_ctl,
-				strerror(errno));
-		}
-		return (err);
-	}
-	if (ioctl(ctl_fd, VMM_DESTROY_VM, &vi) == -1) {
-		err = errno;
-		fprintf(stderr, "couldn't destroy vm \"%s\"", name);
-	}
-	close (ctl_fd);
-	return (err);
-}
-#endif
-
-int
+void
 vm_destroy(struct vmctx *vm)
 {
-	int err;
 	assert(vm != NULL);
 
 	if (vm->fd >= 0)
 		close(vm->fd);
-	err = DESTROY(vm->name);
+	DESTROY(vm->name);
 
 	free(vm);
-	return (err);
 }
 
 int
@@ -256,92 +222,205 @@ vm_parse_memsize(const char *optarg, size_t *ret_memsize)
 	return (error);
 }
 
-#ifdef	__FreeBSD__
-size_t
-vmm_get_mem_total(void)
+uint32_t
+vm_get_lowmem_limit(struct vmctx *ctx)
 {
-	size_t mem_total = 0;
-	size_t oldlen = sizeof(mem_total);
-	int error;
-	error = sysctlbyname("hw.vmm.mem_total", &mem_total, &oldlen, NULL, 0);
-	if (error)
-		return -1;
-	return mem_total;
+
+	return (ctx->lowmem_limit);
 }
 
-size_t
-vmm_get_mem_free(void)
+void
+vm_set_lowmem_limit(struct vmctx *ctx, uint32_t limit)
 {
-	size_t mem_free = 0;
-	size_t oldlen = sizeof(mem_free);
-	int error;
-	error = sysctlbyname("hw.vmm.mem_free", &mem_free, &oldlen, NULL, 0);
-	if (error)
-		return -1;
-	return mem_free;
+
+	ctx->lowmem_limit = limit;
+}
+
+void
+vm_set_memflags(struct vmctx *ctx, int flags)
+{
+
+	ctx->memflags = flags;
 }
-#endif
 
 int
-vm_get_memory_seg(struct vmctx *ctx, vm_paddr_t gpa, size_t *ret_len,
-		  int *wired)
+vm_get_memflags(struct vmctx *ctx)
 {
-	int error;
-	struct vm_memory_segment seg;
-
-	bzero(&seg, sizeof(seg));
-	seg.gpa = gpa;
-	error = ioctl(ctx->fd, VM_GET_MEMORY_SEG, &seg);
-	*ret_len = seg.len;
-	if (wired != NULL)
-		*wired = seg.wired;
+
+	return (ctx->memflags);
+}
+
+/*
+ * Map segment 'segid' starting at 'off' into guest address range [gpa,gpa+len).
+ */
+int
+vm_mmap_memseg(struct vmctx *ctx, vm_paddr_t gpa, int segid, vm_ooffset_t off,
+    size_t len, int prot)
+{
+	struct vm_memmap memmap;
+	int error, flags;
+
+	memmap.gpa = gpa;
+	memmap.segid = segid;
+	memmap.segoff = off;
+	memmap.len = len;
+	memmap.prot = prot;
+	memmap.flags = 0;
+
+	if (ctx->memflags & VM_MEM_F_WIRED)
+		memmap.flags |= VM_MEMMAP_F_WIRED;
+
+	/*
+	 * If this mapping already exists then don't create it again. This
+	 * is the common case for SYSMEM mappings created by bhyveload(8).
+	 */
+	error = vm_mmap_getnext(ctx, &gpa, &segid, &off, &len, &prot, &flags);
+	if (error == 0 && gpa == memmap.gpa) {
+		if (segid != memmap.segid || off != memmap.segoff ||
+		    prot != memmap.prot || flags != memmap.flags) {
+			errno = EEXIST;
+			return (-1);
+		} else {
+			return (0);
+		}
+	}
+
+	error = ioctl(ctx->fd, VM_MMAP_MEMSEG, &memmap);
 	return (error);
 }
 
-uint32_t
-vm_get_lowmem_limit(struct vmctx *ctx)
+int
+vm_mmap_getnext(struct vmctx *ctx, vm_paddr_t *gpa, int *segid,
+    vm_ooffset_t *segoff, size_t *len, int *prot, int *flags)
 {
+	struct vm_memmap memmap;
+	int error;
 
-	return (ctx->lowmem_limit);
+	bzero(&memmap, sizeof(struct vm_memmap));
+	memmap.gpa = *gpa;
+	error = ioctl(ctx->fd, VM_MMAP_GETNEXT, &memmap);
+	if (error == 0) {
+		*gpa = memmap.gpa;
+		*segid = memmap.segid;
+		*segoff = memmap.segoff;
+		*len = memmap.len;
+		*prot = memmap.prot;
+		*flags = memmap.flags;
+	}
+	return (error);
 }
 
-void
-vm_set_lowmem_limit(struct vmctx *ctx, uint32_t limit)
+/*
+ * Return 0 if the segments are identical and non-zero otherwise.
+ *
+ * This is slightly complicated by the fact that only device memory segments
+ * are named.
+ */
+static int
+cmpseg(size_t len, const char *str, size_t len2, const char *str2)
 {
 
-	ctx->lowmem_limit = limit;
+	if (len == len2) {
+		if ((!str && !str2) || (str && str2 && !strcmp(str, str2)))
+			return (0);
+	}
+	return (-1);
 }
 
 static int
-setup_memory_segment(struct vmctx *ctx, vm_paddr_t gpa, size_t len, char **addr)
+vm_alloc_memseg(struct vmctx *ctx, int segid, size_t len, const char *name)
 {
+	struct vm_memseg memseg;
+	size_t n;
 	int error;
-	struct vm_memory_segment seg;
 
 	/*
-	 * Create and optionally map 'len' bytes of memory at guest
-	 * physical address 'gpa'
+	 * If the memory segment has already been created then just return.
+	 * This is the usual case for the SYSMEM segment created by userspace
+	 * loaders like bhyveload(8).
 	 */
-	bzero(&seg, sizeof(seg));
-	seg.gpa = gpa;
-	seg.len = len;
-	error = ioctl(ctx->fd, VM_MAP_MEMORY, &seg);
-	if (error == 0 && addr != NULL) {
-		*addr = mmap(NULL, len, PROT_READ | PROT_WRITE, MAP_SHARED,
-				ctx->fd, gpa);
+	error = vm_get_memseg(ctx, segid, &memseg.len, memseg.name,
+	    sizeof(memseg.name));
+	if (error)
+		return (error);
+
+	if (memseg.len != 0) {
+		if (cmpseg(len, name, memseg.len, VM_MEMSEG_NAME(&memseg))) {
+			errno = EINVAL;
+			return (-1);
+		} else {
+			return (0);
+		}
+	}
+
+	bzero(&memseg, sizeof(struct vm_memseg));
+	memseg.segid = segid;
+	memseg.len = len;
+	if (name != NULL) {
+		n = strlcpy(memseg.name, name, sizeof(memseg.name));
+		if (n >= sizeof(memseg.name)) {
+			errno = ENAMETOOLONG;
+			return (-1);
+		}
 	}
+
+	error = ioctl(ctx->fd, VM_ALLOC_MEMSEG, &memseg);
 	return (error);
 }
 
 int
-vm_setup_memory(struct vmctx *ctx, size_t memsize, enum vm_mmap_style vms)
+vm_get_memseg(struct vmctx *ctx, int segid, size_t *lenp, char *namebuf,
+    size_t bufsize)
 {
-	char **addr;
+	struct vm_memseg memseg;
+	size_t n;
 	int error;
 
-	/* XXX VM_MMAP_SPARSE not implemented yet */
-	assert(vms == VM_MMAP_NONE || vms == VM_MMAP_ALL);
-	ctx->vms = vms;
+	memseg.segid = segid;
+	error = ioctl(ctx->fd, VM_GET_MEMSEG, &memseg);
+	if (error == 0) {
+		*lenp = memseg.len;
+		n = strlcpy(namebuf, memseg.name, bufsize);
+		if (n >= bufsize) {
+			errno = ENAMETOOLONG;
+			error = -1;
+		}
+	}
+	return (error);
+}
+
+static int
+setup_memory_segment(struct vmctx *ctx, vm_paddr_t gpa, size_t len, char *base)
+{
+	char *ptr;
+	int error, flags;
+
+	/* Map 'len' bytes starting at 'gpa' in the guest address space */
+	error = vm_mmap_memseg(ctx, gpa, VM_SYSMEM, gpa, len, PROT_ALL);
+	if (error)
+		return (error);
+
+	flags = MAP_SHARED | MAP_FIXED;
+	if ((ctx->memflags & VM_MEM_F_INCORE) == 0)
+		flags |= MAP_NOCORE;
+
+	/* mmap into the process address space on the host */
+	ptr = mmap(base + gpa, len, PROT_RW, flags, ctx->fd, gpa);
+	if (ptr == MAP_FAILED)
+		return (-1);
+
+	return (0);
+}
+
+int
+vm_setup_memory(struct vmctx *ctx, size_t memsize, enum vm_mmap_style vms)
+{
+	size_t objsize, len;
+	vm_paddr_t gpa;
+	char *baseaddr, *ptr;
+	int error, flags;
+
+	assert(vms == VM_MMAP_ALL);
 
 	/*
 	 * If 'memsize' cannot fit entirely in the 'lowmem' segment then
@@ -349,81 +428,74 @@ vm_setup_memory(struct vmctx *ctx, size_t memsize, enum vm_mmap_style vms)
 	 */
 	if (memsize > ctx->lowmem_limit) {
 		ctx->lowmem = ctx->lowmem_limit;
-		ctx->highmem = memsize - ctx->lowmem;
+		ctx->highmem = memsize - ctx->lowmem_limit;
+		objsize = 4*GB + ctx->highmem;
 	} else {
 		ctx->lowmem = memsize;
 		ctx->highmem = 0;
+		objsize = ctx->lowmem;
 	}
 
-	if (ctx->lowmem > 0) {
-		addr = (vms == VM_MMAP_ALL) ? &ctx->lowermem_addr : NULL;
-		error = setup_memory_segment(ctx, 0, 640*KB, addr);
-		if (error)
-			return (error);
+	error = vm_alloc_memseg(ctx, VM_SYSMEM, objsize, NULL);
+	if (error)
+		return (error);
 
-		addr = (vms == VM_MMAP_ALL) ? &ctx->biosmem_addr : NULL;
-		error = setup_memory_segment(ctx, 768*KB, 256*KB, addr);
-		if (error)
-			return (error);
+	/*
+	 * Stake out a contiguous region covering the guest physical memory
+	 * and the adjoining guard regions.
+	 */
+	len = VM_MMAP_GUARD_SIZE + objsize + VM_MMAP_GUARD_SIZE;
+	flags = MAP_PRIVATE | MAP_ANON | MAP_NOCORE | MAP_ALIGNED_SUPER;
+	ptr = mmap(NULL, len, PROT_NONE, flags, -1, 0);
+	if (ptr == MAP_FAILED)
+		return (-1);
 
-		addr = (vms == VM_MMAP_ALL) ? &ctx->lowmem_addr : NULL;
-		error = setup_memory_segment(ctx, 1*MB, ctx->lowmem - 1*MB, addr);
+	baseaddr = ptr + VM_MMAP_GUARD_SIZE;
+	if (ctx->highmem > 0) {
+		gpa = 4*GB;
+		len = ctx->highmem;
+		error = setup_memory_segment(ctx, gpa, len, baseaddr);
 		if (error)
 			return (error);
 	}
 
-	if (ctx->highmem > 0) {
-		addr = (vms == VM_MMAP_ALL) ? &ctx->highmem_addr : NULL;
-		error = setup_memory_segment(ctx, 4*GB, ctx->highmem, addr);
+	if (ctx->lowmem > 0) {
+		gpa = 0;
+		len = ctx->lowmem;
+		error = setup_memory_segment(ctx, gpa, len, baseaddr);
 		if (error)
 			return (error);
 	}
 
-	return (0);
-}
-
-int
-vm_setup_rom(struct vmctx *ctx, vm_paddr_t gpa, size_t len)
-{
-	ctx->rombase = gpa;
-	ctx->romlimit = gpa + len;
+	ctx->baseaddr = baseaddr;
 
-	return (setup_memory_segment(ctx, gpa, len, &ctx->rom_addr));
+	return (0);
 }
 
+/*
+ * Returns a non-NULL pointer if [gaddr, gaddr+len) is entirely contained in
+ * the lowmem or highmem regions.
+ *
+ * In particular return NULL if [gaddr, gaddr+len) falls in guest MMIO region.
+ * The instruction emulation code depends on this behavior.
+ */
 void *
 vm_map_gpa(struct vmctx *ctx, vm_paddr_t gaddr, size_t len)
 {
 
-	/* XXX VM_MMAP_SPARSE not implemented yet */
-	assert(ctx->vms == VM_MMAP_ALL);
-
-	if (gaddr + len <= 1*MB) {
-		if (gaddr + len <= 640*KB)
-			return ((void *)(ctx->lowermem_addr + gaddr));
-
-		if (768*KB <= gaddr && gaddr + len <= 1*MB) {
-			gaddr -= 768*KB;
-			return ((void *)(ctx->biosmem_addr + gaddr));
-		}
-
-		return (NULL);
-	}
-
-	if (gaddr < ctx->lowmem && gaddr + len <= ctx->lowmem) {
-		gaddr -= 1*MB;
-		return ((void *)(ctx->lowmem_addr + gaddr));
-	}
-
-	if (ctx->rombase <= gaddr && gaddr + len <= ctx->romlimit) {
-		gaddr -= ctx->rombase;
-		return ((void *)(ctx->rom_addr + gaddr));
+	if (ctx->lowmem > 0) {
+		if (gaddr < ctx->lowmem && len <= ctx->lowmem &&
+		    gaddr + len <= ctx->lowmem)
+			return (ctx->baseaddr + gaddr);
 	}
 
-	if (gaddr >= 4*GB) {
-		gaddr -= 4*GB;
-		if (gaddr < ctx->highmem && gaddr + len <= ctx->highmem)
-			return ((void *)(ctx->highmem_addr + gaddr));
+	if (ctx->highmem > 0) {
+                if (gaddr >= 4*GB) {
+			if (gaddr < 4*GB + ctx->highmem &&
+			    len <= ctx->highmem &&
+			    gaddr + len <= 4*GB + ctx->highmem)
+				return (ctx->baseaddr + gaddr);
+		}
 	}
 
 	return (NULL);
@@ -443,6 +515,79 @@ vm_get_highmem_size(struct vmctx *ctx)
 	return (ctx->highmem);
 }
 
+void *
+vm_create_devmem(struct vmctx *ctx, int segid, const char *name, size_t len)
+{
+#ifdef	__FreeBSD__
+	char pathname[MAXPATHLEN];
+#endif
+	size_t len2;
+	char *base, *ptr;
+	int fd, error, flags;
+	off_t mapoff;
+
+	fd = -1;
+	ptr = MAP_FAILED;
+	if (name == NULL || strlen(name) == 0) {
+		errno = EINVAL;
+		goto done;
+	}
+
+	error = vm_alloc_memseg(ctx, segid, len, name);
+	if (error)
+		goto done;
+
+#ifdef	__FreeBSD__
+	strlcpy(pathname, "/dev/vmm.io/", sizeof(pathname));
+	strlcat(pathname, ctx->name, sizeof(pathname));
+	strlcat(pathname, ".", sizeof(pathname));
+	strlcat(pathname, name, sizeof(pathname));
+
+	fd = open(pathname, O_RDWR);
+	if (fd < 0)
+		goto done;
+#else
+	{
+		struct vm_devmem_offset vdo;
+
+		vdo.segid = segid;
+		error = ioctl(ctx->fd, VM_DEVMEM_GETOFFSET, &vdo);
+		if (error == 0) {
+			mapoff = vdo.offset;
+		} else {
+			goto done;
+		}
+	}
+#endif
+
+	/*
+	 * Stake out a contiguous region covering the device memory and the
+	 * adjoining guard regions.
+	 */
+	len2 = VM_MMAP_GUARD_SIZE + len + VM_MMAP_GUARD_SIZE;
+	flags = MAP_PRIVATE | MAP_ANON | MAP_NOCORE | MAP_ALIGNED_SUPER;
+	base = mmap(NULL, len2, PROT_NONE, flags, -1, 0);
+	if (base == MAP_FAILED)
+		goto done;
+
+	flags = MAP_SHARED | MAP_FIXED;
+	if ((ctx->memflags & VM_MEM_F_INCORE) == 0)
+		flags |= MAP_NOCORE;
+
+#ifdef	__FreeBSD__
+	/* mmap the devmem region in the host address space */
+	ptr = mmap(base + VM_MMAP_GUARD_SIZE, len, PROT_RW, flags, fd, 0);
+#else
+	/* mmap the devmem region in the host address space */
+	ptr = mmap(base + VM_MMAP_GUARD_SIZE, len, PROT_RW, flags, ctx->fd,
+	    mapoff);
+#endif
+done:
+	if (fd >= 0)
+		close(fd);
+	return (ptr);
+}
+
 int
 vm_set_desc(struct vmctx *ctx, int vcpu, int reg,
 	    uint64_t base, uint32_t limit, uint32_t access)
@@ -535,23 +680,22 @@ vm_run(struct vmctx *ctx, int vcpu, struct vm_exit *vmexit)
 	return (error);
 }
 
-/* XXX: unused static */
-#if notyet
-static int
-vm_inject_exception_real(struct vmctx *ctx, int vcpu, int vector,
-    int error_code, int error_code_valid)
+int
+vm_suspend(struct vmctx *ctx, enum vm_suspend_how how)
 {
-	struct vm_exception exc;
+	struct vm_suspend vmsuspend;
 
-	bzero(&exc, sizeof(exc));
-	exc.cpuid = vcpu;
-	exc.vector = vector;
-	exc.error_code = error_code;
-	exc.error_code_valid = error_code_valid;
+	bzero(&vmsuspend, sizeof(vmsuspend));
+	vmsuspend.how = how;
+	return (ioctl(ctx->fd, VM_SUSPEND, &vmsuspend));
+}
 
-	return (ioctl(ctx->fd, VM_INJECT_EXCEPTION, &exc));
+int
+vm_reinit(struct vmctx *ctx)
+{
+
+	return (ioctl(ctx->fd, VM_REINIT, 0));
 }
-#endif
 
 int
 vm_inject_exception(struct vmctx *ctx, int vcpu, int vector, int errcode_valid,
@@ -861,7 +1005,6 @@ vm_setup_pptdev_msix(struct vmctx *ctx, int vcpu, int bus, int slot, int func,
 	return ioctl(ctx->fd, VM_PPTDEV_MSIX, &pptmsix);
 }
 
-#ifdef	__FreeBSD__
 uint64_t *
 vm_get_stats(struct vmctx *ctx, int vcpu, struct timeval *ret_tv,
 	     int *ret_entries)
@@ -872,7 +1015,7 @@ vm_get_stats(struct vmctx *ctx, int vcpu, struct timeval *ret_tv,
 
 	vmstats.cpuid = vcpu;
 
-	error = ioctl(ctx->fd, VM_STATS, &vmstats);
+	error = ioctl(ctx->fd, VM_STATS_IOC, &vmstats);
 	if (error == 0) {
 		if (ret_entries)
 			*ret_entries = vmstats.num_entries;
@@ -894,7 +1037,6 @@ vm_get_stat_desc(struct vmctx *ctx, int index)
 	else
 		return (NULL);
 }
-#endif
 
 int
 vm_get_x2apic_state(struct vmctx *ctx, int vcpu, enum x2apic_state *state)
@@ -1115,9 +1257,9 @@ vm_get_hpet_capabilities(struct vmctx *ctx, uint32_t *capabilities)
 	return (error);
 }
 
-static int
-gla2gpa(struct vmctx *ctx, int vcpu, struct vm_guest_paging *paging,
-    uint64_t gla, int prot, int *fault, uint64_t *gpa)
+int
+vm_gla2gpa(struct vmctx *ctx, int vcpu, struct vm_guest_paging *paging,
+    uint64_t gla, int prot, uint64_t *gpa, int *fault)
 {
 	struct vm_gla2gpa gg;
 	int error;
@@ -1136,29 +1278,18 @@ gla2gpa(struct vmctx *ctx, int vcpu, struct vm_guest_paging *paging,
 	return (error);
 }
 
-int
-vm_gla2gpa(struct vmctx *ctx, int vcpu, struct vm_guest_paging *paging,
-    uint64_t gla, int prot, uint64_t *gpa)
-{
-	int error, fault;
-
-	error = gla2gpa(ctx, vcpu, paging, gla, prot, &fault, gpa);
-	if (fault)
-		error = fault;
-	return (error);
-}
-
 #ifndef min
 #define	min(a,b)	(((a) < (b)) ? (a) : (b))
 #endif
 
 int
 vm_copy_setup(struct vmctx *ctx, int vcpu, struct vm_guest_paging *paging,
-    uint64_t gla, size_t len, int prot, struct iovec *iov, int iovcnt)
+    uint64_t gla, size_t len, int prot, struct iovec *iov, int iovcnt,
+    int *fault)
 {
 	void *va;
 	uint64_t gpa;
-	int error, fault, i, n, off;
+	int error, i, n, off;
 
 	for (i = 0; i < iovcnt; i++) {
 		iov[i].iov_base = 0;
@@ -1167,18 +1298,16 @@ vm_copy_setup(struct vmctx *ctx, int vcpu, struct vm_guest_paging *paging,
 
 	while (len) {
 		assert(iovcnt > 0);
-		error = gla2gpa(ctx, vcpu, paging, gla, prot, &fault, &gpa);
-		if (error)
-			return (-1);
-		if (fault)
-			return (1);
+		error = vm_gla2gpa(ctx, vcpu, paging, gla, prot, &gpa, fault);
+		if (error || *fault)
+			return (error);
 
 		off = gpa & PAGE_MASK;
 		n = min(len, PAGE_SIZE - off);
 
 		va = vm_map_gpa(ctx, gpa, n);
 		if (va == NULL)
-			return (-1);
+			return (EFAULT);
 
 		iov->iov_base = va;
 		iov->iov_len = n;
@@ -1239,6 +1368,35 @@ vm_copyout(struct vmctx *ctx, int vcpu, const void *vp, struct iovec *iov,
 	}
 }
 
+static int
+vm_get_cpus(struct vmctx *ctx, int which, cpuset_t *cpus)
+{
+	struct vm_cpuset vm_cpuset;
+	int error;
+
+	bzero(&vm_cpuset, sizeof(struct vm_cpuset));
+	vm_cpuset.which = which;
+	vm_cpuset.cpusetsize = sizeof(cpuset_t);
+	vm_cpuset.cpus = cpus;
+
+	error = ioctl(ctx->fd, VM_GET_CPUS, &vm_cpuset);
+	return (error);
+}
+
+int
+vm_active_cpus(struct vmctx *ctx, cpuset_t *cpus)
+{
+
+	return (vm_get_cpus(ctx, VM_ACTIVE_CPUS, cpus));
+}
+
+int
+vm_suspended_cpus(struct vmctx *ctx, cpuset_t *cpus)
+{
+
+	return (vm_get_cpus(ctx, VM_SUSPENDED_CPUS, cpus));
+}
+
 int
 vm_activate_cpu(struct vmctx *ctx, int vcpu)
 {
@@ -1251,6 +1409,87 @@ vm_activate_cpu(struct vmctx *ctx, int vcpu)
 	return (error);
 }
 
+int
+vm_get_intinfo(struct vmctx *ctx, int vcpu, uint64_t *info1, uint64_t *info2)
+{
+	struct vm_intinfo vmii;
+	int error;
+
+	bzero(&vmii, sizeof(struct vm_intinfo));
+	vmii.vcpuid = vcpu;
+	error = ioctl(ctx->fd, VM_GET_INTINFO, &vmii);
+	if (error == 0) {
+		*info1 = vmii.info1;
+		*info2 = vmii.info2;
+	}
+	return (error);
+}
+
+int
+vm_set_intinfo(struct vmctx *ctx, int vcpu, uint64_t info1)
+{
+	struct vm_intinfo vmii;
+	int error;
+
+	bzero(&vmii, sizeof(struct vm_intinfo));
+	vmii.vcpuid = vcpu;
+	vmii.info1 = info1;
+	error = ioctl(ctx->fd, VM_SET_INTINFO, &vmii);
+	return (error);
+}
+
+int
+vm_rtc_write(struct vmctx *ctx, int offset, uint8_t value)
+{
+	struct vm_rtc_data rtcdata;
+	int error;
+
+	bzero(&rtcdata, sizeof(struct vm_rtc_data));
+	rtcdata.offset = offset;
+	rtcdata.value = value;
+	error = ioctl(ctx->fd, VM_RTC_WRITE, &rtcdata);
+	return (error);
+}
+
+int
+vm_rtc_read(struct vmctx *ctx, int offset, uint8_t *retval)
+{
+	struct vm_rtc_data rtcdata;
+	int error;
+
+	bzero(&rtcdata, sizeof(struct vm_rtc_data));
+	rtcdata.offset = offset;
+	error = ioctl(ctx->fd, VM_RTC_READ, &rtcdata);
+	if (error == 0)
+		*retval = rtcdata.value;
+	return (error);
+}
+
+int
+vm_rtc_settime(struct vmctx *ctx, time_t secs)
+{
+	struct vm_rtc_time rtctime;
+	int error;
+
+	bzero(&rtctime, sizeof(struct vm_rtc_time));
+	rtctime.secs = secs;
+	error = ioctl(ctx->fd, VM_RTC_SETTIME, &rtctime);
+	return (error);
+}
+
+int
+vm_rtc_gettime(struct vmctx *ctx, time_t *secs)
+{
+	struct vm_rtc_time rtctime;
+	int error;
+
+	bzero(&rtctime, sizeof(struct vm_rtc_time));
+	error = ioctl(ctx->fd, VM_RTC_GETTIME, &rtctime);
+	if (error == 0)
+		*secs = rtctime.secs;
+	return (error);
+}
+
 int
 vm_restart_instruction(void *arg, int vcpu)
 {
@@ -1258,3 +1497,47 @@ vm_restart_instruction(void *arg, int vcpu)
 
 	return (ioctl(ctx->fd, VM_RESTART_INSTRUCTION, &vcpu));
 }
+
+int
+vm_get_device_fd(struct vmctx *ctx)
+{
+
+	return (ctx->fd);
+}
+
+#ifdef __FreeBSD__
+const cap_ioctl_t *
+vm_get_ioctls(size_t *len)
+{
+	cap_ioctl_t *cmds;
+	/* keep in sync with machine/vmm_dev.h */
+	static const cap_ioctl_t vm_ioctl_cmds[] = { VM_RUN, VM_SUSPEND, VM_REINIT,
+	    VM_ALLOC_MEMSEG, VM_GET_MEMSEG, VM_MMAP_MEMSEG, VM_MMAP_MEMSEG,
+	    VM_MMAP_GETNEXT, VM_SET_REGISTER, VM_GET_REGISTER,
+	    VM_SET_SEGMENT_DESCRIPTOR, VM_GET_SEGMENT_DESCRIPTOR,
+	    VM_INJECT_EXCEPTION, VM_LAPIC_IRQ, VM_LAPIC_LOCAL_IRQ,
+	    VM_LAPIC_MSI, VM_IOAPIC_ASSERT_IRQ, VM_IOAPIC_DEASSERT_IRQ,
+	    VM_IOAPIC_PULSE_IRQ, VM_IOAPIC_PINCOUNT, VM_ISA_ASSERT_IRQ,
+	    VM_ISA_DEASSERT_IRQ, VM_ISA_PULSE_IRQ, VM_ISA_SET_IRQ_TRIGGER,
+	    VM_SET_CAPABILITY, VM_GET_CAPABILITY, VM_BIND_PPTDEV,
+	    VM_UNBIND_PPTDEV, VM_MAP_PPTDEV_MMIO, VM_PPTDEV_MSI,
+	    VM_PPTDEV_MSIX, VM_INJECT_NMI, VM_STATS, VM_STAT_DESC,
+	    VM_SET_X2APIC_STATE, VM_GET_X2APIC_STATE,
+	    VM_GET_HPET_CAPABILITIES, VM_GET_GPA_PMAP, VM_GLA2GPA,
+	    VM_ACTIVATE_CPU, VM_GET_CPUS, VM_SET_INTINFO, VM_GET_INTINFO,
+	    VM_RTC_WRITE, VM_RTC_READ, VM_RTC_SETTIME, VM_RTC_GETTIME,
+	    VM_RESTART_INSTRUCTION };
+
+	if (len == NULL) {
+		cmds = malloc(sizeof(vm_ioctl_cmds));
+		if (cmds == NULL)
+			return (NULL);
+		bcopy(vm_ioctl_cmds, cmds, sizeof(vm_ioctl_cmds));
+		return (cmds);
+	}
+
+	*len = nitems(vm_ioctl_cmds);
+	return (NULL);
+}
+#endif /* __FreeBSD__ */
+
diff --git a/usr/src/lib/libvmmapi/common/vmmapi.h b/usr/src/lib/libvmmapi/common/vmmapi.h
index d7eb67aa58..f5a1e954d5 100644
--- a/usr/src/lib/libvmmapi/common/vmmapi.h
+++ b/usr/src/lib/libvmmapi/common/vmmapi.h
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/lib/libvmmapi/vmmapi.h 280929 2015-04-01 00:15:31Z tychon $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -42,6 +42,13 @@
 #define	_VMMAPI_H_
 
 #include <sys/param.h>
+#include <sys/cpuset.h>
+
+/*
+ * API version for out-of-tree consumers like grub-bhyve for making compile
+ * time decisions.
+ */
+#define	VMMAPI_VERSION	0103	/* 2 digit major followed by 2 digit minor */
 
 struct iovec;
 struct vmctx;
@@ -57,19 +64,69 @@ enum vm_mmap_style {
 	VM_MMAP_SPARSE,		/* mappings created on-demand */
 };
 
+/*
+ * 'flags' value passed to 'vm_set_memflags()'.
+ */
+#define	VM_MEM_F_INCORE	0x01	/* include guest memory in core file */
+#define	VM_MEM_F_WIRED	0x02	/* guest memory is wired */
+
+/*
+ * Identifiers for memory segments:
+ * - vm_setup_memory() uses VM_SYSMEM for the system memory segment.
+ * - the remaining identifiers can be used to create devmem segments.
+ */
+enum {
+	VM_SYSMEM,
+	VM_BOOTROM,
+	VM_FRAMEBUFFER,
+};
+
+/*
+ * Get the length and name of the memory segment identified by 'segid'.
+ * Note that system memory segments are identified with a nul name.
+ *
+ * Returns 0 on success and non-zero otherwise.
+ */
+int	vm_get_memseg(struct vmctx *ctx, int ident, size_t *lenp, char *name,
+	    size_t namesiz);
+
+/*
+ * Iterate over the guest address space. This function finds an address range
+ * that starts at an address >= *gpa.
+ *
+ * Returns 0 if the next address range was found and non-zero otherwise.
+ */
+int	vm_mmap_getnext(struct vmctx *ctx, vm_paddr_t *gpa, int *segid,
+	    vm_ooffset_t *segoff, size_t *len, int *prot, int *flags);
+/*
+ * Create a device memory segment identified by 'segid'.
+ *
+ * Returns a pointer to the memory segment on success and MAP_FAILED otherwise.
+ */
+void	*vm_create_devmem(struct vmctx *ctx, int segid, const char *name,
+	    size_t len);
+
+/*
+ * Map the memory segment identified by 'segid' into the guest address space
+ * at [gpa,gpa+len) with protection 'prot'.
+ */
+int	vm_mmap_memseg(struct vmctx *ctx, vm_paddr_t gpa, int segid,
+	    vm_ooffset_t segoff, size_t len, int prot);
+
 int	vm_create(const char *name);
+int	vm_get_device_fd(struct vmctx *ctx);
 struct vmctx *vm_open(const char *name);
-int	vm_destroy(struct vmctx *ctx);
+void	vm_destroy(struct vmctx *ctx);
 int	vm_parse_memsize(const char *optarg, size_t *memsize);
-int	vm_get_memory_seg(struct vmctx *ctx, vm_paddr_t gpa, size_t *ret_len,
-			  int *wired);
 int	vm_setup_memory(struct vmctx *ctx, size_t len, enum vm_mmap_style s);
-int	vm_setup_rom(struct vmctx *ctx, vm_paddr_t gpa, size_t len);
 void	*vm_map_gpa(struct vmctx *ctx, vm_paddr_t gaddr, size_t len);
+int	vm_get_gpa_pmap(struct vmctx *, uint64_t gpa, uint64_t *pte, int *num);
 int	vm_gla2gpa(struct vmctx *, int vcpuid, struct vm_guest_paging *paging,
-		   uint64_t gla, int prot, uint64_t *gpa);
+		   uint64_t gla, int prot, uint64_t *gpa, int *fault);
 uint32_t vm_get_lowmem_limit(struct vmctx *ctx);
 void	vm_set_lowmem_limit(struct vmctx *ctx, uint32_t limit);
+void	vm_set_memflags(struct vmctx *ctx, int flags);
+int	vm_get_memflags(struct vmctx *ctx);
 size_t	vm_get_lowmem_size(struct vmctx *ctx);
 size_t	vm_get_highmem_size(struct vmctx *ctx);
 int	vm_set_desc(struct vmctx *ctx, int vcpu, int reg,
@@ -81,6 +138,8 @@ int	vm_get_seg_desc(struct vmctx *ctx, int vcpu, int reg,
 int	vm_set_register(struct vmctx *ctx, int vcpu, int reg, uint64_t val);
 int	vm_get_register(struct vmctx *ctx, int vcpu, int reg, uint64_t *retval);
 int	vm_run(struct vmctx *ctx, int vcpu, struct vm_exit *ret_vmexit);
+int	vm_suspend(struct vmctx *ctx, enum vm_suspend_how how);
+int	vm_reinit(struct vmctx *ctx);
 int	vm_apicid2vcpu(struct vmctx *ctx, int apicid);
 int	vm_inject_exception(struct vmctx *ctx, int vcpu, int vector,
     int errcode_valid, uint32_t errcode, int restart_instruction);
@@ -113,6 +172,13 @@ int	vm_setup_pptdev_msix(struct vmctx *ctx, int vcpu, int bus, int slot,
 	    int func, int idx, uint64_t addr, uint64_t msg,
 	    uint32_t vector_control);
 
+int	vm_get_intinfo(struct vmctx *ctx, int vcpu, uint64_t *i1, uint64_t *i2);
+int	vm_set_intinfo(struct vmctx *ctx, int vcpu, uint64_t exit_intinfo);
+
+#ifdef __FreeBSD__
+const cap_ioctl_t *vm_get_ioctls(size_t *len);
+#endif
+
 /*
  * Return a pointer to the statistics buffer. Note that this is not MT-safe.
  */
@@ -127,11 +193,16 @@ int	vm_get_hpet_capabilities(struct vmctx *ctx, uint32_t *capabilities);
 
 /*
  * Translate the GLA range [gla,gla+len) into GPA segments in 'iov'.
- * The 'iovcnt' should be big enough to accomodate all GPA segments.
- * Returns 0 on success, 1 on a guest fault condition and -1 otherwise.
+ * The 'iovcnt' should be big enough to accommodate all GPA segments.
+ *
+ * retval	fault		Interpretation
+ *   0		  0		Success
+ *   0		  1		An exception was injected into the guest
+ * EFAULT	 N/A		Error
  */
 int	vm_copy_setup(struct vmctx *ctx, int vcpu, struct vm_guest_paging *pg,
-	    uint64_t gla, size_t len, int prot, struct iovec *iov, int iovcnt);
+	    uint64_t gla, size_t len, int prot, struct iovec *iov, int iovcnt,
+	    int *fault);
 void	vm_copyin(struct vmctx *ctx, int vcpu, struct iovec *guest_iov,
 	    void *host_dst, size_t len);
 void	vm_copyout(struct vmctx *ctx, int vcpu, const void *host_src,
@@ -139,9 +210,17 @@ void	vm_copyout(struct vmctx *ctx, int vcpu, const void *host_src,
 void	vm_copy_teardown(struct vmctx *ctx, int vcpu, struct iovec *iov,
 	    int iovcnt);
 
+/* RTC */
+int	vm_rtc_write(struct vmctx *ctx, int offset, uint8_t value);
+int	vm_rtc_read(struct vmctx *ctx, int offset, uint8_t *retval);
+int	vm_rtc_settime(struct vmctx *ctx, time_t secs);
+int	vm_rtc_gettime(struct vmctx *ctx, time_t *secs);
+
 /* Reset vcpu register state */
 int	vcpu_reset(struct vmctx *ctx, int vcpu);
 
+int	vm_active_cpus(struct vmctx *ctx, cpuset_t *cpus);
+int	vm_suspended_cpus(struct vmctx *ctx, cpuset_t *cpus);
 int	vm_activate_cpu(struct vmctx *ctx, int vcpu);
 
 #ifdef	__FreeBSD__
diff --git a/usr/src/uts/i86pc/Makefile.files b/usr/src/uts/i86pc/Makefile.files
index 40eb4036cc..755c4b7270 100644
--- a/usr/src/uts/i86pc/Makefile.files
+++ b/usr/src/uts/i86pc/Makefile.files
@@ -243,7 +243,8 @@ VMM_OBJS += vmm.o \
 	vmm_instruction_emul.o \
 	vmm_ioport.o \
 	vmm_lapic.o \
-	vmm_sol_mem.o \
+	vmm_mem.o \
+	vmm_stat.o \
 	vmm_util.o \
 	x86.o \
 	vdev.o \
@@ -252,12 +253,22 @@ VMM_OBJS += vmm.o \
 	vhpet.o \
 	vioapic.o \
 	vlapic.o \
+	vrtc.o \
+	vpmtmr.o \
 	ept.o \
 	vmcs.o \
 	vmx_msr.o \
 	vmx.o \
 	vmx_support.o \
+	svm.o \
+	svm_msr.o \
+	npt.o \
+	vmcb.o \
+	svm_support.o \
 	amdv.o \
+	sol_iommu.o \
+	sol_ppt.o \
+	vmm_sol_vm.o \
 	vmm_sol_glue.o
 
 VIONA_OBJS += viona.o
diff --git a/usr/src/uts/i86pc/Makefile.i86pc b/usr/src/uts/i86pc/Makefile.i86pc
index 9a5a625b4c..36d5208530 100644
--- a/usr/src/uts/i86pc/Makefile.i86pc
+++ b/usr/src/uts/i86pc/Makefile.i86pc
@@ -24,6 +24,7 @@
 #
 # Copyright (c) 2005, 2010, Oracle and/or its affiliates. All rights reserved.
 # Copyright (c) 2013 Andrew Stormont.  All rights reserved.
+# Copyright 2017 Joyent, Inc.
 #
 #
 #	This makefile contains the common definitions for the i86pc unix
@@ -264,6 +265,7 @@ DRV_KMODS	+= amd_iommu
 DRV_KMODS	+= dr
 DRV_KMODS	+= ioat
 DRV_KMODS	+= fipe
+DRV_KMODS	+= vmm
 
 DRV_KMODS	+= cpudrv
 
diff --git a/usr/src/uts/i86pc/Makefile.rules b/usr/src/uts/i86pc/Makefile.rules
index 9bfbbe5e78..ea723f01be 100644
--- a/usr/src/uts/i86pc/Makefile.rules
+++ b/usr/src/uts/i86pc/Makefile.rules
@@ -235,6 +235,9 @@ $(OBJS_DIR)/%.o:		$(UTSBASE)/i86pc/io/vmm/io/%.c
 $(OBJS_DIR)/%.o:		$(UTSBASE)/i86pc/io/vmm/intel/%.s
 	$(COMPILE.s) -o $@ $<
 
+$(OBJS_DIR)/%.o:		$(UTSBASE)/i86pc/io/vmm/amd/%.s
+	$(COMPILE.s) -o $@ $<
+
 $(OBJS_DIR)/%.o:		$(UTSBASE)/i86pc/io/viona/%.c
 	$(COMPILE.c) -o $@ $<
 	$(CTFCONVERT_O)
diff --git a/usr/src/uts/i86pc/io/vmm/amd/amdv.c b/usr/src/uts/i86pc/io/vmm/amd/amdv.c
index 5cdf283a24..69812681d7 100644
--- a/usr/src/uts/i86pc/io/vmm/amd/amdv.c
+++ b/usr/src/uts/i86pc/io/vmm/amd/amdv.c
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/amd/amdv.c 245678 2013-01-20 03:42:49Z neel $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -40,140 +40,15 @@
  */
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/sys/amd64/vmm/amd/amdv.c 245678 2013-01-20 03:42:49Z neel $");
+__FBSDID("$FreeBSD$");
 
 #include <sys/param.h>
 #include <sys/systm.h>
 #include <sys/errno.h>
-#include <sys/smp.h>
 
 #include <machine/vmm.h>
-#ifdef	__FreeBSD__
 #include "io/iommu.h"
-#endif
 
-static int
-amdv_init(void)
-{
-
-	printf("amdv_init: not implemented\n");
-	return (ENXIO);
-}
-
-static int
-amdv_cleanup(void)
-{
-
-	printf("amdv_cleanup: not implemented\n");
-	return (ENXIO);
-}
-
-static void *
-amdv_vminit(struct vm *vm)
-{
-
-	printf("amdv_vminit: not implemented\n");
-	return (NULL);
-}
-
-static int
-amdv_vmrun(void *arg, int vcpu, register_t rip)
-{
-
-	printf("amdv_vmrun: not implemented\n");
-	return (ENXIO);
-}
-
-static void
-amdv_vmcleanup(void *arg)
-{
-
-	printf("amdv_vmcleanup: not implemented\n");
-	return;
-}
-
-static int
-amdv_vmmmap_set(void *arg, vm_paddr_t gpa, vm_paddr_t hpa, size_t length,
-	    vm_memattr_t attr, int prot, boolean_t spok)
-{
-
-	printf("amdv_vmmmap_set: not implemented\n");
-	return (EINVAL);
-}
-
-static vm_paddr_t
-amdv_vmmmap_get(void *arg, vm_paddr_t gpa)
-{
-
-	printf("amdv_vmmmap_get: not implemented\n");
-	return (EINVAL);
-}
-
-static int
-amdv_getreg(void *arg, int vcpu, int regnum, uint64_t *retval)
-{
-	
-	printf("amdv_getreg: not implemented\n");
-	return (EINVAL);
-}
-
-static int
-amdv_setreg(void *arg, int vcpu, int regnum, uint64_t val)
-{
-	
-	printf("amdv_setreg: not implemented\n");
-	return (EINVAL);
-}
-
-static int
-amdv_getdesc(void *vmi, int vcpu, int num, struct seg_desc *desc)
-{
-
-	printf("amdv_get_desc: not implemented\n");
-	return (EINVAL);
-}
-
-static int
-amdv_setdesc(void *vmi, int vcpu, int num, struct seg_desc *desc)
-{
-
-	printf("amdv_get_desc: not implemented\n");
-	return (EINVAL);
-}
-
-static int
-amdv_getcap(void *arg, int vcpu, int type, int *retval)
-{
-
-	printf("amdv_getcap: not implemented\n");
-	return (EINVAL);
-}
-
-static int
-amdv_setcap(void *arg, int vcpu, int type, int val)
-{
-
-	printf("amdv_setcap: not implemented\n");
-	return (EINVAL);
-}
-
-struct vmm_ops vmm_ops_amd = {
-	amdv_init,
-	amdv_cleanup,
-	amdv_vminit,
-	amdv_vmrun,
-	amdv_vmcleanup,
-	amdv_vmmmap_set,
-	amdv_vmmmap_get,
-	amdv_getreg,
-	amdv_setreg,
-	amdv_getdesc,
-	amdv_setdesc,
-	amdv_getcap,
-	amdv_setcap
-};
-
-#ifdef	__FreeBSD__
 static int
 amd_iommu_init(void)
 {
@@ -236,14 +111,14 @@ amd_iommu_remove_mapping(void *domain, vm_paddr_t gpa, uint64_t len)
 }
 
 static void
-amd_iommu_add_device(void *domain, int bus, int slot, int func)
+amd_iommu_add_device(void *domain, uint16_t rid)
 {
 
 	printf("amd_iommu_add_device: not implemented\n");
 }
 
 static void
-amd_iommu_remove_device(void *domain, int bus, int slot, int func)
+amd_iommu_remove_device(void *domain, uint16_t rid)
 {
 
 	printf("amd_iommu_remove_device: not implemented\n");
@@ -269,4 +144,3 @@ struct iommu_ops iommu_ops_amd = {
 	amd_iommu_remove_device,
 	amd_iommu_invalidate_tlb,
 };
-#endif
diff --git a/usr/src/uts/i86pc/io/vmm/amd/npt.c b/usr/src/uts/i86pc/io/vmm/amd/npt.c
new file mode 100644
index 0000000000..e1c1b79e1b
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/amd/npt.c
@@ -0,0 +1,85 @@
+/*-
+ * Copyright (c) 2013 Anish Gupta (akgupt3@gmail.com)
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/param.h>
+#include <sys/kernel.h>
+#include <sys/systm.h>
+#include <sys/sysctl.h>
+
+#include <vm/vm.h>
+#include <vm/pmap.h>
+#include <vm/vm_extern.h>
+
+#include "npt.h"
+
+SYSCTL_DECL(_hw_vmm);
+SYSCTL_NODE(_hw_vmm, OID_AUTO, npt, CTLFLAG_RW, NULL, NULL);
+
+static int npt_flags;
+SYSCTL_INT(_hw_vmm_npt, OID_AUTO, pmap_flags, CTLFLAG_RD,
+	&npt_flags, 0, NULL);
+
+#define NPT_IPIMASK	0xFF
+
+/*
+ * AMD nested page table init.
+ */
+int
+svm_npt_init(int ipinum)
+{
+	int enable_superpage = 1;
+
+	npt_flags = ipinum & NPT_IPIMASK;
+	TUNABLE_INT_FETCH("hw.vmm.npt.enable_superpage", &enable_superpage);
+	if (enable_superpage)
+		npt_flags |= PMAP_PDE_SUPERPAGE; 
+	
+	return (0);
+}
+
+static int
+npt_pinit(pmap_t pmap)
+{
+
+	return (pmap_pinit_type(pmap, PT_RVI, npt_flags));
+}
+
+struct vmspace *
+svm_npt_alloc(vm_offset_t min, vm_offset_t max)
+{
+	
+	return (vmspace_alloc(min, max, npt_pinit));
+}
+
+void
+svm_npt_free(struct vmspace *vmspace)
+{
+
+	vmspace_free(vmspace);
+}
diff --git a/usr/src/uts/i86pc/io/vmm/amd/npt.h b/usr/src/uts/i86pc/io/vmm/amd/npt.h
new file mode 100644
index 0000000000..5966474711
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/amd/npt.h
@@ -0,0 +1,36 @@
+/*-
+ * Copyright (c) 2013 Anish Gupta (akgupt3@gmail.com)
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _SVM_NPT_H_
+#define _SVM_NPT_H_
+
+int 	svm_npt_init(int ipinum);
+struct	vmspace *svm_npt_alloc(vm_offset_t min, vm_offset_t max);
+void	svm_npt_free(struct vmspace *vmspace);
+
+#endif /* _SVM_NPT_H_ */
diff --git a/usr/src/uts/i86pc/io/vmm/amd/offsets.in b/usr/src/uts/i86pc/io/vmm/amd/offsets.in
new file mode 100644
index 0000000000..f8d2a716d7
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/amd/offsets.in
@@ -0,0 +1,36 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2017 Joyent, Inc.
+ */
+#include <sys/types.h>
+
+#include "amd/svm.h"
+
+svm_regctx
+	sctx_rbx	SCTX_RBX
+	sctx_rcx	SCTX_RCX
+	sctx_rbp	SCTX_RBP
+	sctx_rdx	SCTX_RDX
+	sctx_rdi	SCTX_RDI
+	sctx_rsi	SCTX_RSI
+	sctx_r8		SCTX_R8
+	sctx_r9		SCTX_R9
+	sctx_r10	SCTX_R10
+	sctx_r11	SCTX_R11
+	sctx_r12	SCTX_R12
+	sctx_r13	SCTX_R13
+	sctx_r14	SCTX_R14
+	sctx_r15	SCTX_R15
+
+/* Pull in definition for MSR_GSBASE */
+\#include <machine/specialreg.h>
diff --git a/usr/src/uts/i86pc/io/vmm/amd/svm.c b/usr/src/uts/i86pc/io/vmm/amd/svm.c
new file mode 100644
index 0000000000..c6e3330779
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/amd/svm.c
@@ -0,0 +1,2272 @@
+/*-
+ * Copyright (c) 2013, Anish Gupta (akgupt3@gmail.com)
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/param.h>
+#include <sys/systm.h>
+#include <sys/smp.h>
+#include <sys/kernel.h>
+#include <sys/malloc.h>
+#include <sys/pcpu.h>
+#include <sys/proc.h>
+#include <sys/sysctl.h>
+
+#ifndef __FreeBSD__
+#include <sys/x86_archext.h>
+#endif
+
+#include <vm/vm.h>
+#include <vm/pmap.h>
+
+#include <machine/cpufunc.h>
+#include <machine/psl.h>
+#include <machine/md_var.h>
+#include <machine/specialreg.h>
+#include <machine/smp.h>
+#include <machine/vmm.h>
+#include <machine/vmm_dev.h>
+#include <machine/vmm_instruction_emul.h>
+
+#include "vmm_lapic.h"
+#include "vmm_stat.h"
+#include "vmm_ktr.h"
+#include "vmm_ioport.h"
+#include "vatpic.h"
+#include "vlapic.h"
+#include "vlapic_priv.h"
+
+#include "x86.h"
+#include "vmcb.h"
+#include "svm.h"
+#include "svm_softc.h"
+#include "svm_msr.h"
+#include "npt.h"
+
+SYSCTL_DECL(_hw_vmm);
+SYSCTL_NODE(_hw_vmm, OID_AUTO, svm, CTLFLAG_RW, NULL, NULL);
+
+/*
+ * SVM CPUID function 0x8000_000A, edx bit decoding.
+ */
+#define AMD_CPUID_SVM_NP		BIT(0)  /* Nested paging or RVI */
+#define AMD_CPUID_SVM_LBR		BIT(1)  /* Last branch virtualization */
+#define AMD_CPUID_SVM_SVML		BIT(2)  /* SVM lock */
+#define AMD_CPUID_SVM_NRIP_SAVE		BIT(3)  /* Next RIP is saved */
+#define AMD_CPUID_SVM_TSC_RATE		BIT(4)  /* TSC rate control. */
+#define AMD_CPUID_SVM_VMCB_CLEAN	BIT(5)  /* VMCB state caching */
+#define AMD_CPUID_SVM_FLUSH_BY_ASID	BIT(6)  /* Flush by ASID */
+#define AMD_CPUID_SVM_DECODE_ASSIST	BIT(7)  /* Decode assist */
+#define AMD_CPUID_SVM_PAUSE_INC		BIT(10) /* Pause intercept filter. */
+#define AMD_CPUID_SVM_PAUSE_FTH		BIT(12) /* Pause filter threshold */
+#define	AMD_CPUID_SVM_AVIC		BIT(13)	/* AVIC present */
+
+#define	VMCB_CACHE_DEFAULT	(VMCB_CACHE_ASID 	|	\
+				VMCB_CACHE_IOPM		|	\
+				VMCB_CACHE_I		|	\
+				VMCB_CACHE_TPR		|	\
+				VMCB_CACHE_CR2		|	\
+				VMCB_CACHE_CR		|	\
+				VMCB_CACHE_DT		|	\
+				VMCB_CACHE_SEG		|	\
+				VMCB_CACHE_NP)
+
+static uint32_t vmcb_clean = VMCB_CACHE_DEFAULT;
+SYSCTL_INT(_hw_vmm_svm, OID_AUTO, vmcb_clean, CTLFLAG_RDTUN, &vmcb_clean,
+    0, NULL);
+
+static MALLOC_DEFINE(M_SVM, "svm", "svm");
+static MALLOC_DEFINE(M_SVM_VLAPIC, "svm-vlapic", "svm-vlapic");
+
+#ifdef __FreeBSD__
+/* Per-CPU context area. */
+extern struct pcpu __pcpu[];
+#endif
+
+static uint32_t svm_feature = ~0U;	/* AMD SVM features. */
+SYSCTL_UINT(_hw_vmm_svm, OID_AUTO, features, CTLFLAG_RDTUN, &svm_feature, 0,
+    "SVM features advertised by CPUID.8000000AH:EDX");
+
+static int disable_npf_assist;
+SYSCTL_INT(_hw_vmm_svm, OID_AUTO, disable_npf_assist, CTLFLAG_RWTUN,
+    &disable_npf_assist, 0, NULL);
+
+/* Maximum ASIDs supported by the processor */
+static uint32_t nasid;
+SYSCTL_UINT(_hw_vmm_svm, OID_AUTO, num_asids, CTLFLAG_RDTUN, &nasid, 0,
+    "Number of ASIDs supported by this processor");
+
+/* Current ASID generation for each host cpu */
+static struct asid asid[MAXCPU];
+
+/* 
+ * SVM host state saved area of size 4KB for each core.
+ */
+static uint8_t hsave[MAXCPU][PAGE_SIZE] __aligned(PAGE_SIZE);
+
+static VMM_STAT_AMD(VCPU_EXITINTINFO, "VM exits during event delivery");
+static VMM_STAT_AMD(VCPU_INTINFO_INJECTED, "Events pending at VM entry");
+static VMM_STAT_AMD(VMEXIT_VINTR, "VM exits due to interrupt window");
+
+static int svm_setreg(void *arg, int vcpu, int ident, uint64_t val);
+
+static __inline int
+flush_by_asid(void)
+{
+
+	return (svm_feature & AMD_CPUID_SVM_FLUSH_BY_ASID);
+}
+
+static __inline int
+decode_assist(void)
+{
+
+	return (svm_feature & AMD_CPUID_SVM_DECODE_ASSIST);
+}
+
+static void
+svm_disable(void *arg __unused)
+{
+	uint64_t efer;
+
+	efer = rdmsr(MSR_EFER);
+	efer &= ~EFER_SVM;
+	wrmsr(MSR_EFER, efer);
+}
+
+/*
+ * Disable SVM on all CPUs.
+ */
+static int
+svm_cleanup(void)
+{
+
+	smp_rendezvous(NULL, svm_disable, NULL, NULL);
+	return (0);
+}
+
+/*
+ * Verify that all the features required by bhyve are available.
+ */
+static int
+check_svm_features(void)
+{
+	u_int regs[4];
+
+	/* CPUID Fn8000_000A is for SVM */
+	do_cpuid(0x8000000A, regs);
+	svm_feature &= regs[3];
+
+	/*
+	 * The number of ASIDs can be configured to be less than what is
+	 * supported by the hardware but not more.
+	 */
+	if (nasid == 0 || nasid > regs[1])
+		nasid = regs[1];
+	KASSERT(nasid > 1, ("Insufficient ASIDs for guests: %#x", nasid));
+
+	/* bhyve requires the Nested Paging feature */
+	if (!(svm_feature & AMD_CPUID_SVM_NP)) {
+		printf("SVM: Nested Paging feature not available.\n");
+		return (ENXIO);
+	}
+
+	/* bhyve requires the NRIP Save feature */
+	if (!(svm_feature & AMD_CPUID_SVM_NRIP_SAVE)) {
+		printf("SVM: NRIP Save feature not available.\n");
+		return (ENXIO);
+	}
+
+	return (0);
+}
+
+static void
+svm_enable(void *arg __unused)
+{
+	uint64_t efer;
+
+	efer = rdmsr(MSR_EFER);
+	efer |= EFER_SVM;
+	wrmsr(MSR_EFER, efer);
+
+	wrmsr(MSR_VM_HSAVE_PA, vtophys(hsave[curcpu]));
+}
+
+/*
+ * Return 1 if SVM is enabled on this processor and 0 otherwise.
+ */
+static int
+svm_available(void)
+{
+	uint64_t msr;
+
+#ifdef __FreeBSD__
+	/* Section 15.4 Enabling SVM from APM2. */
+	if ((amd_feature2 & AMDID2_SVM) == 0) {
+		printf("SVM: not available.\n");
+		return (0);
+	}
+#else
+	if (!is_x86_feature(x86_featureset, X86FSET_SVM)) {
+		cmn_err(CE_WARN, "processor does not support SVM operation\n");
+		return (0);
+	}
+#endif
+
+	msr = rdmsr(MSR_VM_CR);
+	if ((msr & VM_CR_SVMDIS) != 0) {
+#ifdef __FreeBSD__
+		printf("SVM: disabled by BIOS.\n");
+#else
+		cmn_err(CE_WARN, "SVM disabled by BIOS.\n");
+#endif
+		return (0);
+	}
+
+	return (1);
+}
+
+static int
+svm_init(int ipinum)
+{
+	int error, cpu;
+
+	if (!svm_available())
+		return (ENXIO);
+
+	error = check_svm_features();
+	if (error)
+		return (error);
+
+	vmcb_clean &= VMCB_CACHE_DEFAULT;
+
+	for (cpu = 0; cpu < MAXCPU; cpu++) {
+		/*
+		 * Initialize the host ASIDs to their "highest" valid values.
+		 *
+		 * The next ASID allocation will rollover both 'gen' and 'num'
+		 * and start off the sequence at {1,1}.
+		 */
+		asid[cpu].gen = ~0UL;
+		asid[cpu].num = nasid - 1;
+	}
+
+	svm_msr_init();
+	svm_npt_init(ipinum);
+
+	/* Enable SVM on all CPUs */
+	smp_rendezvous(NULL, svm_enable, NULL, NULL);
+
+	return (0);
+}
+
+static void
+svm_restore(void)
+{
+
+	svm_enable(NULL);
+}		
+
+/* Pentium compatible MSRs */
+#define MSR_PENTIUM_START 	0	
+#define MSR_PENTIUM_END 	0x1FFF
+/* AMD 6th generation and Intel compatible MSRs */
+#define MSR_AMD6TH_START 	0xC0000000UL	
+#define MSR_AMD6TH_END 		0xC0001FFFUL	
+/* AMD 7th and 8th generation compatible MSRs */
+#define MSR_AMD7TH_START 	0xC0010000UL	
+#define MSR_AMD7TH_END 		0xC0011FFFUL	
+
+/*
+ * Get the index and bit position for a MSR in permission bitmap.
+ * Two bits are used for each MSR: lower bit for read and higher bit for write.
+ */
+static int
+svm_msr_index(uint64_t msr, int *index, int *bit)
+{
+	uint32_t base, off;
+
+	*index = -1;
+	*bit = (msr % 4) * 2;
+	base = 0;
+
+	if (msr <= MSR_PENTIUM_END) {
+		*index = msr / 4;
+		return (0);
+	}
+
+	base += (MSR_PENTIUM_END - MSR_PENTIUM_START + 1); 
+	if (msr >= MSR_AMD6TH_START && msr <= MSR_AMD6TH_END) {
+		off = (msr - MSR_AMD6TH_START); 
+		*index = (off + base) / 4;
+		return (0);
+	} 
+
+	base += (MSR_AMD6TH_END - MSR_AMD6TH_START + 1);
+	if (msr >= MSR_AMD7TH_START && msr <= MSR_AMD7TH_END) {
+		off = (msr - MSR_AMD7TH_START);
+		*index = (off + base) / 4;
+		return (0);
+	}
+
+	return (EINVAL);
+}
+
+/*
+ * Allow vcpu to read or write the 'msr' without trapping into the hypervisor.
+ */
+static void
+svm_msr_perm(uint8_t *perm_bitmap, uint64_t msr, bool read, bool write)
+{
+	int index, bit, error;
+
+	error = svm_msr_index(msr, &index, &bit);
+	KASSERT(error == 0, ("%s: invalid msr %#lx", __func__, msr));
+	KASSERT(index >= 0 && index < SVM_MSR_BITMAP_SIZE,
+	    ("%s: invalid index %d for msr %#lx", __func__, index, msr));
+	KASSERT(bit >= 0 && bit <= 6, ("%s: invalid bit position %d "
+	    "msr %#lx", __func__, bit, msr));
+
+	if (read)
+		perm_bitmap[index] &= ~(1UL << bit);
+
+	if (write)
+		perm_bitmap[index] &= ~(2UL << bit);
+}
+
+static void
+svm_msr_rw_ok(uint8_t *perm_bitmap, uint64_t msr)
+{
+
+	svm_msr_perm(perm_bitmap, msr, true, true);
+}
+
+static void
+svm_msr_rd_ok(uint8_t *perm_bitmap, uint64_t msr)
+{
+
+	svm_msr_perm(perm_bitmap, msr, true, false);
+}
+
+static __inline int
+svm_get_intercept(struct svm_softc *sc, int vcpu, int idx, uint32_t bitmask)
+{
+	struct vmcb_ctrl *ctrl;
+
+	KASSERT(idx >=0 && idx < 5, ("invalid intercept index %d", idx));
+
+	ctrl = svm_get_vmcb_ctrl(sc, vcpu);
+	return (ctrl->intercept[idx] & bitmask ? 1 : 0);
+}
+
+static __inline void
+svm_set_intercept(struct svm_softc *sc, int vcpu, int idx, uint32_t bitmask,
+    int enabled)
+{
+	struct vmcb_ctrl *ctrl;
+	uint32_t oldval;
+
+	KASSERT(idx >=0 && idx < 5, ("invalid intercept index %d", idx));
+
+	ctrl = svm_get_vmcb_ctrl(sc, vcpu);
+	oldval = ctrl->intercept[idx];
+
+	if (enabled)
+		ctrl->intercept[idx] |= bitmask;
+	else
+		ctrl->intercept[idx] &= ~bitmask;
+
+	if (ctrl->intercept[idx] != oldval) {
+		svm_set_dirty(sc, vcpu, VMCB_CACHE_I);
+		VCPU_CTR3(sc->vm, vcpu, "intercept[%d] modified "
+		    "from %#x to %#x", idx, oldval, ctrl->intercept[idx]);
+	}
+}
+
+static __inline void
+svm_disable_intercept(struct svm_softc *sc, int vcpu, int off, uint32_t bitmask)
+{
+
+	svm_set_intercept(sc, vcpu, off, bitmask, 0);
+}
+
+static __inline void
+svm_enable_intercept(struct svm_softc *sc, int vcpu, int off, uint32_t bitmask)
+{
+
+	svm_set_intercept(sc, vcpu, off, bitmask, 1);
+}
+
+static void
+vmcb_init(struct svm_softc *sc, int vcpu, uint64_t iopm_base_pa,
+    uint64_t msrpm_base_pa, uint64_t np_pml4)
+{
+	struct vmcb_ctrl *ctrl;
+	struct vmcb_state *state;
+	uint32_t mask;
+	int n;
+
+	ctrl = svm_get_vmcb_ctrl(sc, vcpu);
+	state = svm_get_vmcb_state(sc, vcpu);
+
+	ctrl->iopm_base_pa = iopm_base_pa;
+	ctrl->msrpm_base_pa = msrpm_base_pa;
+
+	/* Enable nested paging */
+	ctrl->np_enable = 1;
+	ctrl->n_cr3 = np_pml4;
+
+	/*
+	 * Intercept accesses to the control registers that are not shadowed
+	 * in the VMCB - i.e. all except cr0, cr2, cr3, cr4 and cr8.
+	 */
+	for (n = 0; n < 16; n++) {
+		mask = (BIT(n) << 16) | BIT(n);
+		if (n == 0 || n == 2 || n == 3 || n == 4 || n == 8)
+			svm_disable_intercept(sc, vcpu, VMCB_CR_INTCPT, mask);
+		else
+			svm_enable_intercept(sc, vcpu, VMCB_CR_INTCPT, mask);
+	}
+
+
+	/*
+	 * Intercept everything when tracing guest exceptions otherwise
+	 * just intercept machine check exception.
+	 */
+	if (vcpu_trace_exceptions(sc->vm, vcpu)) {
+		for (n = 0; n < 32; n++) {
+			/*
+			 * Skip unimplemented vectors in the exception bitmap.
+			 */
+			if (n == 2 || n == 9) {
+				continue;
+			}
+			svm_enable_intercept(sc, vcpu, VMCB_EXC_INTCPT, BIT(n));
+		}
+	} else {
+		svm_enable_intercept(sc, vcpu, VMCB_EXC_INTCPT, BIT(IDT_MC));
+	}
+
+	/* Intercept various events (for e.g. I/O, MSR and CPUID accesses) */
+	svm_enable_intercept(sc, vcpu, VMCB_CTRL1_INTCPT, VMCB_INTCPT_IO);
+	svm_enable_intercept(sc, vcpu, VMCB_CTRL1_INTCPT, VMCB_INTCPT_MSR);
+	svm_enable_intercept(sc, vcpu, VMCB_CTRL1_INTCPT, VMCB_INTCPT_CPUID);
+	svm_enable_intercept(sc, vcpu, VMCB_CTRL1_INTCPT, VMCB_INTCPT_INTR);
+	svm_enable_intercept(sc, vcpu, VMCB_CTRL1_INTCPT, VMCB_INTCPT_INIT);
+	svm_enable_intercept(sc, vcpu, VMCB_CTRL1_INTCPT, VMCB_INTCPT_NMI);
+	svm_enable_intercept(sc, vcpu, VMCB_CTRL1_INTCPT, VMCB_INTCPT_SMI);
+	svm_enable_intercept(sc, vcpu, VMCB_CTRL1_INTCPT, VMCB_INTCPT_SHUTDOWN);
+	svm_enable_intercept(sc, vcpu, VMCB_CTRL1_INTCPT,
+	    VMCB_INTCPT_FERR_FREEZE);
+
+	svm_enable_intercept(sc, vcpu, VMCB_CTRL2_INTCPT, VMCB_INTCPT_MONITOR);
+	svm_enable_intercept(sc, vcpu, VMCB_CTRL2_INTCPT, VMCB_INTCPT_MWAIT);
+
+	/*
+	 * From section "Canonicalization and Consistency Checks" in APMv2
+	 * the VMRUN intercept bit must be set to pass the consistency check.
+	 */
+	svm_enable_intercept(sc, vcpu, VMCB_CTRL2_INTCPT, VMCB_INTCPT_VMRUN);
+
+	/*
+	 * The ASID will be set to a non-zero value just before VMRUN.
+	 */
+	ctrl->asid = 0;
+
+	/*
+	 * Section 15.21.1, Interrupt Masking in EFLAGS
+	 * Section 15.21.2, Virtualizing APIC.TPR
+	 *
+	 * This must be set for %rflag and %cr8 isolation of guest and host.
+	 */
+	ctrl->v_intr_masking = 1;
+
+	/* Enable Last Branch Record aka LBR for debugging */
+	ctrl->lbr_virt_en = 1;
+	state->dbgctl = BIT(0);
+
+	/* EFER_SVM must always be set when the guest is executing */
+	state->efer = EFER_SVM;
+
+	/* Set up the PAT to power-on state */
+	state->g_pat = PAT_VALUE(0, PAT_WRITE_BACK)	|
+	    PAT_VALUE(1, PAT_WRITE_THROUGH)	|
+	    PAT_VALUE(2, PAT_UNCACHED)		|
+	    PAT_VALUE(3, PAT_UNCACHEABLE)	|
+	    PAT_VALUE(4, PAT_WRITE_BACK)	|
+	    PAT_VALUE(5, PAT_WRITE_THROUGH)	|
+	    PAT_VALUE(6, PAT_UNCACHED)		|
+	    PAT_VALUE(7, PAT_UNCACHEABLE);
+}
+
+/*
+ * Initialize a virtual machine.
+ */
+static void *
+svm_vminit(struct vm *vm, pmap_t pmap)
+{
+	struct svm_softc *svm_sc;
+	struct svm_vcpu *vcpu;
+	vm_paddr_t msrpm_pa, iopm_pa, pml4_pa;
+	int i;
+
+	svm_sc = contigmalloc(sizeof (*svm_sc), M_SVM, M_WAITOK | M_ZERO,
+	    0, ~(vm_paddr_t)0, PAGE_SIZE, 0);
+	svm_sc->vm = vm;
+	svm_sc->nptp = (vm_offset_t)vtophys(pmap->pm_pml4);
+
+	/*
+	 * Intercept read and write accesses to all MSRs.
+	 */
+	memset(svm_sc->msr_bitmap, 0xFF, sizeof(svm_sc->msr_bitmap));
+
+	/*
+	 * Access to the following MSRs is redirected to the VMCB when the
+	 * guest is executing. Therefore it is safe to allow the guest to
+	 * read/write these MSRs directly without hypervisor involvement.
+	 */
+	svm_msr_rw_ok(svm_sc->msr_bitmap, MSR_GSBASE);
+	svm_msr_rw_ok(svm_sc->msr_bitmap, MSR_FSBASE);
+	svm_msr_rw_ok(svm_sc->msr_bitmap, MSR_KGSBASE);
+
+	svm_msr_rw_ok(svm_sc->msr_bitmap, MSR_STAR);
+	svm_msr_rw_ok(svm_sc->msr_bitmap, MSR_LSTAR);
+	svm_msr_rw_ok(svm_sc->msr_bitmap, MSR_CSTAR);
+	svm_msr_rw_ok(svm_sc->msr_bitmap, MSR_SF_MASK);
+	svm_msr_rw_ok(svm_sc->msr_bitmap, MSR_SYSENTER_CS_MSR);
+	svm_msr_rw_ok(svm_sc->msr_bitmap, MSR_SYSENTER_ESP_MSR);
+	svm_msr_rw_ok(svm_sc->msr_bitmap, MSR_SYSENTER_EIP_MSR);
+	svm_msr_rw_ok(svm_sc->msr_bitmap, MSR_PAT);
+
+	svm_msr_rd_ok(svm_sc->msr_bitmap, MSR_TSC);
+
+	/*
+	 * Intercept writes to make sure that the EFER_SVM bit is not cleared.
+	 */
+	svm_msr_rd_ok(svm_sc->msr_bitmap, MSR_EFER);
+
+	/* Intercept access to all I/O ports. */
+	memset(svm_sc->iopm_bitmap, 0xFF, sizeof(svm_sc->iopm_bitmap));
+
+	iopm_pa = vtophys(svm_sc->iopm_bitmap);
+	msrpm_pa = vtophys(svm_sc->msr_bitmap);
+	pml4_pa = svm_sc->nptp;
+	for (i = 0; i < VM_MAXCPU; i++) {
+		vcpu = svm_get_vcpu(svm_sc, i);
+		vcpu->nextrip = ~0;
+		vcpu->lastcpu = NOCPU;
+		vcpu->vmcb_pa = vtophys(&vcpu->vmcb);
+		vmcb_init(svm_sc, i, iopm_pa, msrpm_pa, pml4_pa);
+		svm_msr_guest_init(svm_sc, i);
+	}
+	return (svm_sc);
+}
+
+/*
+ * Collateral for a generic SVM VM-exit.
+ */
+static void
+vm_exit_svm(struct vm_exit *vme, uint64_t code, uint64_t info1, uint64_t info2)
+{
+
+	vme->exitcode = VM_EXITCODE_SVM;
+	vme->u.svm.exitcode = code;
+	vme->u.svm.exitinfo1 = info1;
+	vme->u.svm.exitinfo2 = info2;
+}
+
+static int
+svm_cpl(struct vmcb_state *state)
+{
+
+	/*
+	 * From APMv2:
+	 *   "Retrieve the CPL from the CPL field in the VMCB, not
+	 *    from any segment DPL"
+	 */
+	return (state->cpl);
+}
+
+static enum vm_cpu_mode
+svm_vcpu_mode(struct vmcb *vmcb)
+{
+	struct vmcb_segment seg;
+	struct vmcb_state *state;
+	int error;
+
+	state = &vmcb->state;
+
+	if (state->efer & EFER_LMA) {
+		error = vmcb_seg(vmcb, VM_REG_GUEST_CS, &seg);
+		KASSERT(error == 0, ("%s: vmcb_seg(cs) error %d", __func__,
+		    error));
+
+		/*
+		 * Section 4.8.1 for APM2, check if Code Segment has
+		 * Long attribute set in descriptor.
+		 */
+		if (seg.attrib & VMCB_CS_ATTRIB_L)
+			return (CPU_MODE_64BIT);
+		else
+			return (CPU_MODE_COMPATIBILITY);
+	} else  if (state->cr0 & CR0_PE) {
+		return (CPU_MODE_PROTECTED);
+	} else {
+		return (CPU_MODE_REAL);
+	}
+}
+
+static enum vm_paging_mode
+svm_paging_mode(uint64_t cr0, uint64_t cr4, uint64_t efer)
+{
+
+	if ((cr0 & CR0_PG) == 0)
+		return (PAGING_MODE_FLAT);
+	if ((cr4 & CR4_PAE) == 0)
+		return (PAGING_MODE_32);
+	if (efer & EFER_LME)
+		return (PAGING_MODE_64);
+	else
+		return (PAGING_MODE_PAE);
+}
+
+/*
+ * ins/outs utility routines
+ */
+static uint64_t
+svm_inout_str_index(struct svm_regctx *regs, int in)
+{
+	uint64_t val;
+
+	val = in ? regs->sctx_rdi : regs->sctx_rsi;
+
+	return (val);
+}
+
+static uint64_t
+svm_inout_str_count(struct svm_regctx *regs, int rep)
+{
+	uint64_t val;
+
+	val = rep ? regs->sctx_rcx : 1;
+
+	return (val);
+}
+
+static void
+svm_inout_str_seginfo(struct svm_softc *svm_sc, int vcpu, int64_t info1,
+    int in, struct vm_inout_str *vis)
+{
+	int error, s;
+
+	if (in) {
+		vis->seg_name = VM_REG_GUEST_ES;
+	} else {
+		/* The segment field has standard encoding */
+		s = (info1 >> 10) & 0x7;
+		vis->seg_name = vm_segment_name(s);
+	}
+
+	error = vmcb_getdesc(svm_sc, vcpu, vis->seg_name, &vis->seg_desc);
+	KASSERT(error == 0, ("%s: svm_getdesc error %d", __func__, error));
+}
+
+static int
+svm_inout_str_addrsize(uint64_t info1)
+{
+        uint32_t size;
+
+        size = (info1 >> 7) & 0x7;
+        switch (size) {
+        case 1:
+                return (2);     /* 16 bit */
+        case 2:
+                return (4);     /* 32 bit */
+        case 4:
+                return (8);     /* 64 bit */
+        default:
+                panic("%s: invalid size encoding %d", __func__, size);
+        }
+}
+
+static void
+svm_paging_info(struct vmcb *vmcb, struct vm_guest_paging *paging)
+{
+	struct vmcb_state *state;
+
+	state = &vmcb->state;
+	paging->cr3 = state->cr3;
+	paging->cpl = svm_cpl(state);
+	paging->cpu_mode = svm_vcpu_mode(vmcb);
+	paging->paging_mode = svm_paging_mode(state->cr0, state->cr4,
+	    state->efer);
+}
+
+#define	UNHANDLED 0
+
+/*
+ * Handle guest I/O intercept.
+ */
+static int
+svm_handle_io(struct svm_softc *svm_sc, int vcpu, struct vm_exit *vmexit)
+{
+	struct vmcb_ctrl *ctrl;
+	struct vmcb_state *state;
+	struct svm_regctx *regs;
+	struct vm_inout_str *vis;
+	uint64_t info1;
+	int inout_string;
+
+	state = svm_get_vmcb_state(svm_sc, vcpu);
+	ctrl  = svm_get_vmcb_ctrl(svm_sc, vcpu);
+	regs  = svm_get_guest_regctx(svm_sc, vcpu);
+
+	info1 = ctrl->exitinfo1;
+	inout_string = info1 & BIT(2) ? 1 : 0;
+
+	/*
+	 * The effective segment number in EXITINFO1[12:10] is populated
+	 * only if the processor has the DecodeAssist capability.
+	 *
+	 * XXX this is not specified explicitly in APMv2 but can be verified
+	 * empirically.
+	 */
+	if (inout_string && !decode_assist())
+		return (UNHANDLED);
+
+	vmexit->exitcode 	= VM_EXITCODE_INOUT;
+	vmexit->u.inout.in 	= (info1 & BIT(0)) ? 1 : 0;
+	vmexit->u.inout.string 	= inout_string;
+	vmexit->u.inout.rep 	= (info1 & BIT(3)) ? 1 : 0;
+	vmexit->u.inout.bytes 	= (info1 >> 4) & 0x7;
+	vmexit->u.inout.port 	= (uint16_t)(info1 >> 16);
+	vmexit->u.inout.eax 	= (uint32_t)(state->rax);
+
+	if (inout_string) {
+		vmexit->exitcode = VM_EXITCODE_INOUT_STR;
+		vis = &vmexit->u.inout_str;
+		svm_paging_info(svm_get_vmcb(svm_sc, vcpu), &vis->paging);
+		vis->rflags = state->rflags;
+		vis->cr0 = state->cr0;
+		vis->index = svm_inout_str_index(regs, vmexit->u.inout.in);
+		vis->count = svm_inout_str_count(regs, vmexit->u.inout.rep);
+		vis->addrsize = svm_inout_str_addrsize(info1);
+		svm_inout_str_seginfo(svm_sc, vcpu, info1,
+		    vmexit->u.inout.in, vis);
+	}
+
+	return (UNHANDLED);
+}
+
+static int
+npf_fault_type(uint64_t exitinfo1)
+{
+
+	if (exitinfo1 & VMCB_NPF_INFO1_W)
+		return (VM_PROT_WRITE);
+	else if (exitinfo1 & VMCB_NPF_INFO1_ID)
+		return (VM_PROT_EXECUTE);
+	else
+		return (VM_PROT_READ);
+}
+
+static bool
+svm_npf_emul_fault(uint64_t exitinfo1)
+{
+	
+	if (exitinfo1 & VMCB_NPF_INFO1_ID) {
+		return (false);
+	}
+
+	if (exitinfo1 & VMCB_NPF_INFO1_GPT) {
+		return (false);
+	}
+
+	if ((exitinfo1 & VMCB_NPF_INFO1_GPA) == 0) {
+		return (false);
+	}
+
+	return (true);	
+}
+
+static void
+svm_handle_inst_emul(struct vmcb *vmcb, uint64_t gpa, struct vm_exit *vmexit)
+{
+	struct vm_guest_paging *paging;
+	struct vmcb_segment seg;
+	struct vmcb_ctrl *ctrl;
+	char *inst_bytes;
+	int error, inst_len;
+
+	ctrl = &vmcb->ctrl;
+	paging = &vmexit->u.inst_emul.paging;
+
+	vmexit->exitcode = VM_EXITCODE_INST_EMUL;
+	vmexit->u.inst_emul.gpa = gpa;
+	vmexit->u.inst_emul.gla = VIE_INVALID_GLA;
+	svm_paging_info(vmcb, paging);
+
+	error = vmcb_seg(vmcb, VM_REG_GUEST_CS, &seg);
+	KASSERT(error == 0, ("%s: vmcb_seg(CS) error %d", __func__, error));
+
+	switch(paging->cpu_mode) {
+	case CPU_MODE_REAL:
+		vmexit->u.inst_emul.cs_base = seg.base;
+		vmexit->u.inst_emul.cs_d = 0;
+		break;
+	case CPU_MODE_PROTECTED:
+	case CPU_MODE_COMPATIBILITY:
+		vmexit->u.inst_emul.cs_base = seg.base;
+
+		/*
+		 * Section 4.8.1 of APM2, Default Operand Size or D bit.
+		 */
+		vmexit->u.inst_emul.cs_d = (seg.attrib & VMCB_CS_ATTRIB_D) ?
+		    1 : 0;
+		break;
+	default:
+		vmexit->u.inst_emul.cs_base = 0;
+		vmexit->u.inst_emul.cs_d = 0;
+		break;	
+	}
+
+	/*
+	 * Copy the instruction bytes into 'vie' if available.
+	 */
+	if (decode_assist() && !disable_npf_assist) {
+		inst_len = ctrl->inst_len;
+		inst_bytes = (char *)ctrl->inst_bytes;
+	} else {
+		inst_len = 0;
+		inst_bytes = NULL;
+	}
+	vie_init(&vmexit->u.inst_emul.vie, inst_bytes, inst_len);
+}
+
+#ifdef KTR
+static const char *
+intrtype_to_str(int intr_type)
+{
+	switch (intr_type) {
+	case VMCB_EVENTINJ_TYPE_INTR:
+		return ("hwintr");
+	case VMCB_EVENTINJ_TYPE_NMI:
+		return ("nmi");
+	case VMCB_EVENTINJ_TYPE_INTn:
+		return ("swintr");
+	case VMCB_EVENTINJ_TYPE_EXCEPTION:
+		return ("exception");
+	default:
+		panic("%s: unknown intr_type %d", __func__, intr_type);
+	}
+}
+#endif
+
+/*
+ * Inject an event to vcpu as described in section 15.20, "Event injection".
+ */
+static void
+svm_eventinject(struct svm_softc *sc, int vcpu, int intr_type, int vector,
+		 uint32_t error, bool ec_valid)
+{
+	struct vmcb_ctrl *ctrl;
+
+	ctrl = svm_get_vmcb_ctrl(sc, vcpu);
+
+	KASSERT((ctrl->eventinj & VMCB_EVENTINJ_VALID) == 0,
+	    ("%s: event already pending %#lx", __func__, ctrl->eventinj));
+
+	KASSERT(vector >=0 && vector <= 255, ("%s: invalid vector %d",
+	    __func__, vector));
+
+	switch (intr_type) {
+	case VMCB_EVENTINJ_TYPE_INTR:
+	case VMCB_EVENTINJ_TYPE_NMI:
+	case VMCB_EVENTINJ_TYPE_INTn:
+		break;
+	case VMCB_EVENTINJ_TYPE_EXCEPTION:
+		if (vector >= 0 && vector <= 31 && vector != 2)
+			break;
+		/* FALLTHROUGH */
+	default:
+		panic("%s: invalid intr_type/vector: %d/%d", __func__,
+		    intr_type, vector);
+	}
+	ctrl->eventinj = vector | (intr_type << 8) | VMCB_EVENTINJ_VALID;
+	if (ec_valid) {
+		ctrl->eventinj |= VMCB_EVENTINJ_EC_VALID;
+		ctrl->eventinj |= (uint64_t)error << 32;
+		VCPU_CTR3(sc->vm, vcpu, "Injecting %s at vector %d errcode %#x",
+		    intrtype_to_str(intr_type), vector, error);
+	} else {
+		VCPU_CTR2(sc->vm, vcpu, "Injecting %s at vector %d",
+		    intrtype_to_str(intr_type), vector);
+	}
+}
+
+static void
+svm_update_virqinfo(struct svm_softc *sc, int vcpu)
+{
+	struct vm *vm;
+	struct vlapic *vlapic;
+	struct vmcb_ctrl *ctrl;
+	int pending;
+
+	vm = sc->vm;
+	vlapic = vm_lapic(vm, vcpu);
+	ctrl = svm_get_vmcb_ctrl(sc, vcpu);
+
+	/* Update %cr8 in the emulated vlapic */
+	vlapic_set_cr8(vlapic, ctrl->v_tpr);
+
+	/*
+	 * If V_IRQ indicates that the interrupt injection attempted on then
+	 * last VMRUN was successful then update the vlapic accordingly.
+	 */
+	if (ctrl->v_intr_vector != 0) {
+		pending = ctrl->v_irq;
+		KASSERT(ctrl->v_intr_vector >= 16, ("%s: invalid "
+		    "v_intr_vector %d", __func__, ctrl->v_intr_vector));
+		KASSERT(!ctrl->v_ign_tpr, ("%s: invalid v_ign_tpr", __func__));
+		VCPU_CTR2(vm, vcpu, "v_intr_vector %d %s", ctrl->v_intr_vector,
+		    pending ? "pending" : "accepted");
+		if (!pending)
+			vlapic_intr_accepted(vlapic, ctrl->v_intr_vector);
+	}
+}
+
+static void
+svm_save_intinfo(struct svm_softc *svm_sc, int vcpu)
+{
+	struct vmcb_ctrl *ctrl;
+	uint64_t intinfo;
+
+	ctrl  = svm_get_vmcb_ctrl(svm_sc, vcpu);
+	intinfo = ctrl->exitintinfo;	
+	if (!VMCB_EXITINTINFO_VALID(intinfo))
+		return;
+
+	/*
+	 * From APMv2, Section "Intercepts during IDT interrupt delivery"
+	 *
+	 * If a #VMEXIT happened during event delivery then record the event
+	 * that was being delivered.
+	 */
+	VCPU_CTR2(svm_sc->vm, vcpu, "SVM:Pending INTINFO(0x%lx), vector=%d.\n",
+		intinfo, VMCB_EXITINTINFO_VECTOR(intinfo));
+	vmm_stat_incr(svm_sc->vm, vcpu, VCPU_EXITINTINFO, 1);
+	vm_exit_intinfo(svm_sc->vm, vcpu, intinfo);
+}
+
+static __inline int
+vintr_intercept_enabled(struct svm_softc *sc, int vcpu)
+{
+
+	return (svm_get_intercept(sc, vcpu, VMCB_CTRL1_INTCPT,
+	    VMCB_INTCPT_VINTR));
+}
+
+static __inline void
+enable_intr_window_exiting(struct svm_softc *sc, int vcpu)
+{
+	struct vmcb_ctrl *ctrl;
+
+	ctrl = svm_get_vmcb_ctrl(sc, vcpu);
+
+	if (ctrl->v_irq && ctrl->v_intr_vector == 0) {
+		KASSERT(ctrl->v_ign_tpr, ("%s: invalid v_ign_tpr", __func__));
+		KASSERT(vintr_intercept_enabled(sc, vcpu),
+		    ("%s: vintr intercept should be enabled", __func__));
+		return;
+	}
+
+	VCPU_CTR0(sc->vm, vcpu, "Enable intr window exiting");
+	ctrl->v_irq = 1;
+	ctrl->v_ign_tpr = 1;
+	ctrl->v_intr_vector = 0;
+	svm_set_dirty(sc, vcpu, VMCB_CACHE_TPR);
+	svm_enable_intercept(sc, vcpu, VMCB_CTRL1_INTCPT, VMCB_INTCPT_VINTR);
+}
+
+static __inline void
+disable_intr_window_exiting(struct svm_softc *sc, int vcpu)
+{
+	struct vmcb_ctrl *ctrl;
+
+	ctrl = svm_get_vmcb_ctrl(sc, vcpu);
+
+	if (!ctrl->v_irq && ctrl->v_intr_vector == 0) {
+		KASSERT(!vintr_intercept_enabled(sc, vcpu),
+		    ("%s: vintr intercept should be disabled", __func__));
+		return;
+	}
+
+#ifdef KTR
+	if (ctrl->v_intr_vector == 0)
+		VCPU_CTR0(sc->vm, vcpu, "Disable intr window exiting");
+	else
+		VCPU_CTR0(sc->vm, vcpu, "Clearing V_IRQ interrupt injection");
+#endif
+	ctrl->v_irq = 0;
+	ctrl->v_intr_vector = 0;
+	svm_set_dirty(sc, vcpu, VMCB_CACHE_TPR);
+	svm_disable_intercept(sc, vcpu, VMCB_CTRL1_INTCPT, VMCB_INTCPT_VINTR);
+}
+
+static int
+svm_modify_intr_shadow(struct svm_softc *sc, int vcpu, uint64_t val)
+{
+	struct vmcb_ctrl *ctrl;
+	int oldval, newval;
+
+	ctrl = svm_get_vmcb_ctrl(sc, vcpu);
+	oldval = ctrl->intr_shadow;
+	newval = val ? 1 : 0;
+	if (newval != oldval) {
+		ctrl->intr_shadow = newval;
+		VCPU_CTR1(sc->vm, vcpu, "Setting intr_shadow to %d", newval);
+	}
+	return (0);
+}
+
+static int
+svm_get_intr_shadow(struct svm_softc *sc, int vcpu, uint64_t *val)
+{
+	struct vmcb_ctrl *ctrl;
+
+	ctrl = svm_get_vmcb_ctrl(sc, vcpu);
+	*val = ctrl->intr_shadow;
+	return (0);
+}
+
+/*
+ * Once an NMI is injected it blocks delivery of further NMIs until the handler
+ * executes an IRET. The IRET intercept is enabled when an NMI is injected to
+ * to track when the vcpu is done handling the NMI.
+ */
+static int
+nmi_blocked(struct svm_softc *sc, int vcpu)
+{
+	int blocked;
+
+	blocked = svm_get_intercept(sc, vcpu, VMCB_CTRL1_INTCPT,
+	    VMCB_INTCPT_IRET);
+	return (blocked);
+}
+
+static void
+enable_nmi_blocking(struct svm_softc *sc, int vcpu)
+{
+
+	KASSERT(!nmi_blocked(sc, vcpu), ("vNMI already blocked"));
+	VCPU_CTR0(sc->vm, vcpu, "vNMI blocking enabled");
+	svm_enable_intercept(sc, vcpu, VMCB_CTRL1_INTCPT, VMCB_INTCPT_IRET);
+}
+
+static void
+clear_nmi_blocking(struct svm_softc *sc, int vcpu)
+{
+	int error;
+
+	KASSERT(nmi_blocked(sc, vcpu), ("vNMI already unblocked"));
+	VCPU_CTR0(sc->vm, vcpu, "vNMI blocking cleared");
+	/*
+	 * When the IRET intercept is cleared the vcpu will attempt to execute
+	 * the "iret" when it runs next. However, it is possible to inject
+	 * another NMI into the vcpu before the "iret" has actually executed.
+	 *
+	 * For e.g. if the "iret" encounters a #NPF when accessing the stack
+	 * it will trap back into the hypervisor. If an NMI is pending for
+	 * the vcpu it will be injected into the guest.
+	 *
+	 * XXX this needs to be fixed
+	 */
+	svm_disable_intercept(sc, vcpu, VMCB_CTRL1_INTCPT, VMCB_INTCPT_IRET);
+
+	/*
+	 * Set 'intr_shadow' to prevent an NMI from being injected on the
+	 * immediate VMRUN.
+	 */
+	error = svm_modify_intr_shadow(sc, vcpu, 1);
+	KASSERT(!error, ("%s: error %d setting intr_shadow", __func__, error));
+}
+
+#define	EFER_MBZ_BITS	0xFFFFFFFFFFFF0200UL
+
+static int
+svm_write_efer(struct svm_softc *sc, int vcpu, uint64_t newval, bool *retu)
+{
+	struct vm_exit *vme;
+	struct vmcb_state *state;
+	uint64_t changed, lma, oldval;
+	int error;
+
+	state = svm_get_vmcb_state(sc, vcpu);
+
+	oldval = state->efer;
+	VCPU_CTR2(sc->vm, vcpu, "wrmsr(efer) %#lx/%#lx", oldval, newval);
+
+	newval &= ~0xFE;		/* clear the Read-As-Zero (RAZ) bits */
+	changed = oldval ^ newval;
+
+	if (newval & EFER_MBZ_BITS)
+		goto gpf;
+
+	/* APMv2 Table 14-5 "Long-Mode Consistency Checks" */
+	if (changed & EFER_LME) {
+		if (state->cr0 & CR0_PG)
+			goto gpf;
+	}
+
+	/* EFER.LMA = EFER.LME & CR0.PG */
+	if ((newval & EFER_LME) != 0 && (state->cr0 & CR0_PG) != 0)
+		lma = EFER_LMA;
+	else
+		lma = 0;
+
+	if ((newval & EFER_LMA) != lma)
+		goto gpf;
+
+	if (newval & EFER_NXE) {
+		if (!vm_cpuid_capability(sc->vm, vcpu, VCC_NO_EXECUTE))
+			goto gpf;
+	}
+
+	/*
+	 * XXX bhyve does not enforce segment limits in 64-bit mode. Until
+	 * this is fixed flag guest attempt to set EFER_LMSLE as an error.
+	 */
+	if (newval & EFER_LMSLE) {
+		vme = vm_exitinfo(sc->vm, vcpu);
+		vm_exit_svm(vme, VMCB_EXIT_MSR, 1, 0);
+		*retu = true;
+		return (0);
+	}
+
+	if (newval & EFER_FFXSR) {
+		if (!vm_cpuid_capability(sc->vm, vcpu, VCC_FFXSR))
+			goto gpf;
+	}
+
+	if (newval & EFER_TCE) {
+		if (!vm_cpuid_capability(sc->vm, vcpu, VCC_TCE))
+			goto gpf;
+	}
+
+	error = svm_setreg(sc, vcpu, VM_REG_GUEST_EFER, newval);
+	KASSERT(error == 0, ("%s: error %d updating efer", __func__, error));
+	return (0);
+gpf:
+	vm_inject_gp(sc->vm, vcpu);
+	return (0);
+}
+
+static int
+emulate_wrmsr(struct svm_softc *sc, int vcpu, u_int num, uint64_t val,
+    bool *retu)
+{
+	int error;
+
+	if (lapic_msr(num))
+		error = lapic_wrmsr(sc->vm, vcpu, num, val, retu);
+	else if (num == MSR_EFER)
+		error = svm_write_efer(sc, vcpu, val, retu);
+	else
+		error = svm_wrmsr(sc, vcpu, num, val, retu);
+
+	return (error);
+}
+
+static int
+emulate_rdmsr(struct svm_softc *sc, int vcpu, u_int num, bool *retu)
+{
+	struct vmcb_state *state;
+	struct svm_regctx *ctx;
+	uint64_t result;
+	int error;
+
+	if (lapic_msr(num))
+		error = lapic_rdmsr(sc->vm, vcpu, num, &result, retu);
+	else
+		error = svm_rdmsr(sc, vcpu, num, &result, retu);
+
+	if (error == 0) {
+		state = svm_get_vmcb_state(sc, vcpu);
+		ctx = svm_get_guest_regctx(sc, vcpu);
+		state->rax = result & 0xffffffff;
+		ctx->sctx_rdx = result >> 32;
+	}
+
+	return (error);
+}
+
+#ifdef KTR
+static const char *
+exit_reason_to_str(uint64_t reason)
+{
+	static char reasonbuf[32];
+
+	switch (reason) {
+	case VMCB_EXIT_INVALID:
+		return ("invalvmcb");
+	case VMCB_EXIT_SHUTDOWN:
+		return ("shutdown");
+	case VMCB_EXIT_NPF:
+		return ("nptfault");
+	case VMCB_EXIT_PAUSE:
+		return ("pause");
+	case VMCB_EXIT_HLT:
+		return ("hlt");
+	case VMCB_EXIT_CPUID:
+		return ("cpuid");
+	case VMCB_EXIT_IO:
+		return ("inout");
+	case VMCB_EXIT_MC:
+		return ("mchk");
+	case VMCB_EXIT_INTR:
+		return ("extintr");
+	case VMCB_EXIT_NMI:
+		return ("nmi");
+	case VMCB_EXIT_VINTR:
+		return ("vintr");
+	case VMCB_EXIT_MSR:
+		return ("msr");
+	case VMCB_EXIT_IRET:
+		return ("iret");
+	case VMCB_EXIT_MONITOR:
+		return ("monitor");
+	case VMCB_EXIT_MWAIT:
+		return ("mwait");
+	default:
+		snprintf(reasonbuf, sizeof(reasonbuf), "%#lx", reason);
+		return (reasonbuf);
+	}
+}
+#endif	/* KTR */
+
+/*
+ * From section "State Saved on Exit" in APMv2: nRIP is saved for all #VMEXITs
+ * that are due to instruction intercepts as well as MSR and IOIO intercepts
+ * and exceptions caused by INT3, INTO and BOUND instructions.
+ *
+ * Return 1 if the nRIP is valid and 0 otherwise.
+ */
+static int
+nrip_valid(uint64_t exitcode)
+{
+	switch (exitcode) {
+	case 0x00 ... 0x0F:	/* read of CR0 through CR15 */
+	case 0x10 ... 0x1F:	/* write of CR0 through CR15 */
+	case 0x20 ... 0x2F:	/* read of DR0 through DR15 */
+	case 0x30 ... 0x3F:	/* write of DR0 through DR15 */
+	case 0x43:		/* INT3 */
+	case 0x44:		/* INTO */
+	case 0x45:		/* BOUND */
+	case 0x65 ... 0x7C:	/* VMEXIT_CR0_SEL_WRITE ... VMEXIT_MSR */
+	case 0x80 ... 0x8D:	/* VMEXIT_VMRUN ... VMEXIT_XSETBV */
+		return (1);
+	default:
+		return (0);
+	}
+}
+
+static int
+svm_vmexit(struct svm_softc *svm_sc, int vcpu, struct vm_exit *vmexit)
+{
+	struct vmcb *vmcb;
+	struct vmcb_state *state;
+	struct vmcb_ctrl *ctrl;
+	struct svm_regctx *ctx;
+	uint64_t code, info1, info2, val;
+	uint32_t eax, ecx, edx;
+	int error, errcode_valid, handled, idtvec, reflect;
+	bool retu;
+
+	ctx = svm_get_guest_regctx(svm_sc, vcpu);
+	vmcb = svm_get_vmcb(svm_sc, vcpu);
+	state = &vmcb->state;
+	ctrl = &vmcb->ctrl;
+
+	handled = 0;
+	code = ctrl->exitcode;
+	info1 = ctrl->exitinfo1;
+	info2 = ctrl->exitinfo2;
+
+	vmexit->exitcode = VM_EXITCODE_BOGUS;
+	vmexit->rip = state->rip;
+	vmexit->inst_length = nrip_valid(code) ? ctrl->nrip - state->rip : 0;
+
+	vmm_stat_incr(svm_sc->vm, vcpu, VMEXIT_COUNT, 1);
+
+	/*
+	 * #VMEXIT(INVALID) needs to be handled early because the VMCB is
+	 * in an inconsistent state and can trigger assertions that would
+	 * never happen otherwise.
+	 */
+	if (code == VMCB_EXIT_INVALID) {
+		vm_exit_svm(vmexit, code, info1, info2);
+		return (0);
+	}
+
+	KASSERT((ctrl->eventinj & VMCB_EVENTINJ_VALID) == 0, ("%s: event "
+	    "injection valid bit is set %#lx", __func__, ctrl->eventinj));
+
+	KASSERT(vmexit->inst_length >= 0 && vmexit->inst_length <= 15,
+	    ("invalid inst_length %d: code (%#lx), info1 (%#lx), info2 (%#lx)",
+	    vmexit->inst_length, code, info1, info2));
+
+	svm_update_virqinfo(svm_sc, vcpu);
+	svm_save_intinfo(svm_sc, vcpu);
+
+	switch (code) {
+	case VMCB_EXIT_IRET:
+		/*
+		 * Restart execution at "iret" but with the intercept cleared.
+		 */
+		vmexit->inst_length = 0;
+		clear_nmi_blocking(svm_sc, vcpu);
+		handled = 1;
+		break;
+	case VMCB_EXIT_VINTR:	/* interrupt window exiting */
+		vmm_stat_incr(svm_sc->vm, vcpu, VMEXIT_VINTR, 1);
+		handled = 1;
+		break;
+	case VMCB_EXIT_INTR:	/* external interrupt */
+		vmm_stat_incr(svm_sc->vm, vcpu, VMEXIT_EXTINT, 1);
+		handled = 1;
+		break;
+	case VMCB_EXIT_NMI:	/* external NMI */
+		handled = 1;
+		break;
+	case 0x40 ... 0x5F:
+		vmm_stat_incr(svm_sc->vm, vcpu, VMEXIT_EXCEPTION, 1);
+		reflect = 1;
+		idtvec = code - 0x40;
+		switch (idtvec) {
+		case IDT_MC:
+			/*
+			 * Call the machine check handler by hand. Also don't
+			 * reflect the machine check back into the guest.
+			 */
+			reflect = 0;
+			VCPU_CTR0(svm_sc->vm, vcpu, "Vectoring to MCE handler");
+			__asm __volatile("int $18");
+			break;
+		case IDT_PF:
+			error = svm_setreg(svm_sc, vcpu, VM_REG_GUEST_CR2,
+			    info2);
+			KASSERT(error == 0, ("%s: error %d updating cr2",
+			    __func__, error));
+			/* fallthru */
+		case IDT_NP:
+		case IDT_SS:
+		case IDT_GP:
+		case IDT_AC:
+		case IDT_TS:
+			errcode_valid = 1;
+			break;
+
+		case IDT_DF:
+			errcode_valid = 1;
+			info1 = 0;
+			break;
+
+		case IDT_BP:
+		case IDT_OF:
+		case IDT_BR:
+			/*
+			 * The 'nrip' field is populated for INT3, INTO and
+			 * BOUND exceptions and this also implies that
+			 * 'inst_length' is non-zero.
+			 *
+			 * Reset 'inst_length' to zero so the guest %rip at
+			 * event injection is identical to what it was when
+			 * the exception originally happened.
+			 */
+			VCPU_CTR2(svm_sc->vm, vcpu, "Reset inst_length from %d "
+			    "to zero before injecting exception %d",
+			    vmexit->inst_length, idtvec);
+			vmexit->inst_length = 0;
+			/* fallthru */
+		default:
+			errcode_valid = 0;
+			info1 = 0;
+			break;
+		}
+		KASSERT(vmexit->inst_length == 0, ("invalid inst_length (%d) "
+		    "when reflecting exception %d into guest",
+		    vmexit->inst_length, idtvec));
+
+		if (reflect) {
+			/* Reflect the exception back into the guest */
+			VCPU_CTR2(svm_sc->vm, vcpu, "Reflecting exception "
+			    "%d/%#x into the guest", idtvec, (int)info1);
+			error = vm_inject_exception(svm_sc->vm, vcpu, idtvec,
+			    errcode_valid, info1, 0);
+			KASSERT(error == 0, ("%s: vm_inject_exception error %d",
+			    __func__, error));
+		}
+		handled = 1;
+		break;
+	case VMCB_EXIT_MSR:	/* MSR access. */
+		eax = state->rax;
+		ecx = ctx->sctx_rcx;
+		edx = ctx->sctx_rdx;
+		retu = false;	
+
+		if (info1) {
+			vmm_stat_incr(svm_sc->vm, vcpu, VMEXIT_WRMSR, 1);
+			val = (uint64_t)edx << 32 | eax;
+			VCPU_CTR2(svm_sc->vm, vcpu, "wrmsr %#x val %#lx",
+			    ecx, val);
+			if (emulate_wrmsr(svm_sc, vcpu, ecx, val, &retu)) {
+				vmexit->exitcode = VM_EXITCODE_WRMSR;
+				vmexit->u.msr.code = ecx;
+				vmexit->u.msr.wval = val;
+			} else if (!retu) {
+				handled = 1;
+			} else {
+				KASSERT(vmexit->exitcode != VM_EXITCODE_BOGUS,
+				    ("emulate_wrmsr retu with bogus exitcode"));
+			}
+		} else {
+			VCPU_CTR1(svm_sc->vm, vcpu, "rdmsr %#x", ecx);
+			vmm_stat_incr(svm_sc->vm, vcpu, VMEXIT_RDMSR, 1);
+			if (emulate_rdmsr(svm_sc, vcpu, ecx, &retu)) {
+				vmexit->exitcode = VM_EXITCODE_RDMSR;
+				vmexit->u.msr.code = ecx;
+			} else if (!retu) {
+				handled = 1;
+			} else {
+				KASSERT(vmexit->exitcode != VM_EXITCODE_BOGUS,
+				    ("emulate_rdmsr retu with bogus exitcode"));
+			}
+		}
+		break;
+	case VMCB_EXIT_IO:
+		handled = svm_handle_io(svm_sc, vcpu, vmexit);
+		vmm_stat_incr(svm_sc->vm, vcpu, VMEXIT_INOUT, 1);
+		break;
+	case VMCB_EXIT_CPUID:
+		vmm_stat_incr(svm_sc->vm, vcpu, VMEXIT_CPUID, 1);
+		handled = x86_emulate_cpuid(svm_sc->vm, vcpu,
+		    (uint32_t *)&state->rax,
+		    (uint32_t *)&ctx->sctx_rbx,
+		    (uint32_t *)&ctx->sctx_rcx,
+		    (uint32_t *)&ctx->sctx_rdx);
+		break;
+	case VMCB_EXIT_HLT:
+		vmm_stat_incr(svm_sc->vm, vcpu, VMEXIT_HLT, 1);
+		vmexit->exitcode = VM_EXITCODE_HLT;
+		vmexit->u.hlt.rflags = state->rflags;
+		break;
+	case VMCB_EXIT_PAUSE:
+		vmexit->exitcode = VM_EXITCODE_PAUSE;
+		vmm_stat_incr(svm_sc->vm, vcpu, VMEXIT_PAUSE, 1);
+		break;
+	case VMCB_EXIT_NPF:
+		/* EXITINFO2 contains the faulting guest physical address */
+		if (info1 & VMCB_NPF_INFO1_RSV) {
+			VCPU_CTR2(svm_sc->vm, vcpu, "nested page fault with "
+			    "reserved bits set: info1(%#lx) info2(%#lx)",
+			    info1, info2);
+		} else if (vm_mem_allocated(svm_sc->vm, vcpu, info2)) {
+			vmexit->exitcode = VM_EXITCODE_PAGING;
+			vmexit->u.paging.gpa = info2;
+			vmexit->u.paging.fault_type = npf_fault_type(info1);
+			vmm_stat_incr(svm_sc->vm, vcpu, VMEXIT_NESTED_FAULT, 1);
+			VCPU_CTR3(svm_sc->vm, vcpu, "nested page fault "
+			    "on gpa %#lx/%#lx at rip %#lx",
+			    info2, info1, state->rip);
+		} else if (svm_npf_emul_fault(info1)) {
+			svm_handle_inst_emul(vmcb, info2, vmexit);
+			vmm_stat_incr(svm_sc->vm, vcpu, VMEXIT_INST_EMUL, 1);
+			VCPU_CTR3(svm_sc->vm, vcpu, "inst_emul fault "
+			    "for gpa %#lx/%#lx at rip %#lx",
+			    info2, info1, state->rip);
+		}
+		break;
+	case VMCB_EXIT_MONITOR:
+		vmexit->exitcode = VM_EXITCODE_MONITOR;
+		break;
+	case VMCB_EXIT_MWAIT:
+		vmexit->exitcode = VM_EXITCODE_MWAIT;
+		break;
+	default:
+		vmm_stat_incr(svm_sc->vm, vcpu, VMEXIT_UNKNOWN, 1);
+		break;
+	}	
+
+	VCPU_CTR4(svm_sc->vm, vcpu, "%s %s vmexit at %#lx/%d",
+	    handled ? "handled" : "unhandled", exit_reason_to_str(code),
+	    vmexit->rip, vmexit->inst_length);
+
+	if (handled) {
+		vmexit->rip += vmexit->inst_length;
+		vmexit->inst_length = 0;
+		state->rip = vmexit->rip;
+	} else {
+		if (vmexit->exitcode == VM_EXITCODE_BOGUS) {
+			/*
+			 * If this VM exit was not claimed by anybody then
+			 * treat it as a generic SVM exit.
+			 */
+			vm_exit_svm(vmexit, code, info1, info2);
+		} else {
+			/*
+			 * The exitcode and collateral have been populated.
+			 * The VM exit will be processed further in userland.
+			 */
+		}
+	}
+	return (handled);
+}
+
+static void
+svm_inj_intinfo(struct svm_softc *svm_sc, int vcpu)
+{
+	uint64_t intinfo;
+
+	if (!vm_entry_intinfo(svm_sc->vm, vcpu, &intinfo))
+		return;
+
+	KASSERT(VMCB_EXITINTINFO_VALID(intinfo), ("%s: entry intinfo is not "
+	    "valid: %#lx", __func__, intinfo));
+
+	svm_eventinject(svm_sc, vcpu, VMCB_EXITINTINFO_TYPE(intinfo),
+		VMCB_EXITINTINFO_VECTOR(intinfo),
+		VMCB_EXITINTINFO_EC(intinfo),
+		VMCB_EXITINTINFO_EC_VALID(intinfo));
+	vmm_stat_incr(svm_sc->vm, vcpu, VCPU_INTINFO_INJECTED, 1);
+	VCPU_CTR1(svm_sc->vm, vcpu, "Injected entry intinfo: %#lx", intinfo);
+}
+
+/*
+ * Inject event to virtual cpu.
+ */
+static void
+svm_inj_interrupts(struct svm_softc *sc, int vcpu, struct vlapic *vlapic)
+{
+	struct vmcb_ctrl *ctrl;
+	struct vmcb_state *state;
+	struct svm_vcpu *vcpustate;
+	uint8_t v_tpr;
+	int vector, need_intr_window, pending_apic_vector;
+
+	state = svm_get_vmcb_state(sc, vcpu);
+	ctrl  = svm_get_vmcb_ctrl(sc, vcpu);
+	vcpustate = svm_get_vcpu(sc, vcpu);
+
+	need_intr_window = 0;
+	pending_apic_vector = 0;
+
+	if (vcpustate->nextrip != state->rip) {
+		ctrl->intr_shadow = 0;
+		VCPU_CTR2(sc->vm, vcpu, "Guest interrupt blocking "
+		    "cleared due to rip change: %#lx/%#lx",
+		    vcpustate->nextrip, state->rip);
+	}
+
+	/*
+	 * Inject pending events or exceptions for this vcpu.
+	 *
+	 * An event might be pending because the previous #VMEXIT happened
+	 * during event delivery (i.e. ctrl->exitintinfo).
+	 *
+	 * An event might also be pending because an exception was injected
+	 * by the hypervisor (e.g. #PF during instruction emulation).
+	 */
+	svm_inj_intinfo(sc, vcpu);
+
+	/* NMI event has priority over interrupts. */
+	if (vm_nmi_pending(sc->vm, vcpu)) {
+		if (nmi_blocked(sc, vcpu)) {
+			/*
+			 * Can't inject another NMI if the guest has not
+			 * yet executed an "iret" after the last NMI.
+			 */
+			VCPU_CTR0(sc->vm, vcpu, "Cannot inject NMI due "
+			    "to NMI-blocking");
+		} else if (ctrl->intr_shadow) {
+			/*
+			 * Can't inject an NMI if the vcpu is in an intr_shadow.
+			 */
+			VCPU_CTR0(sc->vm, vcpu, "Cannot inject NMI due to "
+			    "interrupt shadow");
+			need_intr_window = 1;
+			goto done;
+		} else if (ctrl->eventinj & VMCB_EVENTINJ_VALID) {
+			/*
+			 * If there is already an exception/interrupt pending
+			 * then defer the NMI until after that.
+			 */
+			VCPU_CTR1(sc->vm, vcpu, "Cannot inject NMI due to "
+			    "eventinj %#lx", ctrl->eventinj);
+
+			/*
+			 * Use self-IPI to trigger a VM-exit as soon as
+			 * possible after the event injection is completed.
+			 *
+			 * This works only if the external interrupt exiting
+			 * is at a lower priority than the event injection.
+			 *
+			 * Although not explicitly specified in APMv2 the
+			 * relative priorities were verified empirically.
+			 */
+			ipi_cpu(curcpu, IPI_AST);	/* XXX vmm_ipinum? */
+		} else {
+			vm_nmi_clear(sc->vm, vcpu);
+
+			/* Inject NMI, vector number is not used */
+			svm_eventinject(sc, vcpu, VMCB_EVENTINJ_TYPE_NMI,
+			    IDT_NMI, 0, false);
+
+			/* virtual NMI blocking is now in effect */
+			enable_nmi_blocking(sc, vcpu);
+
+			VCPU_CTR0(sc->vm, vcpu, "Injecting vNMI");
+		}
+	}
+
+	if (!vm_extint_pending(sc->vm, vcpu)) {
+		/*
+		 * APIC interrupts are delivered using the V_IRQ offload.
+		 *
+		 * The primary benefit is that the hypervisor doesn't need to
+		 * deal with the various conditions that inhibit interrupts.
+		 * It also means that TPR changes via CR8 will be handled
+		 * without any hypervisor involvement.
+		 *
+		 * Note that the APIC vector must remain pending in the vIRR
+		 * until it is confirmed that it was delivered to the guest.
+		 * This can be confirmed based on the value of V_IRQ at the
+		 * next #VMEXIT (1 = pending, 0 = delivered).
+		 *
+		 * Also note that it is possible that another higher priority
+		 * vector can become pending before this vector is delivered
+		 * to the guest. This is alright because vcpu_notify_event()
+		 * will send an IPI and force the vcpu to trap back into the
+		 * hypervisor. The higher priority vector will be injected on
+		 * the next VMRUN.
+		 */
+		if (vlapic_pending_intr(vlapic, &vector)) {
+			KASSERT(vector >= 16 && vector <= 255,
+			    ("invalid vector %d from local APIC", vector));
+			pending_apic_vector = vector;
+		}
+		goto done;
+	}
+
+	/* Ask the legacy pic for a vector to inject */
+	vatpic_pending_intr(sc->vm, &vector);
+	KASSERT(vector >= 0 && vector <= 255, ("invalid vector %d from INTR",
+	    vector));
+
+	/*
+	 * If the guest has disabled interrupts or is in an interrupt shadow
+	 * then we cannot inject the pending interrupt.
+	 */
+	if ((state->rflags & PSL_I) == 0) {
+		VCPU_CTR2(sc->vm, vcpu, "Cannot inject vector %d due to "
+		    "rflags %#lx", vector, state->rflags);
+		need_intr_window = 1;
+		goto done;
+	}
+
+	if (ctrl->intr_shadow) {
+		VCPU_CTR1(sc->vm, vcpu, "Cannot inject vector %d due to "
+		    "interrupt shadow", vector);
+		need_intr_window = 1;
+		goto done;
+	}
+
+	if (ctrl->eventinj & VMCB_EVENTINJ_VALID) {
+		VCPU_CTR2(sc->vm, vcpu, "Cannot inject vector %d due to "
+		    "eventinj %#lx", vector, ctrl->eventinj);
+		need_intr_window = 1;
+		goto done;
+	}
+
+	/*
+	 * Legacy PIC interrupts are delivered via the event injection
+	 * mechanism.
+	 */
+	svm_eventinject(sc, vcpu, VMCB_EVENTINJ_TYPE_INTR, vector, 0, false);
+
+	vm_extint_clear(sc->vm, vcpu);
+	vatpic_intr_accepted(sc->vm, vector);
+
+	/*
+	 * Force a VM-exit as soon as the vcpu is ready to accept another
+	 * interrupt. This is done because the PIC might have another vector
+	 * that it wants to inject. Also, if the APIC has a pending interrupt
+	 * that was preempted by the ExtInt then it allows us to inject the
+	 * APIC vector as soon as possible.
+	 */
+	need_intr_window = 1;
+done:
+	/*
+	 * The guest can modify the TPR by writing to %CR8. In guest mode
+	 * the processor reflects this write to V_TPR without hypervisor
+	 * intervention.
+	 *
+	 * The guest can also modify the TPR by writing to it via the memory
+	 * mapped APIC page. In this case, the write will be emulated by the
+	 * hypervisor. For this reason V_TPR must be updated before every
+	 * VMRUN.
+	 */
+	v_tpr = vlapic_get_cr8(vlapic);
+	KASSERT(v_tpr <= 15, ("invalid v_tpr %#x", v_tpr));
+	if (ctrl->v_tpr != v_tpr) {
+		VCPU_CTR2(sc->vm, vcpu, "VMCB V_TPR changed from %#x to %#x",
+		    ctrl->v_tpr, v_tpr);
+		ctrl->v_tpr = v_tpr;
+		svm_set_dirty(sc, vcpu, VMCB_CACHE_TPR);
+	}
+
+	if (pending_apic_vector) {
+		/*
+		 * If an APIC vector is being injected then interrupt window
+		 * exiting is not possible on this VMRUN.
+		 */
+		KASSERT(!need_intr_window, ("intr_window exiting impossible"));
+		VCPU_CTR1(sc->vm, vcpu, "Injecting vector %d using V_IRQ",
+		    pending_apic_vector);
+
+		ctrl->v_irq = 1;
+		ctrl->v_ign_tpr = 0;
+		ctrl->v_intr_vector = pending_apic_vector;
+		ctrl->v_intr_prio = pending_apic_vector >> 4;
+		svm_set_dirty(sc, vcpu, VMCB_CACHE_TPR);
+	} else if (need_intr_window) {
+		/*
+		 * We use V_IRQ in conjunction with the VINTR intercept to
+		 * trap into the hypervisor as soon as a virtual interrupt
+		 * can be delivered.
+		 *
+		 * Since injected events are not subject to intercept checks
+		 * we need to ensure that the V_IRQ is not actually going to
+		 * be delivered on VM entry. The KASSERT below enforces this.
+		 */
+		KASSERT((ctrl->eventinj & VMCB_EVENTINJ_VALID) != 0 ||
+		    (state->rflags & PSL_I) == 0 || ctrl->intr_shadow,
+		    ("Bogus intr_window_exiting: eventinj (%#lx), "
+		    "intr_shadow (%u), rflags (%#lx)",
+		    ctrl->eventinj, ctrl->intr_shadow, state->rflags));
+		enable_intr_window_exiting(sc, vcpu);
+	} else {
+		disable_intr_window_exiting(sc, vcpu);
+	}
+}
+
+static __inline void
+restore_host_tss(void)
+{
+#ifdef __FreeBSD__
+	struct system_segment_descriptor *tss_sd;
+
+	/*
+	 * The TSS descriptor was in use prior to launching the guest so it
+	 * has been marked busy.
+	 *
+	 * 'ltr' requires the descriptor to be marked available so change the
+	 * type to "64-bit available TSS".
+	 */
+	tss_sd = PCPU_GET(tss);
+	tss_sd->sd_type = SDT_SYSTSS;
+	ltr(GSEL(GPROC0_SEL, SEL_KPL));
+#else
+	/* XXXJOY: Add logic to restore TSS for us */
+	panic("SVM Restore system TSS");
+#endif
+}
+
+static void
+check_asid(struct svm_softc *sc, int vcpuid, pmap_t pmap, u_int thiscpu)
+{
+	struct svm_vcpu *vcpustate;
+	struct vmcb_ctrl *ctrl;
+	long eptgen;
+	bool alloc_asid;
+
+	KASSERT(CPU_ISSET(thiscpu, &pmap->pm_active), ("%s: nested pmap not "
+	    "active on cpu %u", __func__, thiscpu));
+
+	vcpustate = svm_get_vcpu(sc, vcpuid);
+	ctrl = svm_get_vmcb_ctrl(sc, vcpuid);
+
+	/*
+	 * The TLB entries associated with the vcpu's ASID are not valid
+	 * if either of the following conditions is true:
+	 *
+	 * 1. The vcpu's ASID generation is different than the host cpu's
+	 *    ASID generation. This happens when the vcpu migrates to a new
+	 *    host cpu. It can also happen when the number of vcpus executing
+	 *    on a host cpu is greater than the number of ASIDs available.
+	 *
+	 * 2. The pmap generation number is different than the value cached in
+	 *    the 'vcpustate'. This happens when the host invalidates pages
+	 *    belonging to the guest.
+	 *
+	 *	asidgen		eptgen	      Action
+	 *	mismatch	mismatch
+	 *	   0		   0		(a)
+	 *	   0		   1		(b1) or (b2)
+	 *	   1		   0		(c)
+	 *	   1		   1		(d)
+	 *
+	 * (a) There is no mismatch in eptgen or ASID generation and therefore
+	 *     no further action is needed.
+	 *
+	 * (b1) If the cpu supports FlushByAsid then the vcpu's ASID is
+	 *      retained and the TLB entries associated with this ASID
+	 *      are flushed by VMRUN.
+	 *
+	 * (b2) If the cpu does not support FlushByAsid then a new ASID is
+	 *      allocated.
+	 *
+	 * (c) A new ASID is allocated.
+	 *
+	 * (d) A new ASID is allocated.
+	 */
+
+	alloc_asid = false;
+	eptgen = pmap->pm_eptgen;
+	ctrl->tlb_ctrl = VMCB_TLB_FLUSH_NOTHING;
+
+	if (vcpustate->asid.gen != asid[thiscpu].gen) {
+		alloc_asid = true;	/* (c) and (d) */
+	} else if (vcpustate->eptgen != eptgen) {
+		if (flush_by_asid())
+			ctrl->tlb_ctrl = VMCB_TLB_FLUSH_GUEST;	/* (b1) */
+		else
+			alloc_asid = true;			/* (b2) */
+	} else {
+		/*
+		 * This is the common case (a).
+		 */
+		KASSERT(!alloc_asid, ("ASID allocation not necessary"));
+		KASSERT(ctrl->tlb_ctrl == VMCB_TLB_FLUSH_NOTHING,
+		    ("Invalid VMCB tlb_ctrl: %#x", ctrl->tlb_ctrl));
+	}
+
+	if (alloc_asid) {
+		if (++asid[thiscpu].num >= nasid) {
+			asid[thiscpu].num = 1;
+			if (++asid[thiscpu].gen == 0)
+				asid[thiscpu].gen = 1;
+			/*
+			 * If this cpu does not support "flush-by-asid"
+			 * then flush the entire TLB on a generation
+			 * bump. Subsequent ASID allocation in this
+			 * generation can be done without a TLB flush.
+			 */
+			if (!flush_by_asid())
+				ctrl->tlb_ctrl = VMCB_TLB_FLUSH_ALL;
+		}
+		vcpustate->asid.gen = asid[thiscpu].gen;
+		vcpustate->asid.num = asid[thiscpu].num;
+
+		ctrl->asid = vcpustate->asid.num;
+		svm_set_dirty(sc, vcpuid, VMCB_CACHE_ASID);
+		/*
+		 * If this cpu supports "flush-by-asid" then the TLB
+		 * was not flushed after the generation bump. The TLB
+		 * is flushed selectively after every new ASID allocation.
+		 */
+		if (flush_by_asid())
+			ctrl->tlb_ctrl = VMCB_TLB_FLUSH_GUEST;
+	}
+	vcpustate->eptgen = eptgen;
+
+	KASSERT(ctrl->asid != 0, ("Guest ASID must be non-zero"));
+	KASSERT(ctrl->asid == vcpustate->asid.num,
+	    ("ASID mismatch: %u/%u", ctrl->asid, vcpustate->asid.num));
+}
+
+static __inline void
+disable_gintr(void)
+{
+
+	__asm __volatile("clgi");
+}
+
+static __inline void
+enable_gintr(void)
+{
+
+        __asm __volatile("stgi");
+}
+
+/*
+ * Start vcpu with specified RIP.
+ */
+static int
+svm_vmrun(void *arg, int vcpu, register_t rip, pmap_t pmap, 
+	struct vm_eventinfo *evinfo)
+{
+	struct svm_regctx *gctx;
+	struct svm_softc *svm_sc;
+	struct svm_vcpu *vcpustate;
+	struct vmcb_state *state;
+	struct vmcb_ctrl *ctrl;
+	struct vm_exit *vmexit;
+	struct vlapic *vlapic;
+	struct vm *vm;
+	uint64_t vmcb_pa;
+	int handled;
+
+	svm_sc = arg;
+	vm = svm_sc->vm;
+
+	vcpustate = svm_get_vcpu(svm_sc, vcpu);
+	state = svm_get_vmcb_state(svm_sc, vcpu);
+	ctrl = svm_get_vmcb_ctrl(svm_sc, vcpu);
+	vmexit = vm_exitinfo(vm, vcpu);
+	vlapic = vm_lapic(vm, vcpu);
+
+	gctx = svm_get_guest_regctx(svm_sc, vcpu);
+	vmcb_pa = svm_sc->vcpu[vcpu].vmcb_pa;
+
+	if (vcpustate->lastcpu != curcpu) {
+		/*
+		 * Force new ASID allocation by invalidating the generation.
+		 */
+		vcpustate->asid.gen = 0;
+
+		/*
+		 * Invalidate the VMCB state cache by marking all fields dirty.
+		 */
+		svm_set_dirty(svm_sc, vcpu, 0xffffffff);
+
+		/*
+		 * XXX
+		 * Setting 'vcpustate->lastcpu' here is bit premature because
+		 * we may return from this function without actually executing
+		 * the VMRUN  instruction. This could happen if a rendezvous
+		 * or an AST is pending on the first time through the loop.
+		 *
+		 * This works for now but any new side-effects of vcpu
+		 * migration should take this case into account.
+		 */
+		vcpustate->lastcpu = curcpu;
+		vmm_stat_incr(vm, vcpu, VCPU_MIGRATIONS, 1);
+	}
+
+	svm_msr_guest_enter(svm_sc, vcpu);
+
+	/* Update Guest RIP */
+	state->rip = rip;
+
+	do {
+		/*
+		 * Disable global interrupts to guarantee atomicity during
+		 * loading of guest state. This includes not only the state
+		 * loaded by the "vmrun" instruction but also software state
+		 * maintained by the hypervisor: suspended and rendezvous
+		 * state, NPT generation number, vlapic interrupts etc.
+		 */
+		disable_gintr();
+
+		if (vcpu_suspended(evinfo)) {
+			enable_gintr();
+			vm_exit_suspended(vm, vcpu, state->rip);
+			break;
+		}
+
+		if (vcpu_rendezvous_pending(evinfo)) {
+			enable_gintr();
+			vm_exit_rendezvous(vm, vcpu, state->rip);
+			break;
+		}
+
+		if (vcpu_reqidle(evinfo)) {
+			enable_gintr();
+			vm_exit_reqidle(vm, vcpu, state->rip);
+			break;
+		}
+
+		/* We are asked to give the cpu by scheduler. */
+		if (vcpu_should_yield(vm, vcpu)) {
+			enable_gintr();
+			vm_exit_astpending(vm, vcpu, state->rip);
+			break;
+		}
+
+		svm_inj_interrupts(svm_sc, vcpu, vlapic);
+
+		/* Activate the nested pmap on 'curcpu' */
+		CPU_SET_ATOMIC_ACQ(curcpu, &pmap->pm_active);
+
+		/*
+		 * Check the pmap generation and the ASID generation to
+		 * ensure that the vcpu does not use stale TLB mappings.
+		 */
+		check_asid(svm_sc, vcpu, pmap, curcpu);
+
+		ctrl->vmcb_clean = vmcb_clean & ~vcpustate->dirty;
+		vcpustate->dirty = 0;
+		VCPU_CTR1(vm, vcpu, "vmcb clean %#x", ctrl->vmcb_clean);
+
+		/* Launch Virtual Machine. */
+		VCPU_CTR1(vm, vcpu, "Resume execution at %#lx", state->rip);
+#ifdef __FreeBSD__
+		svm_launch(vmcb_pa, gctx, &__pcpu[curcpu]);
+#else
+		svm_launch(vmcb_pa, gctx, CPU);
+#endif
+
+		CPU_CLR_ATOMIC(curcpu, &pmap->pm_active);
+
+		/*
+		 * The host GDTR and IDTR is saved by VMRUN and restored
+		 * automatically on #VMEXIT. However, the host TSS needs
+		 * to be restored explicitly.
+		 */
+		restore_host_tss();
+
+		/* #VMEXIT disables interrupts so re-enable them here. */ 
+		enable_gintr();
+
+		/* Update 'nextrip' */
+		vcpustate->nextrip = state->rip;
+
+		/* Handle #VMEXIT and if required return to user space. */
+		handled = svm_vmexit(svm_sc, vcpu, vmexit);
+	} while (handled);
+
+	svm_msr_guest_exit(svm_sc, vcpu);
+
+	return (0);
+}
+
+static void
+svm_vmcleanup(void *arg)
+{
+	struct svm_softc *sc = arg;
+
+	contigfree(sc, sizeof (*sc), M_SVM);
+}
+
+static register_t *
+swctx_regptr(struct svm_regctx *regctx, int reg)
+{
+
+	switch (reg) {
+	case VM_REG_GUEST_RBX:
+		return (&regctx->sctx_rbx);
+	case VM_REG_GUEST_RCX:
+		return (&regctx->sctx_rcx);
+	case VM_REG_GUEST_RDX:
+		return (&regctx->sctx_rdx);
+	case VM_REG_GUEST_RDI:
+		return (&regctx->sctx_rdi);
+	case VM_REG_GUEST_RSI:
+		return (&regctx->sctx_rsi);
+	case VM_REG_GUEST_RBP:
+		return (&regctx->sctx_rbp);
+	case VM_REG_GUEST_R8:
+		return (&regctx->sctx_r8);
+	case VM_REG_GUEST_R9:
+		return (&regctx->sctx_r9);
+	case VM_REG_GUEST_R10:
+		return (&regctx->sctx_r10);
+	case VM_REG_GUEST_R11:
+		return (&regctx->sctx_r11);
+	case VM_REG_GUEST_R12:
+		return (&regctx->sctx_r12);
+	case VM_REG_GUEST_R13:
+		return (&regctx->sctx_r13);
+	case VM_REG_GUEST_R14:
+		return (&regctx->sctx_r14);
+	case VM_REG_GUEST_R15:
+		return (&regctx->sctx_r15);
+	default:
+		return (NULL);
+	}
+}
+
+static int
+svm_getreg(void *arg, int vcpu, int ident, uint64_t *val)
+{
+	struct svm_softc *svm_sc;
+	register_t *reg;
+
+	svm_sc = arg;
+
+	if (ident == VM_REG_GUEST_INTR_SHADOW) {
+		return (svm_get_intr_shadow(svm_sc, vcpu, val));
+	}
+
+	if (vmcb_read(svm_sc, vcpu, ident, val) == 0) {
+		return (0);
+	}
+
+	reg = swctx_regptr(svm_get_guest_regctx(svm_sc, vcpu), ident);
+
+	if (reg != NULL) {
+		*val = *reg;
+		return (0);
+	}
+
+	VCPU_CTR1(svm_sc->vm, vcpu, "svm_getreg: unknown register %#x", ident);
+	return (EINVAL);
+}
+
+static int
+svm_setreg(void *arg, int vcpu, int ident, uint64_t val)
+{
+	struct svm_softc *svm_sc;
+	register_t *reg;
+
+	svm_sc = arg;
+
+	if (ident == VM_REG_GUEST_INTR_SHADOW) {
+		return (svm_modify_intr_shadow(svm_sc, vcpu, val));
+	}
+
+	if (vmcb_write(svm_sc, vcpu, ident, val) == 0) {
+		return (0);
+	}
+
+	reg = swctx_regptr(svm_get_guest_regctx(svm_sc, vcpu), ident);
+
+	if (reg != NULL) {
+		*reg = val;
+		return (0);
+	}
+
+	/*
+	 * XXX deal with CR3 and invalidate TLB entries tagged with the
+	 * vcpu's ASID. This needs to be treated differently depending on
+	 * whether 'running' is true/false.
+	 */
+
+	VCPU_CTR1(svm_sc->vm, vcpu, "svm_setreg: unknown register %#x", ident);
+	return (EINVAL);
+}
+
+static int
+svm_setcap(void *arg, int vcpu, int type, int val)
+{
+	struct svm_softc *sc;
+	int error;
+
+	sc = arg;
+	error = 0;
+	switch (type) {
+	case VM_CAP_HALT_EXIT:
+		svm_set_intercept(sc, vcpu, VMCB_CTRL1_INTCPT,
+		    VMCB_INTCPT_HLT, val);
+		break;
+	case VM_CAP_PAUSE_EXIT:
+		svm_set_intercept(sc, vcpu, VMCB_CTRL1_INTCPT,
+		    VMCB_INTCPT_PAUSE, val);
+		break;
+	case VM_CAP_UNRESTRICTED_GUEST:
+		/* Unrestricted guest execution cannot be disabled in SVM */
+		if (val == 0)
+			error = EINVAL;
+		break;
+	default:
+		error = ENOENT;
+		break;
+	}
+	return (error);
+}
+
+static int
+svm_getcap(void *arg, int vcpu, int type, int *retval)
+{
+	struct svm_softc *sc;
+	int error;
+
+	sc = arg;
+	error = 0;
+
+	switch (type) {
+	case VM_CAP_HALT_EXIT:
+		*retval = svm_get_intercept(sc, vcpu, VMCB_CTRL1_INTCPT,
+		    VMCB_INTCPT_HLT);
+		break;
+	case VM_CAP_PAUSE_EXIT:
+		*retval = svm_get_intercept(sc, vcpu, VMCB_CTRL1_INTCPT,
+		    VMCB_INTCPT_PAUSE);
+		break;
+	case VM_CAP_UNRESTRICTED_GUEST:
+		*retval = 1;	/* unrestricted guest is always enabled */
+		break;
+	default:
+		error = ENOENT;
+		break;
+	}
+	return (error);
+}
+
+static struct vlapic *
+svm_vlapic_init(void *arg, int vcpuid)
+{
+	struct svm_softc *svm_sc;
+	struct vlapic *vlapic;
+
+	svm_sc = arg;
+	vlapic = malloc(sizeof(struct vlapic), M_SVM_VLAPIC, M_WAITOK | M_ZERO);
+	vlapic->vm = svm_sc->vm;
+	vlapic->vcpuid = vcpuid;
+	vlapic->apic_page = (struct LAPIC *)&svm_sc->apic_page[vcpuid];
+
+	vlapic_init(vlapic);
+
+	return (vlapic);
+}
+
+static void
+svm_vlapic_cleanup(void *arg, struct vlapic *vlapic)
+{
+
+        vlapic_cleanup(vlapic);
+        free(vlapic, M_SVM_VLAPIC);
+}
+
+struct vmm_ops vmm_ops_amd = {
+	svm_init,
+	svm_cleanup,
+	svm_restore,
+	svm_vminit,
+	svm_vmrun,
+	svm_vmcleanup,
+	svm_getreg,
+	svm_setreg,
+	vmcb_getdesc,
+	vmcb_setdesc,
+	svm_getcap,
+	svm_setcap,
+	svm_npt_alloc,
+	svm_npt_free,
+	svm_vlapic_init,
+	svm_vlapic_cleanup	
+};
diff --git a/usr/src/uts/i86pc/io/vmm/amd/svm.h b/usr/src/uts/i86pc/io/vmm/amd/svm.h
new file mode 100644
index 0000000000..f6c8de2a99
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/amd/svm.h
@@ -0,0 +1,60 @@
+/*-
+ * Copyright (c) 2013 Anish Gupta (akgupt3@gmail.com)
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _SVM_H_
+#define _SVM_H_
+
+/*
+ * Guest register state that is saved outside the VMCB.
+ */
+struct svm_regctx {
+	register_t	sctx_rbp;
+	register_t	sctx_rbx;
+	register_t	sctx_rcx;
+	register_t	sctx_rdx;
+	register_t	sctx_rdi;
+	register_t	sctx_rsi;
+	register_t	sctx_r8;
+	register_t	sctx_r9;
+	register_t	sctx_r10;
+	register_t	sctx_r11;
+	register_t	sctx_r12;
+	register_t	sctx_r13;
+	register_t	sctx_r14;
+	register_t	sctx_r15;
+};
+
+#ifdef __FreeBSD__
+struct pcpu;
+void svm_launch(uint64_t pa, struct svm_regctx *gctx, struct pcpu *pcpu);
+#else
+struct cpu;
+void svm_launch(uint64_t pa, struct svm_regctx *gctx, struct cpu *pcpu);
+#endif
+
+#endif /* _SVM_H_ */
diff --git a/usr/src/uts/i86pc/io/vmm/amd/svm_msr.c b/usr/src/uts/i86pc/io/vmm/amd/svm_msr.c
new file mode 100644
index 0000000000..49208a351c
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/amd/svm_msr.c
@@ -0,0 +1,170 @@
+/*-
+ * Copyright (c) 2014, Neel Natu (neel@freebsd.org)
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/param.h>
+#include <sys/errno.h>
+#include <sys/systm.h>
+
+#include <machine/cpufunc.h>
+#include <machine/specialreg.h>
+#include <machine/vmm.h>
+
+#include "svm.h"
+#include "vmcb.h"
+#include "svm_softc.h"
+#include "svm_msr.h"
+
+#ifndef MSR_AMDK8_IPM
+#define	MSR_AMDK8_IPM	0xc0010055
+#endif
+
+enum {
+	IDX_MSR_LSTAR,
+	IDX_MSR_CSTAR,
+	IDX_MSR_STAR,
+	IDX_MSR_SF_MASK,
+	HOST_MSR_NUM		/* must be the last enumeration */
+};
+
+static uint64_t host_msrs[HOST_MSR_NUM];
+
+void
+svm_msr_init(void)
+{
+	/* 
+	 * It is safe to cache the values of the following MSRs because they
+	 * don't change based on curcpu, curproc or curthread.
+	 */
+	host_msrs[IDX_MSR_LSTAR] = rdmsr(MSR_LSTAR);
+	host_msrs[IDX_MSR_CSTAR] = rdmsr(MSR_CSTAR);
+	host_msrs[IDX_MSR_STAR] = rdmsr(MSR_STAR);
+	host_msrs[IDX_MSR_SF_MASK] = rdmsr(MSR_SF_MASK);
+}
+
+void
+svm_msr_guest_init(struct svm_softc *sc, int vcpu)
+{
+	/*
+	 * All the MSRs accessible to the guest are either saved/restored by
+	 * hardware on every #VMEXIT/VMRUN (e.g., G_PAT) or are saved/restored
+	 * by VMSAVE/VMLOAD (e.g., MSR_GSBASE).
+	 *
+	 * There are no guest MSRs that are saved/restored "by hand" so nothing
+	 * more to do here.
+	 */
+	return;
+}
+
+void
+svm_msr_guest_enter(struct svm_softc *sc, int vcpu)
+{
+	/*
+	 * Save host MSRs (if any) and restore guest MSRs (if any).
+	 */
+}
+
+void
+svm_msr_guest_exit(struct svm_softc *sc, int vcpu)
+{
+	/*
+	 * Save guest MSRs (if any) and restore host MSRs.
+	 */
+	wrmsr(MSR_LSTAR, host_msrs[IDX_MSR_LSTAR]);
+	wrmsr(MSR_CSTAR, host_msrs[IDX_MSR_CSTAR]);
+	wrmsr(MSR_STAR, host_msrs[IDX_MSR_STAR]);
+	wrmsr(MSR_SF_MASK, host_msrs[IDX_MSR_SF_MASK]);
+
+	/* MSR_KGSBASE will be restored on the way back to userspace */
+}
+
+int
+svm_rdmsr(struct svm_softc *sc, int vcpu, u_int num, uint64_t *result,
+    bool *retu)
+{
+	int error = 0;
+
+	switch (num) {
+	case MSR_MCG_CAP:
+	case MSR_MCG_STATUS:
+		*result = 0;
+		break;
+	case MSR_MTRRcap:
+	case MSR_MTRRdefType:
+	case MSR_MTRR4kBase ... MSR_MTRR4kBase + 8:
+	case MSR_MTRR16kBase ... MSR_MTRR16kBase + 1:
+	case MSR_MTRR64kBase:
+	case MSR_SYSCFG:
+		*result = 0;
+		break;
+	case MSR_AMDK8_IPM:
+		*result = 0;
+		break;
+	default:
+		error = EINVAL;
+		break;
+	}
+
+	return (error);
+}
+
+int
+svm_wrmsr(struct svm_softc *sc, int vcpu, u_int num, uint64_t val, bool *retu)
+{
+	int error = 0;
+
+	switch (num) {
+	case MSR_MCG_CAP:
+	case MSR_MCG_STATUS:
+		break;		/* ignore writes */
+	case MSR_MTRRcap:
+		vm_inject_gp(sc->vm, vcpu);
+		break;
+	case MSR_MTRRdefType:
+	case MSR_MTRR4kBase ... MSR_MTRR4kBase + 8:
+	case MSR_MTRR16kBase ... MSR_MTRR16kBase + 1:
+	case MSR_MTRR64kBase:
+	case MSR_SYSCFG:
+		break;		/* Ignore writes */
+	case MSR_AMDK8_IPM:
+		/*
+		 * Ignore writes to the "Interrupt Pending Message" MSR.
+		 */
+		break;
+	case MSR_K8_UCODE_UPDATE:
+		/*
+		 * Ignore writes to microcode update register.
+		 */
+		break;
+	default:
+		error = EINVAL;
+		break;
+	}
+
+	return (error);
+}
diff --git a/usr/src/uts/i86pc/io/vmm/amd/svm_msr.h b/usr/src/uts/i86pc/io/vmm/amd/svm_msr.h
new file mode 100644
index 0000000000..07716c86de
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/amd/svm_msr.h
@@ -0,0 +1,44 @@
+/*-
+ * Copyright (c) 2014 Neel Natu (neel@freebsd.org)
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _SVM_MSR_H_
+#define	_SVM_MSR_H_
+
+struct svm_softc;
+
+void svm_msr_init(void);
+void svm_msr_guest_init(struct svm_softc *sc, int vcpu);
+void svm_msr_guest_enter(struct svm_softc *sc, int vcpu);
+void svm_msr_guest_exit(struct svm_softc *sc, int vcpu);
+
+int svm_wrmsr(struct svm_softc *sc, int vcpu, u_int num, uint64_t val,
+    bool *retu);
+int svm_rdmsr(struct svm_softc *sc, int vcpu, u_int num, uint64_t *result,
+    bool *retu);
+
+#endif	/* _SVM_MSR_H_ */
diff --git a/usr/src/uts/i86pc/io/vmm/amd/svm_softc.h b/usr/src/uts/i86pc/io/vmm/amd/svm_softc.h
new file mode 100644
index 0000000000..de0c3f7890
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/amd/svm_softc.h
@@ -0,0 +1,114 @@
+/*-
+ * Copyright (c) 2013 Anish Gupta (akgupt3@gmail.com)
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _SVM_SOFTC_H_
+#define _SVM_SOFTC_H_
+
+#define SVM_IO_BITMAP_SIZE	(3 * PAGE_SIZE)
+#define SVM_MSR_BITMAP_SIZE	(2 * PAGE_SIZE)
+
+struct asid {
+	uint64_t	gen;	/* range is [1, ~0UL] */
+	uint32_t	num;	/* range is [1, nasid - 1] */
+};
+
+/*
+ * XXX separate out 'struct vmcb' from 'svm_vcpu' to avoid wasting space
+ * due to VMCB alignment requirements.
+ */
+struct svm_vcpu {
+	struct vmcb	vmcb;	 /* hardware saved vcpu context */
+	struct svm_regctx swctx; /* software saved vcpu context */
+	uint64_t	vmcb_pa; /* VMCB physical address */
+	uint64_t	nextrip; /* next instruction to be executed by guest */
+        int		lastcpu; /* host cpu that the vcpu last ran on */
+	uint32_t	dirty;	 /* state cache bits that must be cleared */
+	long		eptgen;	 /* pmap->pm_eptgen when the vcpu last ran */
+	struct asid	asid;
+} __aligned(PAGE_SIZE);
+
+/*
+ * SVM softc, one per virtual machine.
+ */
+struct svm_softc {
+	uint8_t iopm_bitmap[SVM_IO_BITMAP_SIZE];    /* shared by all vcpus */
+	uint8_t msr_bitmap[SVM_MSR_BITMAP_SIZE];    /* shared by all vcpus */
+	uint8_t apic_page[VM_MAXCPU][PAGE_SIZE];
+	struct svm_vcpu vcpu[VM_MAXCPU];
+	vm_offset_t 	nptp;			    /* nested page table */
+	struct vm	*vm;
+} __aligned(PAGE_SIZE);
+
+CTASSERT((offsetof(struct svm_softc, nptp) & PAGE_MASK) == 0);
+
+static __inline struct svm_vcpu *
+svm_get_vcpu(struct svm_softc *sc, int vcpu)
+{
+
+	return (&(sc->vcpu[vcpu]));
+}
+
+static __inline struct vmcb *
+svm_get_vmcb(struct svm_softc *sc, int vcpu)
+{
+
+	return (&(sc->vcpu[vcpu].vmcb));
+}
+
+static __inline struct vmcb_state *
+svm_get_vmcb_state(struct svm_softc *sc, int vcpu)
+{
+
+	return (&(sc->vcpu[vcpu].vmcb.state));
+}
+
+static __inline struct vmcb_ctrl *
+svm_get_vmcb_ctrl(struct svm_softc *sc, int vcpu)
+{
+
+	return (&(sc->vcpu[vcpu].vmcb.ctrl));
+}
+
+static __inline struct svm_regctx *
+svm_get_guest_regctx(struct svm_softc *sc, int vcpu)
+{
+
+	return (&(sc->vcpu[vcpu].swctx));
+}
+
+static __inline void
+svm_set_dirty(struct svm_softc *sc, int vcpu, uint32_t dirtybits)
+{
+        struct svm_vcpu *vcpustate;
+
+        vcpustate = svm_get_vcpu(sc, vcpu);
+
+        vcpustate->dirty |= dirtybits;
+}
+
+#endif /* _SVM_SOFTC_H_ */
diff --git a/usr/src/uts/i86pc/io/vmm/amd/svm_support.s b/usr/src/uts/i86pc/io/vmm/amd/svm_support.s
new file mode 100644
index 0000000000..4258c95d70
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/amd/svm_support.s
@@ -0,0 +1,148 @@
+/*-
+ * Copyright (c) 2013, Anish Gupta (akgupt3@gmail.com)
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+#include <machine/asmacros.h>
+
+#include "svm_assym.h"
+
+/* Porting note: This is named 'svm_support.S' upstream. */
+
+#if defined(lint)
+
+struct svm_regctx;
+struct pcpu;
+
+/*ARGSUSED*/
+void
+svm_launch(uint64_t pa, struct svm_regctx *gctx, struct pcpu *pcpu)
+{}
+
+#else /* lint */
+
+/*
+ * Be friendly to DTrace FBT's prologue/epilogue pattern matching.
+ *
+ * They are also responsible for saving/restoring the host %rbp across VMRUN.
+ */
+#define	VENTER  push %rbp ; mov %rsp,%rbp
+#define	VLEAVE  pop %rbp
+
+#define	VMLOAD	.byte 0x0f, 0x01, 0xda
+#define	VMRUN	.byte 0x0f, 0x01, 0xd8
+#define	VMSAVE	.byte 0x0f, 0x01, 0xdb
+
+/*
+ * svm_launch(uint64_t vmcb, struct svm_regctx *gctx, struct pcpu *pcpu)
+ * %rdi: physical address of VMCB
+ * %rsi: pointer to guest context
+ * %rdx: pointer to the pcpu data
+ */
+ENTRY(svm_launch)
+	VENTER
+
+	/* save pointer to the pcpu data */
+	push %rdx
+
+	/*
+	 * Host register state saved across a VMRUN.
+	 *
+	 * All "callee saved registers" except:
+	 * %rsp: because it is preserved by the processor across VMRUN.
+	 * %rbp: because it is saved/restored by the function prologue/epilogue.
+	 */
+	push %rbx
+	push %r12
+	push %r13
+	push %r14
+	push %r15
+
+	/* Save the physical address of the VMCB in %rax */
+	movq %rdi, %rax
+
+	push %rsi		/* push guest context pointer on the stack */
+
+	/*
+	 * Restore guest state.
+	 */
+	movq SCTX_R8(%rsi), %r8
+	movq SCTX_R9(%rsi), %r9
+	movq SCTX_R10(%rsi), %r10
+	movq SCTX_R11(%rsi), %r11
+	movq SCTX_R12(%rsi), %r12
+	movq SCTX_R13(%rsi), %r13
+	movq SCTX_R14(%rsi), %r14
+	movq SCTX_R15(%rsi), %r15
+	movq SCTX_RBP(%rsi), %rbp
+	movq SCTX_RBX(%rsi), %rbx
+	movq SCTX_RCX(%rsi), %rcx
+	movq SCTX_RDX(%rsi), %rdx
+	movq SCTX_RDI(%rsi), %rdi
+	movq SCTX_RSI(%rsi), %rsi	/* %rsi must be restored last */
+
+	VMLOAD
+	VMRUN
+	VMSAVE
+
+	pop %rax		/* pop guest context pointer from the stack */
+
+	/*
+	 * Save guest state.
+	 */
+	movq %r8, SCTX_R8(%rax)
+	movq %r9, SCTX_R9(%rax)
+	movq %r10, SCTX_R10(%rax)
+	movq %r11, SCTX_R11(%rax)
+	movq %r12, SCTX_R12(%rax)
+	movq %r13, SCTX_R13(%rax)
+	movq %r14, SCTX_R14(%rax)
+	movq %r15, SCTX_R15(%rax)
+	movq %rbp, SCTX_RBP(%rax)
+	movq %rbx, SCTX_RBX(%rax)
+	movq %rcx, SCTX_RCX(%rax)
+	movq %rdx, SCTX_RDX(%rax)
+	movq %rdi, SCTX_RDI(%rax)
+	movq %rsi, SCTX_RSI(%rax)
+
+	/* Restore host state */
+	pop %r15
+	pop %r14
+	pop %r13
+	pop %r12
+	pop %rbx
+
+	/* Restore %GS.base to point to the host's pcpu data */
+	pop %rdx
+	mov %edx, %eax
+	shr $32, %rdx
+	mov $MSR_GSBASE, %ecx
+	wrmsr
+
+	VLEAVE
+	ret
+END(svm_launch)
+
+#endif /* lint */
diff --git a/usr/src/uts/i86pc/io/vmm/amd/vmcb.c b/usr/src/uts/i86pc/io/vmm/amd/vmcb.c
new file mode 100644
index 0000000000..d8601690c4
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/amd/vmcb.c
@@ -0,0 +1,442 @@
+/*-
+ * Copyright (c) 2013 Anish Gupta (akgupt3@gmail.com)
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/param.h>
+#include <sys/systm.h>
+
+#include <machine/segments.h>
+#include <machine/specialreg.h>
+#include <machine/vmm.h>
+
+#include "vmm_ktr.h"
+
+#include "vmcb.h"
+#include "svm.h"
+#include "svm_softc.h"
+
+/*
+ * The VMCB aka Virtual Machine Control Block is a 4KB aligned page
+ * in memory that describes the virtual machine.
+ *
+ * The VMCB contains:
+ * - instructions or events in the guest to intercept
+ * - control bits that modify execution environment of the guest
+ * - guest processor state (e.g. general purpose registers)
+ */
+
+/*
+ * Return VMCB segment area.
+ */
+static struct vmcb_segment *
+vmcb_segptr(struct vmcb *vmcb, int type)
+{
+	struct vmcb_state *state;
+	struct vmcb_segment *seg;
+
+	state = &vmcb->state;
+
+	switch (type) {
+	case VM_REG_GUEST_CS:
+		seg = &state->cs;
+		break;
+
+	case VM_REG_GUEST_DS:
+		seg = &state->ds;
+		break;
+
+	case VM_REG_GUEST_ES:
+		seg = &state->es;
+		break;
+
+	case VM_REG_GUEST_FS:
+		seg = &state->fs;
+		break;
+
+	case VM_REG_GUEST_GS:
+		seg = &state->gs;
+		break;
+
+	case VM_REG_GUEST_SS:
+		seg = &state->ss;
+		break;
+
+	case VM_REG_GUEST_GDTR:
+		seg = &state->gdt;
+		break;
+
+	case VM_REG_GUEST_IDTR:
+		seg = &state->idt;
+		break;
+
+	case VM_REG_GUEST_LDTR:
+		seg = &state->ldt;
+		break;
+
+	case VM_REG_GUEST_TR:
+		seg = &state->tr;
+		break;
+
+	default:
+		seg = NULL;
+		break;
+	}
+
+	return (seg);
+}
+
+static int
+vmcb_access(struct svm_softc *softc, int vcpu, int write, int ident,
+	uint64_t *val)
+{
+	struct vmcb *vmcb;
+	int off, bytes;
+	char *ptr;
+
+	vmcb	= svm_get_vmcb(softc, vcpu);
+	off	= VMCB_ACCESS_OFFSET(ident);
+	bytes	= VMCB_ACCESS_BYTES(ident);
+
+	if ((off + bytes) >= sizeof (struct vmcb))
+		return (EINVAL);
+
+	ptr = (char *)vmcb;
+
+	if (!write)
+		*val = 0;
+
+	switch (bytes) {
+	case 8:
+	case 4:
+	case 2:
+		if (write)
+			memcpy(ptr + off, val, bytes);
+		else
+			memcpy(val, ptr + off, bytes);
+		break;
+	default:
+		VCPU_CTR1(softc->vm, vcpu,
+		    "Invalid size %d for VMCB access: %d", bytes);
+		return (EINVAL);
+	}
+
+	/* Invalidate all VMCB state cached by h/w. */
+	if (write)
+		svm_set_dirty(softc, vcpu, 0xffffffff);
+
+	return (0);
+}
+
+/*
+ * Read from segment selector, control and general purpose register of VMCB.
+ */
+int
+vmcb_read(struct svm_softc *sc, int vcpu, int ident, uint64_t *retval)
+{
+	struct vmcb *vmcb;
+	struct vmcb_state *state;
+	struct vmcb_segment *seg;
+	int err;
+
+	vmcb = svm_get_vmcb(sc, vcpu);
+	state = &vmcb->state;
+	err = 0;
+
+	if (VMCB_ACCESS_OK(ident))
+		return (vmcb_access(sc, vcpu, 0, ident, retval));
+
+	switch (ident) {
+	case VM_REG_GUEST_CR0:
+		*retval = state->cr0;
+		break;
+
+	case VM_REG_GUEST_CR2:
+		*retval = state->cr2;
+		break;
+
+	case VM_REG_GUEST_CR3:
+		*retval = state->cr3;
+		break;
+
+	case VM_REG_GUEST_CR4:
+		*retval = state->cr4;
+		break;
+
+	case VM_REG_GUEST_DR7:
+		*retval = state->dr7;
+		break;
+
+	case VM_REG_GUEST_EFER:
+		*retval = state->efer;
+		break;
+
+	case VM_REG_GUEST_RAX:
+		*retval = state->rax;
+		break;
+
+	case VM_REG_GUEST_RFLAGS:
+		*retval = state->rflags;
+		break;
+
+	case VM_REG_GUEST_RIP:
+		*retval = state->rip;
+		break;
+
+	case VM_REG_GUEST_RSP:
+		*retval = state->rsp;
+		break;
+
+	case VM_REG_GUEST_CS:
+	case VM_REG_GUEST_DS:
+	case VM_REG_GUEST_ES:
+	case VM_REG_GUEST_FS:
+	case VM_REG_GUEST_GS:
+	case VM_REG_GUEST_SS:
+	case VM_REG_GUEST_LDTR:
+	case VM_REG_GUEST_TR:
+		seg = vmcb_segptr(vmcb, ident);
+		KASSERT(seg != NULL, ("%s: unable to get segment %d from VMCB",
+		    __func__, ident));
+		*retval = seg->selector;
+		break;
+
+	case VM_REG_GUEST_GDTR:
+	case VM_REG_GUEST_IDTR:
+		/* GDTR and IDTR don't have segment selectors */
+		err = EINVAL;
+		break;
+	default:
+		err =  EINVAL;
+		break;
+	}
+
+	return (err);
+}
+
+/*
+ * Write to segment selector, control and general purpose register of VMCB.
+ */
+int
+vmcb_write(struct svm_softc *sc, int vcpu, int ident, uint64_t val)
+{
+	struct vmcb *vmcb;
+	struct vmcb_state *state;
+	struct vmcb_segment *seg;
+	int err, dirtyseg;
+
+	vmcb = svm_get_vmcb(sc, vcpu);
+	state = &vmcb->state;
+	dirtyseg = 0;
+	err = 0;
+
+	if (VMCB_ACCESS_OK(ident))
+		return (vmcb_access(sc, vcpu, 1, ident, &val));
+
+	switch (ident) {
+	case VM_REG_GUEST_CR0:
+		state->cr0 = val;
+		svm_set_dirty(sc, vcpu, VMCB_CACHE_CR);
+		break;
+
+	case VM_REG_GUEST_CR2:
+		state->cr2 = val;
+		svm_set_dirty(sc, vcpu, VMCB_CACHE_CR2);
+		break;
+
+	case VM_REG_GUEST_CR3:
+		state->cr3 = val;
+		svm_set_dirty(sc, vcpu, VMCB_CACHE_CR);
+		break;
+
+	case VM_REG_GUEST_CR4:
+		state->cr4 = val;
+		svm_set_dirty(sc, vcpu, VMCB_CACHE_CR);
+		break;
+
+	case VM_REG_GUEST_DR7:
+		state->dr7 = val;
+		break;
+
+	case VM_REG_GUEST_EFER:
+		/* EFER_SVM must always be set when the guest is executing */
+		state->efer = val | EFER_SVM;
+		svm_set_dirty(sc, vcpu, VMCB_CACHE_CR);
+		break;
+
+	case VM_REG_GUEST_RAX:
+		state->rax = val;
+		break;
+
+	case VM_REG_GUEST_RFLAGS:
+		state->rflags = val;
+		break;
+
+	case VM_REG_GUEST_RIP:
+		state->rip = val;
+		break;
+
+	case VM_REG_GUEST_RSP:
+		state->rsp = val;
+		break;
+
+	case VM_REG_GUEST_CS:
+	case VM_REG_GUEST_DS:
+	case VM_REG_GUEST_ES:
+	case VM_REG_GUEST_SS:
+		dirtyseg = 1;		/* FALLTHROUGH */
+	case VM_REG_GUEST_FS:
+	case VM_REG_GUEST_GS:
+	case VM_REG_GUEST_LDTR:
+	case VM_REG_GUEST_TR:
+		seg = vmcb_segptr(vmcb, ident);
+		KASSERT(seg != NULL, ("%s: unable to get segment %d from VMCB",
+		    __func__, ident));
+		seg->selector = val;
+		if (dirtyseg)
+			svm_set_dirty(sc, vcpu, VMCB_CACHE_SEG);
+		break;
+
+	case VM_REG_GUEST_GDTR:
+	case VM_REG_GUEST_IDTR:
+		/* GDTR and IDTR don't have segment selectors */
+		err = EINVAL;
+		break;
+	default:
+		err = EINVAL;
+		break;
+	}
+
+	return (err);
+}
+
+int
+vmcb_seg(struct vmcb *vmcb, int ident, struct vmcb_segment *seg2)
+{
+	struct vmcb_segment *seg;
+
+	seg = vmcb_segptr(vmcb, ident);
+	if (seg != NULL) {
+		bcopy(seg, seg2, sizeof(struct vmcb_segment));
+		return (0);
+	} else {
+		return (EINVAL);
+	}
+}
+
+int
+vmcb_setdesc(void *arg, int vcpu, int reg, struct seg_desc *desc)
+{
+	struct vmcb *vmcb;
+	struct svm_softc *sc;
+	struct vmcb_segment *seg;
+	uint16_t attrib;
+
+	sc = arg;
+	vmcb = svm_get_vmcb(sc, vcpu);
+
+	seg = vmcb_segptr(vmcb, reg);
+	KASSERT(seg != NULL, ("%s: invalid segment descriptor %d",
+	    __func__, reg));
+
+	seg->base = desc->base;
+	seg->limit = desc->limit;
+	if (reg != VM_REG_GUEST_GDTR && reg != VM_REG_GUEST_IDTR) {
+		/*
+		 * Map seg_desc access to VMCB attribute format.
+		 *
+		 * SVM uses the 'P' bit in the segment attributes to indicate a
+		 * NULL segment so clear it if the segment is marked unusable.
+		 */
+		attrib = ((desc->access & 0xF000) >> 4) | (desc->access & 0xFF);
+		if (SEG_DESC_UNUSABLE(desc->access)) {
+			attrib &= ~0x80;
+		}
+		seg->attrib = attrib;
+	}
+
+	VCPU_CTR4(sc->vm, vcpu, "Setting desc %d: base (%#lx), limit (%#x), "
+	    "attrib (%#x)", reg, seg->base, seg->limit, seg->attrib);
+
+	switch (reg) {
+	case VM_REG_GUEST_CS:
+	case VM_REG_GUEST_DS:
+	case VM_REG_GUEST_ES:
+	case VM_REG_GUEST_SS:
+		svm_set_dirty(sc, vcpu, VMCB_CACHE_SEG);
+		break;
+	case VM_REG_GUEST_GDTR:
+	case VM_REG_GUEST_IDTR:
+		svm_set_dirty(sc, vcpu, VMCB_CACHE_DT);
+		break;
+	default:
+		break;
+	}
+
+	return (0);
+}
+
+int
+vmcb_getdesc(void *arg, int vcpu, int reg, struct seg_desc *desc)
+{
+	struct vmcb *vmcb;
+	struct svm_softc *sc;
+	struct vmcb_segment *seg;
+
+	sc = arg;
+	vmcb = svm_get_vmcb(sc, vcpu);
+	seg = vmcb_segptr(vmcb, reg);
+	KASSERT(seg != NULL, ("%s: invalid segment descriptor %d",
+	    __func__, reg));
+
+	desc->base = seg->base;
+	desc->limit = seg->limit;
+	desc->access = 0;
+
+	if (reg != VM_REG_GUEST_GDTR && reg != VM_REG_GUEST_IDTR) {
+		/* Map seg_desc access to VMCB attribute format */
+		desc->access = ((seg->attrib & 0xF00) << 4) |
+		    (seg->attrib & 0xFF);
+
+		/*
+		 * VT-x uses bit 16 to indicate a segment that has been loaded
+		 * with a NULL selector (aka unusable). The 'desc->access'
+		 * field is interpreted in the VT-x format by the
+		 * processor-independent code.
+		 *
+		 * SVM uses the 'P' bit to convey the same information so
+		 * convert it into the VT-x format. For more details refer to
+		 * section "Segment State in the VMCB" in APMv2.
+		 */
+		if (reg != VM_REG_GUEST_CS && reg != VM_REG_GUEST_TR) {
+			if ((desc->access & 0x80) == 0)
+				desc->access |= 0x10000;  /* Unusable segment */
+		}
+	}
+
+	return (0);
+}
diff --git a/usr/src/uts/i86pc/io/vmm/amd/vmcb.h b/usr/src/uts/i86pc/io/vmm/amd/vmcb.h
new file mode 100644
index 0000000000..9c4f582ceb
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/amd/vmcb.h
@@ -0,0 +1,334 @@
+/*-
+ * Copyright (c) 2013 Anish Gupta (akgupt3@gmail.com)
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _VMCB_H_
+#define	_VMCB_H_
+
+struct svm_softc;
+
+#define BIT(n)			(1ULL << n)
+
+/*
+ * Secure Virtual Machine: AMD64 Programmer's Manual Vol2, Chapter 15
+ * Layout of VMCB: AMD64 Programmer's Manual Vol2, Appendix B
+ */
+
+/* vmcb_ctrl->intercept[] array indices */
+#define	VMCB_CR_INTCPT		0
+#define	VMCB_DR_INTCPT		1
+#define	VMCB_EXC_INTCPT		2
+#define	VMCB_CTRL1_INTCPT	3
+#define	VMCB_CTRL2_INTCPT	4
+
+/* intercept[VMCB_CTRL1_INTCPT] fields */
+#define	VMCB_INTCPT_INTR		BIT(0)
+#define	VMCB_INTCPT_NMI			BIT(1)
+#define	VMCB_INTCPT_SMI			BIT(2)
+#define	VMCB_INTCPT_INIT		BIT(3)
+#define	VMCB_INTCPT_VINTR		BIT(4)
+#define	VMCB_INTCPT_CR0_WRITE		BIT(5)
+#define	VMCB_INTCPT_IDTR_READ		BIT(6)
+#define	VMCB_INTCPT_GDTR_READ		BIT(7)
+#define	VMCB_INTCPT_LDTR_READ		BIT(8)
+#define	VMCB_INTCPT_TR_READ		BIT(9)
+#define	VMCB_INTCPT_IDTR_WRITE		BIT(10)
+#define	VMCB_INTCPT_GDTR_WRITE		BIT(11)
+#define	VMCB_INTCPT_LDTR_WRITE		BIT(12)
+#define	VMCB_INTCPT_TR_WRITE		BIT(13)
+#define	VMCB_INTCPT_RDTSC		BIT(14)
+#define	VMCB_INTCPT_RDPMC		BIT(15)
+#define	VMCB_INTCPT_PUSHF		BIT(16)
+#define	VMCB_INTCPT_POPF		BIT(17)
+#define	VMCB_INTCPT_CPUID		BIT(18)
+#define	VMCB_INTCPT_RSM			BIT(19)
+#define	VMCB_INTCPT_IRET		BIT(20)
+#define	VMCB_INTCPT_INTn		BIT(21)
+#define	VMCB_INTCPT_INVD		BIT(22)
+#define	VMCB_INTCPT_PAUSE		BIT(23)
+#define	VMCB_INTCPT_HLT			BIT(24)
+#define	VMCB_INTCPT_INVPG		BIT(25)
+#define	VMCB_INTCPT_INVPGA		BIT(26)
+#define	VMCB_INTCPT_IO			BIT(27)
+#define	VMCB_INTCPT_MSR			BIT(28)
+#define	VMCB_INTCPT_TASK_SWITCH		BIT(29)
+#define	VMCB_INTCPT_FERR_FREEZE		BIT(30)
+#define	VMCB_INTCPT_SHUTDOWN		BIT(31)
+
+/* intercept[VMCB_CTRL2_INTCPT] fields */
+#define	VMCB_INTCPT_VMRUN		BIT(0)
+#define	VMCB_INTCPT_VMMCALL		BIT(1)
+#define	VMCB_INTCPT_VMLOAD		BIT(2)
+#define	VMCB_INTCPT_VMSAVE		BIT(3)
+#define	VMCB_INTCPT_STGI		BIT(4)
+#define	VMCB_INTCPT_CLGI		BIT(5)
+#define	VMCB_INTCPT_SKINIT		BIT(6)
+#define	VMCB_INTCPT_RDTSCP		BIT(7)
+#define	VMCB_INTCPT_ICEBP		BIT(8)
+#define	VMCB_INTCPT_WBINVD		BIT(9)
+#define	VMCB_INTCPT_MONITOR		BIT(10)
+#define	VMCB_INTCPT_MWAIT		BIT(11)
+#define	VMCB_INTCPT_MWAIT_ARMED		BIT(12)
+#define	VMCB_INTCPT_XSETBV		BIT(13)
+
+/* VMCB TLB control */
+#define	VMCB_TLB_FLUSH_NOTHING		0	/* Flush nothing */
+#define	VMCB_TLB_FLUSH_ALL		1	/* Flush entire TLB */
+#define	VMCB_TLB_FLUSH_GUEST		3	/* Flush all guest entries */
+#define	VMCB_TLB_FLUSH_GUEST_NONGLOBAL	7	/* Flush guest non-PG entries */
+
+/* VMCB state caching */
+#define	VMCB_CACHE_NONE		0	/* No caching */
+#define	VMCB_CACHE_I		BIT(0)	/* Intercept, TSC off, Pause filter */
+#define	VMCB_CACHE_IOPM		BIT(1)	/* I/O and MSR permission */
+#define	VMCB_CACHE_ASID		BIT(2)	/* ASID */
+#define	VMCB_CACHE_TPR		BIT(3)	/* V_TPR to V_INTR_VECTOR */
+#define	VMCB_CACHE_NP		BIT(4)	/* Nested Paging */
+#define	VMCB_CACHE_CR		BIT(5)	/* CR0, CR3, CR4 & EFER */
+#define	VMCB_CACHE_DR		BIT(6)	/* Debug registers */
+#define	VMCB_CACHE_DT		BIT(7)	/* GDT/IDT */
+#define	VMCB_CACHE_SEG		BIT(8)	/* User segments, CPL */
+#define	VMCB_CACHE_CR2		BIT(9)	/* page fault address */
+#define	VMCB_CACHE_LBR		BIT(10)	/* Last branch */
+
+/* VMCB control event injection */
+#define	VMCB_EVENTINJ_EC_VALID		BIT(11)	/* Error Code valid */
+#define	VMCB_EVENTINJ_VALID		BIT(31)	/* Event valid */
+
+/* Event types that can be injected */
+#define	VMCB_EVENTINJ_TYPE_INTR		0
+#define	VMCB_EVENTINJ_TYPE_NMI		2
+#define	VMCB_EVENTINJ_TYPE_EXCEPTION	3
+#define	VMCB_EVENTINJ_TYPE_INTn		4
+
+/* VMCB exit code, APM vol2 Appendix C */
+#define	VMCB_EXIT_MC			0x52
+#define	VMCB_EXIT_INTR			0x60
+#define	VMCB_EXIT_NMI			0x61
+#define	VMCB_EXIT_VINTR			0x64
+#define	VMCB_EXIT_PUSHF			0x70
+#define	VMCB_EXIT_POPF			0x71
+#define	VMCB_EXIT_CPUID			0x72
+#define	VMCB_EXIT_IRET			0x74
+#define	VMCB_EXIT_PAUSE			0x77
+#define	VMCB_EXIT_HLT			0x78
+#define	VMCB_EXIT_IO			0x7B
+#define	VMCB_EXIT_MSR			0x7C
+#define	VMCB_EXIT_SHUTDOWN		0x7F
+#define	VMCB_EXIT_VMSAVE		0x83
+#define	VMCB_EXIT_MONITOR		0x8A
+#define	VMCB_EXIT_MWAIT			0x8B
+#define	VMCB_EXIT_NPF			0x400
+#define	VMCB_EXIT_INVALID		-1
+
+/*
+ * Nested page fault.
+ * Bit definitions to decode EXITINFO1.
+ */
+#define	VMCB_NPF_INFO1_P		BIT(0) /* Nested page present. */
+#define	VMCB_NPF_INFO1_W		BIT(1) /* Access was write. */
+#define	VMCB_NPF_INFO1_U		BIT(2) /* Access was user access. */
+#define	VMCB_NPF_INFO1_RSV		BIT(3) /* Reserved bits present. */
+#define	VMCB_NPF_INFO1_ID		BIT(4) /* Code read. */
+
+#define	VMCB_NPF_INFO1_GPA		BIT(32) /* Guest physical address. */
+#define	VMCB_NPF_INFO1_GPT		BIT(33) /* Guest page table. */
+
+/*
+ * EXITINTINFO, Interrupt exit info for all intrecepts.
+ * Section 15.7.2, Intercepts during IDT Interrupt Delivery.
+ */
+#define VMCB_EXITINTINFO_VECTOR(x)	((x) & 0xFF)
+#define VMCB_EXITINTINFO_TYPE(x)	(((x) >> 8) & 0x7)
+#define VMCB_EXITINTINFO_EC_VALID(x)	(((x) & BIT(11)) ? 1 : 0)
+#define VMCB_EXITINTINFO_VALID(x)	(((x) & BIT(31)) ? 1 : 0)
+#define VMCB_EXITINTINFO_EC(x)		(((x) >> 32) & 0xFFFFFFFF)
+
+/* Offset of various VMCB fields. */
+#define	VMCB_OFF_CTRL(x)		(x)
+#define	VMCB_OFF_STATE(x)		((x) + 0x400)
+
+#define	VMCB_OFF_CR_INTERCEPT		VMCB_OFF_CTRL(0x0)
+#define	VMCB_OFF_DR_INTERCEPT		VMCB_OFF_CTRL(0x4)
+#define	VMCB_OFF_EXC_INTERCEPT		VMCB_OFF_CTRL(0x8)
+#define	VMCB_OFF_INST1_INTERCEPT	VMCB_OFF_CTRL(0xC)
+#define	VMCB_OFF_INST2_INTERCEPT	VMCB_OFF_CTRL(0x10)
+#define	VMCB_OFF_IO_PERM		VMCB_OFF_CTRL(0x40)
+#define	VMCB_OFF_MSR_PERM		VMCB_OFF_CTRL(0x48)
+#define	VMCB_OFF_TSC_OFFSET		VMCB_OFF_CTRL(0x50)
+#define	VMCB_OFF_ASID			VMCB_OFF_CTRL(0x58)
+#define	VMCB_OFF_TLB_CTRL		VMCB_OFF_CTRL(0x5C)
+#define	VMCB_OFF_VIRQ			VMCB_OFF_CTRL(0x60)
+#define	VMCB_OFF_EXIT_REASON		VMCB_OFF_CTRL(0x70)
+#define	VMCB_OFF_EXITINFO1		VMCB_OFF_CTRL(0x78)
+#define	VMCB_OFF_EXITINFO2		VMCB_OFF_CTRL(0x80)
+#define	VMCB_OFF_EXITINTINFO		VMCB_OFF_CTRL(0x88)
+#define	VMCB_OFF_AVIC_BAR		VMCB_OFF_CTRL(0x98)
+#define	VMCB_OFF_NPT_BASE		VMCB_OFF_CTRL(0xB0)
+#define	VMCB_OFF_AVIC_PAGE		VMCB_OFF_CTRL(0xE0)
+#define	VMCB_OFF_AVIC_LT		VMCB_OFF_CTRL(0xF0)
+#define	VMCB_OFF_AVIC_PT		VMCB_OFF_CTRL(0xF8)
+#define	VMCB_OFF_SYSENTER_CS		VMCB_OFF_STATE(0x228)
+#define	VMCB_OFF_SYSENTER_ESP		VMCB_OFF_STATE(0x230)
+#define	VMCB_OFF_SYSENTER_EIP		VMCB_OFF_STATE(0x238)
+#define	VMCB_OFF_GUEST_PAT		VMCB_OFF_STATE(0x268)
+
+/*
+ * Encode the VMCB offset and bytes that we want to read from VMCB.
+ */
+#define	VMCB_ACCESS(o, w)		(0x80000000 | (((w) & 0xF) << 16) | \
+					((o) & 0xFFF))
+#define	VMCB_ACCESS_OK(v)               ((v) & 0x80000000 )
+#define	VMCB_ACCESS_BYTES(v)            (((v) >> 16) & 0xF)
+#define	VMCB_ACCESS_OFFSET(v)           ((v) & 0xFFF)
+
+#ifdef _KERNEL
+/* VMCB save state area segment format */
+struct vmcb_segment {
+	uint16_t	selector;
+	uint16_t	attrib;
+	uint32_t	limit;
+	uint64_t	base;
+} __attribute__ ((__packed__));
+CTASSERT(sizeof(struct vmcb_segment) == 16);
+
+/* Code segment descriptor attribute in 12 bit format as saved by VMCB. */
+#define	VMCB_CS_ATTRIB_L		BIT(9)	/* Long mode. */
+#define	VMCB_CS_ATTRIB_D		BIT(10)	/* OPerand size bit. */
+
+/*
+ * The VMCB is divided into two areas - the first one contains various
+ * control bits including the intercept vector and the second one contains
+ * the guest state.
+ */
+
+/* VMCB control area - padded up to 1024 bytes */
+struct vmcb_ctrl {
+	uint32_t intercept[5];	/* all intercepts */
+	uint8_t	 pad1[0x28];	/* Offsets 0x14-0x3B are reserved. */
+	uint16_t pause_filthresh; /* Offset 0x3C, PAUSE filter threshold */
+	uint16_t pause_filcnt;  /* Offset 0x3E, PAUSE filter count */
+	uint64_t iopm_base_pa;	/* 0x40: IOPM_BASE_PA */
+	uint64_t msrpm_base_pa; /* 0x48: MSRPM_BASE_PA */
+	uint64_t tsc_offset;	/* 0x50: TSC_OFFSET */
+	uint32_t asid;		/* 0x58: Guest ASID */
+	uint8_t	 tlb_ctrl;	/* 0x5C: TLB_CONTROL */
+	uint8_t  pad2[3];	/* 0x5D-0x5F: Reserved. */
+	uint8_t	 v_tpr;		/* 0x60: V_TPR, guest CR8 */
+	uint8_t	 v_irq:1;	/* Is virtual interrupt pending? */
+	uint8_t	:7; 		/* Padding */
+	uint8_t v_intr_prio:4;	/* 0x62: Priority for virtual interrupt. */
+	uint8_t v_ign_tpr:1;
+	uint8_t :3;
+	uint8_t	v_intr_masking:1; /* Guest and host sharing of RFLAGS. */
+	uint8_t	:7;
+	uint8_t	v_intr_vector;	/* 0x65: Vector for virtual interrupt. */
+	uint8_t pad3[3];	/* Bit64-40 Reserved. */
+	uint64_t intr_shadow:1; /* 0x68: Interrupt shadow, section15.2.1 APM2 */
+	uint64_t :63;
+	uint64_t exitcode;	/* 0x70, Exitcode */
+	uint64_t exitinfo1;	/* 0x78, EXITINFO1 */
+	uint64_t exitinfo2;	/* 0x80, EXITINFO2 */
+	uint64_t exitintinfo;	/* 0x88, Interrupt exit value. */
+	uint64_t np_enable:1;   /* 0x90, Nested paging enable. */
+	uint64_t :63;
+	uint8_t  pad4[0x10];	/* 0x98-0xA7 reserved. */
+	uint64_t eventinj;	/* 0xA8, Event injection. */
+	uint64_t n_cr3;		/* B0, Nested page table. */
+	uint64_t lbr_virt_en:1;	/* Enable LBR virtualization. */
+	uint64_t :63;
+	uint32_t vmcb_clean;	/* 0xC0: VMCB clean bits for caching */
+	uint32_t :32;		/* 0xC4: Reserved */
+	uint64_t nrip;		/* 0xC8: Guest next nRIP. */
+	uint8_t	inst_len;	/* 0xD0: #NPF decode assist */
+	uint8_t	inst_bytes[15];
+	uint8_t	padd6[0x320];
+} __attribute__ ((__packed__));
+CTASSERT(sizeof(struct vmcb_ctrl) == 1024);
+
+struct vmcb_state {
+	struct   vmcb_segment es;
+	struct   vmcb_segment cs;
+	struct   vmcb_segment ss;
+	struct   vmcb_segment ds;
+	struct   vmcb_segment fs;
+	struct   vmcb_segment gs;
+	struct   vmcb_segment gdt;
+	struct   vmcb_segment ldt;
+	struct   vmcb_segment idt;
+	struct   vmcb_segment tr;
+	uint8_t	 pad1[0x2b];		/* Reserved: 0xA0-0xCA */
+	uint8_t	 cpl;
+	uint8_t  pad2[4];
+	uint64_t efer;
+	uint8_t	 pad3[0x70];		/* Reserved: 0xd8-0x147 */
+	uint64_t cr4;
+	uint64_t cr3;			/* Guest CR3 */
+	uint64_t cr0;
+	uint64_t dr7;
+	uint64_t dr6;
+	uint64_t rflags;
+	uint64_t rip;
+	uint8_t	 pad4[0x58]; 		/* Reserved: 0x180-0x1D7 */
+	uint64_t rsp;
+	uint8_t	 pad5[0x18]; 		/* Reserved 0x1E0-0x1F7 */
+	uint64_t rax;
+	uint64_t star;
+	uint64_t lstar;
+	uint64_t cstar;
+	uint64_t sfmask;
+	uint64_t kernelgsbase;
+	uint64_t sysenter_cs;
+	uint64_t sysenter_esp;
+	uint64_t sysenter_eip;
+	uint64_t cr2;
+	uint8_t	 pad6[0x20];
+	uint64_t g_pat;
+	uint64_t dbgctl;
+	uint64_t br_from;
+	uint64_t br_to;
+	uint64_t int_from;
+	uint64_t int_to;
+	uint8_t	 pad7[0x968];		/* Reserved up to end of VMCB */
+} __attribute__ ((__packed__));
+CTASSERT(sizeof(struct vmcb_state) == 0xC00);
+
+struct vmcb {
+	struct vmcb_ctrl ctrl;
+	struct vmcb_state state;
+} __attribute__ ((__packed__));
+CTASSERT(sizeof(struct vmcb) == PAGE_SIZE);
+CTASSERT(offsetof(struct vmcb, state) == 0x400);
+
+int	vmcb_read(struct svm_softc *sc, int vcpu, int ident, uint64_t *retval);
+int	vmcb_write(struct svm_softc *sc, int vcpu, int ident, uint64_t val);
+int	vmcb_setdesc(void *arg, int vcpu, int ident, struct seg_desc *desc);
+int	vmcb_getdesc(void *arg, int vcpu, int ident, struct seg_desc *desc);
+int	vmcb_seg(struct vmcb *vmcb, int ident, struct vmcb_segment *seg);
+
+#endif /* _KERNEL */
+#endif /* _VMCB_H_ */
diff --git a/usr/src/uts/i86pc/io/vmm/intel/ept.c b/usr/src/uts/i86pc/io/vmm/intel/ept.c
index 5ae9ed2f6a..7553176ef4 100644
--- a/usr/src/uts/i86pc/io/vmm/intel/ept.c
+++ b/usr/src/uts/i86pc/io/vmm/intel/ept.c
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/intel/ept.c 252475 2013-07-01 20:05:43Z grehan $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -39,33 +39,32 @@
  */
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/sys/amd64/vmm/intel/ept.c 252475 2013-07-01 20:05:43Z grehan $");
+__FBSDID("$FreeBSD$");
 
+#include <sys/param.h>
+#include <sys/kernel.h>
 #include <sys/types.h>
-#include <sys/errno.h>
 #include <sys/systm.h>
-#include <sys/malloc.h>
 #include <sys/smp.h>
+#include <sys/sysctl.h>
 
 #include <vm/vm.h>
 #include <vm/pmap.h>
-
-#include <machine/param.h>
-#include <machine/cpufunc.h>
-#include <machine/pmap.h>
-#include <machine/vmparam.h>
+#include <vm/vm_extern.h>
 
 #include <machine/vmm.h>
+
 #include "vmx_cpufunc.h"
-#include "vmx.h"
 #include "ept.h"
 
+#define	EPT_SUPPORTS_EXEC_ONLY(cap)	((cap) & (1UL << 0))
 #define	EPT_PWL4(cap)			((cap) & (1UL << 6))
 #define	EPT_MEMORY_TYPE_WB(cap)		((cap) & (1UL << 14))
 #define	EPT_PDE_SUPERPAGE(cap)		((cap) & (1UL << 16))	/* 2MB pages */
 #define	EPT_PDPTE_SUPERPAGE(cap)	((cap) & (1UL << 17))	/* 1GB pages */
-#define	INVVPID_SUPPORTED(cap)		((cap) & (1UL << 32))
 #define	INVEPT_SUPPORTED(cap)		((cap) & (1UL << 20))
+#define	AD_BITS_SUPPORTED(cap)		((cap) & (1UL << 21))
+#define	INVVPID_SUPPORTED(cap)		((cap) & (1UL << 32))
 
 #define	INVVPID_ALL_TYPES_MASK		0xF0000000000UL
 #define	INVVPID_ALL_TYPES_SUPPORTED(cap)	\
@@ -75,28 +74,22 @@ __FBSDID("$FreeBSD: head/sys/amd64/vmm/intel/ept.c 252475 2013-07-01 20:05:43Z g
 #define	INVEPT_ALL_TYPES_SUPPORTED(cap)		\
 	(((cap) & INVEPT_ALL_TYPES_MASK) == INVEPT_ALL_TYPES_MASK)
 
-#define	EPT_PG_RD			(1 << 0)
-#define	EPT_PG_WR			(1 << 1)
-#define	EPT_PG_EX			(1 << 2)
-#define	EPT_PG_MEMORY_TYPE(x)		((x) << 3)
-#define	EPT_PG_IGNORE_PAT		(1 << 6)
-#define	EPT_PG_SUPERPAGE		(1 << 7)
+#define	EPT_PWLEVELS		4		/* page walk levels */
+#define	EPT_ENABLE_AD_BITS	(1 << 6)
 
-#define	EPT_ADDR_MASK			((uint64_t)-1 << 12)
+SYSCTL_DECL(_hw_vmm);
+SYSCTL_NODE(_hw_vmm, OID_AUTO, ept, CTLFLAG_RW, NULL, NULL);
 
-MALLOC_DECLARE(M_VMX);
+static int ept_enable_ad_bits;
 
-static uint64_t page_sizes_mask;
-
-/*
- * Set this to 1 to have the EPT tables respect the guest PAT settings
- */
-static int ept_pat_passthru;
+static int ept_pmap_flags;
+SYSCTL_INT(_hw_vmm_ept, OID_AUTO, pmap_flags, CTLFLAG_RD,
+    &ept_pmap_flags, 0, NULL);
 
 int
-ept_init(void)
+ept_init(int ipinum)
 {
-	int page_shift;
+	int use_hw_ad_bits, use_superpages, use_exec_only;
 	uint64_t cap;
 
 	cap = rdmsr(MSR_VMX_EPT_VPID_CAP);
@@ -116,17 +109,24 @@ ept_init(void)
 	    !INVEPT_ALL_TYPES_SUPPORTED(cap))
 		return (EINVAL);
 
-	/* Set bits in 'page_sizes_mask' for each valid page size */
-	page_shift = PAGE_SHIFT;
-	page_sizes_mask = 1UL << page_shift;		/* 4KB page */
+	ept_pmap_flags = ipinum & PMAP_NESTED_IPIMASK;
+
+	use_superpages = 1;
+	TUNABLE_INT_FETCH("hw.vmm.ept.use_superpages", &use_superpages);
+	if (use_superpages && EPT_PDE_SUPERPAGE(cap))
+		ept_pmap_flags |= PMAP_PDE_SUPERPAGE;	/* 2MB superpage */
 
-	page_shift += 9;
-	if (EPT_PDE_SUPERPAGE(cap))
-		page_sizes_mask |= 1UL << page_shift;	/* 2MB superpage */
+	use_hw_ad_bits = 1;
+	TUNABLE_INT_FETCH("hw.vmm.ept.use_hw_ad_bits", &use_hw_ad_bits);
+	if (use_hw_ad_bits && AD_BITS_SUPPORTED(cap))
+		ept_enable_ad_bits = 1;
+	else
+		ept_pmap_flags |= PMAP_EMULATE_AD_BITS;
 
-	page_shift += 9;
-	if (EPT_PDPTE_SUPERPAGE(cap))
-		page_sizes_mask |= 1UL << page_shift;	/* 1GB superpage */
+	use_exec_only = 1;
+	TUNABLE_INT_FETCH("hw.vmm.ept.use_exec_only", &use_exec_only);
+	if (use_exec_only && EPT_SUPPORTS_EXEC_ONLY(cap))
+		ept_pmap_flags |= PMAP_SUPPORTS_EXEC_ONLY;
 
 	return (0);
 }
@@ -165,288 +165,53 @@ ept_dump(uint64_t *ptp, int nlevels)
 }
 #endif
 
-static size_t
-ept_create_mapping(uint64_t *ptp, vm_paddr_t gpa, vm_paddr_t hpa, size_t length,
-		   vm_memattr_t attr, vm_prot_t prot, boolean_t spok)
-{
-	int spshift, ptpshift, ptpindex, nlevels;
-
-	/*
-	 * Compute the size of the mapping that we can accomodate.
-	 *
-	 * This is based on three factors:
-	 * - super page sizes supported by the processor
-	 * - alignment of the region starting at 'gpa' and 'hpa'
-	 * - length of the region 'len'
-	 */
-	spshift = PAGE_SHIFT;
-	if (spok)
-		spshift += (EPT_PWLEVELS - 1) * 9;
-	while (spshift >= PAGE_SHIFT) {
-		uint64_t spsize = 1UL << spshift;
-		if ((page_sizes_mask & spsize) != 0 &&
-		    (gpa & (spsize - 1)) == 0 &&
-		    (hpa & (spsize - 1)) == 0 &&
-		    length >= spsize) {
-			break;
-		}
-		spshift -= 9;
-	}
-
-	if (spshift < PAGE_SHIFT) {
-		panic("Invalid spshift for gpa 0x%016lx, hpa 0x%016lx, "
-		      "length 0x%016lx, page_sizes_mask 0x%016lx",
-		      gpa, hpa, length, page_sizes_mask);
-	}
-
-	nlevels = EPT_PWLEVELS;
-	while (--nlevels >= 0) {
-		ptpshift = PAGE_SHIFT + nlevels * 9;
-		ptpindex = (gpa >> ptpshift) & 0x1FF;
-
-		/* We have reached the leaf mapping */
-		if (spshift >= ptpshift)
-			break;
-
-		/*
-		 * We are working on a non-leaf page table page.
-		 *
-		 * Create the next level page table page if necessary and point
-		 * to it from the current page table.
-		 */
-		if (ptp[ptpindex] == 0) {
-#ifdef	__FreeBSD__
-			void *nlp = malloc(PAGE_SIZE, M_VMX, M_WAITOK | M_ZERO);
-#else
-			void *nlp = kmem_zalloc(PAGE_SIZE, KM_SLEEP);
-			ASSERT((((uintptr_t)nlp) & PAGE_MASK) == 0);
-#endif
-			ptp[ptpindex] = vtophys(nlp);
-			ptp[ptpindex] |= EPT_PG_RD | EPT_PG_WR | EPT_PG_EX;
-		}
-
-		/* Work our way down to the next level page table page */
-#ifdef	__FreeBSD__
-		ptp = (uint64_t *)PHYS_TO_DMAP(ptp[ptpindex] & EPT_ADDR_MASK);
-#else
-		ptp = (uint64_t *)hat_kpm_pfn2va(btop(ptp[ptpindex] & EPT_ADDR_MASK));
-#endif
-	}
-
-	if ((gpa & ((1UL << ptpshift) - 1)) != 0) {
-		panic("ept_create_mapping: gpa 0x%016lx and ptpshift %d "
-		      "mismatch\n", gpa, ptpshift);
-	}
-
-	if (prot != VM_PROT_NONE) {
-		/* Do the mapping */
-		ptp[ptpindex] = hpa;
-
-		/* Apply the access controls */
-		if (prot & VM_PROT_READ)
-			ptp[ptpindex] |= EPT_PG_RD;
-		if (prot & VM_PROT_WRITE)
-			ptp[ptpindex] |= EPT_PG_WR;
-		if (prot & VM_PROT_EXECUTE)
-			ptp[ptpindex] |= EPT_PG_EX;
-
-		/*
-		 * By default the PAT type is ignored - this appears to
-		 * be how other hypervisors handle EPT. Allow this to be
-		 * overridden.
-		 */
-		ptp[ptpindex] |= EPT_PG_MEMORY_TYPE(attr);
-		if (!ept_pat_passthru)
-			ptp[ptpindex] |= EPT_PG_IGNORE_PAT;
-
-		if (nlevels > 0)
-			ptp[ptpindex] |= EPT_PG_SUPERPAGE;
-	} else {
-		/* Remove the mapping */
-		ptp[ptpindex] = 0;
-	}
-
-	return (1UL << ptpshift);
-}
-
-static vm_paddr_t
-ept_lookup_mapping(uint64_t *ptp, vm_paddr_t gpa)
-{
-	int nlevels, ptpshift, ptpindex;
-	uint64_t ptpval, hpabase, pgmask;
-
-	nlevels = EPT_PWLEVELS;
-	while (--nlevels >= 0) {
-		ptpshift = PAGE_SHIFT + nlevels * 9;
-		ptpindex = (gpa >> ptpshift) & 0x1FF;
-
-		ptpval = ptp[ptpindex];
-
-		/* Cannot make progress beyond this point */
-		if ((ptpval & (EPT_PG_RD | EPT_PG_WR | EPT_PG_EX)) == 0)
-			break;
-
-		if (nlevels == 0 || (ptpval & EPT_PG_SUPERPAGE)) {
-			pgmask = (1UL << ptpshift) - 1;
-			hpabase = ptpval & ~pgmask;
-			return (hpabase | (gpa & pgmask));
-		}
-
-		/* Work our way down to the next level page table page */
-#ifdef	__FreBSD__
-		ptp = (uint64_t *)PHYS_TO_DMAP(ptpval & EPT_ADDR_MASK);
-#else
-		ptp = (uint64_t *)hat_kpm_pfn2va(btop(ptpval & EPT_ADDR_MASK));
-#endif
-	}
-
-	return ((vm_paddr_t)-1);
-}
-
 static void
-ept_free_pt_entry(pt_entry_t pte)
+invept_single_context(void *arg)
 {
-	if (pte == 0)
-		return;
-
-	/* sanity check */
-	if ((pte & EPT_PG_SUPERPAGE) != 0)
-		panic("ept_free_pt_entry: pte cannot have superpage bit");
+	struct invept_desc desc = *(struct invept_desc *)arg;
 
-	return;
+	invept(INVEPT_TYPE_SINGLE_CONTEXT, desc);
 }
 
-static void
-ept_free_pd_entry(pd_entry_t pde)
+void
+ept_invalidate_mappings(u_long eptp)
 {
-	pt_entry_t	*pt;
-	int		i;
+	struct invept_desc invept_desc = { 0 };
 
-	if (pde == 0)
-		return;
+	invept_desc.eptp = eptp;
 
-	if ((pde & EPT_PG_SUPERPAGE) == 0) {
-#ifdef	__FreeBSD__
-		pt = (pt_entry_t *)PHYS_TO_DMAP(pde & EPT_ADDR_MASK);
-		for (i = 0; i < NPTEPG; i++)
-			ept_free_pt_entry(pt[i]);
-		free(pt, M_VMX);	/* free the page table page */
-#else
-		page_t		*pp;
-		pt = (pt_entry_t *)hat_kpm_pfn2va(btop(pde & EPT_ADDR_MASK));
-		for (i = 0; i < NPTEPG; i++)
-			ept_free_pt_entry(pt[i]);
-		pp = page_numtopp_nolock(btop(pde & EPT_ADDR_MASK));
-		kmem_free((void *)pp->p_offset, PAGE_SIZE);
-#endif
-	}
+	smp_rendezvous(NULL, invept_single_context, NULL, &invept_desc);
 }
 
-static void
-ept_free_pdp_entry(pdp_entry_t pdpe)
+static int
+ept_pinit(pmap_t pmap)
 {
-	pd_entry_t 	*pd;
-	int		 i;
 
-	if (pdpe == 0)
-		return;
-
-	if ((pdpe & EPT_PG_SUPERPAGE) == 0) {
-#ifdef	__FreeBSD__
-		pd = (pd_entry_t *)PHYS_TO_DMAP(pdpe & EPT_ADDR_MASK);
-		for (i = 0; i < NPDEPG; i++)
-			ept_free_pd_entry(pd[i]);
-		free(pd, M_VMX);	/* free the page directory page */
-#else
-		page_t		*pp;
-		pd = (pd_entry_t *)hat_kpm_pfn2va(btop(pdpe & EPT_ADDR_MASK));
-		for (i = 0; i < NPDEPG; i++)
-			ept_free_pd_entry(pd[i]);
-		pp = page_numtopp_nolock(btop(pdpe & EPT_ADDR_MASK));
-		kmem_free((void *)pp->p_offset, PAGE_SIZE);
-#endif
-	}
+	return (pmap_pinit_type(pmap, PT_EPT, ept_pmap_flags));
 }
 
-static void
-ept_free_pml4_entry(pml4_entry_t pml4e)
+struct vmspace *
+ept_vmspace_alloc(vm_offset_t min, vm_offset_t max)
 {
-	pdp_entry_t	*pdp;
-	int		i;
 
-	if (pml4e == 0)
-		return;
-
-	if ((pml4e & EPT_PG_SUPERPAGE) == 0) {
-#ifdef	__FreeBSD__
-		pdp = (pdp_entry_t *)PHYS_TO_DMAP(pml4e & EPT_ADDR_MASK);
-		for (i = 0; i < NPDPEPG; i++)
-			ept_free_pdp_entry(pdp[i]);
-		free(pdp, M_VMX);	/* free the page directory ptr page */
-#else
-		page_t		*pp;
-		pdp = (pdp_entry_t *)hat_kpm_pfn2va(btop(pml4e
-		    & EPT_ADDR_MASK));
-		for (i = 0; i < NPDPEPG; i++)
-			ept_free_pdp_entry(pdp[i]);
-		pp = page_numtopp_nolock(btop(pml4e & EPT_ADDR_MASK));
-		kmem_free((void *)pp->p_offset, PAGE_SIZE);
-#endif
-	}
+	return (vmspace_alloc(min, max, ept_pinit));
 }
 
 void
-ept_vmcleanup(struct vmx *vmx)
-{
-	int 		 i;
-
-	for (i = 0; i < NPML4EPG; i++)
-		ept_free_pml4_entry(vmx->pml4ept[i]);
-}
-
-int
-ept_vmmmap_set(void *arg, vm_paddr_t gpa, vm_paddr_t hpa, size_t len,
-		vm_memattr_t attr, int prot, boolean_t spok)
+ept_vmspace_free(struct vmspace *vmspace)
 {
-	size_t n;
-	struct vmx *vmx = arg;
-
-	while (len > 0) {
-		n = ept_create_mapping(vmx->pml4ept, gpa, hpa, len, attr,
-				       prot, spok);
-		len -= n;
-		gpa += n;
-		hpa += n;
-	}
 
-	return (0);
+	vmspace_free(vmspace);
 }
 
-vm_paddr_t
-ept_vmmmap_get(void *arg, vm_paddr_t gpa)
+uint64_t
+eptp(uint64_t pml4)
 {
-	vm_paddr_t hpa;
-	struct vmx *vmx;
+	uint64_t eptp_val;
 
-	vmx = arg;
-	hpa = ept_lookup_mapping(vmx->pml4ept, gpa);
-	return (hpa);
-}
+	eptp_val = pml4 | (EPT_PWLEVELS - 1) << 3 | PAT_WRITE_BACK;
+	if (ept_enable_ad_bits)
+		eptp_val |= EPT_ENABLE_AD_BITS;
 
-static void
-invept_single_context(void *arg)
-{
-	struct invept_desc desc = *(struct invept_desc *)arg;
-
-	invept(INVEPT_TYPE_SINGLE_CONTEXT, desc);
-}
-
-void
-ept_invalidate_mappings(u_long pml4ept)
-{
-	struct invept_desc invept_desc = { 0 };
-
-	invept_desc.eptp = EPTP(pml4ept);
-
-	smp_rendezvous(NULL, invept_single_context, NULL, &invept_desc);
+	return (eptp_val);
 }
diff --git a/usr/src/uts/i86pc/io/vmm/intel/ept.h b/usr/src/uts/i86pc/io/vmm/intel/ept.h
index d0bcce7ec3..1393e467ee 100644
--- a/usr/src/uts/i86pc/io/vmm/intel/ept.h
+++ b/usr/src/uts/i86pc/io/vmm/intel/ept.h
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/intel/ept.h 245678 2013-01-20 03:42:49Z neel $
+ * $FreeBSD$
  */
 
 #ifndef	_EPT_H_
@@ -31,13 +31,9 @@
 
 struct vmx;
 
-#define	EPT_PWLEVELS	4		/* page walk levels */
-#define	EPTP(pml4)	((pml4) | (EPT_PWLEVELS - 1) << 3 | PAT_WRITE_BACK)
-
-int	ept_init(void);
-int	ept_vmmmap_set(void *arg, vm_paddr_t gpa, vm_paddr_t hpa, size_t length,
-	    vm_memattr_t attr, int prot, boolean_t allow_superpage_mappings);
-vm_paddr_t ept_vmmmap_get(void *arg, vm_paddr_t gpa);
-void	ept_invalidate_mappings(u_long ept_pml4);
-void	ept_vmcleanup(struct vmx *vmx);
+int	ept_init(int ipinum);
+void	ept_invalidate_mappings(u_long eptp);
+struct vmspace *ept_vmspace_alloc(vm_offset_t min, vm_offset_t max);
+void	ept_vmspace_free(struct vmspace *vmspace);
+uint64_t eptp(uint64_t pml4);
 #endif
diff --git a/usr/src/uts/i86pc/io/vmm/intel/offsets.in b/usr/src/uts/i86pc/io/vmm/intel/offsets.in
new file mode 100644
index 0000000000..d60a2d8f5f
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/intel/offsets.in
@@ -0,0 +1,62 @@
+/*
+ * COPYRIGHT 2014 Pluribus Networks Inc.
+ *
+ * All rights reserved. This copyright notice is Copyright Management
+ * Information under 17 USC 1202 and is included to protect this work and
+ * deter copyright infringement.  Removal or alteration of this Copyright
+ * Management Information without the express written permission from
+ * Pluribus Networks Inc is prohibited, and any such unauthorized removal
+ * or alteration will be a violation of federal law.
+ */
+#include <sys/types.h>
+#include <sys/param.h>
+#include <sys/systm.h>
+#include <sys/cpuvar.h>
+
+#include <machine/pmap.h>
+#include <machine/vmm.h>
+
+#include "intel/vmx_cpufunc.h"
+#include "intel/vmx.h"
+#include "vm/vm_glue.h"
+
+vmxctx
+	guest_rdi		VMXCTX_GUEST_RDI
+	guest_rsi		VMXCTX_GUEST_RSI
+	guest_rdx		VMXCTX_GUEST_RDX
+	guest_rcx		VMXCTX_GUEST_RCX
+	guest_r8		VMXCTX_GUEST_R8
+	guest_r9		VMXCTX_GUEST_R9
+	guest_rax		VMXCTX_GUEST_RAX
+	guest_rbx		VMXCTX_GUEST_RBX
+	guest_rbp		VMXCTX_GUEST_RBP
+	guest_r10		VMXCTX_GUEST_R10
+	guest_r11		VMXCTX_GUEST_R11
+	guest_r12		VMXCTX_GUEST_R12
+	guest_r13		VMXCTX_GUEST_R13
+	guest_r14		VMXCTX_GUEST_R14
+	guest_r15		VMXCTX_GUEST_R15
+	guest_cr2		VMXCTX_GUEST_CR2
+	inst_fail_status	VMXCTX_INST_FAIL_STATUS
+	pmap			VMXCTX_PMAP
+
+vmx
+	eptgen		VMX_EPTGEN
+	eptp		VMX_EPTP
+
+pmap
+	pm_active	PM_ACTIVE
+	pm_eptgen	PM_EPTGEN
+
+cpu
+	cpu_id
+
+\#define	VM_SUCCESS		0
+\#define	VM_FAIL_INVALID		1
+\#define	VM_FAIL_VALID		2
+
+\#define	VMX_GUEST_VMEXIT	0
+\#define	VMX_VMRESUME_ERROR	1
+\#define	VMX_VMLAUNCH_ERROR	2
+\#define	VMX_INVEPT_ERROR	3
+\#define	VMX_VMWRITE_ERROR	4
diff --git a/usr/src/uts/i86pc/io/vmm/intel/vmcs.c b/usr/src/uts/i86pc/io/vmm/intel/vmcs.c
index 1f31959ec2..6198184065 100644
--- a/usr/src/uts/i86pc/io/vmm/intel/vmcs.c
+++ b/usr/src/uts/i86pc/io/vmm/intel/vmcs.c
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/intel/vmcs.c 266550 2014-05-22 17:22:37Z neel $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -44,7 +44,7 @@
 #endif
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/sys/amd64/vmm/intel/vmcs.c 266550 2014-05-22 17:22:37Z neel $");
+__FBSDID("$FreeBSD$");
 
 #include <sys/param.h>
 #include <sys/systm.h>
@@ -118,6 +118,14 @@ vmcs_field_encoding(int ident)
 		return (VMCS_GUEST_LDTR_SELECTOR);
 	case VM_REG_GUEST_EFER:
 		return (VMCS_GUEST_IA32_EFER);
+	case VM_REG_GUEST_PDPTE0:
+		return (VMCS_GUEST_PDPTE0);
+	case VM_REG_GUEST_PDPTE1:
+		return (VMCS_GUEST_PDPTE1);
+	case VM_REG_GUEST_PDPTE2:
+		return (VMCS_GUEST_PDPTE2);
+	case VM_REG_GUEST_PDPTE3:
+		return (VMCS_GUEST_PDPTE3);
 	default:
 		return (-1);
 	}
@@ -333,43 +341,15 @@ done:
 	return (error);
 }
 
-#ifndef	__FreeBSD__
 int
-vmcs_set_host_msr_save(struct vmcs *vmcs, u_long h_area, u_int h_count)
-{
-	int error;
-
-	VMPTRLD(vmcs);
-
-	/*
-	 * Host MSRs are loaded from the VM-exit MSR-load area.
-	 */
-	if ((error = vmwrite(VMCS_EXIT_MSR_LOAD, h_area)) != 0)
-		goto done;
-	if ((error = vmwrite(VMCS_EXIT_MSR_LOAD_COUNT, h_count)) != 0)
-		goto done;
-
-	error = 0;
-done:
-	VMCLEAR(vmcs);
-	return (error);
-}
-#endif
-
-int
-vmcs_set_defaults(struct vmcs *vmcs,
-		  u_long host_rip, u_long host_rsp, u_long ept_pml4,
-		  uint32_t pinbased_ctls, uint32_t procbased_ctls,
-		  uint32_t procbased_ctls2, uint32_t exit_ctls,
-		  uint32_t entry_ctls, u_long msr_bitmap, uint16_t vpid)
+vmcs_init(struct vmcs *vmcs)
 {
 	int error, codesel, datasel, tsssel;
 	u_long cr0, cr4, efer;
-	uint64_t eptp, pat, idtrbase;
+	uint64_t pat;
 #ifdef	__FreeBSD__
-	uint64_t fsbase;
+	uint64_t fsbase, idtrbase;
 #endif
-	uint32_t exc_bitmap;
 
 	codesel = vmm_get_host_codesel();
 	datasel = vmm_get_host_datasel();
@@ -380,34 +360,6 @@ vmcs_set_defaults(struct vmcs *vmcs,
 	 */
 	VMPTRLD(vmcs);
 
-	/*
-	 * Load the VMX controls
-	 */
-	if ((error = vmwrite(VMCS_PIN_BASED_CTLS, pinbased_ctls)) != 0)
-		goto done;
-	if ((error = vmwrite(VMCS_PRI_PROC_BASED_CTLS, procbased_ctls)) != 0)
-		goto done;
-	if ((error = vmwrite(VMCS_SEC_PROC_BASED_CTLS, procbased_ctls2)) != 0)
-		goto done;
-	if ((error = vmwrite(VMCS_EXIT_CTLS, exit_ctls)) != 0)
-		goto done;
-	if ((error = vmwrite(VMCS_ENTRY_CTLS, entry_ctls)) != 0)
-		goto done;
-
-	/* Guest state */
-
-	/* Initialize guest IA32_PAT MSR with the default value */
-	pat = PAT_VALUE(0, PAT_WRITE_BACK)	|
-	      PAT_VALUE(1, PAT_WRITE_THROUGH)	|
-	      PAT_VALUE(2, PAT_UNCACHED)	|
-	      PAT_VALUE(3, PAT_UNCACHEABLE)	|
-	      PAT_VALUE(4, PAT_WRITE_BACK)	|
-	      PAT_VALUE(5, PAT_WRITE_THROUGH)	|
-	      PAT_VALUE(6, PAT_UNCACHED)	|
-	      PAT_VALUE(7, PAT_UNCACHEABLE);
-	if ((error = vmwrite(VMCS_GUEST_IA32_PAT, pat)) != 0)
-		goto done;
-
 	/* Host state */
 
 	/* Initialize host IA32_PAT MSR */
@@ -470,36 +422,27 @@ vmcs_set_defaults(struct vmcs *vmcs,
 	fsbase = vmm_get_host_fsbase();
 	if ((error = vmwrite(VMCS_HOST_FS_BASE, fsbase)) != 0)
 		goto done;
-#endif
 
 	idtrbase = vmm_get_host_idtrbase();
 	if ((error = vmwrite(VMCS_HOST_IDTR_BASE, idtrbase)) != 0)
 		goto done;
 
-	/* instruction pointer */
-	if ((error = vmwrite(VMCS_HOST_RIP, host_rip)) != 0)
-		goto done;
-
-	/* stack pointer */
-	if ((error = vmwrite(VMCS_HOST_RSP, host_rsp)) != 0)
-		goto done;
-
-	/* eptp */
-	eptp = EPTP(ept_pml4);
-	if ((error = vmwrite(VMCS_EPTP, eptp)) != 0)
+#else /* __FreeBSD__ */
+	/*
+	 * Configure host sysenter MSRs to be restored on VM exit.
+	 * The thread-specific MSR_INTC_SEP_ESP value is loaded in vmx_run.
+	 */
+	if ((error = vmwrite(VMCS_HOST_IA32_SYSENTER_CS, KCS_SEL)) != 0)
 		goto done;
-
-	/* vpid */
-	if ((error = vmwrite(VMCS_VPID, vpid)) != 0)
+	/* Natively defined as MSR_INTC_SEP_EIP */
+	if ((error = vmwrite(VMCS_HOST_IA32_SYSENTER_EIP,
+	    rdmsr(MSR_SYSENTER_EIP_MSR))) != 0)
 		goto done;
 
-	/* msr bitmap */
-	if ((error = vmwrite(VMCS_MSR_BITMAP, msr_bitmap)) != 0)
-		goto done;
+#endif /* __FreeBSD__ */
 
-	/* exception bitmap */
-	exc_bitmap = 1 << IDT_MC;
-	if ((error = vmwrite(VMCS_EXCEPTION_BITMAP, exc_bitmap)) != 0)
+	/* instruction pointer */
+	if ((error = vmwrite(VMCS_HOST_RIP, (u_long)vmx_exit_guest)) != 0)
 		goto done;
 
 	/* link pointer */
diff --git a/usr/src/uts/i86pc/io/vmm/intel/vmcs.h b/usr/src/uts/i86pc/io/vmm/intel/vmcs.h
index 20e99e8184..4eeaf26f15 100644
--- a/usr/src/uts/i86pc/io/vmm/intel/vmcs.h
+++ b/usr/src/uts/i86pc/io/vmm/intel/vmcs.h
@@ -23,19 +23,40 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/intel/vmcs.h 276098 2014-12-23 02:14:49Z neel $
+ * $FreeBSD$
+ */
+
+/*
+ * Copyright 2017 Joyent, Inc.
  */
 
 #ifndef _VMCS_H_
 #define	_VMCS_H_
 
 #ifdef _KERNEL
+#ifndef _ASM
 struct vmcs {
 	uint32_t	identifier;
 	uint32_t	abort_code;
 	char		_impl_specific[PAGE_SIZE - sizeof(uint32_t) * 2];
+#ifndef __FreeBSD__
+	/*
+	 * Keep the physical address of the VMCS cached adjacent for the
+	 * structure so it can be referenced in contexts which are too delicate
+	 * for a call into the HAT.  For the moment it means wasting a whole
+	 * page on padding for the PA value to maintain alignment, but it
+	 * allows the consumers of 'struct vmcs *' to easily access the value
+	 * without a significant change to the interface.
+	 */
+	uint64_t	vmcs_pa;
+	char		_pa_pad[PAGE_SIZE - sizeof (vm_paddr_t)];
+#endif
 };
+#ifdef __FreeBSD__
 CTASSERT(sizeof(struct vmcs) == PAGE_SIZE);
+#else
+CTASSERT(sizeof(struct vmcs) == (2*PAGE_SIZE));
+#endif
 
 /* MSR save region is composed of an array of 'struct msr_entry' */
 struct msr_entry {
@@ -47,15 +68,6 @@ struct msr_entry {
 
 int vmcs_set_msr_save(struct vmcs *vmcs, u_long g_area, u_int g_count);
 int	vmcs_init(struct vmcs *vmcs);
-#ifndef	__FreeBSD__
-int vmcs_set_host_msr_save(struct vmcs *vmcs, u_long h_area, u_int h_count);
-#endif
-int	vmcs_set_defaults(struct vmcs *vmcs, u_long host_rip, u_long host_rsp,
-			  u_long ept_pml4,
-			  uint32_t pinbased_ctls, uint32_t procbased_ctls,
-			  uint32_t procbased_ctls2, uint32_t exit_ctls,
-			  uint32_t entry_ctls, u_long msr_bitmap,
-			  uint16_t vpid);
 int	vmcs_getreg(struct vmcs *vmcs, int running, int ident, uint64_t *rv);
 int	vmcs_setreg(struct vmcs *vmcs, int running, int ident, uint64_t val);
 int	vmcs_getdesc(struct vmcs *vmcs, int running, int ident,
@@ -86,6 +98,65 @@ vmcs_write(uint32_t encoding, uint64_t val)
 	error = vmwrite(encoding, val);
 	KASSERT(error == 0, ("vmcs_write(%u) error %d", encoding, error));
 }
+
+#ifndef __FreeBSD__
+/*
+ * Due to header complexity combined with the need to cache the physical
+ * address for the VMCS, these must be defined here rather than vmx_cpufunc.h.
+ */
+static __inline int
+vmclear(struct vmcs *vmcs)
+{
+	int error;
+	uint64_t addr = vmcs->vmcs_pa;
+
+	__asm __volatile("vmclear %[addr];"
+			 VMX_SET_ERROR_CODE
+			 : [error] "=r" (error)
+			 : [addr] "m" (*(uint64_t *)&addr)
+			 : "memory");
+	return (error);
+}
+
+static __inline int
+vmptrld(struct vmcs *vmcs)
+{
+	int error;
+	uint64_t addr = vmcs->vmcs_pa;
+
+	__asm __volatile("vmptrld %[addr];"
+			 VMX_SET_ERROR_CODE
+			 : [error] "=r" (error)
+			 : [addr] "m" (*(uint64_t *)&addr)
+			 : "memory");
+	return (error);
+}
+
+static __inline void
+VMCLEAR(struct vmcs *vmcs)
+{
+	int err;
+
+	err = vmclear(vmcs);
+	if (err != 0)
+		panic("%s: vmclear(%p) error %d", __func__, vmcs, err);
+
+	critical_exit();
+}
+
+static __inline void
+VMPTRLD(struct vmcs *vmcs)
+{
+	int err;
+
+	critical_enter();
+
+	err = vmptrld(vmcs);
+	if (err != 0)
+		panic("%s: vmptrld(%p) error %d", __func__, vmcs, err);
+}
+#endif /* __FreeBSD__ */
+
 #endif	/* _VMX_CPUFUNC_H_ */
 
 #define	vmexit_instruction_length()	vmcs_read(VMCS_EXIT_INSTRUCTION_LENGTH)
@@ -99,6 +170,7 @@ vmcs_write(uint32_t encoding, uint64_t val)
 #define	vmcs_idt_vectoring_info()	vmcs_read(VMCS_IDT_VECTORING_INFO)
 #define	vmcs_idt_vectoring_err()	vmcs_read(VMCS_IDT_VECTORING_ERROR)
 
+#endif	/* _ASM */
 #endif	/* _KERNEL */
 
 #define	VMCS_INITIAL			0xffffffffffffffff
diff --git a/usr/src/uts/i86pc/io/vmm/intel/vmx.c b/usr/src/uts/i86pc/io/vmm/intel/vmx.c
index be2490fe44..c1810b8ed9 100644
--- a/usr/src/uts/i86pc/io/vmm/intel/vmx.c
+++ b/usr/src/uts/i86pc/io/vmm/intel/vmx.c
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/intel/vmx.c 284174 2015-06-09 00:14:47Z tychon $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -40,7 +40,7 @@
  */
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/sys/amd64/vmm/intel/vmx.c 284174 2015-06-09 00:14:47Z tychon $");
+__FBSDID("$FreeBSD$");
 
 #include <sys/param.h>
 #include <sys/systm.h>
@@ -51,6 +51,10 @@ __FBSDID("$FreeBSD: head/sys/amd64/vmm/intel/vmx.c 284174 2015-06-09 00:14:47Z t
 #include <sys/proc.h>
 #include <sys/sysctl.h>
 
+#ifndef __FreeBSD__
+#include <sys/x86_archext.h>
+#endif
+
 #include <vm/vm.h>
 #include <vm/pmap.h>
 
@@ -76,6 +80,7 @@ __FBSDID("$FreeBSD: head/sys/amd64/vmm/intel/vmx.c 284174 2015-06-09 00:14:47Z t
 
 #include "ept.h"
 #include "vmx_cpufunc.h"
+#include "vmcs.h"
 #include "vmx.h"
 #include "vmx_msr.h"
 #include "x86.h"
@@ -91,13 +96,30 @@ __FBSDID("$FreeBSD: head/sys/amd64/vmm/intel/vmx.c 284174 2015-06-09 00:14:47Z t
 	(PROCBASED_INT_WINDOW_EXITING	|				\
 	 PROCBASED_NMI_WINDOW_EXITING)
 
+#ifdef __FreeBSD__
+#define	PROCBASED_CTLS_ONE_SETTING 					\
+	(PROCBASED_SECONDARY_CONTROLS	|				\
+	 PROCBASED_MWAIT_EXITING	|				\
+	 PROCBASED_MONITOR_EXITING	|				\
+	 PROCBASED_IO_EXITING		|				\
+	 PROCBASED_MSR_BITMAPS		|				\
+	 PROCBASED_CTLS_WINDOW_SETTING	|				\
+	 PROCBASED_CR8_LOAD_EXITING	|				\
+	 PROCBASED_CR8_STORE_EXITING)
+#else
+/* We consider TSC offset a necessity for unsynched TSC handling */
 #define	PROCBASED_CTLS_ONE_SETTING 					\
 	(PROCBASED_SECONDARY_CONTROLS	|				\
+	 PROCBASED_TSC_OFFSET		|				\
+	 PROCBASED_MWAIT_EXITING	|				\
+	 PROCBASED_MONITOR_EXITING	|				\
 	 PROCBASED_IO_EXITING		|				\
 	 PROCBASED_MSR_BITMAPS		|				\
 	 PROCBASED_CTLS_WINDOW_SETTING	|				\
 	 PROCBASED_CR8_LOAD_EXITING	|				\
 	 PROCBASED_CR8_STORE_EXITING)
+#endif /* __FreeBSD__ */
+
 #define	PROCBASED_CTLS_ZERO_SETTING	\
 	(PROCBASED_CR3_LOAD_EXITING |	\
 	PROCBASED_CR3_STORE_EXITING |	\
@@ -110,13 +132,11 @@ __FBSDID("$FreeBSD: head/sys/amd64/vmm/intel/vmx.c 284174 2015-06-09 00:14:47Z t
 	(VM_EXIT_HOST_LMA			|			\
 	VM_EXIT_SAVE_EFER			|			\
 	VM_EXIT_LOAD_EFER			|			\
-	VM_EXIT_LOAD_PAT			|			\
-	VM_EXIT_SAVE_PAT			|			\
-	VM_EXIT_LOAD_PAT)
+	VM_EXIT_ACKNOWLEDGE_INTERRUPT)
 
 #define	VM_EXIT_CTLS_ZERO_SETTING	VM_EXIT_SAVE_DEBUG_CONTROLS
 
-#define	VM_ENTRY_CTLS_ONE_SETTING	(VM_ENTRY_LOAD_EFER | VM_ENTRY_LOAD_PAT)
+#define	VM_ENTRY_CTLS_ONE_SETTING	(VM_ENTRY_LOAD_EFER)
 
 #define	VM_ENTRY_CTLS_ZERO_SETTING					\
 	(VM_ENTRY_LOAD_DEBUG_CONTROLS		|			\
@@ -134,9 +154,7 @@ SYSCTL_NODE(_hw_vmm, OID_AUTO, vmx, CTLFLAG_RW, NULL, NULL);
 
 int vmxon_enabled[MAXCPU];
 static char vmxon_region[MAXCPU][PAGE_SIZE] __aligned(PAGE_SIZE);
-#ifndef	__FreeBSD__
-static vm_paddr_t vmxon_region_pa[MAXCPU];
-#endif
+static char *vmxon_region_pa[MAXCPU];
 
 static uint32_t pinbased_ctls, procbased_ctls, procbased_ctls2;
 static uint32_t exit_ctls, entry_ctls;
@@ -160,23 +178,37 @@ SYSCTL_INT(_hw_vmm_vmx, OID_AUTO, initialized, CTLFLAG_RD,
 /*
  * Optional capabilities
  */
+SYSCTL_NODE(_hw_vmm_vmx, OID_AUTO, cap, CTLFLAG_RW, NULL, NULL);
+
 static int cap_halt_exit;
+SYSCTL_INT(_hw_vmm_vmx_cap, OID_AUTO, halt_exit, CTLFLAG_RD, &cap_halt_exit, 0,
+    "HLT triggers a VM-exit");
+
 static int cap_pause_exit;
+SYSCTL_INT(_hw_vmm_vmx_cap, OID_AUTO, pause_exit, CTLFLAG_RD, &cap_pause_exit,
+    0, "PAUSE triggers a VM-exit");
+
 static int cap_unrestricted_guest;
+SYSCTL_INT(_hw_vmm_vmx_cap, OID_AUTO, unrestricted_guest, CTLFLAG_RD,
+    &cap_unrestricted_guest, 0, "Unrestricted guests");
+
 static int cap_monitor_trap;
-#if notyet
+SYSCTL_INT(_hw_vmm_vmx_cap, OID_AUTO, monitor_trap, CTLFLAG_RD,
+    &cap_monitor_trap, 0, "Monitor trap flag");
+
 static int cap_invpcid;
-#endif
+SYSCTL_INT(_hw_vmm_vmx_cap, OID_AUTO, invpcid, CTLFLAG_RD, &cap_invpcid,
+    0, "Guests are allowed to use INVPCID");
 
 static int virtual_interrupt_delivery;
-SYSCTL_INT(_hw_vmm_vmx, OID_AUTO, virtual_interrupt_delivery, CTLFLAG_RD,
+SYSCTL_INT(_hw_vmm_vmx_cap, OID_AUTO, virtual_interrupt_delivery, CTLFLAG_RD,
     &virtual_interrupt_delivery, 0, "APICv virtual interrupt delivery support");
 
 static int posted_interrupts;
-SYSCTL_INT(_hw_vmm_vmx, OID_AUTO, posted_interrupts, CTLFLAG_RD,
+SYSCTL_INT(_hw_vmm_vmx_cap, OID_AUTO, posted_interrupts, CTLFLAG_RD,
     &posted_interrupts, 0, "APICv posted interrupt support");
 
-static int pirvec;
+static int pirvec = -1;
 SYSCTL_INT(_hw_vmm_vmx, OID_AUTO, posted_interrupt_vector, CTLFLAG_RD,
     &pirvec, 0, "APICv posted interrupt vector");
 
@@ -195,9 +227,7 @@ SYSCTL_UINT(_hw_vmm_vmx, OID_AUTO, vpid_alloc_failed, CTLFLAG_RD,
 static int vmx_getdesc(void *arg, int vcpu, int reg, struct seg_desc *desc);
 static int vmx_getreg(void *arg, int vcpu, int reg, uint64_t *retval);
 static int vmxctx_setreg(struct vmxctx *vmxctx, int reg, uint64_t val);
-#ifdef	__FreeBSD__
 static void vmx_inject_pir(struct vlapic *vlapic);
-#endif
 
 #ifdef KTR
 static const char *
@@ -284,8 +314,8 @@ exit_reason_to_str(int reason)
 		return "monitor";
 	case EXIT_REASON_PAUSE:
 		return "pause";
-	case EXIT_REASON_MCE:
-		return "mce";
+	case EXIT_REASON_MCE_DURING_ENTRY:
+		return "mce-during-entry";
 	case EXIT_REASON_TPR:
 		return "tpr";
 	case EXIT_REASON_APIC_ACCESS:
@@ -317,86 +347,8 @@ exit_reason_to_str(int reason)
 		return (reasonbuf);
 	}
 }
-
-#ifdef SETJMP_TRACE
-static const char *
-vmx_setjmp_rc2str(int rc)
-{
-	switch (rc) {
-	case VMX_RETURN_DIRECT:
-		return "direct";
-	case VMX_RETURN_LONGJMP:
-		return "longjmp";
-	case VMX_RETURN_VMRESUME:
-		return "vmresume";
-	case VMX_RETURN_VMLAUNCH:
-		return "vmlaunch";
-	case VMX_RETURN_AST:
-		return "ast";
-	default:
-		return "unknown";
-	}
-}
-
-#define	SETJMP_TRACE(vmx, vcpu, vmxctx, regname)			  \
-	VMM_CTR1((vmx)->vm, (vcpu), "setjmp trace " #regname " 0x%016lx", \
-		 (vmxctx)->regname)
-
-static void
-vmx_setjmp_trace(struct vmx *vmx, int vcpu, struct vmxctx *vmxctx, int rc)
-{
-	uint64_t host_rip, host_rsp;
-
-	if (vmxctx != &vmx->ctx[vcpu])
-		panic("vmx_setjmp_trace: invalid vmxctx %p; should be %p",
-			vmxctx, &vmx->ctx[vcpu]);
-
-	VMM_CTR1((vmx)->vm, (vcpu), "vmxctx = %p", vmxctx);
-	VMM_CTR2((vmx)->vm, (vcpu), "setjmp return code %s(%d)",
-		 vmx_setjmp_rc2str(rc), rc);
-
-	host_rsp = host_rip = ~0;
-	vmread(VMCS_HOST_RIP, &host_rip);
-	vmread(VMCS_HOST_RSP, &host_rsp);
-	VMM_CTR2((vmx)->vm, (vcpu), "vmcs host_rip 0x%016lx, host_rsp 0x%016lx",
-		 host_rip, host_rsp);
-
-	SETJMP_TRACE(vmx, vcpu, vmxctx, host_r15);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, host_r14);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, host_r13);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, host_r12);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, host_rbp);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, host_rsp);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, host_rbx);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, host_rip);
-
-	SETJMP_TRACE(vmx, vcpu, vmxctx, guest_rdi);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, guest_rsi);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, guest_rdx);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, guest_rcx);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, guest_r8);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, guest_r9);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, guest_rax);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, guest_rbx);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, guest_rbp);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, guest_r10);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, guest_r11);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, guest_r12);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, guest_r13);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, guest_r14);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, guest_r15);
-	SETJMP_TRACE(vmx, vcpu, vmxctx, guest_cr2);
-}
-#endif
-#else
-static __inline void
-vmx_setjmp_trace(struct vmx *vmx, int vcpu, struct vmxctx *vmxctx, int rc)
-{
-	return;
-}
 #endif	/* KTR */
 
-#if notyet
 static int
 vmx_allow_x2apic_msrs(struct vmx *vmx)
 {
@@ -444,7 +396,6 @@ vmx_allow_x2apic_msrs(struct vmx *vmx)
 
 	return (error);
 }
-#endif
 
 u_long
 vmx_fix_cr0(u_long cr0)
@@ -542,50 +493,6 @@ vpid_init(void)
 	vpid_unr = new_unrhdr(VM_MAXCPU + 1, 0xffff, NULL);
 }
 
-#ifndef	__FreeBSD__
-static void
-msr_save_area_init(struct msr_entry *g_area, int *g_count)
-{
-	int cnt;
-
-	static struct msr_entry guest_msrs[] = {
-		{ MSR_KGSBASE, 0, 0 },
-		{ MSR_LSTAR, 0, 0 },
-		{ MSR_CSTAR, 0, 0 },
-		{ MSR_STAR, 0, 0 },
-		{ MSR_SF_MASK, 0, 0 },
-	};
-
-	cnt = sizeof(guest_msrs) / sizeof(guest_msrs[0]);
-	if (cnt > GUEST_MSR_MAX_ENTRIES)
-		panic("guest msr save area overrun");
-	bcopy(guest_msrs, g_area, sizeof(guest_msrs));
-	*g_count = cnt;
-}
-
-static void
-host_msr_save_area_init(struct msr_entry *h_area, int *h_count)
-{
-	int i, cnt;
-
-	static struct msr_entry host_msrs[] = {
-		{ MSR_LSTAR, 0, 0 },
-		{ MSR_CSTAR, 0, 0 },
-		{ MSR_STAR, 0, 0 },
-		{ MSR_SF_MASK, 0, 0 },
-	};
-
-	cnt = sizeof(host_msrs) / sizeof(host_msrs[0]);
-	if (cnt > HOST_MSR_MAX_ENTRIES)
-		panic("host msr save area overrun");
-	for (i = 0; i < cnt; i++) {
-		host_msrs[i].val = rdmsr(host_msrs[i].index);
-	}
-	bcopy(host_msrs, h_area, sizeof(host_msrs));
-	*h_count = cnt;
-}
-#endif
-
 static void
 vmx_disable(void *arg __unused)
 {
@@ -610,11 +517,8 @@ vmx_disable(void *arg __unused)
 static int
 vmx_cleanup(void)
 {
-	
-#ifdef	__FreeBSD__
-	if (pirvec != 0)
-		vmm_ipi_free(pirvec);
-#endif
+	if (pirvec >= 0)
+		lapic_ipi_free(pirvec);
 
 	if (vpid_unr != NULL) {
 		delete_unrhdr(vpid_unr);
@@ -643,38 +547,41 @@ vmx_enable(void *arg __unused)
 	load_cr4(rcr4() | CR4_VMXE);
 
 	*(uint32_t *)vmxon_region[curcpu] = vmx_revision();
-#ifdef	__FreeBSD__
+#ifdef __FreeBSD__
 	error = vmxon(vmxon_region[curcpu]);
 #else
-	error = vmxon_pa(vmxon_region_pa[curcpu]);
-	ASSERT(error == 0);
+	error = vmxon(vmxon_region_pa[curcpu]);
 #endif
 	if (error == 0)
 		vmxon_enabled[curcpu] = 1;
 }
 
+static void
+vmx_restore(void)
+{
+
+	if (vmxon_enabled[curcpu])
+		vmxon(vmxon_region[curcpu]);
+}
+
 static int
-vmx_init(void)
+vmx_init(int ipinum)
 {
-#define	X86FSET_VMX	35
-	extern uchar_t x86_featureset[];
-	extern boolean_t is_x86_feature(void *featureset, uint_t feature);
-	int error;
-	uint64_t fixed0, fixed1, feature_control;
-	uint32_t tmp;
-#ifndef	__FreeBSD__
-	int i;
-#endif
+	int error, use_tpr_shadow;
+	uint64_t basic, fixed0, fixed1, feature_control;
+	uint32_t tmp, procbased2_vid_bits;
 
+#ifdef __FreeBSD__
 	/* CPUID.1:ECX[bit 5] must be 1 for processor to support VMX */
-#ifdef	__FreeBSD__
 	if (!(cpu_feature2 & CPUID2_VMX)) {
 		printf("vmx_init: processor does not support VMX operation\n");
 		return (ENXIO);
 	}
 #else
 	if (!is_x86_feature(x86_featureset, X86FSET_VMX)) {
-		cmn_err(CE_WARN, "vmx_init: processor does not support VMX operation\n");
+		cmn_err(CE_WARN,
+		    "vmx_init: processor does not support VMX operation\n");
+		return (ENXIO);
 	}
 #endif
 
@@ -689,6 +596,17 @@ vmx_init(void)
 		return (ENXIO);
 	}
 
+	/*
+	 * Verify capabilities MSR_VMX_BASIC:
+	 * - bit 54 indicates support for INS/OUTS decoding
+	 */
+	basic = rdmsr(MSR_VMX_BASIC);
+	if ((basic & (1UL << 54)) == 0) {
+		printf("vmx_init: processor does not support desired basic "
+		    "capabilities\n");
+		return (EINVAL);
+	}
+
 	/* Check support for primary processor-based VM-execution controls */
 	error = vmx_set_ctlreg(MSR_VMX_PROCBASED_CTLS,
 			       MSR_VMX_TRUE_PROCBASED_CTLS,
@@ -776,8 +694,71 @@ vmx_init(void)
 					PROCBASED2_UNRESTRICTED_GUEST, 0,
 				        &tmp) == 0);
 
+	cap_invpcid = (vmx_set_ctlreg(MSR_VMX_PROCBASED_CTLS2,
+	    MSR_VMX_PROCBASED_CTLS2, PROCBASED2_ENABLE_INVPCID, 0,
+	    &tmp) == 0);
+
+	/*
+	 * Check support for virtual interrupt delivery.
+	 */
+	procbased2_vid_bits = (PROCBASED2_VIRTUALIZE_APIC_ACCESSES |
+	    PROCBASED2_VIRTUALIZE_X2APIC_MODE |
+	    PROCBASED2_APIC_REGISTER_VIRTUALIZATION |
+	    PROCBASED2_VIRTUAL_INTERRUPT_DELIVERY);
+
+	use_tpr_shadow = (vmx_set_ctlreg(MSR_VMX_PROCBASED_CTLS,
+	    MSR_VMX_TRUE_PROCBASED_CTLS, PROCBASED_USE_TPR_SHADOW, 0,
+	    &tmp) == 0);
+
+	error = vmx_set_ctlreg(MSR_VMX_PROCBASED_CTLS2, MSR_VMX_PROCBASED_CTLS2,
+	    procbased2_vid_bits, 0, &tmp);
+	if (error == 0 && use_tpr_shadow) {
+		virtual_interrupt_delivery = 1;
+		TUNABLE_INT_FETCH("hw.vmm.vmx.use_apic_vid",
+		    &virtual_interrupt_delivery);
+	}
+
+	if (virtual_interrupt_delivery) {
+		procbased_ctls |= PROCBASED_USE_TPR_SHADOW;
+		procbased_ctls2 |= procbased2_vid_bits;
+		procbased_ctls2 &= ~PROCBASED2_VIRTUALIZE_X2APIC_MODE;
+
+		/*
+		 * No need to emulate accesses to %CR8 if virtual
+		 * interrupt delivery is enabled.
+		 */
+		procbased_ctls &= ~PROCBASED_CR8_LOAD_EXITING;
+		procbased_ctls &= ~PROCBASED_CR8_STORE_EXITING;
+
+		/*
+		 * Check for Posted Interrupts only if Virtual Interrupt
+		 * Delivery is enabled.
+		 */
+		error = vmx_set_ctlreg(MSR_VMX_PINBASED_CTLS,
+		    MSR_VMX_TRUE_PINBASED_CTLS, PINBASED_POSTED_INTERRUPT, 0,
+		    &tmp);
+		if (error == 0) {
+			pirvec = lapic_ipi_alloc(&IDTVEC(justreturn));
+			if (pirvec < 0) {
+#ifdef __FreeBSD__
+				if (bootverbose) {
+					printf("vmx_init: unable to allocate "
+					    "posted interrupt vector\n");
+				}
+#endif
+			} else {
+				posted_interrupts = 1;
+				TUNABLE_INT_FETCH("hw.vmm.vmx.use_apic_pir",
+				    &posted_interrupts);
+			}
+		}
+	}
+
+	if (posted_interrupts)
+		    pinbased_ctls |= PINBASED_POSTED_INTERRUPT;
+
 	/* Initialize EPT */
-	error = ept_init();
+	error = ept_init(ipinum);
 	if (error) {
 		printf("vmx_init: ept initialization failed (%d)\n", error);
 		return (error);
@@ -808,16 +789,21 @@ vmx_init(void)
 	cr4_ones_mask = fixed0 & fixed1;
 	cr4_zeros_mask = ~fixed0 & ~fixed1;
 
-#ifndef	__FreeBSD__
-	for (i = 0; i < MAXCPU; i++) {
-		vmxon_region_pa[i] = vtophys(&vmxon_region[i]);
-	}
-#endif
-
 	vpid_init();
 
 	vmx_msr_init();
 
+#ifndef __FreeBSD__
+	/*
+	 * Since vtophys requires locks to complete, cache the physical
+	 * addresses to the vmxon pages now, rather than attempting the
+	 * translation in the sensitive cross-call context.
+	 */
+	for (uint_t i = 0; i < MAXCPU; i++) {
+		vmxon_region_pa[i] = (char *)vtophys(vmxon_region[i]);
+	}
+#endif /* __FreeBSD__ */
+
 	/* enable VMX operation */
 	smp_rendezvous(NULL, vmx_enable, NULL, NULL);
 
@@ -826,6 +812,47 @@ vmx_init(void)
 	return (0);
 }
 
+static void
+vmx_trigger_hostintr(int vector)
+{
+#ifdef __FreeBSD__
+	uintptr_t func;
+	struct gate_descriptor *gd;
+
+	gd = &idt[vector];
+
+	KASSERT(vector >= 32 && vector <= 255, ("vmx_trigger_hostintr: "
+	    "invalid vector %d", vector));
+	KASSERT(gd->gd_p == 1, ("gate descriptor for vector %d not present",
+	    vector));
+	KASSERT(gd->gd_type == SDT_SYSIGT, ("gate descriptor for vector %d "
+	    "has invalid type %d", vector, gd->gd_type));
+	KASSERT(gd->gd_dpl == SEL_KPL, ("gate descriptor for vector %d "
+	    "has invalid dpl %d", vector, gd->gd_dpl));
+	KASSERT(gd->gd_selector == GSEL(GCODE_SEL, SEL_KPL), ("gate descriptor "
+	    "for vector %d has invalid selector %d", vector, gd->gd_selector));
+	KASSERT(gd->gd_ist == 0, ("gate descriptor for vector %d has invalid "
+	    "IST %d", vector, gd->gd_ist));
+
+	func = ((long)gd->gd_hioffset << 16 | gd->gd_looffset);
+	vmx_call_isr(func);
+#else
+	uintptr_t func;
+	gate_desc_t *dp;
+
+	VERIFY(vector >= 32 && vector <= 255);
+	dp = &CPU->cpu_m.mcpu_idt[vector];
+
+	VERIFY(dp->sgd_ist == 0);
+	VERIFY(dp->sgd_p == 1);
+
+	func = (((uint64_t)dp->sgd_hi64offset << 32) |
+	    ((uint64_t)dp->sgd_hioffset << 16) |
+	    dp->sgd_looffset);
+	vmx_call_isr(func);
+#endif /* __FreeBSD__ */
+}
+
 static int
 vmx_setup_cr_shadow(int which, struct vmcs *vmcs, uint32_t initial)
 {
@@ -859,15 +886,22 @@ vmx_setup_cr_shadow(int which, struct vmcs *vmcs, uint32_t initial)
 #define	vmx_setup_cr4_shadow(vmcs,init)	vmx_setup_cr_shadow(4, (vmcs), (init))
 
 static void *
-vmx_vminit(struct vm *vm)
+vmx_vminit(struct vm *vm, pmap_t pmap)
 {
 	uint16_t vpid[VM_MAXCPU];
-	int i, error, guest_msr_count;
-#ifndef	__FreeBSD__
-	int host_msr_count;
-#endif
+	int i, error;
 	struct vmx *vmx;
 	struct vmcs *vmcs;
+	uint32_t exc_bitmap;
+
+#ifndef __FreeBSD__
+	/*
+	 * Grab an initial TSC reading to apply as an offset so the guest
+	 * TSC(s) appear to start from a zeroed value.
+	 */
+	uint64_t init_time = rdtsc();
+#endif
+
 
 	vmx = malloc(sizeof(struct vmx), M_VMX, M_WAITOK | M_ZERO);
 	if ((uintptr_t)vmx & PAGE_MASK) {
@@ -876,6 +910,8 @@ vmx_vminit(struct vm *vm)
 	}
 	vmx->vm = vm;
 
+	vmx->eptp = eptp(vtophys((vm_offset_t)pmap->pm_pml4));
+
 	/*
 	 * Clean up EPTP-tagged guest physical and combined mappings
 	 *
@@ -885,7 +921,7 @@ vmx_vminit(struct vm *vm)
 	 *
 	 * Combined mappings for this EP4TA are also invalidated for all VPIDs.
 	 */
-	ept_invalidate_mappings(vtophys(vmx->pml4ept));
+	ept_invalidate_mappings(vmx->eptp);
 
 	msr_bitmap_initialize(vmx->msr_bitmap);
 
@@ -903,10 +939,6 @@ vmx_vminit(struct vm *vm)
 	 * VM exit and entry respectively. It is also restored from the
 	 * host VMCS area on a VM exit.
 	 *
-	 * MSR_PAT is saved and restored in the guest VMCS are on a VM exit
-	 * and entry respectively. It is also restored from the host VMCS
-	 * area on a VM exit.
-	 *
 	 * The TSC MSR is exposed read-only. Writes are disallowed as
 	 * that will impact the host TSC.  If the guest does a write
 	 * the "use TSC offsetting" execution control is enabled and the
@@ -919,15 +951,35 @@ vmx_vminit(struct vm *vm)
 	    guest_msr_rw(vmx, MSR_SYSENTER_ESP_MSR) ||
 	    guest_msr_rw(vmx, MSR_SYSENTER_EIP_MSR) ||
 	    guest_msr_rw(vmx, MSR_EFER) ||
-	    guest_msr_rw(vmx, MSR_PAT) ||
 	    guest_msr_ro(vmx, MSR_TSC))
 		panic("vmx_vminit: error setting guest msr access");
 
 	vpid_alloc(vpid, VM_MAXCPU);
 
+	if (virtual_interrupt_delivery) {
+		error = vm_map_mmio(vm, DEFAULT_APIC_BASE, PAGE_SIZE,
+		    APIC_ACCESS_ADDRESS);
+		/* XXX this should really return an error to the caller */
+		KASSERT(error == 0, ("vm_map_mmio(apicbase) error %d", error));
+	}
+
 	for (i = 0; i < VM_MAXCPU; i++) {
+#ifndef __FreeBSD__
+		/*
+		 * Cache physical address lookups for various components which
+		 * may be required inside the critical_enter() section implied
+		 * by VMPTRLD() below.
+		 */
+		vm_paddr_t msr_bitmap_pa = vtophys(vmx->msr_bitmap);
+		vm_paddr_t apic_page_pa = vtophys(&vmx->apic_page[i]);
+		vm_paddr_t pir_desc_pa = vtophys(&vmx->pir_desc[i]);
+#endif /* __FreeBSD__ */
+
 		vmcs = &vmx->vmcs[i];
 		vmcs->identifier = vmx_revision();
+#ifndef __FreeBSD__
+		vmcs->vmcs_pa = (uint64_t)vtophys(vmcs);
+#endif
 		error = vmclear(vmcs);
 		if (error != 0) {
 			panic("vmx_vminit: vmclear error %d on vcpu %d\n",
@@ -936,43 +988,81 @@ vmx_vminit(struct vm *vm)
 
 		vmx_msr_guest_init(vmx, i);
 
-		error = vmcs_set_defaults(vmcs,
-					  (u_long)vmx_longjmp,
-					  (u_long)&vmx->ctx[i],
-					  vtophys(vmx->pml4ept),
-					  pinbased_ctls,
-					  procbased_ctls,
-					  procbased_ctls2,
-					  exit_ctls, entry_ctls,
-					  vtophys(vmx->msr_bitmap),
-					  vpid[i]);
+		error = vmcs_init(vmcs);
+		KASSERT(error == 0, ("vmcs_init error %d", error));
 
-		if (error != 0)
-			panic("vmx_vminit: vmcs_set_defaults error %d", error);
+		VMPTRLD(vmcs);
+		error = 0;
+#ifdef __FreeBSD__
+		/*
+		 * The illumos vmx_enter_guest implementation avoids some of
+		 * the %rsp-manipulation games which are present in the stock
+		 * one from FreeBSD.
+		 */
+		error += vmwrite(VMCS_HOST_RSP, (u_long)&vmx->ctx[i]);
+#endif
+		error += vmwrite(VMCS_EPTP, vmx->eptp);
+		error += vmwrite(VMCS_PIN_BASED_CTLS, pinbased_ctls);
+		error += vmwrite(VMCS_PRI_PROC_BASED_CTLS, procbased_ctls);
+		error += vmwrite(VMCS_SEC_PROC_BASED_CTLS, procbased_ctls2);
+		error += vmwrite(VMCS_EXIT_CTLS, exit_ctls);
+		error += vmwrite(VMCS_ENTRY_CTLS, entry_ctls);
+#ifdef __FreeBSD__
+		error += vmwrite(VMCS_MSR_BITMAP, vtophys(vmx->msr_bitmap));
+#else
+		error += vmwrite(VMCS_MSR_BITMAP, msr_bitmap_pa);
+#endif
+		error += vmwrite(VMCS_VPID, vpid[i]);
+
+#ifndef __FreeBSD__
+		/*
+		 * Record initial TSC offset.  It will be loaded into the VMCS
+		 * during each setup for VMX entry.
+		 */
+		vmx->tsc_offset[i] = (uint64_t)(-init_time);
+		VERIFY(procbased_ctls & PROCBASED_TSC_OFFSET);
+#endif
+
+		/* exception bitmap */
+		if (vcpu_trace_exceptions(vm, i))
+			exc_bitmap = 0xffffffff;
+		else
+			exc_bitmap = 1 << IDT_MC;
+		error += vmwrite(VMCS_EXCEPTION_BITMAP, exc_bitmap);
+
+		if (virtual_interrupt_delivery) {
+			error += vmwrite(VMCS_APIC_ACCESS, APIC_ACCESS_ADDRESS);
+#ifdef __FreeBSD__
+			error += vmwrite(VMCS_VIRTUAL_APIC,
+			    vtophys(&vmx->apic_page[i]));
+#else
+			error += vmwrite(VMCS_VIRTUAL_APIC, apic_page_pa);
+#endif
+			error += vmwrite(VMCS_EOI_EXIT0, 0);
+			error += vmwrite(VMCS_EOI_EXIT1, 0);
+			error += vmwrite(VMCS_EOI_EXIT2, 0);
+			error += vmwrite(VMCS_EOI_EXIT3, 0);
+		}
+		if (posted_interrupts) {
+			error += vmwrite(VMCS_PIR_VECTOR, pirvec);
+#ifdef __FreeBSD__
+			error += vmwrite(VMCS_PIR_DESC,
+			    vtophys(&vmx->pir_desc[i]));
+#else
+			error += vmwrite(VMCS_PIR_DESC, pir_desc_pa);
+#endif
+		}
+		VMCLEAR(vmcs);
+		KASSERT(error == 0, ("vmx_vminit: error customizing the vmcs"));
 
 		vmx->cap[i].set = 0;
 		vmx->cap[i].proc_ctls = procbased_ctls;
+		vmx->cap[i].proc_ctls2 = procbased_ctls2;
 
-		vmx->state[i].lastcpu = -1;
+		vmx->state[i].nextrip = ~0;
+		vmx->state[i].lastcpu = NOCPU;
 		vmx->state[i].vpid = vpid[i];
 
-#ifndef	__FreeBSD__
-		msr_save_area_init(vmx->guest_msrs[i], &guest_msr_count);
-
-		error = vmcs_set_msr_save(vmcs, vtophys(vmx->guest_msrs[i]),
-		    guest_msr_count);
-		if (error != 0)
-			panic("vmcs_set_msr_save error %d", error);
-
-		host_msr_save_area_init(vmx->host_msrs[i], &host_msr_count);
-
-		error = vmcs_set_host_msr_save(&vmx->vmcs[i],
-					       vtophys(vmx->host_msrs[i]),
-					       host_msr_count);
-		if (error != 0)
-			panic("vmcs_set_msr_save error %d", error);
-#endif
-
 		/*
 		 * Set up the CR0/4 shadows, and init the read shadow
 		 * to the power-on register value from the Intel Sys Arch.
@@ -986,6 +1076,8 @@ vmx_vminit(struct vm *vm)
 		error = vmx_setup_cr4_shadow(vmcs, 0);
 		if (error != 0)
 			panic("vmx_setup_cr4_shadow %d", error);
+
+		vmx->ctx[i].pmap = pmap;
 	}
 
 	return (vmx);
@@ -1023,6 +1115,8 @@ vmx_exit_trace(struct vmx *vmx, int vcpu, uint64_t rip, uint32_t exit_reason,
 		 handled ? "handled" : "unhandled",
 		 exit_reason_to_str(exit_reason), rip);
 #endif
+	DTRACE_PROBE3(vmm__vexit, int, vcpu, uint64_t, rip,
+	    uint32_t, exit_reason);
 }
 
 static __inline void
@@ -1033,38 +1127,40 @@ vmx_astpending_trace(struct vmx *vmx, int vcpu, uint64_t rip)
 #endif
 }
 
-static void
-vmx_set_pcpu_defaults(struct vmx *vmx, int vcpu)
+static VMM_STAT_INTEL(VCPU_INVVPID_SAVED, "Number of vpid invalidations saved");
+static VMM_STAT_INTEL(VCPU_INVVPID_DONE, "Number of vpid invalidations done");
+
+/*
+ * Invalidate guest mappings identified by its vpid from the TLB.
+ */
+static __inline void
+vmx_invvpid(struct vmx *vmx, int vcpu, pmap_t pmap, int running)
 {
 	struct vmxstate *vmxstate;
-	struct invvpid_desc invvpid_desc = { 0 };
-#if notyet
-#ifndef	__FreeBSD__
-	desctbr_t idtr, gdtr;
-#endif
-#endif
+	struct invvpid_desc invvpid_desc;
 
 	vmxstate = &vmx->state[vcpu];
-	vmcs_write(VMCS_HOST_FS_BASE, vmm_get_host_fsbase());
-	if (vmxstate->lastcpu == curcpu)
+	if (vmxstate->vpid == 0)
 		return;
 
-	vmxstate->lastcpu = curcpu;
-
-	vmm_stat_incr(vmx->vm, vcpu, VCPU_MIGRATIONS, 1);
-
-	vmcs_write(VMCS_HOST_TR_BASE, vmm_get_host_trbase());
-	vmcs_write(VMCS_HOST_GDTR_BASE, vmm_get_host_gdtrbase());
-	vmcs_write(VMCS_HOST_GS_BASE, vmm_get_host_gsbase());
+	if (!running) {
+		/*
+		 * Set the 'lastcpu' to an invalid host cpu.
+		 *
+		 * This will invalidate TLB entries tagged with the vcpu's
+		 * vpid the next time it runs via vmx_set_pcpu_defaults().
+		 */
+		vmxstate->lastcpu = NOCPU;
+		return;
+	}
 
-#ifndef	__FreeBSD__
-	vmcs_write(VMCS_HOST_IA32_SYSENTER_CS, rdmsr(MSR_SYSENTER_CS_MSR));
-	vmcs_write(VMCS_HOST_IA32_SYSENTER_ESP, rdmsr(MSR_SYSENTER_ESP_MSR));
-	vmcs_write(VMCS_HOST_IA32_SYSENTER_EIP, rdmsr(MSR_SYSENTER_EIP_MSR));
+#ifdef __FreeBSD__
+	KASSERT(curthread->td_critnest > 0, ("%s: vcpu %d running outside "
+	    "critical section", __func__, vcpu));
 #endif
 
 	/*
-	 * If we are using VPIDs then invalidate all mappings tagged with 'vpid'
+	 * Invalidate all mappings tagged with 'vpid'
 	 *
 	 * We do this because this vcpu was executing on a different host
 	 * cpu when it last ran. We do not track whether it invalidated
@@ -1078,20 +1174,83 @@ vmx_set_pcpu_defaults(struct vmx *vmx, int vcpu)
 	 * Note also that this will invalidate mappings tagged with 'vpid'
 	 * for "all" EP4TAs.
 	 */
-	if (vmxstate->vpid != 0) {
+	if (pmap->pm_eptgen == vmx->eptgen[curcpu]) {
+		invvpid_desc._res1 = 0;
+		invvpid_desc._res2 = 0;
 		invvpid_desc.vpid = vmxstate->vpid;
+		invvpid_desc.linear_addr = 0;
 		invvpid(INVVPID_TYPE_SINGLE_CONTEXT, invvpid_desc);
+		vmm_stat_incr(vmx->vm, vcpu, VCPU_INVVPID_DONE, 1);
+	} else {
+		/*
+		 * The invvpid can be skipped if an invept is going to
+		 * be performed before entering the guest. The invept
+		 * will invalidate combined mappings tagged with
+		 * 'vmx->eptp' for all vpids.
+		 */
+		vmm_stat_incr(vmx->vm, vcpu, VCPU_INVVPID_SAVED, 1);
 	}
 }
 
-static void 
-vm_exit_update_rip(struct vm_exit *vmexit)
+#ifndef __FreeBSD__
+/*
+ * Set the TSC adjustment, taking into account the offsets measured between
+ * host physical CPUs.  This is required even if the guest has not set a TSC
+ * offset since vCPUs inherit the TSC offset of whatever physical CPU it has
+ * migrated onto.  Without this mitigation, un-synched host TSCs will convey
+ * the appearance of TSC time-travel to the guest as its vCPUs migrate.
+ */
+static int
+vmx_apply_tsc_adjust(struct vmx *vmx, int vcpu)
 {
+	extern hrtime_t tsc_gethrtime_tick_delta(void);
+	uint64_t host_offset = (uint64_t)tsc_gethrtime_tick_delta();
+	uint64_t guest_offset = vmx->tsc_offset[vcpu];
 	int error;
 
-	error = vmwrite(VMCS_GUEST_RIP, vmexit->rip + vmexit->inst_length);
-	if (error)
-		panic("vmx_run: error %d writing to VMCS_GUEST_RIP", error);
+	ASSERT(vmx->cap[vcpu].proc_ctls & PROCBASED_TSC_OFFSET);
+
+	error = vmwrite(VMCS_TSC_OFFSET, guest_offset + host_offset);
+
+	return (error);
+}
+#endif
+
+static void
+vmx_set_pcpu_defaults(struct vmx *vmx, int vcpu, pmap_t pmap)
+{
+	struct vmxstate *vmxstate;
+
+#ifndef __FreeBSD__
+	/*
+	 * Regardless of whether the VM appears to have migrated between CPUs,
+	 * save the host sysenter stack pointer.  As it points to the kernel
+	 * stack of each thread, the correct value must be maintained for every
+	 * trip into the critical section.
+	 */
+	vmcs_write(VMCS_HOST_IA32_SYSENTER_ESP, rdmsr(MSR_SYSENTER_ESP_MSR));
+#endif
+
+	vmxstate = &vmx->state[vcpu];
+	if (vmxstate->lastcpu == curcpu)
+		return;
+
+	vmxstate->lastcpu = curcpu;
+
+	vmm_stat_incr(vmx->vm, vcpu, VCPU_MIGRATIONS, 1);
+
+#ifndef __FreeBSD__
+	/* Load the per-CPU IDT address */
+	vmcs_write(VMCS_HOST_IDTR_BASE, vmm_get_host_idtrbase());
+#endif
+	vmcs_write(VMCS_HOST_TR_BASE, vmm_get_host_trbase());
+	vmcs_write(VMCS_HOST_GDTR_BASE, vmm_get_host_gdtrbase());
+	vmcs_write(VMCS_HOST_GS_BASE, vmm_get_host_gsbase());
+	vmx_invvpid(vmx, vcpu, pmap, 1);
+
+#ifndef __FreeBSD__
+	VERIFY0(vmx_apply_tsc_adjust(vmx, vcpu));
+#endif
 }
 
 /*
@@ -1114,13 +1273,8 @@ static __inline void
 vmx_clear_int_window_exiting(struct vmx *vmx, int vcpu)
 {
 
-#ifdef	__FreeBSD__
 	KASSERT((vmx->cap[vcpu].proc_ctls & PROCBASED_INT_WINDOW_EXITING) != 0,
 	    ("intr_window_exiting not set: %#x", vmx->cap[vcpu].proc_ctls));
-#else
-	KASSERT((vmx->cap[vcpu].proc_ctls & PROCBASED_INT_WINDOW_EXITING) != 0,
-	    ("intr_window_exiting not set: %x", vmx->cap[vcpu].proc_ctls));
-#endif
 	vmx->cap[vcpu].proc_ctls &= ~PROCBASED_INT_WINDOW_EXITING;
 	vmcs_write(VMCS_PRI_PROC_BASED_CTLS, vmx->cap[vcpu].proc_ctls);
 	VCPU_CTR0(vmx->vm, vcpu, "Disabling interrupt window exiting");
@@ -1141,13 +1295,8 @@ static __inline void
 vmx_clear_nmi_window_exiting(struct vmx *vmx, int vcpu)
 {
 
-#ifdef	__FreeBSD__
 	KASSERT((vmx->cap[vcpu].proc_ctls & PROCBASED_NMI_WINDOW_EXITING) != 0,
 	    ("nmi_window_exiting not set %#x", vmx->cap[vcpu].proc_ctls));
-#else
-	KASSERT((vmx->cap[vcpu].proc_ctls & PROCBASED_NMI_WINDOW_EXITING) != 0,
-	    ("nmi_window_exiting not set %x", vmx->cap[vcpu].proc_ctls));
-#endif
 	vmx->cap[vcpu].proc_ctls &= ~PROCBASED_NMI_WINDOW_EXITING;
 	vmcs_write(VMCS_PRI_PROC_BASED_CTLS, vmx->cap[vcpu].proc_ctls);
 	VCPU_CTR0(vmx->vm, vcpu, "Disabling NMI window exiting");
@@ -1158,6 +1307,7 @@ vmx_set_tsc_offset(struct vmx *vmx, int vcpu, uint64_t offset)
 {
 	int error;
 
+#ifdef __FreeBSD__
 	if ((vmx->cap[vcpu].proc_ctls & PROCBASED_TSC_OFFSET) == 0) {
 		vmx->cap[vcpu].proc_ctls |= PROCBASED_TSC_OFFSET;
 		vmcs_write(VMCS_PRI_PROC_BASED_CTLS, vmx->cap[vcpu].proc_ctls);
@@ -1165,6 +1315,10 @@ vmx_set_tsc_offset(struct vmx *vmx, int vcpu, uint64_t offset)
 	}
 
 	error = vmwrite(VMCS_TSC_OFFSET, offset);
+#else /* __FreeBSD__ */
+	vmx->tsc_offset[vcpu] = offset;
+	error = vmx_apply_tsc_adjust(vmx, vcpu);
+#endif /* __FreeBSD__ */
 
 	return (error);
 }
@@ -1180,22 +1334,12 @@ vmx_inject_nmi(struct vmx *vmx, int vcpu)
 	uint32_t gi, info;
 
 	gi = vmcs_read(VMCS_GUEST_INTERRUPTIBILITY);
-#ifdef	__FreeBSD__
 	KASSERT((gi & NMI_BLOCKING) == 0, ("vmx_inject_nmi: invalid guest "
 	    "interruptibility-state %#x", gi));
-#else
-	KASSERT((gi & NMI_BLOCKING) == 0, ("vmx_inject_nmi: invalid guest "
-	    "interruptibility-state %x", gi));
-#endif
 
 	info = vmcs_read(VMCS_ENTRY_INTR_INFO);
-#ifdef	__FreeBSD__
 	KASSERT((info & VMCS_INTR_VALID) == 0, ("vmx_inject_nmi: invalid "
 	    "VM-entry interruption information %#x", info));
-#else
-	KASSERT((info & VMCS_INTR_VALID) == 0, ("vmx_inject_nmi: invalid "
-	    "VM-entry interruption information %x", info));
-#endif
 
 	/*
 	 * Inject the virtual NMI. The vector must be the NMI IDT entry
@@ -1211,29 +1355,31 @@ vmx_inject_nmi(struct vmx *vmx, int vcpu)
 }
 
 static void
-vmx_inject_interrupts(struct vmx *vmx, int vcpu, struct vlapic *vlapic)
+vmx_inject_interrupts(struct vmx *vmx, int vcpu, struct vlapic *vlapic,
+    uint64_t guestrip)
 {
 	int vector, need_nmi_exiting, extint_pending;
 	uint64_t rflags, entryinfo;
 	uint32_t gi, info;
 
+	if (vmx->state[vcpu].nextrip != guestrip) {
+		gi = vmcs_read(VMCS_GUEST_INTERRUPTIBILITY);
+		if (gi & HWINTR_BLOCKING) {
+			VCPU_CTR2(vmx->vm, vcpu, "Guest interrupt blocking "
+			    "cleared due to rip change: %#lx/%#lx",
+			    vmx->state[vcpu].nextrip, guestrip);
+			gi &= ~HWINTR_BLOCKING;
+			vmcs_write(VMCS_GUEST_INTERRUPTIBILITY, gi);
+		}
+	}
+
 	if (vm_entry_intinfo(vmx->vm, vcpu, &entryinfo)) {
-#ifdef	__FreeBSD__
 		KASSERT((entryinfo & VMCS_INTR_VALID) != 0, ("%s: entry "
 		    "intinfo is not valid: %#lx", __func__, entryinfo));
-#else
-		KASSERT((entryinfo & VMCS_INTR_VALID) != 0, ("%s: entry "
-		    "intinfo is not valid: %lx", __func__, entryinfo));
-#endif
 
 		info = vmcs_read(VMCS_ENTRY_INTR_INFO);
-#ifdef	__FreeBSD__
 		KASSERT((info & VMCS_INTR_VALID) == 0, ("%s: cannot inject "
 		     "pending exception: %#lx/%#x", __func__, entryinfo, info));
-#else
-		KASSERT((info & VMCS_INTR_VALID) == 0, ("%s: cannot inject "
-		     "pending exception: %lx/%x", __func__, entryinfo, info));
-#endif
 
 		info = entryinfo;
 		vector = info & 0xff;
@@ -1286,12 +1432,10 @@ vmx_inject_interrupts(struct vmx *vmx, int vcpu, struct vlapic *vlapic)
 
 	extint_pending = vm_extint_pending(vmx->vm, vcpu);
 
-#ifdef	__FreeBSD__
 	if (!extint_pending && virtual_interrupt_delivery) {
 		vmx_inject_pir(vlapic);
 		return;
 	}
-#endif
 
 	/*
 	 * If interrupt-window exiting is already in effect then don't bother
@@ -1418,7 +1562,6 @@ vmx_restore_nmi_blocking(struct vmx *vmx, int vcpuid)
 	vmcs_write(VMCS_GUEST_INTERRUPTIBILITY, gi);
 }
 
-#if notyet
 static void
 vmx_clear_nmi_blocking(struct vmx *vmx, int vcpuid)
 {
@@ -1429,27 +1572,112 @@ vmx_clear_nmi_blocking(struct vmx *vmx, int vcpuid)
 	gi &= ~VMCS_INTERRUPTIBILITY_NMI_BLOCKING;
 	vmcs_write(VMCS_GUEST_INTERRUPTIBILITY, gi);
 }
-#endif
 
-static uint64_t
-vmx_get_guest_reg(struct vmx *vmx, int vcpu, int ident)
+static void
+vmx_assert_nmi_blocking(struct vmx *vmx, int vcpuid)
 {
-	const struct vmxctx *vmxctx;
+	uint32_t gi;
+
+	gi = vmcs_read(VMCS_GUEST_INTERRUPTIBILITY);
+	KASSERT(gi & VMCS_INTERRUPTIBILITY_NMI_BLOCKING,
+	    ("NMI blocking is not in effect %#x", gi));
+}
+
+static int
+vmx_emulate_xsetbv(struct vmx *vmx, int vcpu, struct vm_exit *vmexit)
+{
+	struct vmxctx *vmxctx;
+	uint64_t xcrval;
+	const struct xsave_limits *limits;
 
 	vmxctx = &vmx->ctx[vcpu];
+	limits = vmm_get_xsave_limits();
 
-	switch (ident) {
-	case 0:
-		return (vmxctx->guest_rax);
-	case 1:
-		return (vmxctx->guest_rcx);
-	case 2:
-		return (vmxctx->guest_rdx);
-	case 3:
-		return (vmxctx->guest_rbx);
-	case 4:
-		return (vmcs_read(VMCS_GUEST_RSP));
-	case 5:
+	/*
+	 * Note that the processor raises a GP# fault on its own if
+	 * xsetbv is executed for CPL != 0, so we do not have to
+	 * emulate that fault here.
+	 */
+
+	/* Only xcr0 is supported. */
+	if (vmxctx->guest_rcx != 0) {
+		vm_inject_gp(vmx->vm, vcpu);
+		return (HANDLED);
+	}
+
+	/* We only handle xcr0 if both the host and guest have XSAVE enabled. */
+	if (!limits->xsave_enabled || !(vmcs_read(VMCS_GUEST_CR4) & CR4_XSAVE)) {
+		vm_inject_ud(vmx->vm, vcpu);
+		return (HANDLED);
+	}
+
+	xcrval = vmxctx->guest_rdx << 32 | (vmxctx->guest_rax & 0xffffffff);
+	if ((xcrval & ~limits->xcr0_allowed) != 0) {
+		vm_inject_gp(vmx->vm, vcpu);
+		return (HANDLED);
+	}
+
+	if (!(xcrval & XFEATURE_ENABLED_X87)) {
+		vm_inject_gp(vmx->vm, vcpu);
+		return (HANDLED);
+	}
+
+	/* AVX (YMM_Hi128) requires SSE. */
+	if (xcrval & XFEATURE_ENABLED_AVX &&
+	    (xcrval & XFEATURE_AVX) != XFEATURE_AVX) {
+		vm_inject_gp(vmx->vm, vcpu);
+		return (HANDLED);
+	}
+
+	/*
+	 * AVX512 requires base AVX (YMM_Hi128) as well as OpMask,
+	 * ZMM_Hi256, and Hi16_ZMM.
+	 */
+	if (xcrval & XFEATURE_AVX512 &&
+	    (xcrval & (XFEATURE_AVX512 | XFEATURE_AVX)) !=
+	    (XFEATURE_AVX512 | XFEATURE_AVX)) {
+		vm_inject_gp(vmx->vm, vcpu);
+		return (HANDLED);
+	}
+
+	/*
+	 * Intel MPX requires both bound register state flags to be
+	 * set.
+	 */
+	if (((xcrval & XFEATURE_ENABLED_BNDREGS) != 0) !=
+	    ((xcrval & XFEATURE_ENABLED_BNDCSR) != 0)) {
+		vm_inject_gp(vmx->vm, vcpu);
+		return (HANDLED);
+	}
+
+	/*
+	 * This runs "inside" vmrun() with the guest's FPU state, so
+	 * modifying xcr0 directly modifies the guest's xcr0, not the
+	 * host's.
+	 */
+	load_xcr(0, xcrval);
+	return (HANDLED);
+}
+
+static uint64_t
+vmx_get_guest_reg(struct vmx *vmx, int vcpu, int ident)
+{
+	const struct vmxctx *vmxctx;
+
+	vmxctx = &vmx->ctx[vcpu];
+
+	switch (ident) {
+	case 0:
+		return (vmxctx->guest_rax);
+	case 1:
+		return (vmxctx->guest_rcx);
+	case 2:
+		return (vmxctx->guest_rdx);
+	case 3:
+		return (vmxctx->guest_rbx);
+	case 4:
+		return (vmcs_read(VMCS_GUEST_RSP));
+	case 5:
 		return (vmxctx->guest_rbp);
 	case 6:
 		return (vmxctx->guest_rsi);
@@ -1745,6 +1973,7 @@ vmexit_inst_emul(struct vm_exit *vmexit, uint64_t gpa, uint64_t gla)
 	paging = &vmexit->u.inst_emul.paging;
 
 	vmexit->exitcode = VM_EXITCODE_INST_EMUL;
+	vmexit->inst_length = 0;
 	vmexit->u.inst_emul.gpa = gpa;
 	vmexit->u.inst_emul.gla = gla;
 	vmx_paging_info(paging);
@@ -1767,7 +1996,6 @@ vmexit_inst_emul(struct vm_exit *vmexit, uint64_t gpa, uint64_t gla)
 	vie_init(&vmexit->u.inst_emul.vie, NULL, 0);
 }
 
-#if notyet
 static int
 ept_fault_type(uint64_t ept_qual)
 {
@@ -1782,7 +2010,6 @@ ept_fault_type(uint64_t ept_qual)
 
 	return (fault_type);
 }
-#endif
 
 static boolean_t
 ept_emulation_fault(uint64_t ept_qual)
@@ -1812,6 +2039,189 @@ ept_emulation_fault(uint64_t ept_qual)
 	return (TRUE);
 }
 
+static __inline int
+apic_access_virtualization(struct vmx *vmx, int vcpuid)
+{
+	uint32_t proc_ctls2;
+
+	proc_ctls2 = vmx->cap[vcpuid].proc_ctls2;
+	return ((proc_ctls2 & PROCBASED2_VIRTUALIZE_APIC_ACCESSES) ? 1 : 0);
+}
+
+static __inline int
+x2apic_virtualization(struct vmx *vmx, int vcpuid)
+{
+	uint32_t proc_ctls2;
+
+	proc_ctls2 = vmx->cap[vcpuid].proc_ctls2;
+	return ((proc_ctls2 & PROCBASED2_VIRTUALIZE_X2APIC_MODE) ? 1 : 0);
+}
+
+static int
+vmx_handle_apic_write(struct vmx *vmx, int vcpuid, struct vlapic *vlapic,
+    uint64_t qual)
+{
+	int error, handled, offset;
+	uint32_t *apic_regs, vector;
+	bool retu;
+
+	handled = HANDLED;
+	offset = APIC_WRITE_OFFSET(qual);
+
+	if (!apic_access_virtualization(vmx, vcpuid)) {
+		/*
+		 * In general there should not be any APIC write VM-exits
+		 * unless APIC-access virtualization is enabled.
+		 *
+		 * However self-IPI virtualization can legitimately trigger
+		 * an APIC-write VM-exit so treat it specially.
+		 */
+		if (x2apic_virtualization(vmx, vcpuid) &&
+		    offset == APIC_OFFSET_SELF_IPI) {
+			apic_regs = (uint32_t *)(vlapic->apic_page);
+			vector = apic_regs[APIC_OFFSET_SELF_IPI / 4];
+			vlapic_self_ipi_handler(vlapic, vector);
+			return (HANDLED);
+		} else
+			return (UNHANDLED);
+	}
+
+	switch (offset) {
+	case APIC_OFFSET_ID:
+		vlapic_id_write_handler(vlapic);
+		break;
+	case APIC_OFFSET_LDR:
+		vlapic_ldr_write_handler(vlapic);
+		break;
+	case APIC_OFFSET_DFR:
+		vlapic_dfr_write_handler(vlapic);
+		break;
+	case APIC_OFFSET_SVR:
+		vlapic_svr_write_handler(vlapic);
+		break;
+	case APIC_OFFSET_ESR:
+		vlapic_esr_write_handler(vlapic);
+		break;
+	case APIC_OFFSET_ICR_LOW:
+		retu = false;
+		error = vlapic_icrlo_write_handler(vlapic, &retu);
+		if (error != 0 || retu)
+			handled = UNHANDLED;
+		break;
+	case APIC_OFFSET_CMCI_LVT:
+	case APIC_OFFSET_TIMER_LVT ... APIC_OFFSET_ERROR_LVT:
+		vlapic_lvt_write_handler(vlapic, offset);
+		break;
+	case APIC_OFFSET_TIMER_ICR:
+		vlapic_icrtmr_write_handler(vlapic);
+		break;
+	case APIC_OFFSET_TIMER_DCR:
+		vlapic_dcr_write_handler(vlapic);
+		break;
+	default:
+		handled = UNHANDLED;
+		break;
+	}
+	return (handled);
+}
+
+static bool
+apic_access_fault(struct vmx *vmx, int vcpuid, uint64_t gpa)
+{
+
+	if (apic_access_virtualization(vmx, vcpuid) &&
+	    (gpa >= DEFAULT_APIC_BASE && gpa < DEFAULT_APIC_BASE + PAGE_SIZE))
+		return (true);
+	else
+		return (false);
+}
+
+static int
+vmx_handle_apic_access(struct vmx *vmx, int vcpuid, struct vm_exit *vmexit)
+{
+	uint64_t qual;
+	int access_type, offset, allowed;
+
+	if (!apic_access_virtualization(vmx, vcpuid))
+		return (UNHANDLED);
+
+	qual = vmexit->u.vmx.exit_qualification;
+	access_type = APIC_ACCESS_TYPE(qual);
+	offset = APIC_ACCESS_OFFSET(qual);
+
+	allowed = 0;
+	if (access_type == 0) {
+		/*
+		 * Read data access to the following registers is expected.
+		 */
+		switch (offset) {
+		case APIC_OFFSET_APR:
+		case APIC_OFFSET_PPR:
+		case APIC_OFFSET_RRR:
+		case APIC_OFFSET_CMCI_LVT:
+		case APIC_OFFSET_TIMER_CCR:
+			allowed = 1;
+			break;
+		default:
+			break;
+		}
+	} else if (access_type == 1) {
+		/*
+		 * Write data access to the following registers is expected.
+		 */
+		switch (offset) {
+		case APIC_OFFSET_VER:
+		case APIC_OFFSET_APR:
+		case APIC_OFFSET_PPR:
+		case APIC_OFFSET_RRR:
+		case APIC_OFFSET_ISR0 ... APIC_OFFSET_ISR7:
+		case APIC_OFFSET_TMR0 ... APIC_OFFSET_TMR7:
+		case APIC_OFFSET_IRR0 ... APIC_OFFSET_IRR7:
+		case APIC_OFFSET_CMCI_LVT:
+		case APIC_OFFSET_TIMER_CCR:
+			allowed = 1;
+			break;
+		default:
+			break;
+		}
+	}
+
+	if (allowed) {
+		vmexit_inst_emul(vmexit, DEFAULT_APIC_BASE + offset,
+		    VIE_INVALID_GLA);
+	}
+
+	/*
+	 * Regardless of whether the APIC-access is allowed this handler
+	 * always returns UNHANDLED:
+	 * - if the access is allowed then it is handled by emulating the
+	 *   instruction that caused the VM-exit (outside the critical section)
+	 * - if the access is not allowed then it will be converted to an
+	 *   exitcode of VM_EXITCODE_VMX and will be dealt with in userland.
+	 */
+	return (UNHANDLED);
+}
+
+static enum task_switch_reason
+vmx_task_switch_reason(uint64_t qual)
+{
+	int reason;
+
+	reason = (qual >> 30) & 0x3;
+	switch (reason) {
+	case 0:
+		return (TSR_CALL);
+	case 1:
+		return (TSR_IRET);
+	case 2:
+		return (TSR_JMP);
+	case 3:
+		return (TSR_IDT_GATE);
+	default:
+		panic("%s: invalid reason %d", __func__, reason);
+	}
+}
+
 static int
 emulate_wrmsr(struct vmx *vmx, int vcpuid, u_int num, uint64_t val, bool *retu)
 {
@@ -1852,32 +2262,145 @@ emulate_rdmsr(struct vmx *vmx, int vcpuid, u_int num, bool *retu)
 	return (error);
 }
 
+#ifndef __FreeBSD__
+#define	__predict_false(x)	(x)
+#endif
+
 static int
 vmx_exit_process(struct vmx *vmx, int vcpu, struct vm_exit *vmexit)
 {
-	int error, handled, in;
-	struct vmcs *vmcs;
+	int error, errcode, errcode_valid, handled, in;
 	struct vmxctx *vmxctx;
+	struct vlapic *vlapic;
 	struct vm_inout_str *vis;
-	uint32_t eax, ecx, edx, idtvec_info, intr_info, inst_info;
-	uint64_t qual, gpa;
-#if notyet
-	uint64_t gla, cr3;
-#endif
+	struct vm_task_switch *ts;
+	uint32_t eax, ecx, edx, idtvec_info, idtvec_err, intr_info, inst_info;
+	uint32_t intr_type, intr_vec, reason;
+	uint64_t exitintinfo, qual, gpa;
 	bool retu;
 
 	CTASSERT((PINBASED_CTLS_ONE_SETTING & PINBASED_VIRTUAL_NMI) != 0);
 	CTASSERT((PINBASED_CTLS_ONE_SETTING & PINBASED_NMI_EXITING) != 0);
 
 	handled = UNHANDLED;
-	vmcs = &vmx->vmcs[vcpu];
 	vmxctx = &vmx->ctx[vcpu];
+
 	qual = vmexit->u.vmx.exit_qualification;
+	reason = vmexit->u.vmx.exit_reason;
 	vmexit->exitcode = VM_EXITCODE_BOGUS;
 
 	vmm_stat_incr(vmx->vm, vcpu, VMEXIT_COUNT, 1);
 
-	switch (vmexit->u.vmx.exit_reason) {
+	/*
+	 * VM-entry failures during or after loading guest state.
+	 *
+	 * These VM-exits are uncommon but must be handled specially
+	 * as most VM-exit fields are not populated as usual.
+	 */
+	if (__predict_false(reason == EXIT_REASON_MCE_DURING_ENTRY)) {
+		VCPU_CTR0(vmx->vm, vcpu, "Handling MCE during VM-entry");
+#ifdef __FreeBSD__
+		__asm __volatile("int $18");
+#else
+		panic("XXX vector to MCE handler");
+#endif
+		return (1);
+	}
+
+	/*
+	 * VM exits that can be triggered during event delivery need to
+	 * be handled specially by re-injecting the event if the IDT
+	 * vectoring information field's valid bit is set.
+	 *
+	 * See "Information for VM Exits During Event Delivery" in Intel SDM
+	 * for details.
+	 */
+	idtvec_info = vmcs_idt_vectoring_info();
+	if (idtvec_info & VMCS_IDT_VEC_VALID) {
+		idtvec_info &= ~(1 << 12); /* clear undefined bit */
+		exitintinfo = idtvec_info;
+		if (idtvec_info & VMCS_IDT_VEC_ERRCODE_VALID) {
+			idtvec_err = vmcs_idt_vectoring_err();
+			exitintinfo |= (uint64_t)idtvec_err << 32;
+		}
+		error = vm_exit_intinfo(vmx->vm, vcpu, exitintinfo);
+		KASSERT(error == 0, ("%s: vm_set_intinfo error %d",
+		    __func__, error));
+
+		/*
+		 * If 'virtual NMIs' are being used and the VM-exit
+		 * happened while injecting an NMI during the previous
+		 * VM-entry, then clear "blocking by NMI" in the
+		 * Guest Interruptibility-State so the NMI can be
+		 * reinjected on the subsequent VM-entry.
+		 *
+		 * However, if the NMI was being delivered through a task
+		 * gate, then the new task must start execution with NMIs
+		 * blocked so don't clear NMI blocking in this case.
+		 */
+		intr_type = idtvec_info & VMCS_INTR_T_MASK;
+		if (intr_type == VMCS_INTR_T_NMI) {
+			if (reason != EXIT_REASON_TASK_SWITCH)
+				vmx_clear_nmi_blocking(vmx, vcpu);
+			else
+				vmx_assert_nmi_blocking(vmx, vcpu);
+		}
+
+		/*
+		 * Update VM-entry instruction length if the event being
+		 * delivered was a software interrupt or software exception.
+		 */
+		if (intr_type == VMCS_INTR_T_SWINTR ||
+		    intr_type == VMCS_INTR_T_PRIV_SWEXCEPTION ||
+		    intr_type == VMCS_INTR_T_SWEXCEPTION) {
+			vmcs_write(VMCS_ENTRY_INST_LENGTH, vmexit->inst_length);
+		}
+	}
+
+	switch (reason) {
+	case EXIT_REASON_TASK_SWITCH:
+		ts = &vmexit->u.task_switch;
+		ts->tsssel = qual & 0xffff;
+		ts->reason = vmx_task_switch_reason(qual);
+		ts->ext = 0;
+		ts->errcode_valid = 0;
+		vmx_paging_info(&ts->paging);
+		/*
+		 * If the task switch was due to a CALL, JMP, IRET, software
+		 * interrupt (INT n) or software exception (INT3, INTO),
+		 * then the saved %rip references the instruction that caused
+		 * the task switch. The instruction length field in the VMCS
+		 * is valid in this case.
+		 *
+		 * In all other cases (e.g., NMI, hardware exception) the
+		 * saved %rip is one that would have been saved in the old TSS
+		 * had the task switch completed normally so the instruction
+		 * length field is not needed in this case and is explicitly
+		 * set to 0.
+		 */
+		if (ts->reason == TSR_IDT_GATE) {
+			KASSERT(idtvec_info & VMCS_IDT_VEC_VALID,
+			    ("invalid idtvec_info %#x for IDT task switch",
+			    idtvec_info));
+			intr_type = idtvec_info & VMCS_INTR_T_MASK;
+			if (intr_type != VMCS_INTR_T_SWINTR &&
+			    intr_type != VMCS_INTR_T_SWEXCEPTION &&
+			    intr_type != VMCS_INTR_T_PRIV_SWEXCEPTION) {
+				/* Task switch triggered by external event */
+				ts->ext = 1;
+				vmexit->inst_length = 0;
+				if (idtvec_info & VMCS_IDT_VEC_ERRCODE_VALID) {
+					ts->errcode_valid = 1;
+					ts->errcode = vmcs_idt_vectoring_err();
+				}
+			}
+		}
+		vmexit->exitcode = VM_EXITCODE_TASK_SWITCH;
+		VCPU_CTR4(vmx->vm, vcpu, "task switch reason %d, tss 0x%04x, "
+		    "%s errcode 0x%016lx", ts->reason, ts->tsssel,
+		    ts->ext ? "external" : "internal",
+		    ((uint64_t)ts->errcode << 32) | ts->errcode_valid);
+		break;
 	case EXIT_REASON_CR_ACCESS:
 		vmm_stat_incr(vmx->vm, vcpu, VMEXIT_CR_ACCESS, 1);
 		switch (qual & 0xf) {
@@ -1939,6 +2462,7 @@ vmx_exit_process(struct vmx *vmx, int vcpu, struct vm_exit *vmexit)
 	case EXIT_REASON_MTF:
 		vmm_stat_incr(vmx->vm, vcpu, VMEXIT_MTRAP, 1);
 		vmexit->exitcode = VM_EXITCODE_MTRAP;
+		vmexit->inst_length = 0;
 		break;
 	case EXIT_REASON_PAUSE:
 		vmm_stat_incr(vmx->vm, vcpu, VMEXIT_PAUSE, 1);
@@ -1966,18 +2490,10 @@ vmx_exit_process(struct vmx *vmx, int vcpu, struct vm_exit *vmexit)
 		 */
 		if (!(intr_info & VMCS_INTR_VALID))
 			return (1);
-#ifdef	__FreeBSD__
 		KASSERT((intr_info & VMCS_INTR_VALID) != 0 &&
 		    (intr_info & VMCS_INTR_T_MASK) == VMCS_INTR_T_HWINTR,
 		    ("VM exit interruption info invalid: %#x", intr_info));
-#else
-		KASSERT((intr_info & VMCS_INTR_VALID) != 0 &&
-		    (intr_info & VMCS_INTR_T_MASK) == VMCS_INTR_T_HWINTR,
-		    ("VM exit interruption info invalid: %x", intr_info));
-#endif
-#if 0	/* XXX */
 		vmx_trigger_hostintr(intr_info & 0xff);
-#endif
 
 		/*
 		 * This is special. We want to treat this as an 'handled'
@@ -2021,13 +2537,11 @@ vmx_exit_process(struct vmx *vmx, int vcpu, struct vm_exit *vmexit)
 	case EXIT_REASON_EXCEPTION:
 		vmm_stat_incr(vmx->vm, vcpu, VMEXIT_EXCEPTION, 1);
 		intr_info = vmcs_read(VMCS_EXIT_INTR_INFO);
-#ifdef	__FreeBSD__
 		KASSERT((intr_info & VMCS_INTR_VALID) != 0,
 		    ("VM exit interruption info invalid: %#x", intr_info));
-#else
-		KASSERT((intr_info & VMCS_INTR_VALID) != 0,
-		    ("VM exit interruption info invalid: %x", intr_info));
-#endif
+
+		intr_vec = intr_info & 0xff;
+		intr_type = intr_info & VMCS_INTR_T_MASK;
 
 		/*
 		 * If Virtual NMIs control is 1 and the VM-exit is due to a
@@ -2036,26 +2550,117 @@ vmx_exit_process(struct vmx *vmx, int vcpu, struct vm_exit *vmexit)
 		 * the guest.
 		 *
 		 * See "Resuming Guest Software after Handling an Exception".
+		 * See "Information for VM Exits Due to Vectored Events".
 		 */
-		/* XXX: properly initialize idtvec_info */
-		idtvec_info = 0;
 		if ((idtvec_info & VMCS_IDT_VEC_VALID) == 0 &&
-		    (intr_info & 0xff) != IDT_DF &&
+		    (intr_vec != IDT_DF) &&
 		    (intr_info & EXIT_QUAL_NMIUDTI) != 0)
 			vmx_restore_nmi_blocking(vmx, vcpu);
 
 		/*
 		 * The NMI has already been handled in vmx_exit_handle_nmi().
 		 */
-		if ((intr_info & VMCS_INTR_T_MASK) == VMCS_INTR_T_NMI)
+		if (intr_type == VMCS_INTR_T_NMI)
 			return (1);
-		break;
+
+		/*
+		 * Call the machine check handler by hand. Also don't reflect
+		 * the machine check back into the guest.
+		 */
+		if (intr_vec == IDT_MC) {
+			VCPU_CTR0(vmx->vm, vcpu, "Vectoring to MCE handler");
+#ifdef __FreeBSD__
+			__asm __volatile("int $18");
+#else
+			panic("XXX vector to MCE handler");
+#endif
+			return (1);
+		}
+
+		if (intr_vec == IDT_PF) {
+			error = vmxctx_setreg(vmxctx, VM_REG_GUEST_CR2, qual);
+			KASSERT(error == 0, ("%s: vmxctx_setreg(cr2) error %d",
+			    __func__, error));
+		}
+
+		/*
+		 * Software exceptions exhibit trap-like behavior. This in
+		 * turn requires populating the VM-entry instruction length
+		 * so that the %rip in the trap frame is past the INT3/INTO
+		 * instruction.
+		 */
+		if (intr_type == VMCS_INTR_T_SWEXCEPTION)
+			vmcs_write(VMCS_ENTRY_INST_LENGTH, vmexit->inst_length);
+
+		/* Reflect all other exceptions back into the guest */
+		errcode_valid = errcode = 0;
+		if (intr_info & VMCS_INTR_DEL_ERRCODE) {
+			errcode_valid = 1;
+			errcode = vmcs_read(VMCS_EXIT_INTR_ERRCODE);
+		}
+		VCPU_CTR2(vmx->vm, vcpu, "Reflecting exception %d/%#x into "
+		    "the guest", intr_vec, errcode);
+		error = vm_inject_exception(vmx->vm, vcpu, intr_vec,
+		    errcode_valid, errcode, 0);
+		KASSERT(error == 0, ("%s: vm_inject_exception error %d",
+		    __func__, error));
+		return (1);
+
 	case EXIT_REASON_EPT_FAULT:
+		/*
+		 * If 'gpa' lies within the address space allocated to
+		 * memory then this must be a nested page fault otherwise
+		 * this must be an instruction that accesses MMIO space.
+		 */
 		gpa = vmcs_gpa();
-		if (ept_emulation_fault(qual)) {
+		if (vm_mem_allocated(vmx->vm, vcpu, gpa) ||
+		    apic_access_fault(vmx, vcpu, gpa)) {
+			vmexit->exitcode = VM_EXITCODE_PAGING;
+			vmexit->inst_length = 0;
+			vmexit->u.paging.gpa = gpa;
+			vmexit->u.paging.fault_type = ept_fault_type(qual);
+			vmm_stat_incr(vmx->vm, vcpu, VMEXIT_NESTED_FAULT, 1);
+		} else if (ept_emulation_fault(qual)) {
 			vmexit_inst_emul(vmexit, gpa, vmcs_gla());
 			vmm_stat_incr(vmx->vm, vcpu, VMEXIT_INST_EMUL, 1);
 		}
+		/*
+		 * If Virtual NMIs control is 1 and the VM-exit is due to an
+		 * EPT fault during the execution of IRET then we must restore
+		 * the state of "virtual-NMI blocking" before resuming.
+		 *
+		 * See description of "NMI unblocking due to IRET" in
+		 * "Exit Qualification for EPT Violations".
+		 */
+		if ((idtvec_info & VMCS_IDT_VEC_VALID) == 0 &&
+		    (qual & EXIT_QUAL_NMIUDTI) != 0)
+			vmx_restore_nmi_blocking(vmx, vcpu);
+		break;
+	case EXIT_REASON_VIRTUALIZED_EOI:
+		vmexit->exitcode = VM_EXITCODE_IOAPIC_EOI;
+		vmexit->u.ioapic_eoi.vector = qual & 0xFF;
+		vmexit->inst_length = 0;	/* trap-like */
+		break;
+	case EXIT_REASON_APIC_ACCESS:
+		handled = vmx_handle_apic_access(vmx, vcpu, vmexit);
+		break;
+	case EXIT_REASON_APIC_WRITE:
+		/*
+		 * APIC-write VM exit is trap-like so the %rip is already
+		 * pointing to the next instruction.
+		 */
+		vmexit->inst_length = 0;
+		vlapic = vm_lapic(vmx->vm, vcpu);
+		handled = vmx_handle_apic_write(vmx, vcpu, vlapic, qual);
+		break;
+	case EXIT_REASON_XSETBV:
+		handled = vmx_emulate_xsetbv(vmx, vcpu, vmexit);
+		break;
+	case EXIT_REASON_MONITOR:
+		vmexit->exitcode = VM_EXITCODE_MONITOR;
+		break;
+	case EXIT_REASON_MWAIT:
+		vmexit->exitcode = VM_EXITCODE_MWAIT;
 		break;
 	default:
 		vmm_stat_incr(vmx->vm, vcpu, VMEXIT_UNKNOWN, 1);
@@ -2073,17 +2678,9 @@ vmx_exit_process(struct vmx *vmx, int vcpu, struct vm_exit *vmexit)
 		 * the one we just processed. Therefore we update the
 		 * guest rip in the VMCS and in 'vmexit'.
 		 */
-		vm_exit_update_rip(vmexit);
 		vmexit->rip += vmexit->inst_length;
 		vmexit->inst_length = 0;
-
-		/*
-		 * Special case for spinning up an AP - exit to userspace to
-		 * give the controlling process a chance to intercept and
-		 * spin up a thread for the AP.
-		 */
-		if (vmexit->exitcode == VM_EXITCODE_SPINUP_AP)
-			handled = 0;
+		vmcs_write(VMCS_GUEST_RIP, vmexit->rip);
 	} else {
 		if (vmexit->exitcode == VM_EXITCODE_BOGUS) {
 			/*
@@ -2104,30 +2701,92 @@ vmx_exit_process(struct vmx *vmx, int vcpu, struct vm_exit *vmexit)
 	return (handled);
 }
 
-static int
-vmx_run(void *arg, int vcpu, register_t rip)
+static void
+vmx_exit_inst_error(struct vmxctx *vmxctx, int rc, struct vm_exit *vmexit)
 {
-#if notyet
-	int error;
+
+	KASSERT(vmxctx->inst_fail_status != VM_SUCCESS,
+	    ("vmx_exit_inst_error: invalid inst_fail_status %d",
+	    vmxctx->inst_fail_status));
+
+	vmexit->inst_length = 0;
+	vmexit->exitcode = VM_EXITCODE_VMX;
+	vmexit->u.vmx.status = vmxctx->inst_fail_status;
+	vmexit->u.vmx.inst_error = vmcs_instruction_error();
+	vmexit->u.vmx.exit_reason = ~0;
+	vmexit->u.vmx.exit_qualification = ~0;
+
+	switch (rc) {
+	case VMX_VMRESUME_ERROR:
+	case VMX_VMLAUNCH_ERROR:
+	case VMX_INVEPT_ERROR:
+#ifndef __FreeBSD__
+	case VMX_VMWRITE_ERROR:
 #endif
-	int vie, rc, handled, astpending;
-	uint32_t exit_reason;
+		vmexit->u.vmx.inst_type = rc;
+		break;
+	default:
+		panic("vm_exit_inst_error: vmx_enter_guest returned %d", rc);
+	}
+}
+
+/*
+ * If the NMI-exiting VM execution control is set to '1' then an NMI in
+ * non-root operation causes a VM-exit. NMI blocking is in effect so it is
+ * sufficient to simply vector to the NMI handler via a software interrupt.
+ * However, this must be done before maskable interrupts are enabled
+ * otherwise the "iret" issued by an interrupt handler will incorrectly
+ * clear NMI blocking.
+ */
+static __inline void
+vmx_exit_handle_nmi(struct vmx *vmx, int vcpuid, struct vm_exit *vmexit)
+{
+	uint32_t intr_info;
+
+	KASSERT((read_rflags() & PSL_I) == 0, ("interrupts enabled"));
+
+	if (vmexit->u.vmx.exit_reason != EXIT_REASON_EXCEPTION)
+		return;
+
+	intr_info = vmcs_read(VMCS_EXIT_INTR_INFO);
+	KASSERT((intr_info & VMCS_INTR_VALID) != 0,
+	    ("VM exit interruption info invalid: %#x", intr_info));
+
+	if ((intr_info & VMCS_INTR_T_MASK) == VMCS_INTR_T_NMI) {
+		KASSERT((intr_info & 0xff) == IDT_NMI, ("VM exit due "
+		    "to NMI has invalid vector: %#x", intr_info));
+		VCPU_CTR0(vmx->vm, vcpuid, "Vectoring to NMI handler");
+#ifdef __FreeBSD__
+		__asm __volatile("int $2");
+#else
+		panic("XXX vector to NMI handler");
+#endif
+	}
+}
+
+static int
+vmx_run(void *arg, int vcpu, register_t rip, pmap_t pmap,
+    struct vm_eventinfo *evinfo)
+{
+	int rc, handled, launched;
 	struct vmx *vmx;
 	struct vm *vm;
 	struct vmxctx *vmxctx;
 	struct vmcs *vmcs;
 	struct vm_exit *vmexit;
 	struct vlapic *vlapic;
-	
+	uint32_t exit_reason;
+
 	vmx = arg;
 	vm = vmx->vm;
 	vmcs = &vmx->vmcs[vcpu];
 	vmxctx = &vmx->ctx[vcpu];
 	vlapic = vm_lapic(vm, vcpu);
-	vmxctx->launched = 0;
+	vmexit = vm_exitinfo(vm, vcpu);
+	launched = 0;
 
-	astpending = 0;
-	vmexit = vm_exitinfo(vmx->vm, vcpu);
+	KASSERT(vmxctx->pmap == pmap,
+	    ("pmap %p different than ctx pmap %p", pmap, vmxctx->pmap));
 
 	vmx_msr_guest_enter(vmx, vcpu);
 
@@ -2139,56 +2798,74 @@ vmx_run(void *arg, int vcpu, register_t rip)
 	 * from a different process than the one that actually runs it.
 	 *
 	 * If the life of a virtual machine was spent entirely in the context
-	 * of a single process we could do this once in vmcs_set_defaults().
+	 * of a single process we could do this once in vmx_vminit().
 	 */
 	vmcs_write(VMCS_HOST_CR3, rcr3());
 
 	vmcs_write(VMCS_GUEST_RIP, rip);
-	vmx_set_pcpu_defaults(vmx, vcpu);
+	vmx_set_pcpu_defaults(vmx, vcpu, pmap);
 	do {
-		vmx_inject_interrupts(vmx, vcpu, vlapic);
-		vmx_run_trace(vmx, vcpu);
-		rc = vmx_setjmp(vmxctx);
-#ifdef SETJMP_TRACE
-		vmx_setjmp_trace(vmx, vcpu, vmxctx, rc);
-#endif
-		switch (rc) {
-		case VMX_RETURN_DIRECT:
-			if (vmxctx->launched == 0) {
-				vmxctx->launched = 1;
-				vmx_launch(vmxctx);
-			} else
-				vmx_resume(vmxctx);
-			panic("vmx_launch/resume should not return");
+		KASSERT(vmcs_guest_rip() == rip, ("%s: vmcs guest rip mismatch "
+		    "%#lx/%#lx", __func__, vmcs_guest_rip(), rip));
+
+		handled = UNHANDLED;
+		/*
+		 * Interrupts are disabled from this point on until the
+		 * guest starts executing. This is done for the following
+		 * reasons:
+		 *
+		 * If an AST is asserted on this thread after the check below,
+		 * then the IPI_AST notification will not be lost, because it
+		 * will cause a VM exit due to external interrupt as soon as
+		 * the guest state is loaded.
+		 *
+		 * A posted interrupt after 'vmx_inject_interrupts()' will
+		 * not be "lost" because it will be held pending in the host
+		 * APIC because interrupts are disabled. The pending interrupt
+		 * will be recognized as soon as the guest state is loaded.
+		 *
+		 * The same reasoning applies to the IPI generated by
+		 * pmap_invalidate_ept().
+		 */
+		/* XXXJOY: this is _long_ time to keep interrupts disabled */
+		disable_intr();
+		vmx_inject_interrupts(vmx, vcpu, vlapic, rip);
+
+		/*
+		 * Check for vcpu suspension after injecting events because
+		 * vmx_inject_interrupts() can suspend the vcpu due to a
+		 * triple fault.
+		 */
+		if (vcpu_suspended(evinfo)) {
+			enable_intr();
+			vm_exit_suspended(vmx->vm, vcpu, rip);
 			break;
-		case VMX_RETURN_LONGJMP:
-			break;			/* vm exit */
-		case VMX_RETURN_AST:
-			astpending = 1;
+		}
+
+		if (vcpu_rendezvous_pending(evinfo)) {
+			enable_intr();
+			vm_exit_rendezvous(vmx->vm, vcpu, rip);
 			break;
-		case VMX_RETURN_VMRESUME:
-			vie = vmcs_instruction_error();
-			if (vmxctx->launch_error == VM_FAIL_INVALID ||
-			    vie != VMRESUME_WITH_NON_LAUNCHED_VMCS) {
-				printf("vmresume error %d vmcs inst error %d\n",
-					vmxctx->launch_error, vie);
-				goto err_exit;
-			}
-			vmx_launch(vmxctx);	/* try to launch the guest */
-			panic("vmx_launch should not return");
+		}
+
+		if (vcpu_reqidle(evinfo)) {
+			enable_intr();
+			vm_exit_reqidle(vmx->vm, vcpu, rip);
 			break;
-		case VMX_RETURN_VMLAUNCH:
-			vie = vmcs_instruction_error();
-#if 1
-			printf("vmlaunch error %d vmcs inst error %d\n",
-				vmxctx->launch_error, vie);
-#endif
-			goto err_exit;
-		default:
-			panic("vmx_setjmp returned %d", rc);
 		}
-		
-		/* collect some basic information for VM exit processing */
+
+		if (vcpu_should_yield(vm, vcpu)) {
+			enable_intr();
+			vm_exit_astpending(vmx->vm, vcpu, rip);
+			vmx_astpending_trace(vmx, vcpu, rip);
+			handled = HANDLED;
+			break;
+		}
+
+		vmx_run_trace(vmx, vcpu);
+		rc = vmx_enter_guest(vmxctx, vmx, launched);
+
+		/* Collect some information for VM exit processing */
 		vmexit->rip = rip = vmcs_guest_rip();
 		vmexit->inst_length = vmexit_instruction_length();
 		vmexit->u.vmx.exit_reason = exit_reason = vmcs_exit_reason();
@@ -2197,21 +2874,17 @@ vmx_run(void *arg, int vcpu, register_t rip)
 		/* Update 'nextrip' */
 		vmx->state[vcpu].nextrip = rip;
 
-		/* enable interrupts */
-		enable_intr();
-
-		if (astpending) {
-			handled = 1;
-			vmexit->inst_length = 0;
-			vmexit->exitcode = VM_EXITCODE_BOGUS;
-			vmx_astpending_trace(vmx, vcpu, rip);
-			vmm_stat_incr(vmx->vm, vcpu, VMEXIT_ASTPENDING, 1);
-			break;
+		if (rc == VMX_GUEST_VMEXIT) {
+			vmx_exit_handle_nmi(vmx, vcpu, vmexit);
+			enable_intr();
+			handled = vmx_exit_process(vmx, vcpu, vmexit);
+		} else {
+			enable_intr();
+			vmx_exit_inst_error(vmxctx, rc, vmexit);
 		}
-
-		handled = vmx_exit_process(vmx, vcpu, vmexit);
+		launched = 1;
 		vmx_exit_trace(vmx, vcpu, rip, exit_reason, handled);
-
+		rip = vmexit->rip;
 	} while (handled);
 
 	/*
@@ -2225,44 +2898,29 @@ vmx_run(void *arg, int vcpu, register_t rip)
 	}
 
 	if (!handled)
-		vmm_stat_incr(vmx->vm, vcpu, VMEXIT_USERSPACE, 1);
+		vmm_stat_incr(vm, vcpu, VMEXIT_USERSPACE, 1);
 
-	VCPU_CTR1(vmx->vm, vcpu, "goto userland: exitcode %d",
+	VCPU_CTR1(vm, vcpu, "returning from vmx_run: exitcode %d",
 	    vmexit->exitcode);
 
 	VMCLEAR(vmcs);
 	vmx_msr_guest_exit(vmx, vcpu);
 
 	return (0);
-
-err_exit:
-	vmexit->exitcode = VM_EXITCODE_VMX;
-	vmexit->u.vmx.exit_reason = (uint32_t)-1;
-	vmexit->u.vmx.exit_qualification = (uint32_t)-1;
-	vmexit->u.vmx.status = ~0;
-	VMCLEAR(vmcs);
-	vmx_msr_guest_exit(vmx, vcpu);
-
-	return (ENOEXEC);
 }
 
 static void
 vmx_vmcleanup(void *arg)
 {
-	int i, error;
+	int i;
 	struct vmx *vmx = arg;
 
+	if (apic_access_virtualization(vmx, 0))
+		vm_unmap_mmio(vmx->vm, DEFAULT_APIC_BASE, PAGE_SIZE);
+
 	for (i = 0; i < VM_MAXCPU; i++)
 		vpid_free(vmx->state[i].vpid);
 
-	/*
-	 * XXXSMP we also need to clear the VMCS active on the other vcpus.
-	 */
-	error = vmclear(&vmx->vmcs[0]);
-	if (error != 0)
-		panic("vmx_vmcleanup: vmclear error %d on vcpu 0", error);
-
-	ept_vmcleanup(vmx);
 	free(vmx, M_VMX);
 
 	return;
@@ -2335,6 +2993,46 @@ vmxctx_setreg(struct vmxctx *vmxctx, int reg, uint64_t val)
 		return (EINVAL);
 }
 
+static int
+vmx_get_intr_shadow(struct vmx *vmx, int vcpu, int running, uint64_t *retval)
+{
+	uint64_t gi;
+	int error;
+
+	error = vmcs_getreg(&vmx->vmcs[vcpu], running, 
+	    VMCS_IDENT(VMCS_GUEST_INTERRUPTIBILITY), &gi);
+	*retval = (gi & HWINTR_BLOCKING) ? 1 : 0;
+	return (error);
+}
+
+static int
+vmx_modify_intr_shadow(struct vmx *vmx, int vcpu, int running, uint64_t val)
+{
+	struct vmcs *vmcs;
+	uint64_t gi;
+	int error, ident;
+
+	/*
+	 * Forcing the vcpu into an interrupt shadow is not supported.
+	 */
+	if (val) {
+		error = EINVAL;
+		goto done;
+	}
+
+	vmcs = &vmx->vmcs[vcpu];
+	ident = VMCS_IDENT(VMCS_GUEST_INTERRUPTIBILITY);
+	error = vmcs_getreg(vmcs, running, ident, &gi);
+	if (error == 0) {
+		gi &= ~HWINTR_BLOCKING;
+		error = vmcs_setreg(vmcs, running, ident, gi);
+	}
+done:
+	VCPU_CTR2(vmx->vm, vcpu, "Setting intr_shadow to %#lx %s", val,
+	    error ? "failed" : "succeeded");
+	return (error);
+}
+
 static int
 vmx_shadow_reg(int reg)
 {
@@ -2366,6 +3064,9 @@ vmx_getreg(void *arg, int vcpu, int reg, uint64_t *retval)
 	if (running && hostcpu != curcpu)
 		panic("vmx_getreg: %s%d is running", vm_name(vmx->vm), vcpu);
 
+	if (reg == VM_REG_GUEST_INTR_SHADOW)
+		return (vmx_get_intr_shadow(vmx, vcpu, running, retval));
+
 	if (vmxctx_getreg(&vmx->ctx[vcpu], reg, retval) == 0)
 		return (0);
 
@@ -2377,12 +3078,16 @@ vmx_setreg(void *arg, int vcpu, int reg, uint64_t val)
 {
 	int error, hostcpu, running, shadow;
 	uint64_t ctls;
+	pmap_t pmap;
 	struct vmx *vmx = arg;
 
 	running = vcpu_is_running(vmx->vm, vcpu, &hostcpu);
 	if (running && hostcpu != curcpu)
 		panic("vmx_setreg: %s%d is running", vm_name(vmx->vm), vcpu);
 
+	if (reg == VM_REG_GUEST_INTR_SHADOW)
+		return (vmx_modify_intr_shadow(vmx, vcpu, running, val));
+
 	if (vmxctx_setreg(&vmx->ctx[vcpu], reg, val) == 0)
 		return (0);
 
@@ -2414,6 +3119,18 @@ vmx_setreg(void *arg, int vcpu, int reg, uint64_t val)
 			error = vmcs_setreg(&vmx->vmcs[vcpu], running,
 				    VMCS_IDENT(shadow), val);
 		}
+
+		if (reg == VM_REG_GUEST_CR3) {
+			/*
+			 * Invalidate the guest vcpu's TLB mappings to emulate
+			 * the behavior of updating %cr3.
+			 *
+			 * XXX the processor retains global mappings when %cr3
+			 * is updated but vmx_invvpid() does not.
+			 */
+			pmap = vmx->ctx[vcpu].pmap;
+			vmx_invvpid(vmx, vcpu, pmap, running);
+		}
 	}
 
 	return (error);
@@ -2473,6 +3190,10 @@ vmx_getcap(void *arg, int vcpu, int type, int *retval)
 		if (cap_unrestricted_guest)
 			ret = 0;
 		break;
+	case VM_CAP_ENABLE_INVPCID:
+		if (cap_invpcid)
+			ret = 0;
+		break;
 	default:
 		break;
 	}
@@ -2529,11 +3250,21 @@ vmx_setcap(void *arg, int vcpu, int type, int val)
 	case VM_CAP_UNRESTRICTED_GUEST:
 		if (cap_unrestricted_guest) {
 			retval = 0;
-			baseval = procbased_ctls2;
+			pptr = &vmx->cap[vcpu].proc_ctls2;
+			baseval = *pptr;
 			flag = PROCBASED2_UNRESTRICTED_GUEST;
 			reg = VMCS_SEC_PROC_BASED_CTLS;
 		}
 		break;
+	case VM_CAP_ENABLE_INVPCID:
+		if (cap_invpcid) {
+			retval = 0;
+			pptr = &vmx->cap[vcpu].proc_ctls2;
+			baseval = *pptr;
+			flag = PROCBASED2_ENABLE_INVPCID;
+			reg = VMCS_SEC_PROC_BASED_CTLS;
+		}
+		break;
 	default:
 		break;
 	}
@@ -2699,6 +3430,49 @@ vmx_set_tmr(struct vlapic *vlapic, int vector, bool level)
 	VMCLEAR(vmcs);
 }
 
+static void
+vmx_enable_x2apic_mode(struct vlapic *vlapic)
+{
+	struct vmx *vmx;
+	struct vmcs *vmcs;
+	uint32_t proc_ctls2;
+	int vcpuid, error;
+
+	vcpuid = vlapic->vcpuid;
+	vmx = ((struct vlapic_vtx *)vlapic)->vmx;
+	vmcs = &vmx->vmcs[vcpuid];
+
+	proc_ctls2 = vmx->cap[vcpuid].proc_ctls2;
+	KASSERT((proc_ctls2 & PROCBASED2_VIRTUALIZE_APIC_ACCESSES) != 0,
+	    ("%s: invalid proc_ctls2 %#x", __func__, proc_ctls2));
+
+	proc_ctls2 &= ~PROCBASED2_VIRTUALIZE_APIC_ACCESSES;
+	proc_ctls2 |= PROCBASED2_VIRTUALIZE_X2APIC_MODE;
+	vmx->cap[vcpuid].proc_ctls2 = proc_ctls2;
+
+	VMPTRLD(vmcs);
+	vmcs_write(VMCS_SEC_PROC_BASED_CTLS, proc_ctls2);
+	VMCLEAR(vmcs);
+
+	if (vlapic->vcpuid == 0) {
+		/*
+		 * The nested page table mappings are shared by all vcpus
+		 * so unmap the APIC access page just once.
+		 */
+		error = vm_unmap_mmio(vmx->vm, DEFAULT_APIC_BASE, PAGE_SIZE);
+		KASSERT(error == 0, ("%s: vm_unmap_mmio error %d",
+		    __func__, error));
+
+		/*
+		 * The MSR bitmap is shared by all vcpus so modify it only
+		 * once in the context of vcpu 0.
+		 */
+		error = vmx_allow_x2apic_msrs(vmx);
+		KASSERT(error == 0, ("%s: vmx_allow_x2apic_msrs error %d",
+		    __func__, error));
+	}
+}
+
 static void
 vmx_post_intr(struct vlapic *vlapic, int hostcpu)
 {
@@ -2710,7 +3484,6 @@ vmx_post_intr(struct vlapic *vlapic, int hostcpu)
  * Transfer the pending interrupts in the PIR descriptor to the IRR
  * in the virtual APIC page.
  */
-#ifdef	__FreeBSD__
 static void
 vmx_inject_pir(struct vlapic *vlapic)
 {
@@ -2800,7 +3573,6 @@ vmx_inject_pir(struct vlapic *vlapic)
 		}
 	}
 }
-#endif
 
 static struct vlapic *
 vmx_vlapic_init(void *arg, int vcpuid)
@@ -2825,9 +3597,7 @@ vmx_vlapic_init(void *arg, int vcpuid)
 		vlapic->ops.pending_intr = vmx_pending_intr;
 		vlapic->ops.intr_accepted = vmx_intr_accepted;
 		vlapic->ops.set_tmr = vmx_set_tmr;
-#ifdef	__FreeBSD__
 		vlapic->ops.enable_x2apic_mode = vmx_enable_x2apic_mode;
-#endif
 	}
 
 	if (posted_interrupts)
@@ -2849,17 +3619,18 @@ vmx_vlapic_cleanup(void *arg, struct vlapic *vlapic)
 struct vmm_ops vmm_ops_intel = {
 	vmx_init,
 	vmx_cleanup,
+	vmx_restore,
 	vmx_vminit,
 	vmx_run,
 	vmx_vmcleanup,
-	ept_vmmmap_set,
-	ept_vmmmap_get,
 	vmx_getreg,
 	vmx_setreg,
 	vmx_getdesc,
 	vmx_setdesc,
 	vmx_getcap,
 	vmx_setcap,
+	ept_vmspace_alloc,
+	ept_vmspace_free,
 	vmx_vlapic_init,
 	vmx_vlapic_cleanup,
 };
diff --git a/usr/src/uts/i86pc/io/vmm/intel/vmx.h b/usr/src/uts/i86pc/io/vmm/intel/vmx.h
index 50ca62b371..04933eef80 100644
--- a/usr/src/uts/i86pc/io/vmm/intel/vmx.h
+++ b/usr/src/uts/i86pc/io/vmm/intel/vmx.h
@@ -23,7 +23,11 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/intel/vmx.h 284174 2015-06-09 00:14:47Z tychon $
+ * $FreeBSD$
+ */
+
+/*
+ * Copyright 2017 Joyent, Inc.
  */
 
 #ifndef _VMX_H_
@@ -31,15 +35,9 @@
 
 #include "vmcs.h"
 
-#ifndef	__FreeBSD__
-#define	GUEST_MSR_MAX_ENTRIES	64		/* arbitrary */
-#define	HOST_MSR_MAX_ENTRIES	64		/* arbitrary */
-#endif
+struct pmap;
 
 struct vmxctx {
-	register_t	tmpstk[32];		/* vmx_return() stack */
-	register_t	tmpstktop;
-
 	register_t	guest_rdi;		/* Guest state */
 	register_t	guest_rsi;
 	register_t	guest_rdx;
@@ -57,6 +55,7 @@ struct vmxctx {
 	register_t	guest_r15;
 	register_t	guest_cr2;
 
+#ifdef __FreeBSD__
 	register_t	host_r15;		/* Host state */
 	register_t	host_r14;
 	register_t	host_r13;
@@ -64,13 +63,18 @@ struct vmxctx {
 	register_t	host_rbp;
 	register_t	host_rsp;
 	register_t	host_rbx;
-	register_t	host_rip;
+#endif /* __FreeBSD__ */
 	/*
 	 * XXX todo debug registers and fpu state
 	 */
-	
-	int		launched;		/* vmcs launch state */
-	int		launch_error;
+
+	int		inst_fail_status;
+
+	/*
+	 * The pmap needs to be deactivated in vmx_enter_guest()
+	 * so keep a copy of the 'pmap' in each vmxctx.
+	 */
+	struct pmap	*pmap;
 };
 
 struct vmxcap {
@@ -105,52 +109,45 @@ enum {
 	IDX_MSR_STAR,
 	IDX_MSR_SF_MASK,
 	IDX_MSR_KGSBASE,
+	IDX_MSR_PAT,
 	GUEST_MSR_NUM		/* must be the last enumeration */
 };
 
 /* virtual machine softc */
 struct vmx {
-	pml4_entry_t	pml4ept[NPML4EPG];
 	struct vmcs	vmcs[VM_MAXCPU];	/* one vmcs per virtual cpu */
 	struct apic_page apic_page[VM_MAXCPU];	/* one apic page per vcpu */
 	char		msr_bitmap[PAGE_SIZE];
 	struct pir_desc	pir_desc[VM_MAXCPU];
-#ifdef	__FreeBSD__
 	uint64_t	guest_msrs[VM_MAXCPU][GUEST_MSR_NUM];
-#else
-	struct msr_entry guest_msrs[VM_MAXCPU][GUEST_MSR_MAX_ENTRIES];
-	struct msr_entry host_msrs[VM_MAXCPU][HOST_MSR_MAX_ENTRIES];
+#ifndef	__FreeBSD__
+	uint64_t	host_msrs[VM_MAXCPU][GUEST_MSR_NUM];
+	uint64_t	tsc_offset[VM_MAXCPU];
 #endif
 	struct vmxctx	ctx[VM_MAXCPU];
 	struct vmxcap	cap[VM_MAXCPU];
 	struct vmxstate	state[VM_MAXCPU];
+	uint64_t	eptp;
 	struct vm	*vm;
+	long		eptgen[MAXCPU];		/* cached pmap->pm_eptgen */
 };
-CTASSERT((offsetof(struct vmx, pml4ept) & PAGE_MASK) == 0);
 CTASSERT((offsetof(struct vmx, vmcs) & PAGE_MASK) == 0);
 CTASSERT((offsetof(struct vmx, msr_bitmap) & PAGE_MASK) == 0);
+CTASSERT((offsetof(struct vmx, pir_desc[0]) & 63) == 0);
 
-#define	VMX_RETURN_DIRECT	0
-#define	VMX_RETURN_LONGJMP	1
-#define	VMX_RETURN_VMRESUME	2
-#define	VMX_RETURN_VMLAUNCH	3
-#define	VMX_RETURN_AST		4
-/*
- * vmx_setjmp() returns:
- * - 0 when it returns directly
- * - 1 when it returns from vmx_longjmp
- * - 2 when it returns from vmx_resume (which would only be in the error case)
- * - 3 when it returns from vmx_launch (which would only be in the error case)
- * - 4 when it returns from vmx_resume or vmx_launch because of AST pending
- */
-int	vmx_setjmp(struct vmxctx *ctx);
-void	vmx_longjmp(void);			/* returns via vmx_setjmp */
-void	vmx_launch(struct vmxctx *ctx) __dead2;	/* may return via vmx_setjmp */
-void	vmx_resume(struct vmxctx *ctx) __dead2;	/* may return via vmx_setjmp */
+#define	VMX_GUEST_VMEXIT	0
+#define	VMX_VMRESUME_ERROR	1
+#define	VMX_VMLAUNCH_ERROR	2
+#define	VMX_INVEPT_ERROR	3
+#define	VMX_VMWRITE_ERROR	4
+int	vmx_enter_guest(struct vmxctx *ctx, struct vmx *vmx, int launched);
+void	vmx_call_isr(uintptr_t entry);
 
 u_long	vmx_fix_cr0(u_long cr0);
 u_long	vmx_fix_cr4(u_long cr4);
 
 int	vmx_set_tsc_offset(struct vmx *vmx, int vcpu, uint64_t offset);
 
+extern char	vmx_exit_guest[];
+
 #endif
diff --git a/usr/src/uts/i86pc/io/vmm/intel/vmx_controls.h b/usr/src/uts/i86pc/io/vmm/intel/vmx_controls.h
index 08b1469f19..2b117ae5cf 100644
--- a/usr/src/uts/i86pc/io/vmm/intel/vmx_controls.h
+++ b/usr/src/uts/i86pc/io/vmm/intel/vmx_controls.h
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/intel/vmx_controls.h 260410 2014-01-07 21:04:49Z neel $
+ * $FreeBSD$
  */
 
 #ifndef _VMX_CONTROLS_H_
diff --git a/usr/src/uts/i86pc/io/vmm/intel/vmx_cpufunc.h b/usr/src/uts/i86pc/io/vmm/intel/vmx_cpufunc.h
index 2344606b06..003b0e2cfc 100644
--- a/usr/src/uts/i86pc/io/vmm/intel/vmx_cpufunc.h
+++ b/usr/src/uts/i86pc/io/vmm/intel/vmx_cpufunc.h
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/intel/vmx_cpufunc.h 245678 2013-01-20 03:42:49Z neel $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -72,7 +72,12 @@ vmxon(char *region)
 	int error;
 	uint64_t addr;
 
+#ifdef __FreeBSD__
 	addr = vtophys(region);
+#else
+	/* This is pre-translated in illumos */
+	addr = (uint64_t)region;
+#endif
 	__asm __volatile("vmxon %[addr];"
 			 VMX_SET_ERROR_CODE
 			 : [error] "=r" (error)
@@ -82,21 +87,7 @@ vmxon(char *region)
 	return (error);
 }
 
-/* returns 0 on success and non-zero on failure */
-static __inline int
-vmxon_pa(vm_paddr_t addr)
-{
-	int error;
-
-	__asm __volatile("vmxon %[addr];"
-			 VMX_SET_ERROR_CODE
-			 : [error] "=r" (error)
-			 : [addr] "m" (*(uint64_t *)&addr)
-			 : "memory");
-
-	return (error);
-}
-
+#ifdef __FreeBSD__
 /* returns 0 on success and non-zero on failure */
 static __inline int
 vmclear(struct vmcs *vmcs)
@@ -112,6 +103,7 @@ vmclear(struct vmcs *vmcs)
 			 : "memory");
 	return (error);
 }
+#endif /* __FreeBSD__ */
 
 static __inline void
 vmxoff(void)
@@ -127,6 +119,7 @@ vmptrst(uint64_t *addr)
 	__asm __volatile("vmptrst %[addr]" :: [addr]"m" (*addr) : "memory");
 }
 
+#ifdef __FreeBSD__
 static __inline int
 vmptrld(struct vmcs *vmcs)
 {
@@ -141,6 +134,7 @@ vmptrld(struct vmcs *vmcs)
 			 : "memory");
 	return (error);
 }
+#endif /* __FreeBSD__ */
 
 static __inline int
 vmwrite(uint64_t reg, uint64_t val)
@@ -170,6 +164,7 @@ vmread(uint64_t r, uint64_t *addr)
 	return (error);
 }
 
+#ifdef __FreeBSD__
 static __inline void
 VMCLEAR(struct vmcs *vmcs)
 {
@@ -193,6 +188,7 @@ VMPTRLD(struct vmcs *vmcs)
 	if (err != 0)
 		panic("%s: vmptrld(%p) error %d", __func__, vmcs, err);
 }
+#endif /* __FreeBSD__ */
 
 #define	INVVPID_TYPE_ADDRESS		0UL
 #define	INVVPID_TYPE_SINGLE_CONTEXT	1UL
diff --git a/usr/src/uts/i86pc/io/vmm/intel/vmx_msr.c b/usr/src/uts/i86pc/io/vmm/intel/vmx_msr.c
index 0b08f7b1af..c46b9c41fa 100644
--- a/usr/src/uts/i86pc/io/vmm/intel/vmx_msr.c
+++ b/usr/src/uts/i86pc/io/vmm/intel/vmx_msr.c
@@ -23,18 +23,17 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/intel/vmx_msr.c 284174 2015-06-09 00:14:47Z tychon $
+ * $FreeBSD$
  */
 /*
  * Copyright 2017 Joyent, Inc.
  */
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/sys/amd64/vmm/intel/vmx_msr.c 284174 2015-06-09 00:14:47Z tychon $");
+__FBSDID("$FreeBSD$");
 
 #include <sys/param.h>
 #include <sys/systm.h>
-#include <sys/cpuset.h>
 
 #include <machine/clock.h>
 #include <machine/cpufunc.h>
@@ -42,10 +41,6 @@ __FBSDID("$FreeBSD: head/sys/amd64/vmm/intel/vmx_msr.c 284174 2015-06-09 00:14:4
 #include <machine/specialreg.h>
 #include <machine/vmm.h>
 
-#ifndef	__FreeBSD__
-#include <vm/pmap.h>
-#endif
-
 #include "vmx.h"
 #include "vmx_msr.h"
 
@@ -187,9 +182,9 @@ msr_bitmap_change_access(char *bitmap, u_int msr, int access)
 static uint64_t misc_enable;
 static uint64_t platform_info;
 static uint64_t turbo_ratio_limit;
-#ifdef	__FreeBSD__
+#ifdef __FreeBSD__
 static uint64_t host_msrs[GUEST_MSR_NUM];
-#endif
+#endif /* __FreeBSD__ */
 
 static bool
 nehalem_cpu(void)
@@ -239,13 +234,33 @@ westmere_cpu(void)
 	return (false);
 }
 
+static bool
+pat_valid(uint64_t val)
+{
+	int i, pa;
+
+	/*
+	 * From Intel SDM: Table "Memory Types That Can Be Encoded With PAT"
+	 *
+	 * Extract PA0 through PA7 and validate that each one encodes a
+	 * valid memory type.
+	 */
+	for (i = 0; i < 8; i++) {
+		pa = (val >> (i * 8)) & 0xff;
+		if (pa == 2 || pa == 3 || pa >= 8)
+			return (false);
+	}
+	return (true);
+}
+
 void
 vmx_msr_init(void)
 {
 	uint64_t bus_freq, ratio;
 	int i;
 
-#ifdef	__FreeBSD__
+#ifdef __FreeBSD__
+	/* XXXJOY: Do we want to do this caching? */
 	/*
 	 * It is safe to cache the values of the following MSRs because
 	 * they don't change based on curcpu, curproc or curthread.
@@ -254,7 +269,7 @@ vmx_msr_init(void)
 	host_msrs[IDX_MSR_CSTAR] = rdmsr(MSR_CSTAR);
 	host_msrs[IDX_MSR_STAR] = rdmsr(MSR_STAR);
 	host_msrs[IDX_MSR_SF_MASK] = rdmsr(MSR_SF_MASK);
-#endif
+#endif /* __FreeBSD__ */
 
 	/*
 	 * Initialize emulated MSRs
@@ -313,6 +328,10 @@ vmx_msr_init(void)
 void
 vmx_msr_guest_init(struct vmx *vmx, int vcpuid)
 {
+	uint64_t *guest_msrs;
+
+	guest_msrs = vmx->guest_msrs[vcpuid];
+
 	/*
 	 * The permissions bitmap is shared between all vcpus so initialize it
 	 * once when initializing the vBSP.
@@ -324,29 +343,52 @@ vmx_msr_guest_init(struct vmx *vmx, int vcpuid)
 		guest_msr_rw(vmx, MSR_SF_MASK);
 		guest_msr_rw(vmx, MSR_KGSBASE);
 	}
+
+	/*
+	 * Initialize guest IA32_PAT MSR with default value after reset.
+	 */
+	guest_msrs[IDX_MSR_PAT] = PAT_VALUE(0, PAT_WRITE_BACK) |
+	    PAT_VALUE(1, PAT_WRITE_THROUGH)	|
+	    PAT_VALUE(2, PAT_UNCACHED)		|
+	    PAT_VALUE(3, PAT_UNCACHEABLE)	|
+	    PAT_VALUE(4, PAT_WRITE_BACK)	|
+	    PAT_VALUE(5, PAT_WRITE_THROUGH)	|
+	    PAT_VALUE(6, PAT_UNCACHED)		|
+	    PAT_VALUE(7, PAT_UNCACHEABLE);
+
 	return;
 }
 
 void
 vmx_msr_guest_enter(struct vmx *vmx, int vcpuid)
 {
-#ifdef	__FreeBSD__
 	uint64_t *guest_msrs = vmx->guest_msrs[vcpuid];
 
+#ifndef __FreeBSD__
+	uint64_t *host_msrs = vmx->host_msrs[vcpuid];
+
+	/* Save host MSRs */
+	host_msrs[IDX_MSR_LSTAR] = rdmsr(MSR_LSTAR);
+	host_msrs[IDX_MSR_CSTAR] = rdmsr(MSR_CSTAR);
+	host_msrs[IDX_MSR_STAR] = rdmsr(MSR_STAR);
+	host_msrs[IDX_MSR_SF_MASK] = rdmsr(MSR_SF_MASK);
+#endif /* __FreeBSD__ */
+
 	/* Save host MSRs (if any) and restore guest MSRs */
 	wrmsr(MSR_LSTAR, guest_msrs[IDX_MSR_LSTAR]);
 	wrmsr(MSR_CSTAR, guest_msrs[IDX_MSR_CSTAR]);
 	wrmsr(MSR_STAR, guest_msrs[IDX_MSR_STAR]);
 	wrmsr(MSR_SF_MASK, guest_msrs[IDX_MSR_SF_MASK]);
 	wrmsr(MSR_KGSBASE, guest_msrs[IDX_MSR_KGSBASE]);
-#endif
 }
 
 void
 vmx_msr_guest_exit(struct vmx *vmx, int vcpuid)
 {
-#ifdef	__FreeBSD__
 	uint64_t *guest_msrs = vmx->guest_msrs[vcpuid];
+#ifndef __FreeBSD__
+	uint64_t *host_msrs = vmx->host_msrs[vcpuid];
+#endif
 
 	/* Save guest MSRs */
 	guest_msrs[IDX_MSR_LSTAR] = rdmsr(MSR_LSTAR);
@@ -362,13 +404,16 @@ vmx_msr_guest_exit(struct vmx *vmx, int vcpuid)
 	wrmsr(MSR_SF_MASK, host_msrs[IDX_MSR_SF_MASK]);
 
 	/* MSR_KGSBASE will be restored on the way back to userspace */
-#endif
 }
 
 int
 vmx_rdmsr(struct vmx *vmx, int vcpuid, u_int num, uint64_t *val, bool *retu)
 {
-	int error = 0;
+	const uint64_t *guest_msrs;
+	int error;
+
+	guest_msrs = vmx->guest_msrs[vcpuid];
+	error = 0;
 
 	switch (num) {
 	case MSR_MCG_CAP:
@@ -392,6 +437,9 @@ vmx_rdmsr(struct vmx *vmx, int vcpuid, u_int num, uint64_t *val, bool *retu)
 	case MSR_TURBO_RATIO_LIMIT1:
 		*val = turbo_ratio_limit;
 		break;
+	case MSR_PAT:
+		*val = guest_msrs[IDX_MSR_PAT];
+		break;
 	default:
 		error = EINVAL;
 		break;
@@ -402,10 +450,13 @@ vmx_rdmsr(struct vmx *vmx, int vcpuid, u_int num, uint64_t *val, bool *retu)
 int
 vmx_wrmsr(struct vmx *vmx, int vcpuid, u_int num, uint64_t val, bool *retu)
 {
+	uint64_t *guest_msrs;
 	uint64_t changed;
 	int error;
 	
+	guest_msrs = vmx->guest_msrs[vcpuid];
 	error = 0;
+
 	switch (num) {
 	case MSR_MCG_CAP:
 	case MSR_MCG_STATUS:
@@ -438,6 +489,12 @@ vmx_wrmsr(struct vmx *vmx, int vcpuid, u_int num, uint64_t val, bool *retu)
 			error = EINVAL;
 
 		break;
+	case MSR_PAT:
+		if (pat_valid(val))
+			guest_msrs[IDX_MSR_PAT] = val;
+		else
+			vm_inject_gp(vmx->vm, vcpuid);
+		break;
 	case MSR_TSC:
 		error = vmx_set_tsc_offset(vmx, vcpuid, val - rdtsc());
 		break;
diff --git a/usr/src/uts/i86pc/io/vmm/intel/vmx_msr.h b/usr/src/uts/i86pc/io/vmm/intel/vmx_msr.h
index 5300d14d9b..e77881c75e 100644
--- a/usr/src/uts/i86pc/io/vmm/intel/vmx_msr.h
+++ b/usr/src/uts/i86pc/io/vmm/intel/vmx_msr.h
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/intel/vmx_msr.h 271888 2014-09-20 02:35:21Z neel $
+ * $FreeBSD$
  */
 
 #ifndef _VMX_MSR_H_
diff --git a/usr/src/uts/i86pc/io/vmm/intel/vmx_support.s b/usr/src/uts/i86pc/io/vmm/intel/vmx_support.s
index 88e11c8137..8efa3ed172 100644
--- a/usr/src/uts/i86pc/io/vmm/intel/vmx_support.s
+++ b/usr/src/uts/i86pc/io/vmm/intel/vmx_support.s
@@ -1,5 +1,6 @@
 /*-
  * Copyright (c) 2011 NetApp, Inc.
+ * Copyright (c) 2013 Neel Natu <neel@freebsd.org>
  * All rights reserved.
  *
  * Redistribution and use in source and binary forms, with or without
@@ -23,7 +24,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/intel/vmx_support.S 245678 2013-01-20 03:42:49Z neel $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -39,78 +40,38 @@
  * Copyright 2017 Joyent, Inc.
  */
 
-#include <machine/asmacros.h>
+#include <sys/asm_linkage.h>
+#include <sys/segments.h>
 
-#if defined(lint)
+/* Porting note: This is named 'vmx_support.S' upstream. */
 
-struct vmxctx;
 
-/*ARGSUSED*/
-int
-vmx_setjmp(struct vmxctx *ctx)
-{ return (0); }
 
-/*ARGSUSED*/
-void
-vmx_return(struct vmxctx *ctxp, int retval)
-{}
+#if defined(lint)
 
-void
-vmx_longjmp(void)
-{}
+struct vmxctx;
+struct vmx;
 
 /*ARGSUSED*/
 void
 vmx_launch(struct vmxctx *ctx)
 {}
 
-/*ARGSUSED*/
 void
-vmx_resume(struct vmxctx *ctx)
+vmx_exit_guest()
 {}
 
-#else /* lint */
-
-#include "vmx_assym.s"
+/*ARGSUSED*/
+int
+vmx_enter_guest(struct vmxctx *ctx, struct vmx *vmx, int launched)
+{
+	return (0);
+}
 
-/*
- * Disable interrupts before updating %rsp in VMX_CHECK_AST or
- * VMX_GUEST_RESTORE.
- *
- * The location that %rsp points to is a 'vmxctx' and not a
- * real stack so we don't want an interrupt handler to trash it
- */
-#define	VMX_DISABLE_INTERRUPTS		cli
+#else /* lint */
 
-/*
- * If the thread hosting the vcpu has an ast pending then take care of it
- * by returning from vmx_setjmp() with a return value of VMX_RETURN_AST.
- *
- * Assumes that %rdi holds a pointer to the 'vmxctx' and that interrupts
- * are disabled.
- */
-#ifdef	__FreeBSD__
-#define	VMX_CHECK_AST							\
-	movq	PCPU(CURTHREAD),%rax;					\
-	testl	$TDF_ASTPENDING | TDF_NEEDRESCHED,TD_FLAGS(%rax);	\
-	je	9f;							\
-	movq	$VMX_RETURN_AST,%rsi;					\
-	movq	%rdi,%rsp;						\
-	addq	$VMXCTX_TMPSTKTOP,%rsp;					\
-	callq	vmx_return;						\
-9:
-#else
-#define	VMX_CHECK_AST							\
-	movq	%gs:CPU_THREAD,%rax;					\
-	movl	T_ASTFLAG(%rax),%eax;					\
-	test	%al,%al;						\
-	je	9f;							\
-	movq	$VMX_RETURN_AST,%rsi;					\
-	movq	%rdi,%rsp;						\
-	addq	$VMXCTX_TMPSTKTOP,%rsp;					\
-	callq	vmx_return;						\
-9:
-#endif
+#include "vmx_assym.h"
+#include "vmcs.h"
 
 /*
  * Assumes that %rdi holds a pointer to the 'vmxctx'.
@@ -123,7 +84,6 @@ vmx_resume(struct vmxctx *ctx)
  * host context in case of an error with 'vmlaunch' or 'vmresume'.
  */
 #define	VMX_GUEST_RESTORE						\
-	movq	%rdi,%rsp;						\
 	movq	VMXCTX_GUEST_CR2(%rdi),%rsi;				\
 	movq	%rsi,%cr2;						\
 	movq	VMXCTX_GUEST_RSI(%rdi),%rsi;				\
@@ -142,163 +102,205 @@ vmx_resume(struct vmxctx *ctx)
 	movq	VMXCTX_GUEST_R15(%rdi),%r15;				\
 	movq	VMXCTX_GUEST_RDI(%rdi),%rdi; /* restore rdi the last */
 
-#define	VM_INSTRUCTION_ERROR(reg)					\
-	jnc 	1f;							\
-	movl 	$VM_FAIL_INVALID,reg;		/* CF is set */		\
-	jmp 	3f;							\
-1:	jnz 	2f;							\
-	movl 	$VM_FAIL_VALID,reg;		/* ZF is set */		\
-	jmp 	3f;							\
-2:	movl 	$VM_SUCCESS,reg;					\
-3:	movl	reg,VMXCTX_LAUNCH_ERROR(%rsp)
-
-	.text
+/* Stack layout (offset from %rsp) for vmx_enter_guest */
+#define	VMXSTK_TMPRDI	0x00	/* temp store %rdi on vmexit		*/
+#define	VMXSTK_R15	0x08	/* callee saved %r15			*/
+#define	VMXSTK_R14	0x10	/* callee saved %r14			*/
+#define	VMXSTK_R13	0x18	/* callee saved %r13			*/
+#define	VMXSTK_R12	0x20	/* callee saved %r12			*/
+#define	VMXSTK_RBX	0x28	/* callee saved %rbx			*/
+#define	VMXSTK_RDX	0x30	/* save-args %rdx (int launched)	*/
+#define	VMXSTK_RSI	0x38	/* save-args %rsi (struct vmx *vmx)	*/
+#define	VMXSTK_RDI	0x40	/* save-args %rdi (struct vmxctx *ctx)	*/
+#define	VMXSTK_FP	0x48	/* frame pointer %rbp			*/
+#define	VMXSTKSIZE	VMXSTK_FP
+
 /*
- * int vmx_setjmp(ctxp)
- * %rdi = ctxp
- *
- * Return value is '0' when it returns directly from here.
- * Return value is '1' when it returns after a vm exit through vmx_longjmp.
+ * vmx_enter_guest(struct vmxctx *vmxctx, int launched)
+ * Interrupts must be disabled on entry.
  */
-ENTRY(vmx_setjmp)
-	movq	(%rsp),%rax			/* return address */
-	movq    %r15,VMXCTX_HOST_R15(%rdi)
-	movq    %r14,VMXCTX_HOST_R14(%rdi)
-	movq    %r13,VMXCTX_HOST_R13(%rdi)
-	movq    %r12,VMXCTX_HOST_R12(%rdi)
-	movq    %rbp,VMXCTX_HOST_RBP(%rdi)
-	movq    %rsp,VMXCTX_HOST_RSP(%rdi)
-	movq    %rbx,VMXCTX_HOST_RBX(%rdi)
-	movq    %rax,VMXCTX_HOST_RIP(%rdi)
+ENTRY_NP(vmx_enter_guest)
+	pushq	%rbp
+	movq	%rsp, %rbp
+	subq	$VMXSTKSIZE, %rsp
+	movq	%r15, VMXSTK_R15(%rsp)
+	movq	%r14, VMXSTK_R14(%rsp)
+	movq	%r13, VMXSTK_R13(%rsp)
+	movq	%r12, VMXSTK_R12(%rsp)
+	movq	%rbx, VMXSTK_RBX(%rsp)
+	movq	%rdx, VMXSTK_RDX(%rsp)
+	movq	%rsi, VMXSTK_RSI(%rsp)
+	movq	%rdi, VMXSTK_RDI(%rsp)
+
+	movq	%rdi, %r12	/* vmxctx */
+	movq	%rsi, %r13	/* vmx */
+	movl	%edx, %r14d	/* launch state */
+	movq	VMXCTX_PMAP(%rdi), %rbx
+
+	/* Activate guest pmap on this cpu. */
+	leaq	PM_ACTIVE(%rbx), %rdi
+	movl	%gs:CPU_ID, %esi
+	call	cpuset_atomic_add
+	movq	%r12, %rdi
 
 	/*
-	 * XXX save host debug registers
+	 * If 'vmx->eptgen[curcpu]' is not identical to 'pmap->pm_eptgen'
+	 * then we must invalidate all mappings associated with this EPTP.
 	 */
-	movl	$VMX_RETURN_DIRECT,%eax
-	ret
-END(vmx_setjmp)
-
-/*
- * void vmx_return(struct vmxctx *ctxp, int retval)
- * %rdi = ctxp
- * %rsi = retval
- * Return to vmm context through vmx_setjmp() with a value of 'retval'.
- */
-ENTRY(vmx_return)
-	/* Restore host context. */
-	movq	VMXCTX_HOST_R15(%rdi),%r15
-	movq	VMXCTX_HOST_R14(%rdi),%r14
-	movq	VMXCTX_HOST_R13(%rdi),%r13
-	movq	VMXCTX_HOST_R12(%rdi),%r12
-	movq	VMXCTX_HOST_RBP(%rdi),%rbp
-	movq	VMXCTX_HOST_RSP(%rdi),%rsp
-	movq	VMXCTX_HOST_RBX(%rdi),%rbx
-	movq	VMXCTX_HOST_RIP(%rdi),%rax
-	movq	%rax,(%rsp)			/* return address */
+	movq	PM_EPTGEN(%rbx), %r10
+	movl	%gs:CPU_ID, %eax
+	cmpq	%r10, VMX_EPTGEN(%r13, %rax, 8)
+	je	guest_restore
+
+	/* Refresh 'vmx->eptgen[curcpu]' */
+	movq	%r10, VMX_EPTGEN(%r13, %rax, 8)
+
+	/* Setup the invept descriptor on the host stack */
+	pushq	$0x0
+	pushq	VMX_EPTP(%r13)
+	movl	$0x1, %eax	/* Single context invalidate */
+	invept	(%rsp), %rax
+	leaq	0x10(%rsp), %rsp
+	jbe	invept_error		/* Check invept instruction error */
+
+guest_restore:
+	/* Write the current %rsp into the VMCS to be restored on vmexit */
+	movl	$VMCS_HOST_RSP, %eax
+	vmwrite	%rsp, %rax
+	jbe	vmwrite_error
+
+	/* Check if vmresume is adequate or a full vmlaunch is required */
+	cmpl	$0, %r14d
+	je	do_launch
 
+	VMX_GUEST_RESTORE
+	vmresume
 	/*
-	 * XXX restore host debug registers
+	 * In the common case, 'vmresume' returns back to the host through
+	 * 'vmx_exit_guest'. If there is an error we return VMX_VMRESUME_ERROR
+	 * to the caller.
 	 */
-	movl	%esi,%eax
-	ret
-END(vmx_return)
+	leaq	VMXSTK_FP(%rsp), %rbp
+	movq	VMXSTK_RDI(%rsp), %rdi
+	movl	$VMX_VMRESUME_ERROR, %eax
+	jmp	decode_inst_error
 
-/*
- * void vmx_longjmp(void)
- * %rsp points to the struct vmxctx
- */
-ENTRY(vmx_longjmp)
+do_launch:
+	VMX_GUEST_RESTORE
+	vmlaunch
 	/*
-	 * Save guest state that is not automatically saved in the vmcs.
+	 * In the common case, 'vmlaunch' returns back to the host through
+	 * 'vmx_exit_guest'. If there is an error we return VMX_VMLAUNCH_ERROR
+	 * to the caller.
 	 */
-	movq	%rdi,VMXCTX_GUEST_RDI(%rsp)
-	movq	%rsi,VMXCTX_GUEST_RSI(%rsp)
-	movq	%rdx,VMXCTX_GUEST_RDX(%rsp)
-	movq	%rcx,VMXCTX_GUEST_RCX(%rsp)
-	movq	%r8,VMXCTX_GUEST_R8(%rsp)
-	movq	%r9,VMXCTX_GUEST_R9(%rsp)
-	movq	%rax,VMXCTX_GUEST_RAX(%rsp)
-	movq	%rbx,VMXCTX_GUEST_RBX(%rsp)
-	movq	%rbp,VMXCTX_GUEST_RBP(%rsp)
-	movq	%r10,VMXCTX_GUEST_R10(%rsp)
-	movq	%r11,VMXCTX_GUEST_R11(%rsp)
-	movq	%r12,VMXCTX_GUEST_R12(%rsp)
-	movq	%r13,VMXCTX_GUEST_R13(%rsp)
-	movq	%r14,VMXCTX_GUEST_R14(%rsp)
-	movq	%r15,VMXCTX_GUEST_R15(%rsp)
-
-	movq	%cr2,%rdi
-	movq	%rdi,VMXCTX_GUEST_CR2(%rsp)
-
-	movq	%rsp,%rdi
-	movq	$VMX_RETURN_LONGJMP,%rsi
-
-	addq	$VMXCTX_TMPSTKTOP,%rsp
-	callq	vmx_return
-END(vmx_longjmp)
+	leaq	VMXSTK_FP(%rsp), %rbp
+	movq	VMXSTK_RDI(%rsp), %rdi
+	movl	$VMX_VMLAUNCH_ERROR, %eax
+	jmp	decode_inst_error
+
+vmwrite_error:
+	movl	$VMX_VMWRITE_ERROR, %eax
+	jmp	decode_inst_error
+invept_error:
+	movl	$VMX_INVEPT_ERROR, %eax
+	jmp	decode_inst_error
+decode_inst_error:
+	movl	$VM_FAIL_VALID, %r11d
+	jz	inst_error
+	movl	$VM_FAIL_INVALID, %r11d
+inst_error:
+	movl	%r11d, VMXCTX_INST_FAIL_STATUS(%rdi)
+
+	movq	VMXCTX_PMAP(%rdi), %rdi
+	leaq	PM_ACTIVE(%rdi), %rdi
+	movl	%gs:CPU_ID, %esi
+	movq	%rax, %r12
+	call	cpuset_atomic_del
+	movq	%r12, %rax
+
+	movq	VMXSTK_RBX(%rsp), %rbx
+	movq	VMXSTK_R12(%rsp), %r12
+	movq	VMXSTK_R13(%rsp), %r13
+	movq	VMXSTK_R14(%rsp), %r14
+	movq	VMXSTK_R15(%rsp), %r15
+	addq	$VMXSTKSIZE, %rsp
+	popq	%rbp
+	ret
 
 /*
- * void vmx_resume(struct vmxctx *ctxp)
- * %rdi = ctxp
- *
- * Although the return type is a 'void' this function may return indirectly
- * through vmx_setjmp() with a return value of 2.
+ * Non-error VM-exit from the guest. Make this a label so it can
+ * be used by C code when setting up the VMCS.
+ * The VMCS-restored %rsp points to the struct vmxctx
  */
-ENTRY(vmx_resume)
-	VMX_DISABLE_INTERRUPTS
-
-	VMX_CHECK_AST
-
+.align	ASM_ENTRY_ALIGN;
+ALTENTRY(vmx_exit_guest)
 	/*
-	 * Restore guest state that is not automatically loaded from the vmcs.
+	 * Save guest state that is not automatically saved in the vmcs.
 	 */
-	VMX_GUEST_RESTORE
-
-	vmresume
+	movq	%rdi, VMXSTK_TMPRDI(%rsp)
+	movq	VMXSTK_RDI(%rsp), %rdi
+	movq	%rbp, VMXCTX_GUEST_RBP(%rdi)
+	leaq	VMXSTK_FP(%rsp), %rbp
+
+	movq	%rsi, VMXCTX_GUEST_RSI(%rdi)
+	movq	%rdx, VMXCTX_GUEST_RDX(%rdi)
+	movq	%rcx, VMXCTX_GUEST_RCX(%rdi)
+	movq	%r8, VMXCTX_GUEST_R8(%rdi)
+	movq	%r9, VMXCTX_GUEST_R9(%rdi)
+	movq	%rax, VMXCTX_GUEST_RAX(%rdi)
+	movq	%rbx, VMXCTX_GUEST_RBX(%rdi)
+	movq	%r10, VMXCTX_GUEST_R10(%rdi)
+	movq	%r11, VMXCTX_GUEST_R11(%rdi)
+	movq	%r12, VMXCTX_GUEST_R12(%rdi)
+	movq	%r13, VMXCTX_GUEST_R13(%rdi)
+	movq	%r14, VMXCTX_GUEST_R14(%rdi)
+	movq	%r15, VMXCTX_GUEST_R15(%rdi)
+
+	movq	%cr2, %rbx
+	movq	%rbx, VMXCTX_GUEST_CR2(%rdi)
+	movq	VMXSTK_TMPRDI(%rsp), %rdx
+	movq	%rdx, VMXCTX_GUEST_RDI(%rdi)
+
+	/* Deactivate guest pmap on this cpu. */
+	movq	VMXCTX_PMAP(%rdi), %rdi
+	leaq	PM_ACTIVE(%rdi), %rdi
+	movl	%gs:CPU_ID, %esi
+	call	cpuset_atomic_del
 
 	/*
-	 * Capture the reason why vmresume failed.
+	 * This will return to the caller of 'vmx_enter_guest()' with a return
+	 * value of VMX_GUEST_VMEXIT.
 	 */
-	VM_INSTRUCTION_ERROR(%eax)
-
-	/* Return via vmx_setjmp with return value of VMX_RETURN_VMRESUME */
-	movq	%rsp,%rdi
-	movq	$VMX_RETURN_VMRESUME,%rsi
-
-	addq	$VMXCTX_TMPSTKTOP,%rsp
-	callq	vmx_return
-END(vmx_resume)
+	movl	$VMX_GUEST_VMEXIT, %eax
+	movq	VMXSTK_RBX(%rsp), %rbx
+	movq	VMXSTK_R12(%rsp), %r12
+	movq	VMXSTK_R13(%rsp), %r13
+	movq	VMXSTK_R14(%rsp), %r14
+	movq	VMXSTK_R15(%rsp), %r15
+	addq	$VMXSTKSIZE, %rsp
+	popq	%rbp
+	ret
+SET_SIZE(vmx_enter_guest)
 
 /*
- * void vmx_launch(struct vmxctx *ctxp)
- * %rdi = ctxp
+ * %rdi = interrupt handler entry point
  *
- * Although the return type is a 'void' this function may return indirectly
- * through vmx_setjmp() with a return value of 3.
+ * Calling sequence described in the "Instruction Set Reference" for the "INT"
+ * instruction in Intel SDM, Vol 2.
  */
-ENTRY(vmx_launch)
-	VMX_DISABLE_INTERRUPTS
-
-	VMX_CHECK_AST
-
-	/*
-	 * Restore guest state that is not automatically loaded from the vmcs.
-	 */
-	VMX_GUEST_RESTORE
-
-	vmlaunch
-
-	/*
-	 * Capture the reason why vmlaunch failed.
-	 */
-	VM_INSTRUCTION_ERROR(%eax)
-
-	/* Return via vmx_setjmp with return value of VMX_RETURN_VMLAUNCH */
-	movq	%rsp,%rdi
-	movq	$VMX_RETURN_VMLAUNCH,%rsi
-
-	addq	$VMXCTX_TMPSTKTOP,%rsp
-	callq	vmx_return
-END(vmx_launch)
+ENTRY_NP(vmx_call_isr)
+	pushq	%rbp
+	movq	%rsp, %rbp
+	movq	%rsp, %r11
+	andq	$~0xf, %rsp	/* align stack */
+	pushq	$KDS_SEL	/* %ss */
+	pushq	%r11		/* %rsp */
+	pushfq			/* %rflags */
+	pushq	$KCS_SEL	/* %cs */
+	cli
+	call	*%rdi		/* %rip (and call) */
+	popq	%rbp
+	ret
+SET_SIZE(vmx_call_isr)
 
 #endif /* lint */
diff --git a/usr/src/uts/i86pc/io/vmm/intel/vtd.c b/usr/src/uts/i86pc/io/vmm/intel/vtd.c
new file mode 100644
index 0000000000..f3b7a98a9d
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/intel/vtd.c
@@ -0,0 +1,688 @@
+/*-
+ * Copyright (c) 2011 NetApp, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY NETAPP, INC ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL NETAPP, INC OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/param.h>
+#include <sys/kernel.h>
+#include <sys/systm.h>
+#include <sys/malloc.h>
+
+#include <vm/vm.h>
+#include <vm/pmap.h>
+
+#include <dev/pci/pcireg.h>
+
+#include <machine/vmparam.h>
+#include <contrib/dev/acpica/include/acpi.h>
+
+#include "io/iommu.h"
+
+/*
+ * Documented in the "Intel Virtualization Technology for Directed I/O",
+ * Architecture Spec, September 2008.
+ */
+
+/* Section 10.4 "Register Descriptions" */
+struct vtdmap {
+	volatile uint32_t	version;
+	volatile uint32_t	res0;
+	volatile uint64_t	cap;
+	volatile uint64_t	ext_cap;
+	volatile uint32_t	gcr;
+	volatile uint32_t	gsr;
+	volatile uint64_t	rta;
+	volatile uint64_t	ccr;
+};
+
+#define	VTD_CAP_SAGAW(cap)	(((cap) >> 8) & 0x1F)
+#define	VTD_CAP_ND(cap)		((cap) & 0x7)
+#define	VTD_CAP_CM(cap)		(((cap) >> 7) & 0x1)
+#define	VTD_CAP_SPS(cap)	(((cap) >> 34) & 0xF)
+#define	VTD_CAP_RWBF(cap)	(((cap) >> 4) & 0x1)
+
+#define	VTD_ECAP_DI(ecap)	(((ecap) >> 2) & 0x1)
+#define	VTD_ECAP_COHERENCY(ecap) ((ecap) & 0x1)
+#define	VTD_ECAP_IRO(ecap)	(((ecap) >> 8) & 0x3FF)
+
+#define	VTD_GCR_WBF		(1 << 27)
+#define	VTD_GCR_SRTP		(1 << 30)
+#define	VTD_GCR_TE		(1U << 31)
+
+#define	VTD_GSR_WBFS		(1 << 27)
+#define	VTD_GSR_RTPS		(1 << 30)
+#define	VTD_GSR_TES		(1U << 31)
+
+#define	VTD_CCR_ICC		(1UL << 63)	/* invalidate context cache */
+#define	VTD_CCR_CIRG_GLOBAL	(1UL << 61)	/* global invalidation */
+
+#define	VTD_IIR_IVT		(1UL << 63)	/* invalidation IOTLB */
+#define	VTD_IIR_IIRG_GLOBAL	(1ULL << 60)	/* global IOTLB invalidation */
+#define	VTD_IIR_IIRG_DOMAIN	(2ULL << 60)	/* domain IOTLB invalidation */
+#define	VTD_IIR_IIRG_PAGE	(3ULL << 60)	/* page IOTLB invalidation */
+#define	VTD_IIR_DRAIN_READS	(1ULL << 49)	/* drain pending DMA reads */
+#define	VTD_IIR_DRAIN_WRITES	(1ULL << 48)	/* drain pending DMA writes */
+#define	VTD_IIR_DOMAIN_P	32
+
+#define	VTD_ROOT_PRESENT	0x1
+#define	VTD_CTX_PRESENT		0x1
+#define	VTD_CTX_TT_ALL		(1UL << 2)
+
+#define	VTD_PTE_RD		(1UL << 0)
+#define	VTD_PTE_WR		(1UL << 1)
+#define	VTD_PTE_SUPERPAGE	(1UL << 7)
+#define	VTD_PTE_ADDR_M		(0x000FFFFFFFFFF000UL)
+
+#define VTD_RID2IDX(rid)	(((rid) & 0xff) * 2)
+
+struct domain {
+	uint64_t	*ptp;		/* first level page table page */
+	int		pt_levels;	/* number of page table levels */
+	int		addrwidth;	/* 'AW' field in context entry */
+	int		spsmask;	/* supported super page sizes */
+	u_int		id;		/* domain id */
+	vm_paddr_t	maxaddr;	/* highest address to be mapped */
+	SLIST_ENTRY(domain) next;
+};
+
+static SLIST_HEAD(, domain) domhead;
+
+#define	DRHD_MAX_UNITS	8
+static int		drhd_num;
+static struct vtdmap	*vtdmaps[DRHD_MAX_UNITS];
+static int		max_domains;
+typedef int		(*drhd_ident_func_t)(void);
+
+static uint64_t root_table[PAGE_SIZE / sizeof(uint64_t)] __aligned(4096);
+static uint64_t ctx_tables[256][PAGE_SIZE / sizeof(uint64_t)] __aligned(4096);
+
+static MALLOC_DEFINE(M_VTD, "vtd", "vtd");
+
+static int
+vtd_max_domains(struct vtdmap *vtdmap)
+{
+	int nd;
+
+	nd = VTD_CAP_ND(vtdmap->cap);
+
+	switch (nd) {
+	case 0:
+		return (16);
+	case 1:
+		return (64);
+	case 2:
+		return (256);
+	case 3:
+		return (1024);
+	case 4:
+		return (4 * 1024);
+	case 5:
+		return (16 * 1024);
+	case 6:
+		return (64 * 1024);
+	default:
+		panic("vtd_max_domains: invalid value of nd (0x%0x)", nd);
+	}
+}
+
+static u_int
+domain_id(void)
+{
+	u_int id;
+	struct domain *dom;
+
+	/* Skip domain id 0 - it is reserved when Caching Mode field is set */
+	for (id = 1; id < max_domains; id++) {
+		SLIST_FOREACH(dom, &domhead, next) {
+			if (dom->id == id)
+				break;
+		}
+		if (dom == NULL)
+			break;		/* found it */
+	}
+	
+	if (id >= max_domains)
+		panic("domain ids exhausted");
+
+	return (id);
+}
+
+static void
+vtd_wbflush(struct vtdmap *vtdmap)
+{
+
+	if (VTD_ECAP_COHERENCY(vtdmap->ext_cap) == 0)
+		pmap_invalidate_cache();
+
+	if (VTD_CAP_RWBF(vtdmap->cap)) {
+		vtdmap->gcr = VTD_GCR_WBF;
+		while ((vtdmap->gsr & VTD_GSR_WBFS) != 0)
+			;
+	}
+}
+
+static void
+vtd_ctx_global_invalidate(struct vtdmap *vtdmap)
+{
+
+	vtdmap->ccr = VTD_CCR_ICC | VTD_CCR_CIRG_GLOBAL;
+	while ((vtdmap->ccr & VTD_CCR_ICC) != 0)
+		;
+}
+
+static void
+vtd_iotlb_global_invalidate(struct vtdmap *vtdmap)
+{
+	int offset;
+	volatile uint64_t *iotlb_reg, val;
+
+	vtd_wbflush(vtdmap);
+
+	offset = VTD_ECAP_IRO(vtdmap->ext_cap) * 16;
+	iotlb_reg = (volatile uint64_t *)((caddr_t)vtdmap + offset + 8);
+	
+	*iotlb_reg =  VTD_IIR_IVT | VTD_IIR_IIRG_GLOBAL |
+		      VTD_IIR_DRAIN_READS | VTD_IIR_DRAIN_WRITES;
+
+	while (1) {
+		val = *iotlb_reg;
+		if ((val & VTD_IIR_IVT) == 0)
+			break;
+	}
+}
+
+static void
+vtd_translation_enable(struct vtdmap *vtdmap)
+{
+
+	vtdmap->gcr = VTD_GCR_TE;
+	while ((vtdmap->gsr & VTD_GSR_TES) == 0)
+		;
+}
+
+static void
+vtd_translation_disable(struct vtdmap *vtdmap)
+{
+
+	vtdmap->gcr = 0;
+	while ((vtdmap->gsr & VTD_GSR_TES) != 0)
+		;
+}
+
+static int
+vtd_init(void)
+{
+	int i, units, remaining;
+	struct vtdmap *vtdmap;
+	vm_paddr_t ctx_paddr;
+	char *end, envname[32];
+	unsigned long mapaddr;
+	ACPI_STATUS status;
+	ACPI_TABLE_DMAR *dmar;
+	ACPI_DMAR_HEADER *hdr;
+	ACPI_DMAR_HARDWARE_UNIT *drhd;
+
+	/*
+	 * Allow the user to override the ACPI DMAR table by specifying the
+	 * physical address of each remapping unit.
+	 *
+	 * The following example specifies two remapping units at
+	 * physical addresses 0xfed90000 and 0xfeda0000 respectively.
+	 * set vtd.regmap.0.addr=0xfed90000
+	 * set vtd.regmap.1.addr=0xfeda0000
+	 */
+	for (units = 0; units < DRHD_MAX_UNITS; units++) {
+		snprintf(envname, sizeof(envname), "vtd.regmap.%d.addr", units);
+		if (getenv_ulong(envname, &mapaddr) == 0)
+			break;
+		vtdmaps[units] = (struct vtdmap *)PHYS_TO_DMAP(mapaddr);
+	}
+
+	if (units > 0)
+		goto skip_dmar;
+
+	/* Search for DMAR table. */
+	status = AcpiGetTable(ACPI_SIG_DMAR, 0, (ACPI_TABLE_HEADER **)&dmar);
+	if (ACPI_FAILURE(status))
+		return (ENXIO);
+
+	end = (char *)dmar + dmar->Header.Length;
+	remaining = dmar->Header.Length - sizeof(ACPI_TABLE_DMAR);
+	while (remaining > sizeof(ACPI_DMAR_HEADER)) {
+		hdr = (ACPI_DMAR_HEADER *)(end - remaining);
+		if (hdr->Length > remaining)
+			break;
+		/*
+		 * From Intel VT-d arch spec, version 1.3:
+		 * BIOS implementations must report mapping structures
+		 * in numerical order, i.e. All remapping structures of
+		 * type 0 (DRHD) enumerated before remapping structures of
+		 * type 1 (RMRR) and so forth.
+		 */
+		if (hdr->Type != ACPI_DMAR_TYPE_HARDWARE_UNIT)
+			break;
+
+		drhd = (ACPI_DMAR_HARDWARE_UNIT *)hdr;
+		vtdmaps[units++] = (struct vtdmap *)PHYS_TO_DMAP(drhd->Address);
+		if (units >= DRHD_MAX_UNITS)
+			break;
+		remaining -= hdr->Length;
+	}
+
+	if (units <= 0)
+		return (ENXIO);
+
+skip_dmar:
+	drhd_num = units;
+	vtdmap = vtdmaps[0];
+
+	if (VTD_CAP_CM(vtdmap->cap) != 0)
+		panic("vtd_init: invalid caching mode");
+
+	max_domains = vtd_max_domains(vtdmap);
+
+	/*
+	 * Set up the root-table to point to the context-entry tables
+	 */
+	for (i = 0; i < 256; i++) {
+		ctx_paddr = vtophys(ctx_tables[i]);
+		if (ctx_paddr & PAGE_MASK)
+			panic("ctx table (0x%0lx) not page aligned", ctx_paddr);
+
+		root_table[i * 2] = ctx_paddr | VTD_ROOT_PRESENT;
+	}
+
+	return (0);
+}
+
+static void
+vtd_cleanup(void)
+{
+}
+
+static void
+vtd_enable(void)
+{
+	int i;
+	struct vtdmap *vtdmap;
+
+	for (i = 0; i < drhd_num; i++) {
+		vtdmap = vtdmaps[i];
+		vtd_wbflush(vtdmap);
+
+		/* Update the root table address */
+		vtdmap->rta = vtophys(root_table);
+		vtdmap->gcr = VTD_GCR_SRTP;
+		while ((vtdmap->gsr & VTD_GSR_RTPS) == 0)
+			;
+
+		vtd_ctx_global_invalidate(vtdmap);
+		vtd_iotlb_global_invalidate(vtdmap);
+
+		vtd_translation_enable(vtdmap);
+	}
+}
+
+static void
+vtd_disable(void)
+{
+	int i;
+	struct vtdmap *vtdmap;
+
+	for (i = 0; i < drhd_num; i++) {
+		vtdmap = vtdmaps[i];
+		vtd_translation_disable(vtdmap);
+	}
+}
+
+static void
+vtd_add_device(void *arg, uint16_t rid)
+{
+	int idx;
+	uint64_t *ctxp;
+	struct domain *dom = arg;
+	vm_paddr_t pt_paddr;
+	struct vtdmap *vtdmap;
+	uint8_t bus;
+
+	vtdmap = vtdmaps[0];
+	bus = PCI_RID2BUS(rid);
+	ctxp = ctx_tables[bus];
+	pt_paddr = vtophys(dom->ptp);
+	idx = VTD_RID2IDX(rid);
+
+	if (ctxp[idx] & VTD_CTX_PRESENT) {
+		panic("vtd_add_device: device %x is already owned by "
+		      "domain %d", rid,
+		      (uint16_t)(ctxp[idx + 1] >> 8));
+	}
+
+	/*
+	 * Order is important. The 'present' bit is set only after all fields
+	 * of the context pointer are initialized.
+	 */
+	ctxp[idx + 1] = dom->addrwidth | (dom->id << 8);
+
+	if (VTD_ECAP_DI(vtdmap->ext_cap))
+		ctxp[idx] = VTD_CTX_TT_ALL;
+	else
+		ctxp[idx] = 0;
+
+	ctxp[idx] |= pt_paddr | VTD_CTX_PRESENT;
+
+	/*
+	 * 'Not Present' entries are not cached in either the Context Cache
+	 * or in the IOTLB, so there is no need to invalidate either of them.
+	 */
+}
+
+static void
+vtd_remove_device(void *arg, uint16_t rid)
+{
+	int i, idx;
+	uint64_t *ctxp;
+	struct vtdmap *vtdmap;
+	uint8_t bus;
+
+	bus = PCI_RID2BUS(rid);
+	ctxp = ctx_tables[bus];
+	idx = VTD_RID2IDX(rid);
+
+	/*
+	 * Order is important. The 'present' bit is must be cleared first.
+	 */
+	ctxp[idx] = 0;
+	ctxp[idx + 1] = 0;
+
+	/*
+	 * Invalidate the Context Cache and the IOTLB.
+	 *
+	 * XXX use device-selective invalidation for Context Cache
+	 * XXX use domain-selective invalidation for IOTLB
+	 */
+	for (i = 0; i < drhd_num; i++) {
+		vtdmap = vtdmaps[i];
+		vtd_ctx_global_invalidate(vtdmap);
+		vtd_iotlb_global_invalidate(vtdmap);
+	}
+}
+
+#define	CREATE_MAPPING	0
+#define	REMOVE_MAPPING	1
+
+static uint64_t
+vtd_update_mapping(void *arg, vm_paddr_t gpa, vm_paddr_t hpa, uint64_t len,
+		   int remove)
+{
+	struct domain *dom;
+	int i, spshift, ptpshift, ptpindex, nlevels;
+	uint64_t spsize, *ptp;
+
+	dom = arg;
+	ptpindex = 0;
+	ptpshift = 0;
+
+	KASSERT(gpa + len > gpa, ("%s: invalid gpa range %#lx/%#lx", __func__,
+	    gpa, len));
+	KASSERT(gpa + len <= dom->maxaddr, ("%s: gpa range %#lx/%#lx beyond "
+	    "domain maxaddr %#lx", __func__, gpa, len, dom->maxaddr));
+
+	if (gpa & PAGE_MASK)
+		panic("vtd_create_mapping: unaligned gpa 0x%0lx", gpa);
+
+	if (hpa & PAGE_MASK)
+		panic("vtd_create_mapping: unaligned hpa 0x%0lx", hpa);
+
+	if (len & PAGE_MASK)
+		panic("vtd_create_mapping: unaligned len 0x%0lx", len);
+
+	/*
+	 * Compute the size of the mapping that we can accommodate.
+	 *
+	 * This is based on three factors:
+	 * - supported super page size
+	 * - alignment of the region starting at 'gpa' and 'hpa'
+	 * - length of the region 'len'
+	 */
+	spshift = 48;
+	for (i = 3; i >= 0; i--) {
+		spsize = 1UL << spshift;
+		if ((dom->spsmask & (1 << i)) != 0 &&
+		    (gpa & (spsize - 1)) == 0 &&
+		    (hpa & (spsize - 1)) == 0 &&
+		    (len >= spsize)) {
+			break;
+		}
+		spshift -= 9;
+	}
+
+	ptp = dom->ptp;
+	nlevels = dom->pt_levels;
+	while (--nlevels >= 0) {
+		ptpshift = 12 + nlevels * 9;
+		ptpindex = (gpa >> ptpshift) & 0x1FF;
+
+		/* We have reached the leaf mapping */
+		if (spshift >= ptpshift) {
+			break;
+		}
+
+		/*
+		 * We are working on a non-leaf page table page.
+		 *
+		 * Create a downstream page table page if necessary and point
+		 * to it from the current page table.
+		 */
+		if (ptp[ptpindex] == 0) {
+			void *nlp = malloc(PAGE_SIZE, M_VTD, M_WAITOK | M_ZERO);
+			ptp[ptpindex] = vtophys(nlp)| VTD_PTE_RD | VTD_PTE_WR;
+		}
+
+		ptp = (uint64_t *)PHYS_TO_DMAP(ptp[ptpindex] & VTD_PTE_ADDR_M);
+	}
+
+	if ((gpa & ((1UL << ptpshift) - 1)) != 0)
+		panic("gpa 0x%lx and ptpshift %d mismatch", gpa, ptpshift);
+
+	/*
+	 * Update the 'gpa' -> 'hpa' mapping
+	 */
+	if (remove) {
+		ptp[ptpindex] = 0;
+	} else {
+		ptp[ptpindex] = hpa | VTD_PTE_RD | VTD_PTE_WR;
+
+		if (nlevels > 0)
+			ptp[ptpindex] |= VTD_PTE_SUPERPAGE;
+	}
+
+	return (1UL << ptpshift);
+}
+
+static uint64_t
+vtd_create_mapping(void *arg, vm_paddr_t gpa, vm_paddr_t hpa, uint64_t len)
+{
+
+	return (vtd_update_mapping(arg, gpa, hpa, len, CREATE_MAPPING));
+}
+
+static uint64_t
+vtd_remove_mapping(void *arg, vm_paddr_t gpa, uint64_t len)
+{
+
+	return (vtd_update_mapping(arg, gpa, 0, len, REMOVE_MAPPING));
+}
+
+static void
+vtd_invalidate_tlb(void *dom)
+{
+	int i;
+	struct vtdmap *vtdmap;
+
+	/*
+	 * Invalidate the IOTLB.
+	 * XXX use domain-selective invalidation for IOTLB
+	 */
+	for (i = 0; i < drhd_num; i++) {
+		vtdmap = vtdmaps[i];
+		vtd_iotlb_global_invalidate(vtdmap);
+	}
+}
+
+static void *
+vtd_create_domain(vm_paddr_t maxaddr)
+{
+	struct domain *dom;
+	vm_paddr_t addr;
+	int tmp, i, gaw, agaw, sagaw, res, pt_levels, addrwidth;
+	struct vtdmap *vtdmap;
+
+	if (drhd_num <= 0)
+		panic("vtd_create_domain: no dma remapping hardware available");
+
+	vtdmap = vtdmaps[0];
+
+	/*
+	 * Calculate AGAW.
+	 * Section 3.4.2 "Adjusted Guest Address Width", Architecture Spec.
+	 */
+	addr = 0;
+	for (gaw = 0; addr < maxaddr; gaw++)
+		addr = 1ULL << gaw;
+
+	res = (gaw - 12) % 9;
+	if (res == 0)
+		agaw = gaw;
+	else
+		agaw = gaw + 9 - res;
+
+	if (agaw > 64)
+		agaw = 64;
+
+	/*
+	 * Select the smallest Supported AGAW and the corresponding number
+	 * of page table levels.
+	 */
+	pt_levels = 2;
+	sagaw = 30;
+	addrwidth = 0;
+	tmp = VTD_CAP_SAGAW(vtdmap->cap);
+	for (i = 0; i < 5; i++) {
+		if ((tmp & (1 << i)) != 0 && sagaw >= agaw)
+			break;
+		pt_levels++;
+		addrwidth++;
+		sagaw += 9;
+		if (sagaw > 64)
+			sagaw = 64;
+	}
+
+	if (i >= 5) {
+		panic("vtd_create_domain: SAGAW 0x%lx does not support AGAW %d",
+		      VTD_CAP_SAGAW(vtdmap->cap), agaw);
+	}
+
+	dom = malloc(sizeof(struct domain), M_VTD, M_ZERO | M_WAITOK);
+	dom->pt_levels = pt_levels;
+	dom->addrwidth = addrwidth;
+	dom->id = domain_id();
+	dom->maxaddr = maxaddr;
+	dom->ptp = malloc(PAGE_SIZE, M_VTD, M_ZERO | M_WAITOK);
+	if ((uintptr_t)dom->ptp & PAGE_MASK)
+		panic("vtd_create_domain: ptp (%p) not page aligned", dom->ptp);
+
+#ifdef notyet
+	/*
+	 * XXX superpage mappings for the iommu do not work correctly.
+	 *
+	 * By default all physical memory is mapped into the host_domain.
+	 * When a VM is allocated wired memory the pages belonging to it
+	 * are removed from the host_domain and added to the vm's domain.
+	 *
+	 * If the page being removed was mapped using a superpage mapping
+	 * in the host_domain then we need to demote the mapping before
+	 * removing the page.
+	 *
+	 * There is not any code to deal with the demotion at the moment
+	 * so we disable superpage mappings altogether.
+	 */
+	dom->spsmask = VTD_CAP_SPS(vtdmap->cap);
+#endif
+
+	SLIST_INSERT_HEAD(&domhead, dom, next);
+
+	return (dom);
+}
+
+static void
+vtd_free_ptp(uint64_t *ptp, int level)
+{
+	int i;
+	uint64_t *nlp;
+
+	if (level > 1) {
+		for (i = 0; i < 512; i++) {
+			if ((ptp[i] & (VTD_PTE_RD | VTD_PTE_WR)) == 0)
+				continue;
+			if ((ptp[i] & VTD_PTE_SUPERPAGE) != 0)
+				continue;
+			nlp = (uint64_t *)PHYS_TO_DMAP(ptp[i] & VTD_PTE_ADDR_M);
+			vtd_free_ptp(nlp, level - 1);
+		}
+	}
+
+	bzero(ptp, PAGE_SIZE);
+	free(ptp, M_VTD);
+}
+
+static void
+vtd_destroy_domain(void *arg)
+{
+	struct domain *dom;
+	
+	dom = arg;
+
+	SLIST_REMOVE(&domhead, dom, domain, next);
+	vtd_free_ptp(dom->ptp, dom->pt_levels);
+	free(dom, M_VTD);
+}
+
+struct iommu_ops iommu_ops_intel = {
+	vtd_init,
+	vtd_cleanup,
+	vtd_enable,
+	vtd_disable,
+	vtd_create_domain,
+	vtd_destroy_domain,
+	vtd_create_mapping,
+	vtd_remove_mapping,
+	vtd_add_device,
+	vtd_remove_device,
+	vtd_invalidate_tlb,
+};
diff --git a/usr/src/uts/i86pc/io/vmm/io/iommu.h b/usr/src/uts/i86pc/io/vmm/io/iommu.h
new file mode 100644
index 0000000000..a941c779aa
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/io/iommu.h
@@ -0,0 +1,74 @@
+/*-
+ * Copyright (c) 2011 NetApp, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY NETAPP, INC ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL NETAPP, INC OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _IO_IOMMU_H_
+#define	_IO_IOMMU_H_
+
+typedef int (*iommu_init_func_t)(void);
+typedef void (*iommu_cleanup_func_t)(void);
+typedef void (*iommu_enable_func_t)(void);
+typedef void (*iommu_disable_func_t)(void);
+typedef void *(*iommu_create_domain_t)(vm_paddr_t maxaddr);
+typedef void (*iommu_destroy_domain_t)(void *domain);
+typedef uint64_t (*iommu_create_mapping_t)(void *domain, vm_paddr_t gpa,
+					   vm_paddr_t hpa, uint64_t len);
+typedef uint64_t (*iommu_remove_mapping_t)(void *domain, vm_paddr_t gpa,
+					   uint64_t len);
+typedef void (*iommu_add_device_t)(void *domain, uint16_t rid);
+typedef void (*iommu_remove_device_t)(void *dom, uint16_t rid);
+typedef void (*iommu_invalidate_tlb_t)(void *dom);
+
+struct iommu_ops {
+	iommu_init_func_t	init;		/* module wide */
+	iommu_cleanup_func_t	cleanup;
+	iommu_enable_func_t	enable;
+	iommu_disable_func_t	disable;
+
+	iommu_create_domain_t	create_domain;	/* domain-specific */
+	iommu_destroy_domain_t	destroy_domain;
+	iommu_create_mapping_t	create_mapping;
+	iommu_remove_mapping_t	remove_mapping;
+	iommu_add_device_t	add_device;
+	iommu_remove_device_t	remove_device;
+	iommu_invalidate_tlb_t	invalidate_tlb;
+};
+
+extern struct iommu_ops iommu_ops_intel;
+extern struct iommu_ops iommu_ops_amd;
+
+void	iommu_cleanup(void);
+void	*iommu_host_domain(void);
+void	*iommu_create_domain(vm_paddr_t maxaddr);
+void	iommu_destroy_domain(void *dom);
+void	iommu_create_mapping(void *dom, vm_paddr_t gpa, vm_paddr_t hpa,
+			     size_t len);
+void	iommu_remove_mapping(void *dom, vm_paddr_t gpa, size_t len);
+void	iommu_add_device(void *dom, uint16_t rid);
+void	iommu_remove_device(void *dom, uint16_t rid);
+void	iommu_invalidate_tlb(void *domain);
+#endif
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_ipi.h b/usr/src/uts/i86pc/io/vmm/io/ppt.h
similarity index 58%
rename from usr/src/uts/i86pc/io/vmm/vmm_ipi.h
rename to usr/src/uts/i86pc/io/vmm/io/ppt.h
index 4dff03ba1f..8078896095 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_ipi.h
+++ b/usr/src/uts/i86pc/io/vmm/io/ppt.h
@@ -23,15 +23,32 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/vmm_ipi.h 260466 2014-01-09 03:25:54Z neel $
+ * $FreeBSD$
  */
 
-#ifndef _VMM_IPI_H_
-#define _VMM_IPI_H_
+#ifndef _IO_PPT_H_
+#define	_IO_PPT_H_
 
-#ifdef	__FreeBSD__
-int	vmm_ipi_alloc(void);
-void	vmm_ipi_free(int num);
-#endif
+int	ppt_unassign_all(struct vm *vm);
+int	ppt_map_mmio(struct vm *vm, int bus, int slot, int func,
+		     vm_paddr_t gpa, size_t len, vm_paddr_t hpa);
+int	ppt_setup_msi(struct vm *vm, int vcpu, int bus, int slot, int func,
+		      uint64_t addr, uint64_t msg, int numvec);
+int	ppt_setup_msix(struct vm *vm, int vcpu, int bus, int slot, int func,
+		int idx, uint64_t addr, uint64_t msg, uint32_t vector_control);
+int	ppt_assigned_devices(struct vm *vm);
+boolean_t ppt_is_mmio(struct vm *vm, vm_paddr_t gpa);
+
+/*
+ * Returns the number of devices sequestered by the ppt driver for assignment
+ * to virtual machines.
+ */
+int	ppt_avail_devices(void);
 
+/*
+ * The following functions should never be called directly.
+ * Use 'vm_assign_pptdev()' and 'vm_unassign_pptdev()' instead.
+ */
+int	ppt_assign_device(struct vm *vm, int bus, int slot, int func);
+int	ppt_unassign_device(struct vm *vm, int bus, int slot, int func);
 #endif
diff --git a/usr/src/uts/i86pc/io/vmm/io/sol_iommu.c b/usr/src/uts/i86pc/io/vmm/io/sol_iommu.c
new file mode 100644
index 0000000000..989e88e17b
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/io/sol_iommu.c
@@ -0,0 +1,86 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2017 Joyent, Inc.
+ */
+
+#include <sys/types.h>
+#include <sys/param.h>
+#include <sys/cmn_err.h>
+
+/*
+ * IOMMU Stub
+ *
+ * Until proper iommu support can be wired into bhyve, stub out all the
+ * functions to either fail, if reasonable, or panic.
+ */
+
+void
+iommu_cleanup(void)
+{
+}
+
+void *
+iommu_host_domain(void)
+{
+	return (NULL);
+}
+
+/*ARGSUSED*/
+void *
+iommu_create_domain(vm_paddr_t maxaddr)
+{
+	return (NULL);
+}
+
+/*ARGSUSED*/
+void
+iommu_destroy_domain(void *dom)
+{
+	panic("unimplemented");
+}
+
+/*ARGSUSED*/
+void
+iommu_create_mapping(void *dom, vm_paddr_t gpa, vm_paddr_t hpa, size_t len)
+{
+	panic("unimplemented");
+}
+
+/*ARGSUSED*/
+void
+iommu_remove_mapping(void *dom, vm_paddr_t gpa, size_t len)
+{
+	panic("unimplemented");
+}
+
+/*ARGSUSED*/
+void
+iommu_add_device(void *dom, uint16_t rid)
+{
+	panic("unimplemented");
+}
+
+/*ARGSUSED*/
+void
+iommu_remove_device(void *dom, uint16_t rid)
+{
+	panic("unimplemented");
+}
+
+/*ARGSUSED*/
+void
+iommu_invalidate_tlb(void *domain)
+{
+	panic("unimplemented");
+}
+
diff --git a/usr/src/uts/i86pc/io/vmm/io/sol_ppt.c b/usr/src/uts/i86pc/io/vmm/io/sol_ppt.c
new file mode 100644
index 0000000000..9d5b1f5cdc
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/io/sol_ppt.c
@@ -0,0 +1,92 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2017 Joyent, Inc.
+ */
+
+#include <sys/types.h>
+#include <sys/errno.h>
+#include <sys/cmn_err.h>
+
+#include <sys/vmm.h>
+
+/*
+ * PCI Pass-Through Stub
+ *
+ * Until proper passthrough support can be wired into bhyve, stub out all the
+ * functions to either fail or no-op.
+ */
+
+int
+ppt_unassign_all(struct vm *vm)
+{
+	return (0);
+}
+
+/*ARGSUSED*/
+int
+ppt_map_mmio(struct vm *vm, int bus, int slot, int func, vm_paddr_t gpa,
+    size_t len, vm_paddr_t hpa)
+{
+	return (ENXIO);
+}
+
+/*ARGSUSED*/
+int
+ppt_setup_msi(struct vm *vm, int vcpu, int bus, int slot, int func,
+    uint64_t addr, uint64_t msg, int numvec)
+{
+	return (ENXIO);
+}
+
+/*ARGSUSED*/
+int
+ppt_setup_msix(struct vm *vm, int vcpu, int bus, int slot, int func, int idx,
+    uint64_t addr, uint64_t msg, uint32_t vector_control)
+{
+	return (ENXIO);
+}
+
+/*ARGSUSED*/
+int
+ppt_assigned_devices(struct vm *vm)
+{
+	return (0);
+}
+
+/*ARGSUSED*/
+boolean_t
+ppt_is_mmio(struct vm *vm, vm_paddr_t gpa)
+{
+	return (B_FALSE);
+}
+
+/*ARGSUSED*/
+int
+ppt_avail_devices(void)
+{
+	return (0);
+}
+
+/*ARGSUSED*/
+int
+ppt_assign_device(struct vm *vm, int bus, int slot, int func)
+{
+	return (ENOENT);
+}
+
+/*ARGSUSED*/
+int
+ppt_unassign_device(struct vm *vm, int bus, int slot, int func)
+{
+	return (ENXIO);
+}
diff --git a/usr/src/uts/i86pc/io/vmm/io/vatpic.c b/usr/src/uts/i86pc/io/vmm/io/vatpic.c
index a93b252c91..6e94f5bd9a 100644
--- a/usr/src/uts/i86pc/io/vmm/io/vatpic.c
+++ b/usr/src/uts/i86pc/io/vmm/io/vatpic.c
@@ -25,12 +25,11 @@
  */
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/sys/amd64/vmm/io/vatpic.c 279683 2015-03-06 02:05:45Z tychon $");
+__FBSDID("$FreeBSD$");
 
 #include <sys/param.h>
 #include <sys/types.h>
 #include <sys/queue.h>
-#include <sys/cpuset.h>
 #include <sys/kernel.h>
 #include <sys/lock.h>
 #include <sys/malloc.h>
diff --git a/usr/src/uts/i86pc/io/vmm/io/vatpic.h b/usr/src/uts/i86pc/io/vmm/io/vatpic.h
index ef5e51b158..d4a1be1820 100644
--- a/usr/src/uts/i86pc/io/vmm/io/vatpic.h
+++ b/usr/src/uts/i86pc/io/vmm/io/vatpic.h
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/io/vatpic.h 273706 2014-10-26 19:03:06Z neel $
+ * $FreeBSD$
  */
 
 #ifndef _VATPIC_H_
diff --git a/usr/src/uts/i86pc/io/vmm/io/vatpit.c b/usr/src/uts/i86pc/io/vmm/io/vatpit.c
index ce17bdc92c..d39c759f77 100644
--- a/usr/src/uts/i86pc/io/vmm/io/vatpit.c
+++ b/usr/src/uts/i86pc/io/vmm/io/vatpit.c
@@ -26,12 +26,11 @@
  */
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/sys/amd64/vmm/io/vatpit.c 273706 2014-10-26 19:03:06Z neel $");
+__FBSDID("$FreeBSD$");
 
 #include <sys/param.h>
 #include <sys/types.h>
 #include <sys/queue.h>
-#include <sys/cpuset.h>
 #include <sys/kernel.h>
 #include <sys/lock.h>
 #include <sys/malloc.h>
@@ -437,7 +436,7 @@ vatpit_init(struct vm *vm)
 	vatpit->freq_sbt = bttosbt(bt);
 
 	for (i = 0; i < 3; i++) {
-		callout_init(&vatpit->channel[i].callout, true);
+		callout_init(&vatpit->channel[i].callout, 1);
 		arg = &vatpit->channel[i].callout_arg;
 		arg->vatpit = vatpit;
 		arg->channel_num = i;
diff --git a/usr/src/uts/i86pc/io/vmm/io/vatpit.h b/usr/src/uts/i86pc/io/vmm/io/vatpit.h
index f20ad73e47..5719c9c1aa 100644
--- a/usr/src/uts/i86pc/io/vmm/io/vatpit.h
+++ b/usr/src/uts/i86pc/io/vmm/io/vatpit.h
@@ -24,7 +24,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/io/vatpit.h 273706 2014-10-26 19:03:06Z neel $
+ * $FreeBSD$
  */
 
 #ifndef _VATPIT_H_
diff --git a/usr/src/uts/i86pc/io/vmm/io/vhpet.c b/usr/src/uts/i86pc/io/vmm/io/vhpet.c
index 25f6013da0..a8711eac6a 100644
--- a/usr/src/uts/i86pc/io/vmm/io/vhpet.c
+++ b/usr/src/uts/i86pc/io/vmm/io/vhpet.c
@@ -24,11 +24,11 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/io/vhpet.c 263035 2014-03-11 16:56:00Z tychon $
+ * $FreeBSD$
  */
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/sys/amd64/vmm/io/vhpet.c 263035 2014-03-11 16:56:00Z tychon $");
+__FBSDID("$FreeBSD$");
 
 #include <sys/param.h>
 #include <sys/lock.h>
@@ -36,7 +36,6 @@ __FBSDID("$FreeBSD: head/sys/amd64/vmm/io/vhpet.c 263035 2014-03-11 16:56:00Z ty
 #include <sys/kernel.h>
 #include <sys/malloc.h>
 #include <sys/systm.h>
-#include <sys/cpuset.h>
 
 #include <dev/acpica/acpi_hpet.h>
 
@@ -104,7 +103,6 @@ vhpet_capabilities(void)
 	uint64_t cap = 0;
 
 	cap |= 0x8086 << 16;			/* vendor id */
-	cap |= HPET_CAP_LEG_RT;			/* legacy routing capable */
 	cap |= (VHPET_NUM_TIMERS - 1) << 8;	/* number of timers */
 	cap |= 1;				/* revision */
 	cap &= ~HPET_CAP_COUNT_SIZE;		/* 32-bit timer */
@@ -127,15 +125,6 @@ vhpet_timer_msi_enabled(struct vhpet *vhpet, int n)
 {
 	const uint64_t msi_enable = HPET_TCAP_FSB_INT_DEL | HPET_TCNF_FSB_EN;
 
-	/*
-	 * LegacyReplacement Route configuration takes precedence over MSI
-	 * for timers 0 and 1.
-	 */
-	if (n == 0 || n == 1) {
-		if (vhpet->config & HPET_CNF_LEG_RT)
-			return (false);
-	}
-
 	if ((vhpet->timer[n].cap_config & msi_enable) == msi_enable)
 		return (true);
 	else
@@ -152,41 +141,9 @@ vhpet_timer_ioapic_pin(struct vhpet *vhpet, int n)
 	if (vhpet_timer_msi_enabled(vhpet, n))
 		return (0);
 
-	if (vhpet->config & HPET_CNF_LEG_RT) {
-		/*
-		 * In "legacy routing" timers 0 and 1 are connected to
-		 * ioapic pins 2 and 8 respectively.
-		 */
-		switch (n) {
-		case 0:
-			return (2);
-		case 1:
-			return (8);
-		}
-	}
-
 	return ((vhpet->timer[n].cap_config & HPET_TCNF_INT_ROUTE) >> 9);
 }
 
-static __inline int
-vhpet_timer_atpic_pin(struct vhpet *vhpet, int n)
-{
-	if (vhpet->config & HPET_CNF_LEG_RT) {
-		/*
-		 * In "legacy routing" timers 0 and 1 are connected to
-		 * 8259 master pin 0 and slave pin 0 respectively.
-		 */
-		switch (n) {
-		case 0:
-			return (0);
-		case 1:
-			return (8);
-		}
-	}
-
-	return (-1);
-}
-
 static uint32_t
 vhpet_counter(struct vhpet *vhpet, sbintime_t *nowptr)
 {
@@ -211,7 +168,7 @@ vhpet_counter(struct vhpet *vhpet, sbintime_t *nowptr)
 		/*
 		 * The sbinuptime corresponding to the 'countbase' is
 		 * meaningless when the counter is disabled. Make sure
-		 * that the the caller doesn't want to use it.
+		 * that the caller doesn't want to use it.
 		 */
 		KASSERT(nowptr == NULL, ("vhpet_counter: nowptr must be NULL"));
 	}
@@ -221,17 +178,12 @@ vhpet_counter(struct vhpet *vhpet, sbintime_t *nowptr)
 static void
 vhpet_timer_clear_isr(struct vhpet *vhpet, int n)
 {
-	int pin, legacy_pin;
+	int pin;
 
 	if (vhpet->isr & (1 << n)) {
 		pin = vhpet_timer_ioapic_pin(vhpet, n);
 		KASSERT(pin != 0, ("vhpet timer %d irq incorrectly routed", n));
 		vioapic_deassert_irq(vhpet->vm, pin);
-
-		legacy_pin = vhpet_timer_atpic_pin(vhpet, n);
-		if (legacy_pin != -1)
-			vatpic_deassert_irq(vhpet->vm, legacy_pin);
-
 		vhpet->isr &= ~(1 << n);
 	}
 }
@@ -257,12 +209,6 @@ vhpet_timer_edge_trig(struct vhpet *vhpet, int n)
 	KASSERT(!vhpet_timer_msi_enabled(vhpet, n), ("vhpet_timer_edge_trig: "
 	    "timer %d is using MSI", n));
 
-	/* The legacy replacement interrupts are always edge triggered */
-	if (vhpet->config & HPET_CNF_LEG_RT) {
-		if (n == 0 || n == 1)
-			return (true);
-	}
-
 	if ((vhpet->timer[n].cap_config & HPET_TCNF_INT_TYPE) == 0)
 		return (true);
 	else
@@ -272,7 +218,7 @@ vhpet_timer_edge_trig(struct vhpet *vhpet, int n)
 static void
 vhpet_timer_interrupt(struct vhpet *vhpet, int n)
 {
-	int pin, legacy_pin;
+	int pin;
 
 	/* If interrupts are not enabled for this timer then just return. */
 	if (!vhpet_timer_interrupt_enabled(vhpet, n))
@@ -298,17 +244,11 @@ vhpet_timer_interrupt(struct vhpet *vhpet, int n)
 		return;
 	}
 
-	legacy_pin = vhpet_timer_atpic_pin(vhpet, n);
-
 	if (vhpet_timer_edge_trig(vhpet, n)) {
 		vioapic_pulse_irq(vhpet->vm, pin);
-		if (legacy_pin != -1)
-			vatpic_pulse_irq(vhpet->vm, legacy_pin);
 	} else {
 		vhpet->isr |= 1 << n;
 		vioapic_assert_irq(vhpet->vm, pin);
-		if (legacy_pin != -1)
-			vatpic_assert_irq(vhpet->vm, legacy_pin);
 	}
 }
 
@@ -402,10 +342,6 @@ vhpet_start_timer(struct vhpet *vhpet, int n, uint32_t counter, sbintime_t now)
 {
 	sbintime_t delta, precision;
 
-	/* If interrupts are not enabled for this timer then just return. */
-	if (!vhpet_timer_interrupt_enabled(vhpet, n))
-		return;
-
 	if (vhpet->timer[n].comprate != 0)
 		vhpet_adjust_compval(vhpet, n, counter);
 	else {
@@ -588,6 +524,13 @@ vhpet_mmio_write(void *vm, int vcpuid, uint64_t gpa, uint64_t val, int size,
 		counter = vhpet_counter(vhpet, nowptr);
 		oldval = vhpet->config;
 		update_register(&vhpet->config, data, mask);
+
+		/*
+		 * LegacyReplacement Routing is not supported so clear the
+		 * bit explicitly.
+		 */
+		vhpet->config &= ~HPET_CNF_LEG_RT;
+
 		if ((oldval ^ vhpet->config) & HPET_CNF_ENABLE) {
 			if (vhpet_counter_enabled(vhpet)) {
 				vhpet_start_counting(vhpet);
diff --git a/usr/src/uts/i86pc/io/vmm/io/vhpet.h b/usr/src/uts/i86pc/io/vmm/io/vhpet.h
index 868809d166..330e01739a 100644
--- a/usr/src/uts/i86pc/io/vmm/io/vhpet.h
+++ b/usr/src/uts/i86pc/io/vmm/io/vhpet.h
@@ -24,7 +24,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/io/vhpet.h 258579 2013-11-25 19:04:51Z neel $
+ * $FreeBSD$
  */
 
 #ifndef _VHPET_H_
diff --git a/usr/src/uts/i86pc/io/vmm/io/vioapic.c b/usr/src/uts/i86pc/io/vmm/io/vioapic.c
index 77fe58151a..c3e15c34bf 100644
--- a/usr/src/uts/i86pc/io/vmm/io/vioapic.c
+++ b/usr/src/uts/i86pc/io/vmm/io/vioapic.c
@@ -24,7 +24,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/io/vioapic.c 262139 2014-02-17 22:57:51Z neel $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -41,11 +41,10 @@
  */
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/sys/amd64/vmm/io/vioapic.c 262139 2014-02-17 22:57:51Z neel $");
+__FBSDID("$FreeBSD$");
 
 #include <sys/param.h>
 #include <sys/queue.h>
-#include <sys/cpuset.h>
 #include <sys/lock.h>
 #include <sys/mutex.h>
 #include <sys/systm.h>
@@ -239,7 +238,6 @@ vioapic_pulse_irq(struct vm *vm, int irq)
  * Reset the vlapic's trigger-mode register to reflect the ioapic pin
  * configuration.
  */
-#if 0	/* XXX */
 static void
 vioapic_update_tmr(struct vm *vm, int vcpuid, void *arg)
 {
@@ -279,7 +277,6 @@ vioapic_update_tmr(struct vm *vm, int vcpuid, void *arg)
 	}
 	VIOAPIC_UNLOCK(vioapic);
 }
-#endif
 
 static uint32_t
 vioapic_read(struct vioapic *vioapic, int vcpuid, uint32_t addr)
@@ -322,9 +319,7 @@ vioapic_write(struct vioapic *vioapic, int vcpuid, uint32_t addr, uint32_t data)
 	uint64_t data64, mask64;
 	uint64_t last, changed;
 	int regnum, pin, lshift;
-#if 0	/* XXX */
 	cpuset_t allvcpus;
-#endif
 
 	regnum = addr & 0xff;
 	switch (regnum) {
@@ -368,11 +363,9 @@ vioapic_write(struct vioapic *vioapic, int vcpuid, uint32_t addr, uint32_t data)
 			VIOAPIC_CTR1(vioapic, "ioapic pin%d: recalculate "
 			    "vlapic trigger-mode register", pin);
 			VIOAPIC_UNLOCK(vioapic);
-#if 0	/* XXX */
 			allvcpus = vm_active_cpus(vioapic->vm);
 			vm_smp_rendezvous(vioapic->vm, vcpuid, allvcpus,
 			    vioapic_update_tmr, NULL);
-#endif
 			VIOAPIC_LOCK(vioapic);
 		}
 
diff --git a/usr/src/uts/i86pc/io/vmm/io/vioapic.h b/usr/src/uts/i86pc/io/vmm/io/vioapic.h
index 9479ebb10e..7e9c2e875a 100644
--- a/usr/src/uts/i86pc/io/vmm/io/vioapic.h
+++ b/usr/src/uts/i86pc/io/vmm/io/vioapic.h
@@ -24,7 +24,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/io/vioapic.h 258699 2013-11-27 22:18:08Z neel $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -45,10 +45,6 @@
 #define	VIOAPIC_BASE	0xFEC00000
 #define	VIOAPIC_SIZE	4096
 
-#include "vdev.h"
-
-struct vm;
-
 struct vioapic *vioapic_init(struct vm *vm);
 void	vioapic_cleanup(struct vioapic *vioapic);
 
diff --git a/usr/src/uts/i86pc/io/vmm/io/vlapic.c b/usr/src/uts/i86pc/io/vmm/io/vlapic.c
index 5bf306fd6d..6fed322180 100644
--- a/usr/src/uts/i86pc/io/vmm/io/vlapic.c
+++ b/usr/src/uts/i86pc/io/vmm/io/vlapic.c
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/io/vlapic.c 273375 2014-10-21 07:10:43Z neel $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -40,7 +40,7 @@
  */
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/sys/amd64/vmm/io/vlapic.c 273375 2014-10-21 07:10:43Z neel $");
+__FBSDID("$FreeBSD$");
 
 #include <sys/param.h>
 #include <sys/lock.h>
@@ -58,7 +58,6 @@ __FBSDID("$FreeBSD: head/sys/amd64/vmm/io/vlapic.c 273375 2014-10-21 07:10:43Z n
 
 #include <machine/vmm.h>
 
-#include "vmm_ipi.h"
 #include "vmm_lapic.h"
 #include "vmm_ktr.h"
 #include "vmm_stat.h"
@@ -83,7 +82,12 @@ __FBSDID("$FreeBSD: head/sys/amd64/vmm/io/vlapic.c 273375 2014-10-21 07:10:43Z n
 #define	VLAPIC_TIMER_UNLOCK(vlapic)	mtx_unlock_spin(&((vlapic)->timer_mtx))
 #define	VLAPIC_TIMER_LOCKED(vlapic)	mtx_owned(&((vlapic)->timer_mtx))
 
-#define VLAPIC_BUS_FREQ	tsc_freq
+/*
+ * APIC timer frequency:
+ * - arbitrary but chosen to be in the ballpark of contemporary hardware.
+ * - power-of-two to avoid loss of precision when converted to a bintime.
+ */
+#define VLAPIC_BUS_FREQ		(128 * 1024 * 1024)
 
 static __inline uint32_t
 vlapic_get_id(struct vlapic *vlapic)
@@ -123,14 +127,12 @@ vlapic_dfr_write_handler(struct vlapic *vlapic)
 	lapic->dfr &= APIC_DFR_MODEL_MASK;
 	lapic->dfr |= APIC_DFR_RESERVED;
 
-	if ((lapic->dfr & APIC_DFR_MODEL_MASK) == APIC_DFR_MODEL_FLAT) {
+	if ((lapic->dfr & APIC_DFR_MODEL_MASK) == APIC_DFR_MODEL_FLAT)
 		VLAPIC_CTR0(vlapic, "vlapic DFR in Flat Model");
-	} else if ((lapic->dfr & APIC_DFR_MODEL_MASK) ==
-	    APIC_DFR_MODEL_CLUSTER) {
+	else if ((lapic->dfr & APIC_DFR_MODEL_MASK) == APIC_DFR_MODEL_CLUSTER)
 		VLAPIC_CTR0(vlapic, "vlapic DFR in Cluster Model");
-	} else {
+	else
 		VLAPIC_CTR1(vlapic, "DFR in Unknown Model %#x", lapic->dfr);
-	}
 }
 
 void
@@ -262,7 +264,6 @@ vlapic_dcr_write_handler(struct vlapic *vlapic)
 	VLAPIC_TIMER_UNLOCK(vlapic);
 }
 
-
 void
 vlapic_esr_write_handler(struct vlapic *vlapic)
 {
@@ -569,6 +570,8 @@ vlapic_update_ppr(struct vlapic *vlapic)
 	VLAPIC_CTR1(vlapic, "vlapic_update_ppr 0x%02x", ppr);
 }
 
+static VMM_STAT(VLAPIC_GRATUITOUS_EOI, "EOI without any in-service interrupt");
+
 static void
 vlapic_process_eoi(struct vlapic *vlapic)
 {
@@ -579,11 +582,7 @@ vlapic_process_eoi(struct vlapic *vlapic)
 	isrptr = &lapic->isr0;
 	tmrptr = &lapic->tmr0;
 
-	/*
-	 * The x86 architecture reserves the the first 32 vectors for use
-	 * by the processor.
-	 */
-	for (i = 7; i > 0; i--) {
+	for (i = 7; i >= 0; i--) {
 		idx = i * 4;
 		bitpos = fls(isrptr[idx]);
 		if (bitpos-- != 0) {
@@ -592,17 +591,21 @@ vlapic_process_eoi(struct vlapic *vlapic)
 				      vlapic->isrvec_stk_top);
 			}
 			isrptr[idx] &= ~(1 << bitpos);
+			vector = i * 32 + bitpos;
+			VCPU_CTR1(vlapic->vm, vlapic->vcpuid, "EOI vector %d",
+			    vector);
 			VLAPIC_CTR_ISR(vlapic, "vlapic_process_eoi");
 			vlapic->isrvec_stk_top--;
 			vlapic_update_ppr(vlapic);
 			if ((tmrptr[idx] & (1 << bitpos)) != 0) {
-				vector = i * 32 + bitpos;
 				vioapic_process_eoi(vlapic->vm, vlapic->vcpuid,
 				    vector);
 			}
 			return;
 		}
 	}
+	VCPU_CTR0(vlapic->vm, vlapic->vcpuid, "Gratuitous EOI");
+	vmm_stat_incr(vlapic->vm, vlapic->vcpuid, VLAPIC_GRATUITOUS_EOI, 1);
 }
 
 static __inline int
@@ -868,7 +871,7 @@ vlapic_calcdest(struct vm *vm, cpuset_t *dmask, uint32_t dest, bool phys,
 	} else {
 		/*
 		 * In the "Flat Model" the MDA is interpreted as an 8-bit wide
-		 * bitmask. This model is only avilable in the xAPIC mode.
+		 * bitmask. This model is only available in the xAPIC mode.
 		 */
 		mda_flat_ldest = dest & 0xff;
 
@@ -1121,11 +1124,7 @@ vlapic_pending_intr(struct vlapic *vlapic, int *vecptr)
 
 	irrptr = &lapic->irr0;
 
-	/*
-	 * The x86 architecture reserves the the first 32 vectors for use
-	 * by the processor.
-	 */
-	for (i = 7; i > 0; i--) {
+	for (i = 7; i >= 0; i--) {
 		idx = i * 4;
 		val = atomic_load_acq_int(&irrptr[idx]);
 		bitpos = fls(val);
diff --git a/usr/src/uts/i86pc/io/vmm/io/vlapic.h b/usr/src/uts/i86pc/io/vmm/io/vlapic.h
index 3fa705d818..0e68b2fe82 100644
--- a/usr/src/uts/i86pc/io/vmm/io/vlapic.h
+++ b/usr/src/uts/i86pc/io/vmm/io/vlapic.h
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/io/vlapic.h 262281 2014-02-21 06:03:54Z neel $
+ * $FreeBSD$
  */
 
 #ifndef _VLAPIC_H_
diff --git a/usr/src/uts/i86pc/io/vmm/io/vlapic_priv.h b/usr/src/uts/i86pc/io/vmm/io/vlapic_priv.h
index f9bd2e0e8b..08592c8489 100644
--- a/usr/src/uts/i86pc/io/vmm/io/vlapic_priv.h
+++ b/usr/src/uts/i86pc/io/vmm/io/vlapic_priv.h
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/io/vlapic_priv.h 263211 2014-03-15 23:09:34Z tychon $
+ * $FreeBSD$
  */
 
 #ifndef _VLAPIC_PRIV_H_
diff --git a/usr/src/uts/i86pc/io/vmm/io/vpmtmr.c b/usr/src/uts/i86pc/io/vmm/io/vpmtmr.c
new file mode 100644
index 0000000000..1e7bb93d7b
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/io/vpmtmr.c
@@ -0,0 +1,103 @@
+/*-
+ * Copyright (c) 2014, Neel Natu (neel@freebsd.org)
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/param.h>
+#include <sys/queue.h>
+#include <sys/kernel.h>
+#include <sys/malloc.h>
+#include <sys/systm.h>
+
+#include <machine/vmm.h>
+
+#include "vpmtmr.h"
+
+/*
+ * The ACPI Power Management timer is a free-running 24- or 32-bit
+ * timer with a frequency of 3.579545MHz
+ *
+ * This implementation will be 32-bits
+ */
+
+#define PMTMR_FREQ	3579545  /* 3.579545MHz */
+
+struct vpmtmr {
+	sbintime_t	freq_sbt;
+	sbintime_t	baseuptime;
+	uint32_t	baseval;
+};
+
+static MALLOC_DEFINE(M_VPMTMR, "vpmtmr", "bhyve virtual acpi timer");
+
+struct vpmtmr *
+vpmtmr_init(struct vm *vm)
+{
+	struct vpmtmr *vpmtmr;
+	struct bintime bt;
+
+	vpmtmr = malloc(sizeof(struct vpmtmr), M_VPMTMR, M_WAITOK | M_ZERO);
+	vpmtmr->baseuptime = sbinuptime();
+	vpmtmr->baseval = 0;
+
+	FREQ2BT(PMTMR_FREQ, &bt);
+	vpmtmr->freq_sbt = bttosbt(bt);
+
+	return (vpmtmr);
+}
+
+void
+vpmtmr_cleanup(struct vpmtmr *vpmtmr)
+{
+
+	free(vpmtmr, M_VPMTMR);
+}
+
+int
+vpmtmr_handler(struct vm *vm, int vcpuid, bool in, int port, int bytes,
+    uint32_t *val)
+{
+	struct vpmtmr *vpmtmr;
+	sbintime_t now, delta;
+
+	if (!in || bytes != 4)
+		return (-1);
+
+	vpmtmr = vm_pmtmr(vm);
+
+	/*
+	 * No locking needed because 'baseuptime' and 'baseval' are
+	 * written only during initialization.
+	 */
+	now = sbinuptime();
+	delta = now - vpmtmr->baseuptime;
+	KASSERT(delta >= 0, ("vpmtmr_handler: uptime went backwards: "
+	    "%#lx to %#lx", vpmtmr->baseuptime, now));
+	*val = vpmtmr->baseval + delta / vpmtmr->freq_sbt;
+
+	return (0);
+}
diff --git a/usr/src/uts/i86pc/io/vmm/io/vpmtmr.h b/usr/src/uts/i86pc/io/vmm/io/vpmtmr.h
new file mode 100644
index 0000000000..039a28145b
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/io/vpmtmr.h
@@ -0,0 +1,42 @@
+/*-
+ * Copyright (c) 2014 Neel Natu (neel@freebsd.org)
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _VPMTMR_H_
+#define	_VPMTMR_H_
+
+#define	IO_PMTMR 0x408
+
+struct vpmtmr;
+
+struct vpmtmr *vpmtmr_init(struct vm *vm);
+void vpmtmr_cleanup(struct vpmtmr *pmtmr);
+
+int vpmtmr_handler(struct vm *vm, int vcpuid, bool in, int port, int bytes,
+    uint32_t *val);
+
+#endif
diff --git a/usr/src/uts/i86pc/io/vmm/io/vrtc.c b/usr/src/uts/i86pc/io/vmm/io/vrtc.c
new file mode 100644
index 0000000000..18ebc4b98f
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/io/vrtc.c
@@ -0,0 +1,1019 @@
+/*-
+ * Copyright (c) 2014, Neel Natu (neel@freebsd.org)
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/param.h>
+#include <sys/systm.h>
+#include <sys/queue.h>
+#include <sys/kernel.h>
+#include <sys/malloc.h>
+#include <sys/lock.h>
+#include <sys/mutex.h>
+#include <sys/clock.h>
+#include <sys/sysctl.h>
+
+#include <machine/vmm.h>
+
+#include <isa/rtc.h>
+
+#include "vmm_ktr.h"
+#include "vatpic.h"
+#include "vioapic.h"
+#include "vrtc.h"
+
+/* Register layout of the RTC */
+struct rtcdev {
+	uint8_t	sec;
+	uint8_t	alarm_sec;
+	uint8_t	min;
+	uint8_t	alarm_min;
+	uint8_t	hour;
+	uint8_t	alarm_hour;
+	uint8_t	day_of_week;
+	uint8_t	day_of_month;
+	uint8_t	month;
+	uint8_t	year;
+	uint8_t	reg_a;
+	uint8_t	reg_b;
+	uint8_t	reg_c;
+	uint8_t	reg_d;
+	uint8_t	nvram[36];
+	uint8_t	century;
+	uint8_t	nvram2[128 - 51];
+} __packed;
+CTASSERT(sizeof(struct rtcdev) == 128);
+CTASSERT(offsetof(struct rtcdev, century) == RTC_CENTURY);
+
+struct vrtc {
+	struct vm	*vm;
+	struct mtx	mtx;
+	struct callout	callout;
+	u_int		addr;		/* RTC register to read or write */
+	sbintime_t	base_uptime;
+	time_t		base_rtctime;
+	struct rtcdev	rtcdev;
+};
+
+#define	VRTC_LOCK(vrtc)		mtx_lock(&((vrtc)->mtx))
+#define	VRTC_UNLOCK(vrtc)	mtx_unlock(&((vrtc)->mtx))
+#define	VRTC_LOCKED(vrtc)	mtx_owned(&((vrtc)->mtx))
+
+/*
+ * RTC time is considered "broken" if:
+ * - RTC updates are halted by the guest
+ * - RTC date/time fields have invalid values
+ */
+#define	VRTC_BROKEN_TIME	((time_t)-1)
+
+#define	RTC_IRQ			8
+#define	RTCSB_BIN		0x04
+#define	RTCSB_ALL_INTRS		(RTCSB_UINTR | RTCSB_AINTR | RTCSB_PINTR)
+#define	rtc_halted(vrtc)	((vrtc->rtcdev.reg_b & RTCSB_HALT) != 0)
+#define	aintr_enabled(vrtc)	(((vrtc)->rtcdev.reg_b & RTCSB_AINTR) != 0)
+#define	pintr_enabled(vrtc)	(((vrtc)->rtcdev.reg_b & RTCSB_PINTR) != 0)
+#define	uintr_enabled(vrtc)	(((vrtc)->rtcdev.reg_b & RTCSB_UINTR) != 0)
+
+static void vrtc_callout_handler(void *arg);
+static void vrtc_set_reg_c(struct vrtc *vrtc, uint8_t newval);
+
+static MALLOC_DEFINE(M_VRTC, "vrtc", "bhyve virtual rtc");
+
+SYSCTL_DECL(_hw_vmm);
+SYSCTL_NODE(_hw_vmm, OID_AUTO, vrtc, CTLFLAG_RW, NULL, NULL);
+
+static int rtc_flag_broken_time = 1;
+SYSCTL_INT(_hw_vmm_vrtc, OID_AUTO, flag_broken_time, CTLFLAG_RDTUN,
+    &rtc_flag_broken_time, 0, "Stop guest when invalid RTC time is detected");
+
+static __inline bool
+divider_enabled(int reg_a)
+{
+	/*
+	 * The RTC is counting only when dividers are not held in reset.
+	 */
+	return ((reg_a & 0x70) == 0x20);
+}
+
+static __inline bool
+update_enabled(struct vrtc *vrtc)
+{
+	/*
+	 * RTC date/time can be updated only if:
+	 * - divider is not held in reset
+	 * - guest has not disabled updates
+	 * - the date/time fields have valid contents
+	 */
+	if (!divider_enabled(vrtc->rtcdev.reg_a))
+		return (false);
+
+	if (rtc_halted(vrtc))
+		return (false);
+
+	if (vrtc->base_rtctime == VRTC_BROKEN_TIME)
+		return (false);
+
+	return (true);
+}
+
+static time_t
+vrtc_curtime(struct vrtc *vrtc, sbintime_t *basetime)
+{
+	sbintime_t now, delta;
+	time_t t, secs;
+
+	KASSERT(VRTC_LOCKED(vrtc), ("%s: vrtc not locked", __func__));
+
+	t = vrtc->base_rtctime;
+	*basetime = vrtc->base_uptime;
+	if (update_enabled(vrtc)) {
+		now = sbinuptime();
+		delta = now - vrtc->base_uptime;
+		KASSERT(delta >= 0, ("vrtc_curtime: uptime went backwards: "
+		    "%#lx to %#lx", vrtc->base_uptime, now));
+		secs = delta / SBT_1S;
+		t += secs;
+		*basetime += secs * SBT_1S;
+	}
+	return (t);
+}
+
+static __inline uint8_t
+rtcset(struct rtcdev *rtc, int val)
+{
+
+	KASSERT(val >= 0 && val < 100, ("%s: invalid bin2bcd index %d",
+	    __func__, val));
+
+	return ((rtc->reg_b & RTCSB_BIN) ? val : bin2bcd_data[val]);
+}
+
+static void
+secs_to_rtc(time_t rtctime, struct vrtc *vrtc, int force_update)
+{
+	struct clocktime ct;
+	struct timespec ts;
+	struct rtcdev *rtc;
+	int hour;
+
+	KASSERT(VRTC_LOCKED(vrtc), ("%s: vrtc not locked", __func__));
+
+	if (rtctime < 0) {
+		KASSERT(rtctime == VRTC_BROKEN_TIME,
+		    ("%s: invalid vrtc time %#lx", __func__, rtctime));
+		return;
+	}
+
+	/*
+	 * If the RTC is halted then the guest has "ownership" of the
+	 * date/time fields. Don't update the RTC date/time fields in
+	 * this case (unless forced).
+	 */
+	if (rtc_halted(vrtc) && !force_update)
+		return;
+
+	ts.tv_sec = rtctime;
+	ts.tv_nsec = 0;
+	clock_ts_to_ct(&ts, &ct);
+
+	KASSERT(ct.sec >= 0 && ct.sec <= 59, ("invalid clocktime sec %d",
+	    ct.sec));
+	KASSERT(ct.min >= 0 && ct.min <= 59, ("invalid clocktime min %d",
+	    ct.min));
+	KASSERT(ct.hour >= 0 && ct.hour <= 23, ("invalid clocktime hour %d",
+	    ct.hour));
+	KASSERT(ct.dow >= 0 && ct.dow <= 6, ("invalid clocktime wday %d",
+	    ct.dow));
+	KASSERT(ct.day >= 1 && ct.day <= 31, ("invalid clocktime mday %d",
+	    ct.day));
+	KASSERT(ct.mon >= 1 && ct.mon <= 12, ("invalid clocktime month %d",
+	    ct.mon));
+	KASSERT(ct.year >= POSIX_BASE_YEAR, ("invalid clocktime year %d",
+	    ct.year));
+
+	rtc = &vrtc->rtcdev;
+	rtc->sec = rtcset(rtc, ct.sec);
+	rtc->min = rtcset(rtc, ct.min);
+
+	if (rtc->reg_b & RTCSB_24HR) {
+		hour = ct.hour;
+	} else {
+		/*
+		 * Convert to the 12-hour format.
+		 */
+		switch (ct.hour) {
+		case 0:			/* 12 AM */
+		case 12:		/* 12 PM */
+			hour = 12;
+			break;
+		default:
+			/*
+			 * The remaining 'ct.hour' values are interpreted as:
+			 * [1  - 11] ->  1 - 11 AM
+			 * [13 - 23] ->  1 - 11 PM
+			 */
+			hour = ct.hour % 12;
+			break;
+		}
+	}
+
+	rtc->hour = rtcset(rtc, hour);
+
+	if ((rtc->reg_b & RTCSB_24HR) == 0 && ct.hour >= 12)
+		rtc->hour |= 0x80;	    /* set MSB to indicate PM */
+
+	rtc->day_of_week = rtcset(rtc, ct.dow + 1);
+	rtc->day_of_month = rtcset(rtc, ct.day);
+	rtc->month = rtcset(rtc, ct.mon);
+	rtc->year = rtcset(rtc, ct.year % 100);
+	rtc->century = rtcset(rtc, ct.year / 100);
+}
+
+static int
+rtcget(struct rtcdev *rtc, int val, int *retval)
+{
+	uint8_t upper, lower;
+
+	if (rtc->reg_b & RTCSB_BIN) {
+		*retval = val;
+		return (0);
+	}
+
+	lower = val & 0xf;
+	upper = (val >> 4) & 0xf;
+
+	if (lower > 9 || upper > 9)
+		return (-1);
+
+	*retval = upper * 10 + lower;
+	return (0);
+}
+
+static time_t
+rtc_to_secs(struct vrtc *vrtc)
+{
+	struct clocktime ct;
+	struct timespec ts;
+	struct rtcdev *rtc;
+	struct vm *vm;
+	int century, error, hour, pm, year;
+
+	KASSERT(VRTC_LOCKED(vrtc), ("%s: vrtc not locked", __func__));
+
+	vm = vrtc->vm;
+	rtc = &vrtc->rtcdev;
+
+	bzero(&ct, sizeof(struct clocktime));
+
+	error = rtcget(rtc, rtc->sec, &ct.sec);
+	if (error || ct.sec < 0 || ct.sec > 59) {
+		VM_CTR2(vm, "Invalid RTC sec %#x/%d", rtc->sec, ct.sec);
+		goto fail;
+	}
+
+	error = rtcget(rtc, rtc->min, &ct.min);
+	if (error || ct.min < 0 || ct.min > 59) {
+		VM_CTR2(vm, "Invalid RTC min %#x/%d", rtc->min, ct.min);
+		goto fail;
+	}
+
+	pm = 0;
+	hour = rtc->hour;
+	if ((rtc->reg_b & RTCSB_24HR) == 0) {
+		if (hour & 0x80) {
+			hour &= ~0x80;
+			pm = 1;
+		}
+	}
+	error = rtcget(rtc, hour, &ct.hour);
+	if ((rtc->reg_b & RTCSB_24HR) == 0) {
+		if (ct.hour >= 1 && ct.hour <= 12) {
+			/*
+			 * Convert from 12-hour format to internal 24-hour
+			 * representation as follows:
+			 *
+			 *    12-hour format		ct.hour
+			 *	12	AM		0
+			 *	1 - 11	AM		1 - 11
+			 *	12	PM		12
+			 *	1 - 11	PM		13 - 23
+			 */
+			if (ct.hour == 12)
+				ct.hour = 0;
+			if (pm)
+				ct.hour += 12;
+		} else {
+			VM_CTR2(vm, "Invalid RTC 12-hour format %#x/%d",
+			    rtc->hour, ct.hour);
+			goto fail;
+		}
+	}
+
+	if (error || ct.hour < 0 || ct.hour > 23) {
+		VM_CTR2(vm, "Invalid RTC hour %#x/%d", rtc->hour, ct.hour);
+		goto fail;
+	}
+
+	/*
+	 * Ignore 'rtc->dow' because some guests like Linux don't bother
+	 * setting it at all while others like OpenBSD/i386 set it incorrectly. 
+	 *
+	 * clock_ct_to_ts() does not depend on 'ct.dow' anyways so ignore it.
+	 */
+	ct.dow = -1;
+
+	error = rtcget(rtc, rtc->day_of_month, &ct.day);
+	if (error || ct.day < 1 || ct.day > 31) {
+		VM_CTR2(vm, "Invalid RTC mday %#x/%d", rtc->day_of_month,
+		    ct.day);
+		goto fail;
+	}
+
+	error = rtcget(rtc, rtc->month, &ct.mon);
+	if (error || ct.mon < 1 || ct.mon > 12) {
+		VM_CTR2(vm, "Invalid RTC month %#x/%d", rtc->month, ct.mon);
+		goto fail;
+	}
+
+	error = rtcget(rtc, rtc->year, &year);
+	if (error || year < 0 || year > 99) {
+		VM_CTR2(vm, "Invalid RTC year %#x/%d", rtc->year, year);
+		goto fail;
+	}
+
+	error = rtcget(rtc, rtc->century, &century);
+	ct.year = century * 100 + year;
+	if (error || ct.year < POSIX_BASE_YEAR) {
+		VM_CTR2(vm, "Invalid RTC century %#x/%d", rtc->century,
+		    ct.year);
+		goto fail;
+	}
+
+	error = clock_ct_to_ts(&ct, &ts);
+	if (error || ts.tv_sec < 0) {
+		VM_CTR3(vm, "Invalid RTC clocktime.date %04d-%02d-%02d",
+		    ct.year, ct.mon, ct.day);
+		VM_CTR3(vm, "Invalid RTC clocktime.time %02d:%02d:%02d",
+		    ct.hour, ct.min, ct.sec);
+		goto fail;
+	}
+	return (ts.tv_sec);		/* success */
+fail:
+	/*
+	 * Stop updating the RTC if the date/time fields programmed by
+	 * the guest are invalid.
+	 */
+	VM_CTR0(vrtc->vm, "Invalid RTC date/time programming detected");
+	return (VRTC_BROKEN_TIME);
+}
+
+static int
+vrtc_time_update(struct vrtc *vrtc, time_t newtime, sbintime_t newbase)
+{
+	struct rtcdev *rtc;
+	sbintime_t oldbase;
+	time_t oldtime;
+	uint8_t alarm_sec, alarm_min, alarm_hour;
+
+	KASSERT(VRTC_LOCKED(vrtc), ("%s: vrtc not locked", __func__));
+
+	rtc = &vrtc->rtcdev;
+	alarm_sec = rtc->alarm_sec;
+	alarm_min = rtc->alarm_min;
+	alarm_hour = rtc->alarm_hour;
+
+	oldtime = vrtc->base_rtctime;
+	VM_CTR2(vrtc->vm, "Updating RTC secs from %#lx to %#lx",
+	    oldtime, newtime);
+
+	oldbase = vrtc->base_uptime;
+	VM_CTR2(vrtc->vm, "Updating RTC base uptime from %#lx to %#lx",
+	    oldbase, newbase);
+	vrtc->base_uptime = newbase;
+
+	if (newtime == oldtime)
+		return (0);
+
+	/*
+	 * If 'newtime' indicates that RTC updates are disabled then just
+	 * record that and return. There is no need to do alarm interrupt
+	 * processing in this case.
+	 */
+	if (newtime == VRTC_BROKEN_TIME) {
+		vrtc->base_rtctime = VRTC_BROKEN_TIME;
+		return (0);
+	}
+
+	/*
+	 * Return an error if RTC updates are halted by the guest.
+	 */
+	if (rtc_halted(vrtc)) {
+		VM_CTR0(vrtc->vm, "RTC update halted by guest");
+		return (EBUSY);
+	}
+
+	do {
+		/*
+		 * If the alarm interrupt is enabled and 'oldtime' is valid
+		 * then visit all the seconds between 'oldtime' and 'newtime'
+		 * to check for the alarm condition.
+		 *
+		 * Otherwise move the RTC time forward directly to 'newtime'.
+		 */
+		if (aintr_enabled(vrtc) && oldtime != VRTC_BROKEN_TIME)
+			vrtc->base_rtctime++;
+		else
+			vrtc->base_rtctime = newtime;
+
+		if (aintr_enabled(vrtc)) {
+			/*
+			 * Update the RTC date/time fields before checking
+			 * if the alarm conditions are satisfied.
+			 */
+			secs_to_rtc(vrtc->base_rtctime, vrtc, 0);
+
+			if ((alarm_sec >= 0xC0 || alarm_sec == rtc->sec) &&
+			    (alarm_min >= 0xC0 || alarm_min == rtc->min) &&
+			    (alarm_hour >= 0xC0 || alarm_hour == rtc->hour)) {
+				vrtc_set_reg_c(vrtc, rtc->reg_c | RTCIR_ALARM);
+			}
+		}
+	} while (vrtc->base_rtctime != newtime);
+
+	if (uintr_enabled(vrtc))
+		vrtc_set_reg_c(vrtc, rtc->reg_c | RTCIR_UPDATE);
+
+	return (0);
+}
+
+static sbintime_t
+vrtc_freq(struct vrtc *vrtc)
+{
+	int ratesel;
+
+	static sbintime_t pf[16] = {
+		0,
+		SBT_1S / 256,
+		SBT_1S / 128,
+		SBT_1S / 8192,
+		SBT_1S / 4096,
+		SBT_1S / 2048,
+		SBT_1S / 1024,
+		SBT_1S / 512,
+		SBT_1S / 256,
+		SBT_1S / 128,
+		SBT_1S / 64,
+		SBT_1S / 32,
+		SBT_1S / 16,
+		SBT_1S / 8,
+		SBT_1S / 4,
+		SBT_1S / 2,
+	};
+
+	KASSERT(VRTC_LOCKED(vrtc), ("%s: vrtc not locked", __func__));
+
+	/*
+	 * If both periodic and alarm interrupts are enabled then use the
+	 * periodic frequency to drive the callout. The minimum periodic
+	 * frequency (2 Hz) is higher than the alarm frequency (1 Hz) so
+	 * piggyback the alarm on top of it. The same argument applies to
+	 * the update interrupt.
+	 */
+	if (pintr_enabled(vrtc) && divider_enabled(vrtc->rtcdev.reg_a)) {
+		ratesel = vrtc->rtcdev.reg_a & 0xf;
+		return (pf[ratesel]);
+	} else if (aintr_enabled(vrtc) && update_enabled(vrtc)) {
+		return (SBT_1S);
+	} else if (uintr_enabled(vrtc) && update_enabled(vrtc)) {
+		return (SBT_1S);
+	} else {
+		return (0);
+	}
+}
+
+static void
+vrtc_callout_reset(struct vrtc *vrtc, sbintime_t freqsbt)
+{
+
+	KASSERT(VRTC_LOCKED(vrtc), ("%s: vrtc not locked", __func__));
+
+	if (freqsbt == 0) {
+		if (callout_active(&vrtc->callout)) {
+			VM_CTR0(vrtc->vm, "RTC callout stopped");
+			callout_stop(&vrtc->callout);
+		}
+		return;
+	}
+	VM_CTR1(vrtc->vm, "RTC callout frequency %d hz", SBT_1S / freqsbt);
+	callout_reset_sbt(&vrtc->callout, freqsbt, 0, vrtc_callout_handler,
+	    vrtc, 0);
+}
+
+static void
+vrtc_callout_handler(void *arg)
+{
+	struct vrtc *vrtc = arg;
+	sbintime_t freqsbt, basetime;
+	time_t rtctime;
+	int error;
+
+	VM_CTR0(vrtc->vm, "vrtc callout fired");
+
+	VRTC_LOCK(vrtc);
+	if (callout_pending(&vrtc->callout))	/* callout was reset */
+		goto done;
+
+	if (!callout_active(&vrtc->callout))	/* callout was stopped */
+		goto done;
+
+	callout_deactivate(&vrtc->callout);
+
+	KASSERT((vrtc->rtcdev.reg_b & RTCSB_ALL_INTRS) != 0,
+	    ("gratuitous vrtc callout"));
+
+	if (pintr_enabled(vrtc))
+		vrtc_set_reg_c(vrtc, vrtc->rtcdev.reg_c | RTCIR_PERIOD);
+
+	if (aintr_enabled(vrtc) || uintr_enabled(vrtc)) {
+		rtctime = vrtc_curtime(vrtc, &basetime);
+		error = vrtc_time_update(vrtc, rtctime, basetime);
+		KASSERT(error == 0, ("%s: vrtc_time_update error %d",
+		    __func__, error));
+	}
+
+	freqsbt = vrtc_freq(vrtc);
+	KASSERT(freqsbt != 0, ("%s: vrtc frequency cannot be zero", __func__));
+	vrtc_callout_reset(vrtc, freqsbt);
+done:
+	VRTC_UNLOCK(vrtc);
+}
+
+static __inline void
+vrtc_callout_check(struct vrtc *vrtc, sbintime_t freq)
+{
+	int active;
+
+	active = callout_active(&vrtc->callout) ? 1 : 0;
+	KASSERT((freq == 0 && !active) || (freq != 0 && active),
+	    ("vrtc callout %s with frequency %#lx",
+	    active ? "active" : "inactive", freq));
+}
+
+static void
+vrtc_set_reg_c(struct vrtc *vrtc, uint8_t newval)
+{
+	struct rtcdev *rtc;
+	int oldirqf, newirqf;
+	uint8_t oldval, changed;
+
+	KASSERT(VRTC_LOCKED(vrtc), ("%s: vrtc not locked", __func__));
+
+	rtc = &vrtc->rtcdev;
+	newval &= RTCIR_ALARM | RTCIR_PERIOD | RTCIR_UPDATE;
+
+	oldirqf = rtc->reg_c & RTCIR_INT;
+	if ((aintr_enabled(vrtc) && (newval & RTCIR_ALARM) != 0) ||
+	    (pintr_enabled(vrtc) && (newval & RTCIR_PERIOD) != 0) ||
+	    (uintr_enabled(vrtc) && (newval & RTCIR_UPDATE) != 0)) {
+		newirqf = RTCIR_INT;
+	} else {
+		newirqf = 0;
+	}
+
+	oldval = rtc->reg_c;
+	rtc->reg_c = newirqf | newval;
+	changed = oldval ^ rtc->reg_c;
+	if (changed) {
+		VM_CTR2(vrtc->vm, "RTC reg_c changed from %#x to %#x",
+		    oldval, rtc->reg_c);
+	}
+
+	if (!oldirqf && newirqf) {
+		VM_CTR1(vrtc->vm, "RTC irq %d asserted", RTC_IRQ);
+		vatpic_pulse_irq(vrtc->vm, RTC_IRQ);
+		vioapic_pulse_irq(vrtc->vm, RTC_IRQ);
+	} else if (oldirqf && !newirqf) {
+		VM_CTR1(vrtc->vm, "RTC irq %d deasserted", RTC_IRQ);
+	}
+}
+
+static int
+vrtc_set_reg_b(struct vrtc *vrtc, uint8_t newval)
+{
+	struct rtcdev *rtc;
+	sbintime_t oldfreq, newfreq, basetime;
+	time_t curtime, rtctime;
+	int error;
+	uint8_t oldval, changed;
+
+	KASSERT(VRTC_LOCKED(vrtc), ("%s: vrtc not locked", __func__));
+
+	rtc = &vrtc->rtcdev;
+	oldval = rtc->reg_b;
+	oldfreq = vrtc_freq(vrtc);
+
+	rtc->reg_b = newval;
+	changed = oldval ^ newval;
+	if (changed) {
+		VM_CTR2(vrtc->vm, "RTC reg_b changed from %#x to %#x",
+		    oldval, newval);
+	}
+
+	if (changed & RTCSB_HALT) {
+		if ((newval & RTCSB_HALT) == 0) {
+			rtctime = rtc_to_secs(vrtc);
+			basetime = sbinuptime();
+			if (rtctime == VRTC_BROKEN_TIME) {
+				if (rtc_flag_broken_time)
+					return (-1);
+			}
+		} else {
+			curtime = vrtc_curtime(vrtc, &basetime);
+			KASSERT(curtime == vrtc->base_rtctime, ("%s: mismatch "
+			    "between vrtc basetime (%#lx) and curtime (%#lx)",
+			    __func__, vrtc->base_rtctime, curtime));
+
+			/*
+			 * Force a refresh of the RTC date/time fields so
+			 * they reflect the time right before the guest set
+			 * the HALT bit.
+			 */
+			secs_to_rtc(curtime, vrtc, 1);
+
+			/*
+			 * Updates are halted so mark 'base_rtctime' to denote
+			 * that the RTC date/time is in flux.
+			 */
+			rtctime = VRTC_BROKEN_TIME;
+			rtc->reg_b &= ~RTCSB_UINTR;
+		}
+		error = vrtc_time_update(vrtc, rtctime, basetime);
+		KASSERT(error == 0, ("vrtc_time_update error %d", error));
+	}
+
+	/*
+	 * Side effect of changes to the interrupt enable bits.
+	 */
+	if (changed & RTCSB_ALL_INTRS)
+		vrtc_set_reg_c(vrtc, vrtc->rtcdev.reg_c);
+
+	/*
+	 * Change the callout frequency if it has changed.
+	 */
+	newfreq = vrtc_freq(vrtc);
+	if (newfreq != oldfreq)
+		vrtc_callout_reset(vrtc, newfreq);
+	else
+		vrtc_callout_check(vrtc, newfreq);
+
+	/*
+	 * The side effect of bits that control the RTC date/time format
+	 * is handled lazily when those fields are actually read.
+	 */
+	return (0);
+}
+
+static void
+vrtc_set_reg_a(struct vrtc *vrtc, uint8_t newval)
+{
+	sbintime_t oldfreq, newfreq;
+	uint8_t oldval, changed;
+
+	KASSERT(VRTC_LOCKED(vrtc), ("%s: vrtc not locked", __func__));
+
+	newval &= ~RTCSA_TUP;
+	oldval = vrtc->rtcdev.reg_a;
+	oldfreq = vrtc_freq(vrtc);
+
+	if (divider_enabled(oldval) && !divider_enabled(newval)) {
+		VM_CTR2(vrtc->vm, "RTC divider held in reset at %#lx/%#lx",
+		    vrtc->base_rtctime, vrtc->base_uptime);
+	} else if (!divider_enabled(oldval) && divider_enabled(newval)) {
+		/*
+		 * If the dividers are coming out of reset then update
+		 * 'base_uptime' before this happens. This is done to
+		 * maintain the illusion that the RTC date/time was frozen
+		 * while the dividers were disabled.
+		 */
+		vrtc->base_uptime = sbinuptime();
+		VM_CTR2(vrtc->vm, "RTC divider out of reset at %#lx/%#lx",
+		    vrtc->base_rtctime, vrtc->base_uptime);
+	} else {
+		/* NOTHING */
+	}
+
+	vrtc->rtcdev.reg_a = newval;
+	changed = oldval ^ newval;
+	if (changed) {
+		VM_CTR2(vrtc->vm, "RTC reg_a changed from %#x to %#x",
+		    oldval, newval);
+	}
+
+	/*
+	 * Side effect of changes to rate select and divider enable bits.
+	 */
+	newfreq = vrtc_freq(vrtc);
+	if (newfreq != oldfreq)
+		vrtc_callout_reset(vrtc, newfreq);
+	else
+		vrtc_callout_check(vrtc, newfreq);
+}
+
+int
+vrtc_set_time(struct vm *vm, time_t secs)
+{
+	struct vrtc *vrtc;
+	int error;
+
+	vrtc = vm_rtc(vm);
+	VRTC_LOCK(vrtc);
+	error = vrtc_time_update(vrtc, secs, sbinuptime());
+	VRTC_UNLOCK(vrtc);
+
+	if (error) {
+		VM_CTR2(vrtc->vm, "Error %d setting RTC time to %#lx", error,
+		    secs);
+	} else {
+		VM_CTR1(vrtc->vm, "RTC time set to %#lx", secs);
+	}
+
+	return (error);
+}
+
+time_t
+vrtc_get_time(struct vm *vm)
+{
+	struct vrtc *vrtc;
+	sbintime_t basetime;
+	time_t t;
+
+	vrtc = vm_rtc(vm);
+	VRTC_LOCK(vrtc);
+	t = vrtc_curtime(vrtc, &basetime);
+	VRTC_UNLOCK(vrtc);
+
+	return (t);
+}
+
+int
+vrtc_nvram_write(struct vm *vm, int offset, uint8_t value)
+{
+	struct vrtc *vrtc;
+	uint8_t *ptr;
+
+	vrtc = vm_rtc(vm);
+
+	/*
+	 * Don't allow writes to RTC control registers or the date/time fields.
+	 */
+	if (offset < offsetof(struct rtcdev, nvram[0]) ||
+	    offset == RTC_CENTURY || offset >= sizeof(struct rtcdev)) {
+		VM_CTR1(vrtc->vm, "RTC nvram write to invalid offset %d",
+		    offset);
+		return (EINVAL);
+	}
+
+	VRTC_LOCK(vrtc);
+	ptr = (uint8_t *)(&vrtc->rtcdev);
+	ptr[offset] = value;
+	VM_CTR2(vrtc->vm, "RTC nvram write %#x to offset %#x", value, offset);
+	VRTC_UNLOCK(vrtc);
+
+	return (0);
+}
+
+int
+vrtc_nvram_read(struct vm *vm, int offset, uint8_t *retval)
+{
+	struct vrtc *vrtc;
+	sbintime_t basetime;
+	time_t curtime;
+	uint8_t *ptr;
+
+	/*
+	 * Allow all offsets in the RTC to be read.
+	 */
+	if (offset < 0 || offset >= sizeof(struct rtcdev))
+		return (EINVAL);
+
+	vrtc = vm_rtc(vm);
+	VRTC_LOCK(vrtc);
+
+	/*
+	 * Update RTC date/time fields if necessary.
+	 */
+	if (offset < 10 || offset == RTC_CENTURY) {
+		curtime = vrtc_curtime(vrtc, &basetime);
+		secs_to_rtc(curtime, vrtc, 0);
+	}
+
+	ptr = (uint8_t *)(&vrtc->rtcdev);
+	*retval = ptr[offset];
+
+	VRTC_UNLOCK(vrtc);
+	return (0);
+}
+
+int
+vrtc_addr_handler(struct vm *vm, int vcpuid, bool in, int port, int bytes,
+    uint32_t *val)
+{
+	struct vrtc *vrtc;
+
+	vrtc = vm_rtc(vm);
+
+	if (bytes != 1)
+		return (-1);
+
+	if (in) {
+		*val = 0xff;
+		return (0);
+	}
+
+	VRTC_LOCK(vrtc);
+	vrtc->addr = *val & 0x7f;
+	VRTC_UNLOCK(vrtc);
+
+	return (0);
+}
+
+int
+vrtc_data_handler(struct vm *vm, int vcpuid, bool in, int port, int bytes,
+    uint32_t *val)
+{
+	struct vrtc *vrtc;
+	struct rtcdev *rtc;
+	sbintime_t basetime;
+	time_t curtime;
+	int error, offset;
+
+	vrtc = vm_rtc(vm);
+	rtc = &vrtc->rtcdev;
+
+	if (bytes != 1)
+		return (-1);
+
+	VRTC_LOCK(vrtc);
+	offset = vrtc->addr;
+	if (offset >= sizeof(struct rtcdev)) {
+		VRTC_UNLOCK(vrtc);
+		return (-1);
+	}
+
+	error = 0;
+	curtime = vrtc_curtime(vrtc, &basetime);
+	vrtc_time_update(vrtc, curtime, basetime);
+
+	/*
+	 * Update RTC date/time fields if necessary.
+	 *
+	 * This is not just for reads of the RTC. The side-effect of writing
+	 * the century byte requires other RTC date/time fields (e.g. sec)
+	 * to be updated here.
+	 */
+	if (offset < 10 || offset == RTC_CENTURY)
+		secs_to_rtc(curtime, vrtc, 0);
+
+	if (in) {
+		if (offset == 12) {
+			/*
+			 * XXX
+			 * reg_c interrupt flags are updated only if the
+			 * corresponding interrupt enable bit in reg_b is set.
+			 */
+			*val = vrtc->rtcdev.reg_c;
+			vrtc_set_reg_c(vrtc, 0);
+		} else {
+			*val = *((uint8_t *)rtc + offset);
+		}
+		VCPU_CTR2(vm, vcpuid, "Read value %#x from RTC offset %#x",
+		    *val, offset);
+	} else {
+		switch (offset) {
+		case 10:
+			VCPU_CTR1(vm, vcpuid, "RTC reg_a set to %#x", *val);
+			vrtc_set_reg_a(vrtc, *val);
+			break;
+		case 11:
+			VCPU_CTR1(vm, vcpuid, "RTC reg_b set to %#x", *val);
+			error = vrtc_set_reg_b(vrtc, *val);
+			break;
+		case 12:
+			VCPU_CTR1(vm, vcpuid, "RTC reg_c set to %#x (ignored)",
+			    *val);
+			break;
+		case 13:
+			VCPU_CTR1(vm, vcpuid, "RTC reg_d set to %#x (ignored)",
+			    *val);
+			break;
+		case 0:
+			/*
+			 * High order bit of 'seconds' is readonly.
+			 */
+			*val &= 0x7f;
+			/* FALLTHRU */
+		default:
+			VCPU_CTR2(vm, vcpuid, "RTC offset %#x set to %#x",
+			    offset, *val);
+			*((uint8_t *)rtc + offset) = *val;
+			break;
+		}
+
+		/*
+		 * XXX some guests (e.g. OpenBSD) write the century byte
+		 * outside of RTCSB_HALT so re-calculate the RTC date/time.
+		 */
+		if (offset == RTC_CENTURY && !rtc_halted(vrtc)) {
+			curtime = rtc_to_secs(vrtc);
+			error = vrtc_time_update(vrtc, curtime, sbinuptime());
+			KASSERT(!error, ("vrtc_time_update error %d", error));
+			if (curtime == VRTC_BROKEN_TIME && rtc_flag_broken_time)
+				error = -1;
+		}
+	}
+	VRTC_UNLOCK(vrtc);
+	return (error);
+}
+
+void
+vrtc_reset(struct vrtc *vrtc)
+{
+	struct rtcdev *rtc;
+
+	VRTC_LOCK(vrtc);
+
+	rtc = &vrtc->rtcdev;
+	vrtc_set_reg_b(vrtc, rtc->reg_b & ~(RTCSB_ALL_INTRS | RTCSB_SQWE));
+	vrtc_set_reg_c(vrtc, 0);
+	KASSERT(!callout_active(&vrtc->callout), ("rtc callout still active"));
+
+	VRTC_UNLOCK(vrtc);
+}
+
+struct vrtc *
+vrtc_init(struct vm *vm)
+{
+	struct vrtc *vrtc;
+	struct rtcdev *rtc;
+	time_t curtime;
+
+	vrtc = malloc(sizeof(struct vrtc), M_VRTC, M_WAITOK | M_ZERO);
+	vrtc->vm = vm;
+	mtx_init(&vrtc->mtx, "vrtc lock", NULL, MTX_DEF);
+	callout_init(&vrtc->callout, 1);
+
+	/* Allow dividers to keep time but disable everything else */
+	rtc = &vrtc->rtcdev;
+	rtc->reg_a = 0x20;
+	rtc->reg_b = RTCSB_24HR;
+	rtc->reg_c = 0;
+	rtc->reg_d = RTCSD_PWR;
+
+	/* Reset the index register to a safe value. */
+	vrtc->addr = RTC_STATUSD;
+
+	/*
+	 * Initialize RTC time to 00:00:00 Jan 1, 1970.
+	 */
+	curtime = 0;
+
+	VRTC_LOCK(vrtc);
+	vrtc->base_rtctime = VRTC_BROKEN_TIME;
+	vrtc_time_update(vrtc, curtime, sbinuptime());
+	secs_to_rtc(curtime, vrtc, 0);
+	VRTC_UNLOCK(vrtc);
+
+	return (vrtc);
+}
+
+void
+vrtc_cleanup(struct vrtc *vrtc)
+{
+
+	callout_drain(&vrtc->callout);
+	free(vrtc, M_VRTC);
+}
diff --git a/usr/src/uts/i86pc/io/vmm/io/vrtc.h b/usr/src/uts/i86pc/io/vmm/io/vrtc.h
new file mode 100644
index 0000000000..6fbbc9c810
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/io/vrtc.h
@@ -0,0 +1,50 @@
+/*-
+ * Copyright (c) 2014 Neel Natu (neel@freebsd.org)
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice unmodified, this list of conditions, and the following
+ *    disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
+ * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.
+ * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,
+ * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT
+ * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF
+ * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#ifndef _VRTC_H_
+#define	_VRTC_H_
+
+#include <isa/isareg.h>
+
+struct vrtc;
+
+struct vrtc *vrtc_init(struct vm *vm);
+void vrtc_cleanup(struct vrtc *vrtc);
+void vrtc_reset(struct vrtc *vrtc);
+
+time_t vrtc_get_time(struct vm *vm);
+int vrtc_set_time(struct vm *vm, time_t secs);
+int vrtc_nvram_write(struct vm *vm, int offset, uint8_t value);
+int vrtc_nvram_read(struct vm *vm, int offset, uint8_t *retval);
+
+int vrtc_addr_handler(struct vm *vm, int vcpuid, bool in, int port, int bytes,
+    uint32_t *val);
+int vrtc_data_handler(struct vm *vm, int vcpuid, bool in, int port, int bytes,
+    uint32_t *val);
+
+#endif
diff --git a/usr/src/uts/i86pc/io/vmm/offsets.in b/usr/src/uts/i86pc/io/vmm/offsets.in
deleted file mode 100644
index 4b1fe1d6b6..0000000000
--- a/usr/src/uts/i86pc/io/vmm/offsets.in
+++ /dev/null
@@ -1,72 +0,0 @@
-/*
- * This file and its contents are supplied under the terms of the
- * Common Development and Distribution License ("CDDL"), version 1.0.
- * You may only use this file in accordance with the terms of version
- * 1.0 of the CDDL.
- *
- * A full copy of the text of the CDDL should have accompanied this
- * source.  A copy of the CDDL is also available via the Internet at
- * http://www.illumos.org/license/CDDL.
- */
-
-/*
- * Copyright 2014 Pluribus Networks Inc.
- */
-
-#include <sys/types.h>
-#include <sys/param.h>
-#include <sys/systm.h>
-#include <sys/cpuvar.h>
-
-#include <machine/pmap.h>
-
-#include <machine/vmm.h>
-#include "intel/vmx_cpufunc.h"
-#include "intel/vmx.h"
-
-vmxctx
-	tmpstktop	VMXCTX_TMPSTKTOP
-	guest_rdi	VMXCTX_GUEST_RDI
-	guest_rsi	VMXCTX_GUEST_RSI
-	guest_rdx	VMXCTX_GUEST_RDX
-	guest_rcx	VMXCTX_GUEST_RCX
-	guest_r8	VMXCTX_GUEST_R8
-	guest_r9	VMXCTX_GUEST_R9
-	guest_rax	VMXCTX_GUEST_RAX
-	guest_rbx	VMXCTX_GUEST_RBX
-	guest_rbp	VMXCTX_GUEST_RBP
-	guest_r10	VMXCTX_GUEST_R10
-	guest_r11	VMXCTX_GUEST_R11
-	guest_r12	VMXCTX_GUEST_R12
-	guest_r13	VMXCTX_GUEST_R13
-	guest_r14	VMXCTX_GUEST_R14
-	guest_r15	VMXCTX_GUEST_R15
-	guest_cr2	VMXCTX_GUEST_CR2
-	host_r15	VMXCTX_HOST_R15
-	host_r14	VMXCTX_HOST_R14
-	host_r13	VMXCTX_HOST_R13
-	host_r12	VMXCTX_HOST_R12
-	host_rbp	VMXCTX_HOST_RBP
-	host_rsp	VMXCTX_HOST_RSP
-	host_rbx	VMXCTX_HOST_RBX
-	host_rip	VMXCTX_HOST_RIP
-	launch_error	VMXCTX_LAUNCH_ERROR
-
-vmx			VMX_SIZE
-
-\#define	VM_SUCCESS		0
-\#define	VM_FAIL_INVALID		1
-\#define	VM_FAIL_VALID		2
-
-\#define	VMX_RETURN_DIRECT	0
-\#define	VMX_RETURN_LONGJMP	1
-\#define	VMX_RETURN_VMRESUME	2
-\#define	VMX_RETURN_VMLAUNCH	3
-\#define	VMX_RETURN_AST		4
-
-cpu
-	cpu_thread
-
-_kthread
-	t_lwp
-	_tu._ts._t_astflag	T_ASTFLAG
diff --git a/usr/src/uts/i86pc/io/vmm/vm/pmap.h b/usr/src/uts/i86pc/io/vmm/vm/pmap.h
new file mode 100644
index 0000000000..512fc4acee
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/vm/pmap.h
@@ -0,0 +1,27 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2017 Joyent, Inc.
+ */
+
+#ifndef _PMAP_VM_
+#define	_PMAP_VM_
+
+#include <machine/pmap.h>
+#include "vm_glue.h"
+
+void	pmap_invalidate_cache(void);
+void	pmap_get_mapping(pmap_t pmap, vm_offset_t va, uint64_t *ptr, int *num);
+int	pmap_emulate_accessed_dirty(pmap_t pmap, vm_offset_t va, int ftype);
+long	pmap_wired_count(pmap_t pmap);
+
+#endif /* _PMAP_VM_ */
diff --git a/usr/src/uts/i86pc/io/vmm/vm/vm_extern.h b/usr/src/uts/i86pc/io/vmm/vm/vm_extern.h
new file mode 100644
index 0000000000..92a959960a
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/vm/vm_extern.h
@@ -0,0 +1,35 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2017 Joyent, Inc.
+ */
+
+#ifndef _VM_EXTERN_H_
+#define	_VM_EXTERN_H_
+
+#include <sys/types.h>
+#include <vm/vm.h>
+
+struct vmspace;
+struct pmap;
+
+typedef int (*pmap_pinit_t)(struct pmap *pmap);
+
+struct vmspace *vmspace_alloc(vm_offset_t, vm_offset_t, pmap_pinit_t);
+void vmspace_free(struct vmspace *);
+
+int vm_fault(vm_map_t, vm_offset_t, vm_prot_t, int);
+int vm_fault_quick_hold_pages(vm_map_t map, vm_offset_t addr, vm_size_t len,
+    vm_prot_t prot, vm_page_t *ma, int max_count);
+
+
+#endif /* _VM_EXTERN_H_ */
diff --git a/usr/src/uts/i86pc/io/vmm/vm/vm_glue.h b/usr/src/uts/i86pc/io/vmm/vm/vm_glue.h
new file mode 100644
index 0000000000..cf1808c799
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/vm/vm_glue.h
@@ -0,0 +1,80 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2017 Joyent, Inc.
+ */
+
+#ifndef	_VM_GLUE_
+#define	_VM_GLUE_
+
+#include <vm/pmap.h>
+#include <vm/vm.h>
+#include <sys/cpuvar.h>
+
+struct vmspace;
+struct vm_map;
+struct pmap;
+struct vm_object;
+
+struct vm_map {
+	struct vmspace *vmm_space;
+};
+
+struct pmap {
+	void		*pm_pml4;
+	cpuset_t	pm_active;
+	long		pm_eptgen;
+
+	/* Implementation private */
+	enum pmap_type	pm_type;
+	void		*pm_map;
+};
+
+struct vmspace {
+	struct vm_map vm_map;
+
+	/* Implementation private */
+	kmutex_t	vms_lock;
+	struct pmap	vms_pmap;
+	uintptr_t	vms_size;	/* fixed after creation */
+
+	list_t		vms_maplist;
+};
+
+typedef pfn_t (*vm_pager_fn_t)(vm_object_t, uintptr_t, pfn_t *, uint_t *);
+
+struct vm_object {
+	kmutex_t	vmo_lock;
+	uint_t		vmo_refcnt;
+	objtype_t	vmo_type;
+	size_t		vmo_size;
+	vm_memattr_t	vmo_attr;
+	vm_pager_fn_t	vmo_pager;
+	void		*vmo_data;
+};
+
+struct vm_page {
+	kmutex_t		vmp_lock;
+	pfn_t			vmp_pfn;
+	struct vm_object	*vmp_obj_held;
+};
+
+/* Illumos-specific functions for setup and operation */
+int vm_segmap_obj(struct vmspace *, vm_object_t, struct as *, caddr_t *,
+    uint_t, uint_t, uint_t);
+int vm_segmap_space(struct vmspace *, off_t , struct as *, caddr_t *, off_t,
+    uint_t, uint_t, uint_t);
+void *vmspace_find_kva(struct vmspace *, uintptr_t, size_t);
+void vmm_arena_init(void);
+boolean_t vmm_arena_fini(void);
+
+#endif /* _VM_GLUE_ */
diff --git a/usr/src/uts/i86pc/io/vmm/vm/vm_map.h b/usr/src/uts/i86pc/io/vmm/vm/vm_map.h
new file mode 100644
index 0000000000..20b74d4d36
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/vm/vm_map.h
@@ -0,0 +1,63 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2017 Joyent, Inc.
+ */
+
+#ifndef	_VM_MAP_
+#define	_VM_MAP_
+
+#include "vm_glue.h"
+
+/*
+ * vm_map_wire and vm_map_unwire option flags
+ */
+#define VM_MAP_WIRE_SYSTEM	0	/* wiring in a kernel map */
+#define VM_MAP_WIRE_USER	1	/* wiring in a user map */
+
+#define VM_MAP_WIRE_NOHOLES	0	/* region must not have holes */
+#define VM_MAP_WIRE_HOLESOK	2	/* region may have holes */
+
+#define VM_MAP_WIRE_WRITE	4	/* Validate writable. */
+
+/*
+ * The following "find_space" options are supported by vm_map_find().
+ *
+ * For VMFS_ALIGNED_SPACE, the desired alignment is specified to
+ * the macro argument as log base 2 of the desired alignment.
+ */
+#define	VMFS_NO_SPACE		0	/* don't find; use the given range */
+#define	VMFS_ANY_SPACE		1	/* find range with any alignment */
+#define	VMFS_OPTIMAL_SPACE	2	/* find range with optimal alignment */
+#define	VMFS_SUPER_SPACE	3	/* find superpage-aligned range */
+#define	VMFS_ALIGNED_SPACE(x) ((x) << 8) /* find range with fixed alignment */
+
+/*
+ * vm_fault option flags
+ */
+#define	VM_FAULT_NORMAL	0	/* Nothing special */
+#define	VM_FAULT_WIRE	1	/* Wire the mapped page */
+#define	VM_FAULT_DIRTY	2	/* Dirty the page; use w/VM_PROT_COPY */
+
+
+
+pmap_t vmspace_pmap(struct vmspace *);
+
+int vm_map_find(vm_map_t, vm_object_t, vm_ooffset_t, vm_offset_t *, vm_size_t,
+    vm_offset_t, int, vm_prot_t, vm_prot_t, int);
+int vm_map_remove(vm_map_t, vm_offset_t, vm_offset_t);
+int vm_map_wire(vm_map_t map, vm_offset_t start, vm_offset_t end, int flags);
+
+long vmspace_resident_count(struct vmspace *vmspace);
+
+
+#endif /* _VM_MAP_ */
diff --git a/usr/src/uts/i86pc/io/vmm/vm/vm_object.h b/usr/src/uts/i86pc/io/vmm/vm/vm_object.h
new file mode 100644
index 0000000000..9aa7d91cf9
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/vm/vm_object.h
@@ -0,0 +1,30 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2017 Joyent, Inc.
+ */
+
+#ifndef	_VM_OBJECT_
+#define	_VM_OBJECT_
+
+#include "vm_glue.h"
+
+vm_object_t vm_object_allocate(objtype_t, vm_pindex_t);
+void vm_object_deallocate(vm_object_t);
+void vm_object_reference(vm_object_t);
+int vm_object_set_memattr(vm_object_t, vm_memattr_t);
+
+
+#define	VM_OBJECT_WLOCK(vmo)	mutex_enter(&(vmo)->vmo_lock)
+#define	VM_OBJECT_WUNLOCK(vmo)	mutex_exit(&(vmo)->vmo_lock)
+
+#endif /* _VM_OBJECT_ */
diff --git a/usr/src/uts/i86pc/io/vmm/vm/vm_page.h b/usr/src/uts/i86pc/io/vmm/vm/vm_page.h
new file mode 100644
index 0000000000..4559fe6d4c
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/vm/vm_page.h
@@ -0,0 +1,28 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2017 Joyent, Inc.
+ */
+
+
+#ifndef	_VM_PAGE_
+#define	_VM_PAGE_
+
+#include "vm_glue.h"
+
+void vm_page_lock(vm_page_t);
+void vm_page_unhold(vm_page_t);
+void vm_page_unlock(vm_page_t);
+
+#define	VM_PAGE_TO_PHYS(page)	(mmu_ptob((uintptr_t)((page)->vmp_pfn)))
+
+#endif /* _VM_PAGE_ */
diff --git a/usr/src/compat/freebsd/vm/pmap.h b/usr/src/uts/i86pc/io/vmm/vm/vm_pager.h
similarity index 68%
rename from usr/src/compat/freebsd/vm/pmap.h
rename to usr/src/uts/i86pc/io/vmm/vm/vm_pager.h
index 5958c4b101..11aa344f61 100644
--- a/usr/src/compat/freebsd/vm/pmap.h
+++ b/usr/src/uts/i86pc/io/vmm/vm/vm_pager.h
@@ -10,12 +10,14 @@
  */
 
 /*
- * Copyright 2014 Pluribus Networks Inc.
+ * Copyright 2017 Joyent, Inc.
  */
 
-#ifndef _COMPAT_FREEBSD_VM_PMAP_H_
-#define	_COMPAT_FREEBSD_VM_PMAP_H_
+#ifndef	_VM_PAGER_
+#define	_VM_PAGER_
 
-#include <machine/pmap.h>
+vm_object_t vm_pager_allocate(objtype_t, void *, vm_ooffset_t, vm_prot_t,
+    vm_ooffset_t, void *);
 
-#endif	/* _COMPAT_FREEBSD_VM_PMAP_H_ */
+
+#endif /* _VM_PAGER_ */
diff --git a/usr/src/uts/i86pc/io/vmm/vmm.c b/usr/src/uts/i86pc/io/vmm/vmm.c
index 54a20e8e56..0cd82a9309 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm.c
+++ b/usr/src/uts/i86pc/io/vmm/vmm.c
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/vmm.c 280929 2015-04-01 00:15:31Z tychon $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -40,7 +40,7 @@
  */
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/sys/amd64/vmm/vmm.c 280929 2015-04-01 00:15:31Z tychon $");
+__FBSDID("$FreeBSD$");
 
 #include <sys/param.h>
 #include <sys/systm.h>
@@ -52,16 +52,25 @@ __FBSDID("$FreeBSD: head/sys/amd64/vmm/vmm.c 280929 2015-04-01 00:15:31Z tychon
 #include <sys/lock.h>
 #include <sys/mutex.h>
 #include <sys/proc.h>
+#include <sys/rwlock.h>
 #include <sys/sched.h>
 #include <sys/smp.h>
-#include <x86/psl.h>
 #include <sys/systm.h>
 
 #include <vm/vm.h>
-
-#include <machine/vm.h>
+#include <vm/vm_object.h>
+#include <vm/vm_map.h>
+#include <vm/vm_page.h>
+#include <vm/pmap.h>
+#include <vm/vm_extern.h>
+#include <vm/vm_param.h>
+
+#ifdef __FreeBSD__
+#include <machine/cpu.h>
+#endif
 #include <machine/pcb.h>
 #include <machine/smp.h>
+#include <x86/psl.h>
 #include <x86/apicreg.h>
 
 #include <machine/vmm.h>
@@ -78,83 +87,124 @@ __FBSDID("$FreeBSD: head/sys/amd64/vmm/vmm.c 280929 2015-04-01 00:15:31Z tychon
 #include "vhpet.h"
 #include "vioapic.h"
 #include "vlapic.h"
-#include "vmm_ipi.h"
+#include "vpmtmr.h"
+#include "vrtc.h"
 #include "vmm_stat.h"
 #include "vmm_lapic.h"
 
-#ifdef	__FreeBSD__
 #include "io/ppt.h"
 #include "io/iommu.h"
-#endif
 
-struct vhpet;
-struct vioapic;
 struct vlapic;
 
+/*
+ * Initialization:
+ * (a) allocated when vcpu is created
+ * (i) initialized when vcpu is created and when it is reinitialized
+ * (o) initialized the first time the vcpu is created
+ * (x) initialized before use
+ */
 struct vcpu {
-	int		flags;
-	enum vcpu_state	state;
-	struct mtx	mtx;
-	int		hostcpu;	/* host cpuid this vcpu last ran on */
-	struct vlapic	*vlapic;
-	int		 vcpuid;
-	struct savefpu	*guestfpu;	/* guest fpu state */
-	void		*stats;
-	struct vm_exit	exitinfo;
+	struct mtx 	mtx;		/* (o) protects 'state' and 'hostcpu' */
+	enum vcpu_state	state;		/* (o) vcpu state */
+#ifndef __FreeBSD__
+	kcondvar_t	vcpu_cv;	/* (o) cpu waiter cv */
+	kcondvar_t	state_cv;	/* (o) IDLE-transition cv */
+#endif /* __FreeBSD__ */
+	int		hostcpu;	/* (o) vcpu's host cpu */
+	int		reqidle;	/* (i) request vcpu to idle */
+	struct vlapic	*vlapic;	/* (i) APIC device model */
+	enum x2apic_state x2apic_state;	/* (i) APIC mode */
+	uint64_t	exitintinfo;	/* (i) events pending at VM exit */
+	int		nmi_pending;	/* (i) NMI pending */
+	int		extint_pending;	/* (i) INTR pending */
+	int	exception_pending;	/* (i) exception pending */
+	int	exc_vector;		/* (x) exception collateral */
+	int	exc_errcode_valid;
+	uint32_t exc_errcode;
+	struct savefpu	*guestfpu;	/* (a,i) guest fpu state */
+	uint64_t	guest_xcr0;	/* (i) guest %xcr0 register */
+	void		*stats;		/* (a,i) statistics */
+	struct vm_exit	exitinfo;	/* (x) exit reason and collateral */
 	uint64_t	nextrip;	/* (x) next instruction to execute */
-	enum x2apic_state x2apic_state;
-	uint64_t	exitintinfo;
-	int		nmi_pending;
-	int		extint_pending;
-	struct vm_exception exception;
-	int		exception_pending;
 };
 
+#define	vcpu_lock_initialized(v) mtx_initialized(&((v)->mtx))
 #define	vcpu_lock_init(v)	mtx_init(&((v)->mtx), "vcpu lock", 0, MTX_SPIN)
 #define	vcpu_lock(v)		mtx_lock_spin(&((v)->mtx))
 #define	vcpu_unlock(v)		mtx_unlock_spin(&((v)->mtx))
 #define	vcpu_assert_locked(v)	mtx_assert(&((v)->mtx), MA_OWNED)
 
-#define	VM_MAX_MEMORY_SEGMENTS	8
+struct mem_seg {
+	size_t	len;
+	bool	sysmem;
+	struct vm_object *object;
+};
+#define	VM_MAX_MEMSEGS	3
+
+struct mem_map {
+	vm_paddr_t	gpa;
+	size_t		len;
+	vm_ooffset_t	segoff;
+	int		segid;
+	int		prot;
+	int		flags;
+};
+#define	VM_MAX_MEMMAPS	4
 
+/*
+ * Initialization:
+ * (o) initialized the first time the VM is created
+ * (i) initialized when VM is created and when it is reinitialized
+ * (x) initialized before use
+ */
 struct vm {
-	void		*cookie;	/* processor-specific data */
-	void		*iommu;		/* iommu-specific data */
-	struct vcpu	vcpu[VM_MAXCPU];
-	struct vhpet	*vhpet;
-	struct vioapic	*vioapic;	/* virtual ioapic */
-	struct vatpic	*vatpic;	/* virtual atpic */
-	struct vatpit	*vatpit;	/* virtual atpit */
-	int		num_mem_segs;
-	struct vm_memory_segment mem_segs[VM_MAX_MEMORY_SEGMENTS];
-	char		name[VM_MAX_NAMELEN];
-
-	/*
-	 * Set of active vcpus.
-	 * An active vcpu is one that has been started implicitly (BSP) or
-	 * explicitly (AP) by sending it a startup ipi.
-	 */
-	cpuset_t	active_cpus;
-
+	void		*cookie;		/* (i) cpu-specific data */
+	void		*iommu;			/* (x) iommu-specific data */
+	struct vhpet	*vhpet;			/* (i) virtual HPET */
+	struct vioapic	*vioapic;		/* (i) virtual ioapic */
+	struct vatpic	*vatpic;		/* (i) virtual atpic */
+	struct vatpit	*vatpit;		/* (i) virtual atpit */
+	struct vpmtmr	*vpmtmr;		/* (i) virtual ACPI PM timer */
+	struct vrtc	*vrtc;			/* (o) virtual RTC */
+	volatile cpuset_t active_cpus;		/* (i) active vcpus */
+	int		suspend;		/* (i) stop VM execution */
+	volatile cpuset_t suspended_cpus; 	/* (i) suspended vcpus */
+	volatile cpuset_t halted_cpus;		/* (x) cpus in a hard halt */
+	cpuset_t	rendezvous_req_cpus;	/* (x) rendezvous requested */
+	cpuset_t	rendezvous_done_cpus;	/* (x) rendezvous finished */
+	void		*rendezvous_arg;	/* (x) rendezvous func/arg */
 	vm_rendezvous_func_t rendezvous_func;
+	struct mtx	rendezvous_mtx;		/* (o) rendezvous lock */
+#ifndef __FreeBSD__
+	kcondvar_t	rendezvous_cv;		/* (0) rendezvous condvar */
+#endif
+	struct mem_map	mem_maps[VM_MAX_MEMMAPS]; /* (i) guest address space */
+	struct mem_seg	mem_segs[VM_MAX_MEMSEGS]; /* (o) guest memory regions */
+	struct vmspace	*vmspace;		/* (o) guest's address space */
+	char		name[VM_MAX_NAMELEN];	/* (o) virtual machine name */
+	struct vcpu	vcpu[VM_MAXCPU];	/* (i) guest vcpus */
+#ifndef __FreeBSD__
+	krwlock_t	ioport_rwlock;
+	list_t		ioport_hooks;
+#endif /* __FreeBSD__ */
 };
 
 static int vmm_initialized;
 
 static struct vmm_ops *ops;
-#define	VMM_INIT()	(ops != NULL ? (*ops->init)() : 0)
+#define	VMM_INIT(num)	(ops != NULL ? (*ops->init)(num) : 0)
 #define	VMM_CLEANUP()	(ops != NULL ? (*ops->cleanup)() : 0)
+#define	VMM_RESUME()	(ops != NULL ? (*ops->resume)() : 0)
 
-#define	VMINIT(vm)	(ops != NULL ? (*ops->vminit)(vm): NULL)
-#define	VMRUN(vmi, vcpu, rip) \
-	(ops != NULL ? (*ops->vmrun)(vmi, vcpu, rip) : ENXIO)
+#define	VMINIT(vm, pmap) (ops != NULL ? (*ops->vminit)(vm, pmap): NULL)
+#define	VMRUN(vmi, vcpu, rip, pmap, evinfo) \
+	(ops != NULL ? (*ops->vmrun)(vmi, vcpu, rip, pmap, evinfo) : ENXIO)
 #define	VMCLEANUP(vmi)	(ops != NULL ? (*ops->vmcleanup)(vmi) : NULL)
-#define	VMMMAP_SET(vmi, gpa, hpa, len, attr, prot, spm)			\
-    	(ops != NULL ? 							\
-    	(*ops->vmmmap_set)(vmi, gpa, hpa, len, attr, prot, spm) :	\
-	ENXIO)
-#define	VMMMAP_GET(vmi, gpa) \
-	(ops != NULL ? (*ops->vmmmap_get)(vmi, gpa) : ENXIO)
+#define	VMSPACE_ALLOC(min, max) \
+	(ops != NULL ? (*ops->vmspace_alloc)(min, max) : NULL)
+#define	VMSPACE_FREE(vmspace) \
+	(ops != NULL ? (*ops->vmspace_free)(vmspace) : ENXIO)
 #define	VMGETREG(vmi, vcpu, num, retval)		\
 	(ops != NULL ? (*ops->vmgetreg)(vmi, vcpu, num, retval) : ENXIO)
 #define	VMSETREG(vmi, vcpu, num, val)		\
@@ -180,40 +230,111 @@ static MALLOC_DEFINE(M_VM, "vm", "vm");
 /* statistics */
 static VMM_STAT(VCPU_TOTAL_RUNTIME, "vcpu total runtime");
 
+SYSCTL_NODE(_hw, OID_AUTO, vmm, CTLFLAG_RW, NULL, NULL);
+
+/*
+ * Halt the guest if all vcpus are executing a HLT instruction with
+ * interrupts disabled.
+ */
+static int halt_detection_enabled = 1;
+SYSCTL_INT(_hw_vmm, OID_AUTO, halt_detection, CTLFLAG_RDTUN,
+    &halt_detection_enabled, 0,
+    "Halt VM if all vcpus execute HLT with interrupts disabled");
+
 static int vmm_ipinum;
 SYSCTL_INT(_hw_vmm, OID_AUTO, ipinum, CTLFLAG_RD, &vmm_ipinum, 0,
     "IPI vector used for vcpu notifications");
 
+static int trace_guest_exceptions;
+SYSCTL_INT(_hw_vmm, OID_AUTO, trace_guest_exceptions, CTLFLAG_RDTUN,
+    &trace_guest_exceptions, 0,
+    "Trap into hypervisor on all guest exceptions and reflect them back");
+
+static void vm_free_memmap(struct vm *vm, int ident);
+static bool sysmem_mapping(struct vm *vm, struct mem_map *mm);
+static void vcpu_notify_event_locked(struct vcpu *vcpu, bool lapic_intr);
+
+#ifndef __FreeBSD__
+typedef struct vm_ioport_hook {
+	list_node_t	vmih_node;
+	uint_t		vmih_ioport;
+	void		*vmih_arg;
+	vmm_rmem_cb_t	vmih_rmem_cb;
+	vmm_wmem_cb_t	vmih_wmem_cb;
+} vm_ioport_hook_t;
+#endif /* __FreeBSD__ */
+
+#ifdef KTR
+static const char *
+vcpu_state2str(enum vcpu_state state)
+{
+
+	switch (state) {
+	case VCPU_IDLE:
+		return ("idle");
+	case VCPU_FROZEN:
+		return ("frozen");
+	case VCPU_RUNNING:
+		return ("running");
+	case VCPU_SLEEPING:
+		return ("sleeping");
+	default:
+		return ("unknown");
+	}
+}
+#endif
+
 static void
-vcpu_cleanup(struct vm *vm, int i)
+vcpu_cleanup(struct vm *vm, int i, bool destroy)
 {
 	struct vcpu *vcpu = &vm->vcpu[i];
 
 	VLAPIC_CLEANUP(vm->cookie, vcpu->vlapic);
-#ifdef	__FreeBSD__
-	vmm_stat_free(vcpu->stats);	
-#endif
-	fpu_save_area_free(vcpu->guestfpu);
+	if (destroy) {
+		vmm_stat_free(vcpu->stats);
+		fpu_save_area_free(vcpu->guestfpu);
+	}
 }
 
 static void
-vcpu_init(struct vm *vm, uint32_t vcpu_id)
+vcpu_init(struct vm *vm, int vcpu_id, bool create)
 {
 	struct vcpu *vcpu;
-	
+
+	KASSERT(vcpu_id >= 0 && vcpu_id < VM_MAXCPU,
+	    ("vcpu_init: invalid vcpu %d", vcpu_id));
+
 	vcpu = &vm->vcpu[vcpu_id];
 
-	vcpu_lock_init(vcpu);
-	vcpu->hostcpu = NOCPU;
-	vcpu->vcpuid = vcpu_id;
+	if (create) {
+#ifdef __FreeBSD__
+		KASSERT(!vcpu_lock_initialized(vcpu), ("vcpu %d already "
+		    "initialized", vcpu_id));
+#endif
+		vcpu_lock_init(vcpu);
+		vcpu->state = VCPU_IDLE;
+		vcpu->hostcpu = NOCPU;
+		vcpu->guestfpu = fpu_save_area_alloc();
+		vcpu->stats = vmm_stat_alloc();
+	}
+
 	vcpu->vlapic = VLAPIC_INIT(vm->cookie, vcpu_id);
 	vm_set_x2apic_state(vm, vcpu_id, X2APIC_DISABLED);
+	vcpu->reqidle = 0;
 	vcpu->exitintinfo = 0;
-	vcpu->guestfpu = fpu_save_area_alloc();
+	vcpu->nmi_pending = 0;
+	vcpu->extint_pending = 0;
+	vcpu->exception_pending = 0;
+	vcpu->guest_xcr0 = XFEATURE_ENABLED_X87;
 	fpu_save_area_reset(vcpu->guestfpu);
-#ifdef	__FreeBSD__
-	vcpu->stats = vmm_stat_alloc();
-#endif
+	vmm_stat_init(vcpu->stats);
+}
+
+int
+vcpu_trace_exceptions(struct vm *vm, int vcpuid)
+{
+
+	return (trace_guest_exceptions);
 }
 
 struct vm_exit *
@@ -229,14 +350,28 @@ vm_exitinfo(struct vm *vm, int cpuid)
 	return (&vcpu->exitinfo);
 }
 
+#ifdef __FreeBSD__
+static void
+vmm_resume(void)
+{
+	VMM_RESUME();
+}
+#endif
+
 static int
 vmm_init(void)
 {
 	int error;
 
 	vmm_host_state_init();
-#ifdef	__FreeBSD__
-	vmm_ipi_init();
+
+#ifdef __FreeBSD__
+	vmm_ipinum = lapic_ipi_alloc(&IDTVEC(justreturn));
+	if (vmm_ipinum < 0)
+		vmm_ipinum = IPI_AST;
+#else
+	/* XXX: verify for EPT settings */
+	vmm_ipinum = 0;
 #endif
 
 	error = vmm_mem_init();
@@ -250,10 +385,15 @@ vmm_init(void)
 	else
 		return (ENXIO);
 
-	return (VMM_INIT());
+#ifdef __FreeBSD__
+	vmm_resume_p = vmm_resume;
+#endif
+
+	return (VMM_INIT(vmm_ipinum));
 }
 
-#ifdef	__FreeBSD__
+#ifdef __FreeBSD__
+
 static int
 vmm_handler(module_t mod, int what, void *arg)
 {
@@ -262,8 +402,6 @@ vmm_handler(module_t mod, int what, void *arg)
 	switch (what) {
 	case MOD_LOAD:
 		vmmdev_init();
-		if (ppt_num_devices() > 0)
-			iommu_init();
 		error = vmm_init();
 		if (error == 0)
 			vmm_initialized = 1;
@@ -271,8 +409,10 @@ vmm_handler(module_t mod, int what, void *arg)
 	case MOD_UNLOAD:
 		error = vmmdev_cleanup();
 		if (error == 0) {
+			vmm_resume_p = NULL;
 			iommu_cleanup();
-			vmm_ipi_cleanup();
+			if (vmm_ipinum != IPI_AST)
+				lapic_ipi_free(vmm_ipinum);
 			error = VMM_CLEANUP();
 			/*
 			 * Something bad happened - prevent new
@@ -298,17 +438,14 @@ static moduledata_t vmm_kmod = {
 /*
  * vmm initialization has the following dependencies:
  *
- * - iommu initialization must happen after the pci passthru driver has had
- *   a chance to attach to any passthru devices (after SI_SUB_CONFIGURE).
- *
  * - VT-x initialization requires smp_rendezvous() and therefore must happen
  *   after SMP is fully functional (after SI_SUB_SMP).
  */
 DECLARE_MODULE(vmm, vmm_kmod, SI_SUB_SMP + 1, SI_ORDER_ANY);
 MODULE_VERSION(vmm, 1);
 
-SYSCTL_NODE(_hw, OID_AUTO, vmm, CTLFLAG_RW, NULL, NULL);
-#else
+#else /* __FreeBSD__ */
+
 int
 vmm_mod_load()
 {
@@ -335,20 +472,47 @@ vmm_mod_unload()
 
 	return (0);
 }
-#endif
+
+#endif /* __FreeBSD__ */
+
+static void
+vm_init(struct vm *vm, bool create)
+{
+	int i;
+
+	vm->cookie = VMINIT(vm, vmspace_pmap(vm->vmspace));
+	vm->iommu = NULL;
+	vm->vioapic = vioapic_init(vm);
+	vm->vhpet = vhpet_init(vm);
+	vm->vatpic = vatpic_init(vm);
+	vm->vatpit = vatpit_init(vm);
+	vm->vpmtmr = vpmtmr_init(vm);
+	if (create)
+		vm->vrtc = vrtc_init(vm);
+#ifndef __FreeBSD__
+	if (create) {
+		rw_init(&vm->ioport_rwlock, NULL, RW_DEFAULT, NULL);
+		list_create(&vm->ioport_hooks, sizeof (vm_ioport_hook_t),
+		    offsetof (vm_ioport_hook_t, vmih_node));
+	} else {
+		VERIFY(list_is_empty(&vm->ioport_hooks));
+	}
+#endif /* __FreeBSD__ */
+
+	CPU_ZERO(&vm->active_cpus);
+
+	vm->suspend = 0;
+	CPU_ZERO(&vm->suspended_cpus);
+
+	for (i = 0; i < VM_MAXCPU; i++)
+		vcpu_init(vm, i, create);
+}
 
 int
 vm_create(const char *name, struct vm **retvm)
 {
-	int i;
 	struct vm *vm;
-#ifdef	__FreeBSD__
-	vm_paddr_t maxaddr;
-#endif
-
-#if notyet
-	const int BSP = 0;
-#endif
+	struct vmspace *vmspace;
 
 	/*
 	 * If vmm.ko could not be successfully initialized then don't attempt
@@ -360,101 +524,94 @@ vm_create(const char *name, struct vm **retvm)
 	if (name == NULL || strlen(name) >= VM_MAX_NAMELEN)
 		return (EINVAL);
 
+	vmspace = VMSPACE_ALLOC(0, VM_MAXUSER_ADDRESS);
+	if (vmspace == NULL)
+		return (ENOMEM);
+
 	vm = malloc(sizeof(struct vm), M_VM, M_WAITOK | M_ZERO);
 	strcpy(vm->name, name);
-	vm->cookie = VMINIT(vm);
+	vm->vmspace = vmspace;
+	mtx_init(&vm->rendezvous_mtx, "vm rendezvous lock", 0, MTX_DEF);
 
-	vm->vioapic = vioapic_init(vm);
-	vm->vhpet = vhpet_init(vm);
-	vm->vatpic = vatpic_init(vm);
-	vm->vatpit = vatpit_init(vm);
-
-	for (i = 0; i < VM_MAXCPU; i++) {
-		vcpu_init(vm, i);
-	}
-
-#ifdef	__FreeBSD__
-	maxaddr = vmm_mem_maxaddr();
-	vm->iommu = iommu_create_domain(maxaddr);
-#endif
+	vm_init(vm, true);
 
 	*retvm = vm;
 	return (0);
 }
 
 static void
-vm_free_mem_seg(struct vm *vm, struct vm_memory_segment *seg)
+vm_cleanup(struct vm *vm, bool destroy)
 {
-	size_t len;
-	vm_paddr_t hpa;
-#ifdef	__FreeBSD__
-	void *host_domain;
+	struct mem_map *mm;
+	int i;
 
-	host_domain = iommu_host_domain();
-#endif
+	ppt_unassign_all(vm);
 
-	len = 0;
-	while (len < seg->len) {
-		hpa = vm_gpa2hpa(vm, seg->gpa + len, PAGE_SIZE);
-		if (hpa == (vm_paddr_t)-1) {
-			panic("vm_free_mem_segs: cannot free hpa "
-			      "associated with gpa 0x%016lx", seg->gpa + len);
-		}
+	if (vm->iommu != NULL)
+		iommu_destroy_domain(vm->iommu);
 
-#ifdef	__FreeBSD__
-		/*
-		 * Remove the 'gpa' to 'hpa' mapping in VMs domain.
-		 * And resurrect the 1:1 mapping for 'hpa' in 'host_domain'.
-		 */
-		iommu_remove_mapping(vm->iommu, seg->gpa + len, PAGE_SIZE);
-		iommu_create_mapping(host_domain, hpa, hpa, PAGE_SIZE);
-#endif
+	if (destroy)
+		vrtc_cleanup(vm->vrtc);
+	else
+		vrtc_reset(vm->vrtc);
+	vpmtmr_cleanup(vm->vpmtmr);
+	vatpit_cleanup(vm->vatpit);
+	vhpet_cleanup(vm->vhpet);
+	vatpic_cleanup(vm->vatpic);
+	vioapic_cleanup(vm->vioapic);
 
-		vmm_mem_free(hpa, PAGE_SIZE);
+	for (i = 0; i < VM_MAXCPU; i++)
+		vcpu_cleanup(vm, i, destroy);
 
-		len += PAGE_SIZE;
-	}
+	VMCLEANUP(vm->cookie);
 
-#ifdef	__FreeBSD__
 	/*
-	 * Invalidate cached translations associated with 'vm->iommu' since
-	 * we have now moved some pages from it.
+	 * System memory is removed from the guest address space only when
+	 * the VM is destroyed. This is because the mapping remains the same
+	 * across VM reset.
+	 *
+	 * Device memory can be relocated by the guest (e.g. using PCI BARs)
+	 * so those mappings are removed on a VM reset.
 	 */
-	iommu_invalidate_tlb(vm->iommu);
-#endif
+	for (i = 0; i < VM_MAX_MEMMAPS; i++) {
+		mm = &vm->mem_maps[i];
+		if (destroy || !sysmem_mapping(vm, mm))
+			vm_free_memmap(vm, i);
+	}
+
+	if (destroy) {
+		for (i = 0; i < VM_MAX_MEMSEGS; i++)
+			vm_free_memseg(vm, i);
 
-	bzero(seg, sizeof(struct vm_memory_segment));
+		VMSPACE_FREE(vm->vmspace);
+		vm->vmspace = NULL;
+	}
 }
 
 void
 vm_destroy(struct vm *vm)
 {
-	int i;
-
-#ifdef	__FreeBSD__
-	ppt_unassign_all(vm);
-#endif
-
-	for (i = 0; i < vm->num_mem_segs; i++)
-		vm_free_mem_seg(vm, &vm->mem_segs[i]);
-
-	vm->num_mem_segs = 0;
-
-	for (i = 0; i < VM_MAXCPU; i++)
-		vcpu_cleanup(vm, i);
-
-	vatpit_cleanup(vm->vatpit);
-	vhpet_cleanup(vm->vhpet);
-	vatpic_cleanup(vm->vatpic);
-	vioapic_cleanup(vm->vioapic);
+	vm_cleanup(vm, true);
+	free(vm, M_VM);
+}
 
-#ifdef	__FreeBSD__
-	iommu_destroy_domain(vm->iommu);
-#endif
+int
+vm_reinit(struct vm *vm)
+{
+	int error;
 
-	VMCLEANUP(vm->cookie);
+	/*
+	 * A virtual machine can be reset only if all vcpus are suspended.
+	 */
+	if (CPU_CMP(&vm->suspended_cpus, &vm->active_cpus) == 0) {
+		vm_cleanup(vm, false);
+		vm_init(vm, false);
+		error = 0;
+	} else {
+		error = EBUSY;
+	}
 
-	free(vm, M_VM);
+	return (error);
 }
 
 const char *
@@ -463,168 +620,405 @@ vm_name(struct vm *vm)
 	return (vm->name);
 }
 
-#ifdef	__FreeBSD__
 int
 vm_map_mmio(struct vm *vm, vm_paddr_t gpa, size_t len, vm_paddr_t hpa)
 {
-	const boolean_t spok = TRUE;	/* superpage mappings are ok */
+	vm_object_t obj;
 
-	return (VMMMAP_SET(vm->cookie, gpa, hpa, len, VM_MEMATTR_UNCACHEABLE,
-			   VM_PROT_RW, spok));
+	if ((obj = vmm_mmio_alloc(vm->vmspace, gpa, len, hpa)) == NULL)
+		return (ENOMEM);
+	else
+		return (0);
 }
 
 int
 vm_unmap_mmio(struct vm *vm, vm_paddr_t gpa, size_t len)
 {
-	const boolean_t spok = TRUE;	/* superpage mappings are ok */
 
-	return (VMMMAP_SET(vm->cookie, gpa, 0, len, 0,
-			   VM_PROT_NONE, spok));
+	vmm_mmio_free(vm->vmspace, gpa, len);
+	return (0);
 }
-#endif
 
 /*
- * Returns TRUE if 'gpa' is available for allocation and FALSE otherwise
+ * Return 'true' if 'gpa' is allocated in the guest address space.
+ *
+ * This function is called in the context of a running vcpu which acts as
+ * an implicit lock on 'vm->mem_maps[]'.
  */
-static boolean_t
-vm_gpa_available(struct vm *vm, vm_paddr_t gpa)
+bool
+vm_mem_allocated(struct vm *vm, int vcpuid, vm_paddr_t gpa)
 {
+	struct mem_map *mm;
 	int i;
-	vm_paddr_t gpabase, gpalimit;
 
-	if (gpa & PAGE_MASK)
-		panic("vm_gpa_available: gpa (0x%016lx) not page aligned", gpa);
+#ifdef INVARIANTS
+	int hostcpu, state;
+	state = vcpu_get_state(vm, vcpuid, &hostcpu);
+	KASSERT(state == VCPU_RUNNING && hostcpu == curcpu,
+	    ("%s: invalid vcpu state %d/%d", __func__, state, hostcpu));
+#endif
 
-	for (i = 0; i < vm->num_mem_segs; i++) {
-		gpabase = vm->mem_segs[i].gpa;
-		gpalimit = gpabase + vm->mem_segs[i].len;
-		if (gpa >= gpabase && gpa < gpalimit)
-			return (FALSE);
+	for (i = 0; i < VM_MAX_MEMMAPS; i++) {
+		mm = &vm->mem_maps[i];
+		if (mm->len != 0 && gpa >= mm->gpa && gpa < mm->gpa + mm->len)
+			return (true);		/* 'gpa' is sysmem or devmem */
 	}
 
-	return (TRUE);
+	if (ppt_is_mmio(vm, gpa))
+		return (true);			/* 'gpa' is pci passthru mmio */
+
+	return (false);
 }
 
 int
-vm_malloc(struct vm *vm, vm_paddr_t gpa, size_t len)
+vm_alloc_memseg(struct vm *vm, int ident, size_t len, bool sysmem)
 {
-	int error, available, allocated;
-	struct vm_memory_segment *seg;
-	vm_paddr_t g, hpa;
-#ifdef	__FreeBSD__
-	void *host_domain;
-#endif
+	struct mem_seg *seg;
+	vm_object_t obj;
 
-	const boolean_t spok = TRUE;	/* superpage mappings are ok */
+	if (ident < 0 || ident >= VM_MAX_MEMSEGS)
+		return (EINVAL);
 
-	if ((gpa & PAGE_MASK) || (len & PAGE_MASK) || len == 0)
+	if (len == 0 || (len & PAGE_MASK))
 		return (EINVAL);
-	
-	available = allocated = 0;
-	g = gpa;
-	while (g < gpa + len) {
-		if (vm_gpa_available(vm, g))
-			available++;
+
+	seg = &vm->mem_segs[ident];
+	if (seg->object != NULL) {
+		if (seg->len == len && seg->sysmem == sysmem)
+			return (EEXIST);
 		else
-			allocated++;
+			return (EINVAL);
+	}
+
+	obj = vm_object_allocate(OBJT_DEFAULT, len >> PAGE_SHIFT);
+	if (obj == NULL)
+		return (ENOMEM);
+
+	seg->len = len;
+	seg->object = obj;
+	seg->sysmem = sysmem;
+	return (0);
+}
+
+int
+vm_get_memseg(struct vm *vm, int ident, size_t *len, bool *sysmem,
+    vm_object_t *objptr)
+{
+	struct mem_seg *seg;
 
-		g += PAGE_SIZE;
+	if (ident < 0 || ident >= VM_MAX_MEMSEGS)
+		return (EINVAL);
+
+	seg = &vm->mem_segs[ident];
+	if (len)
+		*len = seg->len;
+	if (sysmem)
+		*sysmem = seg->sysmem;
+	if (objptr)
+		*objptr = seg->object;
+	return (0);
+}
+
+void
+vm_free_memseg(struct vm *vm, int ident)
+{
+	struct mem_seg *seg;
+
+	KASSERT(ident >= 0 && ident < VM_MAX_MEMSEGS,
+	    ("%s: invalid memseg ident %d", __func__, ident));
+
+	seg = &vm->mem_segs[ident];
+	if (seg->object != NULL) {
+		vm_object_deallocate(seg->object);
+		bzero(seg, sizeof(struct mem_seg));
 	}
+}
 
-	/*
-	 * If there are some allocated and some available pages in the address
-	 * range then it is an error.
-	 */
-	if (allocated && available)
+int
+vm_mmap_memseg(struct vm *vm, vm_paddr_t gpa, int segid, vm_ooffset_t first,
+    size_t len, int prot, int flags)
+{
+	struct mem_seg *seg;
+	struct mem_map *m, *map;
+	vm_ooffset_t last;
+	int i, error;
+
+	if (prot == 0 || (prot & ~(VM_PROT_ALL)) != 0)
 		return (EINVAL);
 
-	/*
-	 * If the entire address range being requested has already been
-	 * allocated then there isn't anything more to do.
-	 */
-	if (allocated && available == 0)
-		return (0);
+	if (flags & ~VM_MEMMAP_F_WIRED)
+		return (EINVAL);
 
-	if (vm->num_mem_segs >= VM_MAX_MEMORY_SEGMENTS)
-		return (E2BIG);
+	if (segid < 0 || segid >= VM_MAX_MEMSEGS)
+		return (EINVAL);
 
-#ifdef	__FreeBSD__
-	host_domain = iommu_host_domain();
-#endif
+	seg = &vm->mem_segs[segid];
+	if (seg->object == NULL)
+		return (EINVAL);
 
-	seg = &vm->mem_segs[vm->num_mem_segs];
+	last = first + len;
+	if (first < 0 || first >= last || last > seg->len)
+		return (EINVAL);
+
+	if ((gpa | first | last) & PAGE_MASK)
+		return (EINVAL);
 
-	error = 0;
-	seg->gpa = gpa;
-	seg->len = 0;
-	while (seg->len < len) {
-		hpa = vmm_mem_alloc(PAGE_SIZE);
-		if (hpa == 0) {
-			error = ENOMEM;
+	map = NULL;
+	for (i = 0; i < VM_MAX_MEMMAPS; i++) {
+		m = &vm->mem_maps[i];
+		if (m->len == 0) {
+			map = m;
 			break;
 		}
+	}
 
-		error = VMMMAP_SET(vm->cookie, gpa + seg->len, hpa, PAGE_SIZE,
-				   VM_MEMATTR_WRITE_BACK, VM_PROT_ALL, spok);
-		if (error)
-			break;
+	if (map == NULL)
+		return (ENOSPC);
 
-#ifdef	__FreeBSD__
-		/*
-		 * Remove the 1:1 mapping for 'hpa' from the 'host_domain'.
-		 * Add mapping for 'gpa + seg->len' to 'hpa' in the VMs domain.
-		 */
-		iommu_remove_mapping(host_domain, hpa, PAGE_SIZE);
-		iommu_create_mapping(vm->iommu, gpa + seg->len, hpa, PAGE_SIZE);
-#endif
+	error = vm_map_find(&vm->vmspace->vm_map, seg->object, first, &gpa,
+	    len, 0, VMFS_NO_SPACE, prot, prot, 0);
+	if (error != KERN_SUCCESS)
+		return (EFAULT);
+
+	vm_object_reference(seg->object);
 
-		seg->len += PAGE_SIZE;
+	if ((flags & VM_MEMMAP_F_WIRED) != 0) {
+		error = vm_map_wire(&vm->vmspace->vm_map, gpa, gpa + len,
+		    VM_MAP_WIRE_USER | VM_MAP_WIRE_NOHOLES);
+		if (error != KERN_SUCCESS) {
+			vm_map_remove(&vm->vmspace->vm_map, gpa, gpa + len);
+			return (EFAULT);
+		}
 	}
 
-	if (error) {
-		vm_free_mem_seg(vm, seg);
-		return (error);
+	map->gpa = gpa;
+	map->len = len;
+	map->segoff = first;
+	map->segid = segid;
+	map->prot = prot;
+	map->flags = flags;
+	return (0);
+}
+
+int
+vm_mmap_getnext(struct vm *vm, vm_paddr_t *gpa, int *segid,
+    vm_ooffset_t *segoff, size_t *len, int *prot, int *flags)
+{
+	struct mem_map *mm, *mmnext;
+	int i;
+
+	mmnext = NULL;
+	for (i = 0; i < VM_MAX_MEMMAPS; i++) {
+		mm = &vm->mem_maps[i];
+		if (mm->len == 0 || mm->gpa < *gpa)
+			continue;
+		if (mmnext == NULL || mm->gpa < mmnext->gpa)
+			mmnext = mm;
+	}
+
+	if (mmnext != NULL) {
+		*gpa = mmnext->gpa;
+		if (segid)
+			*segid = mmnext->segid;
+		if (segoff)
+			*segoff = mmnext->segoff;
+		if (len)
+			*len = mmnext->len;
+		if (prot)
+			*prot = mmnext->prot;
+		if (flags)
+			*flags = mmnext->flags;
+		return (0);
+	} else {
+		return (ENOENT);
+	}
+}
+
+static void
+vm_free_memmap(struct vm *vm, int ident)
+{
+	struct mem_map *mm;
+	int error;
+
+	mm = &vm->mem_maps[ident];
+	if (mm->len) {
+		error = vm_map_remove(&vm->vmspace->vm_map, mm->gpa,
+		    mm->gpa + mm->len);
+		KASSERT(error == KERN_SUCCESS, ("%s: vm_map_remove error %d",
+		    __func__, error));
+		bzero(mm, sizeof(struct mem_map));
+	}
+}
+
+static __inline bool
+sysmem_mapping(struct vm *vm, struct mem_map *mm)
+{
+
+	if (mm->len != 0 && vm->mem_segs[mm->segid].sysmem)
+		return (true);
+	else
+		return (false);
+}
+
+static vm_paddr_t
+sysmem_maxaddr(struct vm *vm)
+{
+	struct mem_map *mm;
+	vm_paddr_t maxaddr;
+	int i;
+
+	maxaddr = 0;
+	for (i = 0; i < VM_MAX_MEMMAPS; i++) {
+		mm = &vm->mem_maps[i];
+		if (sysmem_mapping(vm, mm)) {
+			if (maxaddr < mm->gpa + mm->len)
+				maxaddr = mm->gpa + mm->len;
+		}
+	}
+	return (maxaddr);
+}
+
+static void
+vm_iommu_modify(struct vm *vm, boolean_t map)
+{
+	int i, sz;
+	vm_paddr_t gpa, hpa;
+	struct mem_map *mm;
+	void *vp, *cookie, *host_domain;
+
+	sz = PAGE_SIZE;
+	host_domain = iommu_host_domain();
+
+	for (i = 0; i < VM_MAX_MEMMAPS; i++) {
+		mm = &vm->mem_maps[i];
+		if (!sysmem_mapping(vm, mm))
+			continue;
+
+		if (map) {
+			KASSERT((mm->flags & VM_MEMMAP_F_IOMMU) == 0,
+			    ("iommu map found invalid memmap %#lx/%#lx/%#x",
+			    mm->gpa, mm->len, mm->flags));
+			if ((mm->flags & VM_MEMMAP_F_WIRED) == 0)
+				continue;
+			mm->flags |= VM_MEMMAP_F_IOMMU;
+		} else {
+			if ((mm->flags & VM_MEMMAP_F_IOMMU) == 0)
+				continue;
+			mm->flags &= ~VM_MEMMAP_F_IOMMU;
+			KASSERT((mm->flags & VM_MEMMAP_F_WIRED) != 0,
+			    ("iommu unmap found invalid memmap %#lx/%#lx/%#x",
+			    mm->gpa, mm->len, mm->flags));
+		}
+
+		gpa = mm->gpa;
+		while (gpa < mm->gpa + mm->len) {
+			vp = vm_gpa_hold(vm, -1, gpa, PAGE_SIZE, VM_PROT_WRITE,
+					 &cookie);
+			KASSERT(vp != NULL, ("vm(%s) could not map gpa %#lx",
+			    vm_name(vm), gpa));
+
+			vm_gpa_release(cookie);
+
+			hpa = DMAP_TO_PHYS((uintptr_t)vp);
+			if (map) {
+				iommu_create_mapping(vm->iommu, gpa, hpa, sz);
+				iommu_remove_mapping(host_domain, hpa, sz);
+			} else {
+				iommu_remove_mapping(vm->iommu, gpa, sz);
+				iommu_create_mapping(host_domain, hpa, hpa, sz);
+			}
+
+			gpa += PAGE_SIZE;
+		}
 	}
 
-#ifdef	__FreeBSD__
 	/*
-	 * Invalidate cached translations associated with 'host_domain' since
-	 * we have now moved some pages from it.
+	 * Invalidate the cached translations associated with the domain
+	 * from which pages were removed.
 	 */
-	iommu_invalidate_tlb(host_domain);
-#endif
+	if (map)
+		iommu_invalidate_tlb(host_domain);
+	else
+		iommu_invalidate_tlb(vm->iommu);
+}
+
+#define	vm_iommu_unmap(vm)	vm_iommu_modify((vm), FALSE)
+#define	vm_iommu_map(vm)	vm_iommu_modify((vm), TRUE)
+
+int
+vm_unassign_pptdev(struct vm *vm, int bus, int slot, int func)
+{
+	int error;
+
+	error = ppt_unassign_device(vm, bus, slot, func);
+	if (error)
+		return (error);
 
-	vm->num_mem_segs++;
+	if (ppt_assigned_devices(vm) == 0)
+		vm_iommu_unmap(vm);
 
 	return (0);
 }
 
-vm_paddr_t
-vm_gpa2hpa(struct vm *vm, vm_paddr_t gpa, size_t len)
+int
+vm_assign_pptdev(struct vm *vm, int bus, int slot, int func)
 {
-	vm_paddr_t nextpage;
+	int error;
+	vm_paddr_t maxaddr;
 
-	nextpage = rounddown(gpa + PAGE_SIZE, PAGE_SIZE);
-	if (len > nextpage - gpa)
-		panic("vm_gpa2hpa: invalid gpa/len: 0x%016lx/%lu", gpa, len);
+	/* Set up the IOMMU to do the 'gpa' to 'hpa' translation */
+	if (ppt_assigned_devices(vm) == 0) {
+		KASSERT(vm->iommu == NULL,
+		    ("vm_assign_pptdev: iommu must be NULL"));
+		maxaddr = sysmem_maxaddr(vm);
+		vm->iommu = iommu_create_domain(maxaddr);
+		if (vm->iommu == NULL)
+			return (ENXIO);
+		vm_iommu_map(vm);
+	}
 
-	return (VMMMAP_GET(vm->cookie, gpa));
+	error = ppt_assign_device(vm, bus, slot, func);
+	return (error);
 }
 
 void *
-vm_gpa_hold(struct vm *vm, vm_paddr_t gpa, size_t len, int reqprot,
+vm_gpa_hold(struct vm *vm, int vcpuid, vm_paddr_t gpa, size_t len, int reqprot,
 	    void **cookie)
 {
-#ifdef	__FreeBSD__
-	int count, pageoff;
+	int i, count, pageoff;
+	struct mem_map *mm;
 	vm_page_t m;
-
+#ifdef INVARIANTS
+	/*
+	 * All vcpus are frozen by ioctls that modify the memory map
+	 * (e.g. VM_MMAP_MEMSEG). Therefore 'vm->memmap[]' stability is
+	 * guaranteed if at least one vcpu is in the VCPU_FROZEN state.
+	 */
+	int state;
+	KASSERT(vcpuid >= -1 && vcpuid < VM_MAXCPU, ("%s: invalid vcpuid %d",
+	    __func__, vcpuid));
+	for (i = 0; i < VM_MAXCPU; i++) {
+		if (vcpuid != -1 && vcpuid != i)
+			continue;
+		state = vcpu_get_state(vm, i, NULL);
+		KASSERT(state == VCPU_FROZEN, ("%s: invalid vcpu state %d",
+		    __func__, state));
+	}
+#endif
 	pageoff = gpa & PAGE_MASK;
 	if (len > PAGE_SIZE - pageoff)
 		panic("vm_gpa_hold: invalid gpa/len: 0x%016lx/%lu", gpa, len);
 
-	count = vm_fault_quick_hold_pages(&vm->vmspace->vm_map,
-	    trunc_page(gpa), PAGE_SIZE, reqprot, &m, 1);
+	count = 0;
+	for (i = 0; i < VM_MAX_MEMMAPS; i++) {
+		mm = &vm->mem_maps[i];
+		if (sysmem_mapping(vm, mm) && gpa >= mm->gpa &&
+		    gpa < mm->gpa + mm->len) {
+			count = vm_fault_quick_hold_pages(&vm->vmspace->vm_map,
+			    trunc_page(gpa), PAGE_SIZE, reqprot, &m, 1);
+			break;
+		}
+	}
 
 	if (count == 1) {
 		*cookie = m;
@@ -633,47 +1027,16 @@ vm_gpa_hold(struct vm *vm, vm_paddr_t gpa, size_t len, int reqprot,
 		*cookie = NULL;
 		return (NULL);
 	}
-#else
-	int pageoff;
-	vm_paddr_t hpa;
-
-	pageoff = gpa & PAGE_MASK;
-	if (len > PAGE_SIZE - pageoff)
-		panic("vm_gpa_hold: invalid gpa/len: 0x%016lx/%lu", gpa, len);
-
-	hpa = vm_gpa2hpa(vm, gpa, len);
-	if (hpa == (vm_paddr_t)-1)
-		return (NULL);
-
-	return (hat_kpm_pfn2va(btop(hpa)) + pageoff);
-#endif
 }
 
 void
 vm_gpa_release(void *cookie)
 {
-#ifdef	__FreeBSD__
 	vm_page_t m = cookie;
 
 	vm_page_lock(m);
 	vm_page_unhold(m);
 	vm_page_unlock(m);
-#endif
-}
-
-int
-vm_gpabase2memseg(struct vm *vm, vm_paddr_t gpabase,
-		  struct vm_memory_segment *seg)
-{
-	int i;
-
-	for (i = 0; i < vm->num_mem_segs; i++) {
-		if (gpabase == vm->mem_segs[i].gpa) {
-			*seg = vm->mem_segs[i];
-			return (0);
-		}
-	}
-	return (-1);
 }
 
 int
@@ -782,6 +1145,10 @@ restore_guest_fpustate(struct vcpu *vcpu)
 	fpu_stop_emulating();
 	fpurestore(vcpu->guestfpu);
 
+	/* restore guest XCR0 if XSAVE is enabled in the host */
+	if (rcr4() & CR4_XSAVE)
+		load_xcr(0, vcpu->guest_xcr0);
+
 	/*
 	 * The FPU is now "dirty" with the guest's state so turn on emulation
 	 * to trap any access to the FPU by the host.
@@ -796,6 +1163,12 @@ save_guest_fpustate(struct vcpu *vcpu)
 	if ((rcr0() & CR0_TS) == 0)
 		panic("fpu emulation not enabled in host!");
 
+	/* save guest XCR0 and restore host XCR0 */
+	if (rcr4() & CR4_XSAVE) {
+		vcpu->guest_xcr0 = rxcr(0);
+		load_xcr(0, vmm_get_host_xcr0());
+	}
+
 	/* save guest FPU state */
 	fpu_stop_emulating();
 	fpusave(vcpu->guestfpu);
@@ -805,11 +1178,13 @@ save_guest_fpustate(struct vcpu *vcpu)
 static VMM_STAT(VCPU_IDLE_TICKS, "number of ticks vcpu was idle");
 
 static int
-vcpu_set_state_locked(struct vcpu *vcpu, enum vcpu_state newstate,
+vcpu_set_state_locked(struct vm *vm, int vcpuid, enum vcpu_state newstate,
     bool from_idle)
 {
+	struct vcpu *vcpu;
 	int error;
 
+	vcpu = &vm->vcpu[vcpuid];
 	vcpu_assert_locked(vcpu);
 
 	/*
@@ -818,8 +1193,17 @@ vcpu_set_state_locked(struct vcpu *vcpu, enum vcpu_state newstate,
 	 * ioctl() operating on a vcpu at any point.
 	 */
 	if (from_idle) {
-		while (vcpu->state != VCPU_IDLE)
+		while (vcpu->state != VCPU_IDLE) {
+			vcpu->reqidle = 1;
+			vcpu_notify_event_locked(vcpu, false);
+			VCPU_CTR1(vm, vcpuid, "vcpu state change from %s to "
+			    "idle requested", vcpu_state2str(vcpu->state));
+#ifdef __FreeBSD__
 			msleep_spin(&vcpu->state, &vcpu->mtx, "vmstat", hz);
+#else
+			cv_wait(&vcpu->state_cv, &vcpu->mtx.m);
+#endif
+		}
 	} else {
 		KASSERT(vcpu->state != VCPU_IDLE, ("invalid transition from "
 		    "vcpu idle state"));
@@ -856,14 +1240,22 @@ vcpu_set_state_locked(struct vcpu *vcpu, enum vcpu_state newstate,
 	if (error)
 		return (EBUSY);
 
+	VCPU_CTR2(vm, vcpuid, "vcpu state changed from %s to %s",
+	    vcpu_state2str(vcpu->state), vcpu_state2str(newstate));
+
 	vcpu->state = newstate;
 	if (newstate == VCPU_RUNNING)
 		vcpu->hostcpu = curcpu;
 	else
 		vcpu->hostcpu = NOCPU;
 
-	if (newstate == VCPU_IDLE)
+	if (newstate == VCPU_IDLE) {
+#ifdef __FreeBSD__
 		wakeup(&vcpu->state);
+#else
+		cv_broadcast(&vcpu->state_cv);
+#endif
+	}
 
 	return (0);
 }
@@ -878,76 +1270,215 @@ vcpu_require_state(struct vm *vm, int vcpuid, enum vcpu_state newstate)
 }
 
 static void
-vcpu_require_state_locked(struct vcpu *vcpu, enum vcpu_state newstate)
+vcpu_require_state_locked(struct vm *vm, int vcpuid, enum vcpu_state newstate)
 {
 	int error;
 
-	if ((error = vcpu_set_state_locked(vcpu, newstate, false)) != 0)
+	if ((error = vcpu_set_state_locked(vm, vcpuid, newstate, false)) != 0)
 		panic("Error %d setting state to %d", error, newstate);
 }
 
+static void
+vm_set_rendezvous_func(struct vm *vm, vm_rendezvous_func_t func)
+{
+
+	KASSERT(mtx_owned(&vm->rendezvous_mtx), ("rendezvous_mtx not locked"));
+
+	/*
+	 * Update 'rendezvous_func' and execute a write memory barrier to
+	 * ensure that it is visible across all host cpus. This is not needed
+	 * for correctness but it does ensure that all the vcpus will notice
+	 * that the rendezvous is requested immediately.
+	 */
+	vm->rendezvous_func = func;
+#ifdef __FreeBSD__
+	wmb();
+#else
+	membar_producer();
+#endif
+}
+
+#define	RENDEZVOUS_CTR0(vm, vcpuid, fmt)				\
+	do {								\
+		if (vcpuid >= 0)					\
+			VCPU_CTR0(vm, vcpuid, fmt);			\
+		else							\
+			VM_CTR0(vm, fmt);				\
+	} while (0)
+
+static void
+vm_handle_rendezvous(struct vm *vm, int vcpuid)
+{
+
+	KASSERT(vcpuid == -1 || (vcpuid >= 0 && vcpuid < VM_MAXCPU),
+	    ("vm_handle_rendezvous: invalid vcpuid %d", vcpuid));
+
+	mtx_lock(&vm->rendezvous_mtx);
+	while (vm->rendezvous_func != NULL) {
+		/* 'rendezvous_req_cpus' must be a subset of 'active_cpus' */
+		CPU_AND(&vm->rendezvous_req_cpus, &vm->active_cpus);
+
+		if (vcpuid != -1 &&
+		    CPU_ISSET(vcpuid, &vm->rendezvous_req_cpus) &&
+		    !CPU_ISSET(vcpuid, &vm->rendezvous_done_cpus)) {
+			VCPU_CTR0(vm, vcpuid, "Calling rendezvous func");
+			(*vm->rendezvous_func)(vm, vcpuid, vm->rendezvous_arg);
+			CPU_SET(vcpuid, &vm->rendezvous_done_cpus);
+		}
+		if (CPU_CMP(&vm->rendezvous_req_cpus,
+		    &vm->rendezvous_done_cpus) == 0) {
+			VCPU_CTR0(vm, vcpuid, "Rendezvous completed");
+			vm_set_rendezvous_func(vm, NULL);
+#ifdef __FreeBSD__
+			wakeup(&vm->rendezvous_func);
+#else
+			cv_broadcast(&vm->rendezvous_cv);
+#endif
+			break;
+		}
+		RENDEZVOUS_CTR0(vm, vcpuid, "Wait for rendezvous completion");
+#ifdef __FreeBSD__
+		mtx_sleep(&vm->rendezvous_func, &vm->rendezvous_mtx, 0,
+		    "vmrndv", 0);
+#else
+		cv_wait(&vm->rendezvous_cv, &vm->rendezvous_mtx.m);
+#endif
+	}
+	mtx_unlock(&vm->rendezvous_mtx);
+}
+
 /*
  * Emulate a guest 'hlt' by sleeping until the vcpu is ready to run.
  */
 static int
 vm_handle_hlt(struct vm *vm, int vcpuid, bool intr_disabled, bool *retu)
 {
-#ifdef	__FreeBSD__
-	struct vm_exit *vmexit;
-#endif
 	struct vcpu *vcpu;
-	int t, timo, spindown;
+	const char *wmesg;
+	int t, vcpu_halted, vm_halted;
+
+	KASSERT(!CPU_ISSET(vcpuid, &vm->halted_cpus), ("vcpu already halted"));
 
 	vcpu = &vm->vcpu[vcpuid];
-	spindown = 0;
+	vcpu_halted = 0;
+	vm_halted = 0;
 
 	vcpu_lock(vcpu);
+	while (1) {
+		/*
+		 * Do a final check for pending NMI or interrupts before
+		 * really putting this thread to sleep. Also check for
+		 * software events that would cause this vcpu to wakeup.
+		 *
+		 * These interrupts/events could have happened after the
+		 * vcpu returned from VMRUN() and before it acquired the
+		 * vcpu lock above.
+		 */
+		if (vm->rendezvous_func != NULL || vm->suspend || vcpu->reqidle)
+			break;
+		if (vm_nmi_pending(vm, vcpuid))
+			break;
+		if (!intr_disabled) {
+			if (vm_extint_pending(vm, vcpuid) ||
+			    vlapic_pending_intr(vcpu->vlapic, NULL)) {
+				break;
+			}
+		}
 
-	/*
-	 * Do a final check for pending NMI or interrupts before
-	 * really putting this thread to sleep.
-	 *
-	 * These interrupts could have happened any time after we
-	 * returned from VMRUN() and before we grabbed the vcpu lock.
-	 */
-	if (vm->rendezvous_func == NULL &&
-	    !vm_nmi_pending(vm, vcpuid) &&
-	    (intr_disabled || !vlapic_pending_intr(vcpu->vlapic, NULL))) {
-		t = ticks;
-		vcpu_require_state_locked(vcpu, VCPU_SLEEPING);
-		if (vlapic_enabled(vcpu->vlapic)) {
-			/*
-			 * XXX msleep_spin() is not interruptible so use the
-			 * 'timo' to put an upper bound on the sleep time.
-			 */
-			timo = hz;
-			msleep_spin(vcpu, &vcpu->mtx, "vmidle", timo);
+		/* Don't go to sleep if the vcpu thread needs to yield */
+		if (vcpu_should_yield(vm, vcpuid))
+			break;
+
+		/*
+		 * Some Linux guests implement "halt" by having all vcpus
+		 * execute HLT with interrupts disabled. 'halted_cpus' keeps
+		 * track of the vcpus that have entered this state. When all
+		 * vcpus enter the halted state the virtual machine is halted.
+		 */
+		if (intr_disabled) {
+			wmesg = "vmhalt";
+			VCPU_CTR0(vm, vcpuid, "Halted");
+			if (!vcpu_halted && halt_detection_enabled) {
+				vcpu_halted = 1;
+				CPU_SET_ATOMIC(vcpuid, &vm->halted_cpus);
+			}
+			if (CPU_CMP(&vm->halted_cpus, &vm->active_cpus) == 0) {
+				vm_halted = 1;
+				break;
+			}
 		} else {
-			/*
-			 * Spindown the vcpu if the apic is disabled and it
-			 * had entered the halted state.
-			 */
-			spindown = 1;
+			wmesg = "vmidle";
 		}
-		vcpu_require_state_locked(vcpu, VCPU_FROZEN);
+
+		t = ticks;
+		vcpu_require_state_locked(vm, vcpuid, VCPU_SLEEPING);
+#ifdef __FreeBSD__
+		/*
+		 * XXX msleep_spin() cannot be interrupted by signals so
+		 * wake up periodically to check pending signals.
+		 */
+		msleep_spin(vcpu, &vcpu->mtx, wmesg, hz);
+#else
+		/*
+		 * Fortunately, cv_wait_sig can be interrupted by signals, so
+		 * there is no need to periodically wake up.
+		 */
+		(void) cv_wait_sig(&vcpu->vcpu_cv, &vcpu->mtx.m);
+#endif
+		vcpu_require_state_locked(vm, vcpuid, VCPU_FROZEN);
 		vmm_stat_incr(vm, vcpuid, VCPU_IDLE_TICKS, ticks - t);
 	}
+
+	if (vcpu_halted)
+		CPU_CLR_ATOMIC(vcpuid, &vm->halted_cpus);
+
 	vcpu_unlock(vcpu);
 
-#ifdef	__FreeBSD__
-	/*
-	 * Since 'vm_deactivate_cpu()' grabs a sleep mutex we must call it
-	 * outside the confines of the vcpu spinlock.
-	 */
-	if (spindown) {
-		*retu = true;
-		vmexit = vm_exitinfo(vm, vcpuid);
-		vmexit->exitcode = VM_EXITCODE_SPINDOWN_CPU;
-		vm_deactivate_cpu(vm, vcpuid);
-		VCPU_CTR0(vm, vcpuid, "spinning down cpu");
+	if (vm_halted)
+		vm_suspend(vm, VM_SUSPEND_HALT);
+
+	return (0);
+}
+
+static int
+vm_handle_paging(struct vm *vm, int vcpuid, bool *retu)
+{
+	int rv, ftype;
+	struct vm_map *map;
+	struct vcpu *vcpu;
+	struct vm_exit *vme;
+
+	vcpu = &vm->vcpu[vcpuid];
+	vme = &vcpu->exitinfo;
+
+	KASSERT(vme->inst_length == 0, ("%s: invalid inst_length %d",
+	    __func__, vme->inst_length));
+
+	ftype = vme->u.paging.fault_type;
+	KASSERT(ftype == VM_PROT_READ ||
+	    ftype == VM_PROT_WRITE || ftype == VM_PROT_EXECUTE,
+	    ("vm_handle_paging: invalid fault_type %d", ftype));
+
+	if (ftype == VM_PROT_READ || ftype == VM_PROT_WRITE) {
+		rv = pmap_emulate_accessed_dirty(vmspace_pmap(vm->vmspace),
+		    vme->u.paging.gpa, ftype);
+		if (rv == 0) {
+			VCPU_CTR2(vm, vcpuid, "%s bit emulation for gpa %#lx",
+			    ftype == VM_PROT_READ ? "accessed" : "dirty",
+			    vme->u.paging.gpa);
+			goto done;
+		}
 	}
-#endif
 
+	map = &vm->vmspace->vm_map;
+	rv = vm_fault(map, vme->u.paging.gpa, ftype, VM_FAULT_NORMAL);
+
+	VCPU_CTR3(vm, vcpuid, "vm_handle_paging rv = %d, gpa = %#lx, "
+	    "ftype = %d", rv, vme->u.paging.gpa, ftype);
+
+	if (rv != KERN_SUCCESS)
+		return (EFAULT);
+done:
 	return (0);
 }
 
@@ -962,11 +1493,14 @@ vm_handle_inst_emul(struct vm *vm, int vcpuid, bool *retu)
 	mem_region_read_t mread;
 	mem_region_write_t mwrite;
 	enum vm_cpu_mode cpu_mode;
-	int cs_d, error, length;
+	int cs_d, error, fault;
 
 	vcpu = &vm->vcpu[vcpuid];
 	vme = &vcpu->exitinfo;
 
+	KASSERT(vme->inst_length == 0, ("%s: invalid inst_length %d",
+	    __func__, vme->inst_length));
+
 	gla = vme->u.inst_emul.gla;
 	gpa = vme->u.inst_emul.gpa;
 	cs_base = vme->u.inst_emul.cs_base;
@@ -975,66 +1509,210 @@ vm_handle_inst_emul(struct vm *vm, int vcpuid, bool *retu)
 	paging = &vme->u.inst_emul.paging;
 	cpu_mode = paging->cpu_mode;
 
-	VCPU_CTR1(vm, vcpuid, "inst_emul fault accessing gpa %#lx", gpa);
+	VCPU_CTR1(vm, vcpuid, "inst_emul fault accessing gpa %#lx", gpa);
+
+	/* Fetch, decode and emulate the faulting instruction */
+	if (vie->num_valid == 0) {
+		error = vmm_fetch_instruction(vm, vcpuid, paging, vme->rip +
+		    cs_base, VIE_INST_SIZE, vie, &fault);
+	} else {
+		/*
+		 * The instruction bytes have already been copied into 'vie'
+		 */
+		error = fault = 0;
+	}
+	if (error || fault)
+		return (error);
+
+	if (vmm_decode_instruction(vm, vcpuid, gla, cpu_mode, cs_d, vie) != 0) {
+		VCPU_CTR1(vm, vcpuid, "Error decoding instruction at %#lx",
+		    vme->rip + cs_base);
+		*retu = true;	    /* dump instruction bytes in userspace */
+		return (0);
+	}
+
+	/*
+	 * Update 'nextrip' based on the length of the emulated instruction.
+	 */
+	vme->inst_length = vie->num_processed;
+	vcpu->nextrip += vie->num_processed;
+	VCPU_CTR1(vm, vcpuid, "nextrip updated to %#lx after instruction "
+	    "decoding", vcpu->nextrip);
+ 
+	/* return to userland unless this is an in-kernel emulated device */
+	if (gpa >= DEFAULT_APIC_BASE && gpa < DEFAULT_APIC_BASE + PAGE_SIZE) {
+		mread = lapic_mmio_read;
+		mwrite = lapic_mmio_write;
+	} else if (gpa >= VIOAPIC_BASE && gpa < VIOAPIC_BASE + VIOAPIC_SIZE) {
+		mread = vioapic_mmio_read;
+		mwrite = vioapic_mmio_write;
+	} else if (gpa >= VHPET_BASE && gpa < VHPET_BASE + VHPET_SIZE) {
+		mread = vhpet_mmio_read;
+		mwrite = vhpet_mmio_write;
+	} else {
+		*retu = true;
+		return (0);
+	}
+
+	error = vmm_emulate_instruction(vm, vcpuid, gpa, vie, paging,
+	    mread, mwrite, retu);
+
+	return (error);
+}
+
+static int
+vm_handle_suspend(struct vm *vm, int vcpuid, bool *retu)
+{
+	int i, done;
+	struct vcpu *vcpu;
+
+	done = 0;
+	vcpu = &vm->vcpu[vcpuid];
+
+	CPU_SET_ATOMIC(vcpuid, &vm->suspended_cpus);
+
+	/*
+	 * Wait until all 'active_cpus' have suspended themselves.
+	 *
+	 * Since a VM may be suspended at any time including when one or
+	 * more vcpus are doing a rendezvous we need to call the rendezvous
+	 * handler while we are waiting to prevent a deadlock.
+	 */
+	vcpu_lock(vcpu);
+	while (1) {
+		if (CPU_CMP(&vm->suspended_cpus, &vm->active_cpus) == 0) {
+			VCPU_CTR0(vm, vcpuid, "All vcpus suspended");
+			break;
+		}
+
+		if (vm->rendezvous_func == NULL) {
+			VCPU_CTR0(vm, vcpuid, "Sleeping during suspend");
+			vcpu_require_state_locked(vm, vcpuid, VCPU_SLEEPING);
+#ifdef __FreeBSD__
+			msleep_spin(vcpu, &vcpu->mtx, "vmsusp", hz);
+#else
+			cv_wait(&vcpu->vcpu_cv, &vcpu->mtx.m);
+#endif
+			vcpu_require_state_locked(vm, vcpuid, VCPU_FROZEN);
+		} else {
+			VCPU_CTR0(vm, vcpuid, "Rendezvous during suspend");
+			vcpu_unlock(vcpu);
+			vm_handle_rendezvous(vm, vcpuid);
+			vcpu_lock(vcpu);
+		}
+	}
+	vcpu_unlock(vcpu);
+
+	/*
+	 * Wakeup the other sleeping vcpus and return to userspace.
+	 */
+	for (i = 0; i < VM_MAXCPU; i++) {
+		if (CPU_ISSET(i, &vm->suspended_cpus)) {
+			vcpu_notify_event(vm, i, false);
+		}
+	}
+
+	*retu = true;
+	return (0);
+}
+
+static int
+vm_handle_reqidle(struct vm *vm, int vcpuid, bool *retu)
+{
+	struct vcpu *vcpu = &vm->vcpu[vcpuid];
+
+	vcpu_lock(vcpu);
+	KASSERT(vcpu->reqidle, ("invalid vcpu reqidle %d", vcpu->reqidle));
+	vcpu->reqidle = 0;
+	vcpu_unlock(vcpu);
+	*retu = true;
+	return (0);
+}
+
+int
+vm_suspend(struct vm *vm, enum vm_suspend_how how)
+{
+	int i;
+
+	if (how <= VM_SUSPEND_NONE || how >= VM_SUSPEND_LAST)
+		return (EINVAL);
 
-	/* Fetch, decode and emulate the faulting instruction */
-	if (vie->num_valid == 0) {
-		/*
-		 * If the instruction length is not known then assume a
-		 * maximum size instruction.
-		 */
-		length = vme->inst_length ? vme->inst_length : VIE_INST_SIZE;
-		error = vmm_fetch_instruction(vm, vcpuid, paging, vme->rip +
-		    cs_base, length, vie);
-	} else {
-		/*
-		 * The instruction bytes have already been copied into 'vie'
-		 */
-		error = 0;
+	if (atomic_cmpset_int((uint_t *)&vm->suspend, 0, how) == 0) {
+		VM_CTR2(vm, "virtual machine already suspended %d/%d",
+		    vm->suspend, how);
+		return (EALREADY);
 	}
-	if (error == 1)
-		return (0);		/* Resume guest to handle page fault */
-	else if (error == -1)
-		return (EFAULT);
-	else if (error != 0)
-		panic("%s: vmm_fetch_instruction error %d", __func__, error);
 
-	if (vmm_decode_instruction(vm, vcpuid, gla, cpu_mode, cs_d, vie) != 0)
-		return (EFAULT);
+	VM_CTR1(vm, "virtual machine successfully suspended %d", how);
 
 	/*
-	 * If the instruction length was not specified then update it now
-	 * along with 'nextrip'.
+	 * Notify all active vcpus that they are now suspended.
 	 */
-	if (vme->inst_length == 0) {
-		vme->inst_length = vie->num_processed;
-		vcpu->nextrip += vie->num_processed;
-	}
- 
-	/* return to userland unless this is an in-kernel emulated device */
-	if (gpa >= DEFAULT_APIC_BASE && gpa < DEFAULT_APIC_BASE + PAGE_SIZE) {
-		mread = lapic_mmio_read;
-		mwrite = lapic_mmio_write;
-	} else if (gpa >= VIOAPIC_BASE && gpa < VIOAPIC_BASE + VIOAPIC_SIZE) {
-		mread = vioapic_mmio_read;
-		mwrite = vioapic_mmio_write;
-	} else if (gpa >= VHPET_BASE && gpa < VHPET_BASE + VHPET_SIZE) {
-		mread = vhpet_mmio_read;
-		mwrite = vhpet_mmio_write;
-	} else {
-		*retu = true;
-		return (0);
+	for (i = 0; i < VM_MAXCPU; i++) {
+		if (CPU_ISSET(i, &vm->active_cpus))
+			vcpu_notify_event(vm, i, false);
 	}
 
-	error = vmm_emulate_instruction(vm, vcpuid, gpa, vie, paging,
-	    mread, mwrite, retu);
+	return (0);
+}
 
-	return (error);
+void
+vm_exit_suspended(struct vm *vm, int vcpuid, uint64_t rip)
+{
+	struct vm_exit *vmexit;
+
+	KASSERT(vm->suspend > VM_SUSPEND_NONE && vm->suspend < VM_SUSPEND_LAST,
+	    ("vm_exit_suspended: invalid suspend type %d", vm->suspend));
+
+	vmexit = vm_exitinfo(vm, vcpuid);
+	vmexit->rip = rip;
+	vmexit->inst_length = 0;
+	vmexit->exitcode = VM_EXITCODE_SUSPENDED;
+	vmexit->u.suspended.how = vm->suspend;
+}
+
+void
+vm_exit_rendezvous(struct vm *vm, int vcpuid, uint64_t rip)
+{
+	struct vm_exit *vmexit;
+
+	KASSERT(vm->rendezvous_func != NULL, ("rendezvous not in progress"));
+
+	vmexit = vm_exitinfo(vm, vcpuid);
+	vmexit->rip = rip;
+	vmexit->inst_length = 0;
+	vmexit->exitcode = VM_EXITCODE_RENDEZVOUS;
+	vmm_stat_incr(vm, vcpuid, VMEXIT_RENDEZVOUS, 1);
+}
+
+void
+vm_exit_reqidle(struct vm *vm, int vcpuid, uint64_t rip)
+{
+	struct vm_exit *vmexit;
+
+	vmexit = vm_exitinfo(vm, vcpuid);
+	vmexit->rip = rip;
+	vmexit->inst_length = 0;
+	vmexit->exitcode = VM_EXITCODE_REQIDLE;
+	vmm_stat_incr(vm, vcpuid, VMEXIT_REQIDLE, 1);
+}
+
+void
+vm_exit_astpending(struct vm *vm, int vcpuid, uint64_t rip)
+{
+	struct vm_exit *vmexit;
+
+	vmexit = vm_exitinfo(vm, vcpuid);
+	vmexit->rip = rip;
+	vmexit->inst_length = 0;
+	vmexit->exitcode = VM_EXITCODE_BOGUS;
+	vmm_stat_incr(vm, vcpuid, VMEXIT_ASTPENDING, 1);
 }
 
 int
 vm_run(struct vm *vm, struct vm_run *vmrun)
 {
+	struct vm_eventinfo evinfo;
 	int error, vcpuid;
 	struct vcpu *vcpu;
 #ifdef	__FreeBSD__
@@ -1043,22 +1721,39 @@ vm_run(struct vm *vm, struct vm_run *vmrun)
 	uint64_t tscval;
 	struct vm_exit *vme;
 	bool retu, intr_disabled;
+	pmap_t pmap;
 
 	vcpuid = vmrun->cpuid;
 
 	if (vcpuid < 0 || vcpuid >= VM_MAXCPU)
 		return (EINVAL);
 
+	if (!CPU_ISSET(vcpuid, &vm->active_cpus))
+		return (EINVAL);
+
+	if (CPU_ISSET(vcpuid, &vm->suspended_cpus))
+		return (EINVAL);
+
+	pmap = vmspace_pmap(vm->vmspace);
 	vcpu = &vm->vcpu[vcpuid];
 	vme = &vcpu->exitinfo;
+	evinfo.rptr = &vm->rendezvous_func;
+	evinfo.sptr = &vm->suspend;
+	evinfo.iptr = &vcpu->reqidle;
 restart:
 	critical_enter();
 
+	KASSERT(!CPU_ISSET(curcpu, &pmap->pm_active),
+	    ("vm_run: absurd pm_active"));
+
 	tscval = rdtsc();
 
 #ifdef	__FreeBSD__
 	pcb = PCPU_GET(curpcb);
 	set_pcb_flags(pcb, PCB_FULL_IRET);
+#else
+	/* Force a trip through update_sregs to reload %fs/%gs and friends */
+	ttolwp(curthread)->lwp_pcb.pcb_rupdate = 1;
 #endif
 
 #ifndef	__FreeBSD__
@@ -1068,7 +1763,7 @@ restart:
 	restore_guest_fpustate(vcpu);
 
 	vcpu_require_state(vm, vcpuid, VCPU_RUNNING);
-	error = VMRUN(vm->cookie, vcpuid, vcpu->nextrip);
+	error = VMRUN(vm->cookie, vcpuid, vcpu->nextrip, pmap, &evinfo);
 	vcpu_require_state(vm, vcpuid, VCPU_FROZEN);
 
 	save_guest_fpustate(vcpu);
@@ -1085,10 +1780,27 @@ restart:
 		retu = false;
 		vcpu->nextrip = vme->rip + vme->inst_length;
 		switch (vme->exitcode) {
+		case VM_EXITCODE_REQIDLE:
+			error = vm_handle_reqidle(vm, vcpuid, &retu);
+			break;
+		case VM_EXITCODE_SUSPENDED:
+			error = vm_handle_suspend(vm, vcpuid, &retu);
+			break;
+		case VM_EXITCODE_IOAPIC_EOI:
+			vioapic_process_eoi(vm, vcpuid,
+			    vme->u.ioapic_eoi.vector);
+			break;
+		case VM_EXITCODE_RENDEZVOUS:
+			vm_handle_rendezvous(vm, vcpuid);
+			error = 0;
+			break;
 		case VM_EXITCODE_HLT:
 			intr_disabled = ((vme->u.hlt.rflags & PSL_I) == 0);
 			error = vm_handle_hlt(vm, vcpuid, intr_disabled, &retu);
 			break;
+		case VM_EXITCODE_PAGING:
+			error = vm_handle_paging(vm, vcpuid, &retu);
+			break;
 		case VM_EXITCODE_INST_EMUL:
 			error = vm_handle_inst_emul(vm, vcpuid, &retu);
 			break;
@@ -1096,18 +1808,23 @@ restart:
 		case VM_EXITCODE_INOUT_STR:
 			error = vm_handle_inout(vm, vcpuid, vme, &retu);
 			break;
+		case VM_EXITCODE_MONITOR:
+		case VM_EXITCODE_MWAIT:
+			vm_inject_ud(vm, vcpuid);
+			break;
 		default:
 			retu = true;	/* handled in userland */
 			break;
 		}
 	}
 
-	if (error == 0 && retu == false) {
+	if (error == 0 && retu == false)
 		goto restart;
-	}
+
+	VCPU_CTR2(vm, vcpuid, "retu %d/%d", error, vme->exitcode);
 
 	/* copy the exit information */
-	bcopy(vme, &vmrun->vm_exit, sizeof(struct vm_exit));
+	bcopy(vme, &vmrun->vm_exit, sizeof (struct vm_exit));
 	return (error);
 }
 
@@ -1295,11 +2012,11 @@ vcpu_exception_intinfo(struct vcpu *vcpu)
 	uint64_t info = 0;
 
 	if (vcpu->exception_pending) {
-		info = vcpu->exception.vector & 0xff;
+		info = vcpu->exc_vector & 0xff;
 		info |= VM_INTINFO_VALID | VM_INTINFO_HWEXCEPTION;
-		if (vcpu->exception.error_code_valid) {
+		if (vcpu->exc_errcode_valid) {
 			info |= VM_INTINFO_DEL_ERRCODE;
-			info |= (uint64_t)vcpu->exception.error_code << 32;
+			info |= (uint64_t)vcpu->exc_errcode << 32;
 		}
 	}
 	return (info);
@@ -1324,7 +2041,7 @@ vm_entry_intinfo(struct vm *vm, int vcpuid, uint64_t *retinfo)
 		info2 = vcpu_exception_intinfo(vcpu);
 		vcpu->exception_pending = 0;
 		VCPU_CTR2(vm, vcpuid, "Exception %d delivered: %#lx",
-		    vcpu->exception.vector, info2);
+		    vcpu->exc_vector, info2);
 	}
 
 	if ((info1 & VM_INTINFO_VALID) && (info2 & VM_INTINFO_VALID)) {
@@ -1348,76 +2065,93 @@ vm_entry_intinfo(struct vm *vm, int vcpuid, uint64_t *retinfo)
 }
 
 int
-vm_inject_exception(struct vm *vm, int vcpuid, struct vm_exception *exception)
+vm_get_intinfo(struct vm *vm, int vcpuid, uint64_t *info1, uint64_t *info2)
+{
+	struct vcpu *vcpu;
+
+	if (vcpuid < 0 || vcpuid >= VM_MAXCPU)
+		return (EINVAL);
+
+	vcpu = &vm->vcpu[vcpuid];
+	*info1 = vcpu->exitintinfo;
+	*info2 = vcpu_exception_intinfo(vcpu);
+	return (0);
+}
+
+int
+vm_inject_exception(struct vm *vm, int vcpuid, int vector, int errcode_valid,
+    uint32_t errcode, int restart_instruction)
 {
 	struct vcpu *vcpu;
+	uint64_t regval;
+	int error;
 
 	if (vcpuid < 0 || vcpuid >= VM_MAXCPU)
 		return (EINVAL);
 
-	if (exception->vector < 0 || exception->vector >= 32)
+	if (vector < 0 || vector >= 32)
+		return (EINVAL);
+
+	/*
+	 * A double fault exception should never be injected directly into
+	 * the guest. It is a derived exception that results from specific
+	 * combinations of nested faults.
+	 */
+	if (vector == IDT_DF)
 		return (EINVAL);
 
 	vcpu = &vm->vcpu[vcpuid];
 
 	if (vcpu->exception_pending) {
 		VCPU_CTR2(vm, vcpuid, "Unable to inject exception %d due to "
-		    "pending exception %d", exception->vector,
-		    vcpu->exception.vector);
+		    "pending exception %d", vector, vcpu->exc_vector);
 		return (EBUSY);
 	}
 
-	vcpu->exception_pending = 1;
-	vcpu->exception = *exception;
-	VCPU_CTR1(vm, vcpuid, "Exception %d pending", exception->vector);
-	return (0);
-}
+	if (errcode_valid) {
+		/*
+		 * Exceptions don't deliver an error code in real mode.
+		 */
+		error = vm_get_register(vm, vcpuid, VM_REG_GUEST_CR0, &regval);
+		KASSERT(!error, ("%s: error %d getting CR0", __func__, error));
+		if (!(regval & CR0_PE))
+			errcode_valid = 0;
+	}
 
-int
-vm_exception_pending(struct vm *vm, int vcpuid, struct vm_exception *exception)
-{
-	struct vcpu *vcpu;
-	int pending;
+	/*
+	 * From section 26.6.1 "Interruptibility State" in Intel SDM:
+	 *
+	 * Event blocking by "STI" or "MOV SS" is cleared after guest executes
+	 * one instruction or incurs an exception.
+	 */
+	error = vm_set_register(vm, vcpuid, VM_REG_GUEST_INTR_SHADOW, 0);
+	KASSERT(error == 0, ("%s: error %d clearing interrupt shadow",
+	    __func__, error));
 
-	KASSERT(vcpuid >= 0 && vcpuid < VM_MAXCPU, ("invalid vcpu %d", vcpuid));
+	if (restart_instruction)
+		vm_restart_instruction(vm, vcpuid);
 
-	vcpu = &vm->vcpu[vcpuid];
-	pending = vcpu->exception_pending;
-	if (pending) {
-		vcpu->exception_pending = 0;
-		*exception = vcpu->exception;
-		VCPU_CTR1(vm, vcpuid, "Exception %d delivered",
-		    exception->vector);
-	}
-	return (pending);
+	vcpu->exception_pending = 1;
+	vcpu->exc_vector = vector;
+	vcpu->exc_errcode = errcode;
+	vcpu->exc_errcode_valid = errcode_valid;
+	VCPU_CTR1(vm, vcpuid, "Exception %d pending", vector);
+	return (0);
 }
 
 void
 vm_inject_fault(void *vmarg, int vcpuid, int vector, int errcode_valid,
     int errcode)
 {
-	struct vm_exception exception;
-	struct vm_exit *vmexit;
 	struct vm *vm;
-	int error;
+	int error, restart_instruction;
 
 	vm = vmarg;
+	restart_instruction = 1;
 
-	exception.vector = vector;
-	exception.error_code = errcode;
-	exception.error_code_valid = errcode_valid;
-	error = vm_inject_exception(vm, vcpuid, &exception);
+	error = vm_inject_exception(vm, vcpuid, vector, errcode_valid,
+	    errcode, restart_instruction);
 	KASSERT(error == 0, ("vm_inject_exception error %d", error));
-
-	/*
-	 * A fault-like exception allows the instruction to be restarted
-	 * after the exception handler returns.
-	 *
-	 * By setting the inst_length to 0 we ensure that the instruction
-	 * pointer remains at the faulting instruction.
-	 */
-	vmexit = vm_exitinfo(vm, vcpuid);
-	vmexit->inst_length = 0;
 }
 
 void
@@ -1450,7 +2184,6 @@ vm_inject_nmi(struct vm *vm, int vcpuid)
 
 	vcpu->nmi_pending = 1;
 	vcpu_notify_event(vm, vcpuid, false);
-
 	return (0);
 }
 
@@ -1498,7 +2231,6 @@ vm_inject_extint(struct vm *vm, int vcpuid)
 
 	vcpu->extint_pending = 1;
 	vcpu_notify_event(vm, vcpuid, false);
-
 	return (0);
 }
 
@@ -1556,22 +2288,24 @@ vm_set_capability(struct vm *vm, int vcpu, int type, int val)
 	return (VMSETCAP(vm->cookie, vcpu, type, val));
 }
 
-struct vhpet *
-vm_hpet(struct vm *vm)
+struct vlapic *
+vm_lapic(struct vm *vm, int cpu)
 {
-	return (vm->vhpet);
+	return (vm->vcpu[cpu].vlapic);
 }
 
 struct vioapic *
 vm_ioapic(struct vm *vm)
 {
+
 	return (vm->vioapic);
 }
 
-struct vlapic *
-vm_lapic(struct vm *vm, int cpu)
+struct vhpet *
+vm_hpet(struct vm *vm)
 {
-	return (vm->vcpu[cpu].vlapic);
+
+	return (vm->vhpet);
 }
 
 #ifdef	__FreeBSD__
@@ -1596,7 +2330,7 @@ vmm_is_pptdev(int bus, int slot, int func)
 	/* set pptdevs="1/2/3 4/5/6 7/8/9 10/11/12" */
 	found = 0;
 	for (i = 0; names[i] != NULL && !found; i++) {
-		cp = val = getenv(names[i]);
+		cp = val = kern_getenv(names[i]);
 		while (cp != NULL && *cp != '\0') {
 			if ((cp2 = strchr(cp, ' ')) != NULL)
 				*cp2 = '\0';
@@ -1638,7 +2372,7 @@ vcpu_set_state(struct vm *vm, int vcpuid, enum vcpu_state newstate,
 	vcpu = &vm->vcpu[vcpuid];
 
 	vcpu_lock(vcpu);
-	error = vcpu_set_state_locked(vcpu, newstate, from_idle);
+	error = vcpu_set_state_locked(vm, vcpuid, newstate, from_idle);
 	vcpu_unlock(vcpu);
 
 	return (error);
@@ -1686,6 +2420,13 @@ vm_active_cpus(struct vm *vm)
 	return (vm->active_cpus);
 }
 
+cpuset_t
+vm_suspended_cpus(struct vm *vm)
+{
+
+	return (vm->suspended_cpus);
+}
+
 void *
 vcpu_stats(struct vm *vm, int vcpuid)
 {
@@ -1727,15 +2468,11 @@ vm_set_x2apic_state(struct vm *vm, int vcpuid, enum x2apic_state state)
  * - If the vcpu is running on a different host_cpu then an IPI will be directed
  *   to the host_cpu to cause the vcpu to trap into the hypervisor.
  */
-void
-vcpu_notify_event(struct vm *vm, int vcpuid, bool lapic_intr)
+static void
+vcpu_notify_event_locked(struct vcpu *vcpu, bool lapic_intr)
 {
 	int hostcpu;
-	struct vcpu *vcpu;
-
-	vcpu = &vm->vcpu[vcpuid];
 
-	vcpu_lock(vcpu);
 	hostcpu = vcpu->hostcpu;
 	if (vcpu->state == VCPU_RUNNING) {
 		KASSERT(hostcpu != NOCPU, ("vcpu running on invalid hostcpu"));
@@ -1757,12 +2494,33 @@ vcpu_notify_event(struct vm *vm, int vcpuid, bool lapic_intr)
 	} else {
 		KASSERT(hostcpu == NOCPU, ("vcpu state %d not consistent "
 		    "with hostcpu %d", vcpu->state, hostcpu));
-		if (vcpu->state == VCPU_SLEEPING)
+		if (vcpu->state == VCPU_SLEEPING) {
+#ifdef __FreeBSD__
 			wakeup_one(vcpu);
+#else
+			cv_signal(&vcpu->vcpu_cv);
+#endif
+		}
 	}
+}
+
+void
+vcpu_notify_event(struct vm *vm, int vcpuid, bool lapic_intr)
+{
+	struct vcpu *vcpu = &vm->vcpu[vcpuid];
+
+	vcpu_lock(vcpu);
+	vcpu_notify_event_locked(vcpu, lapic_intr);
 	vcpu_unlock(vcpu);
 }
 
+struct vmspace *
+vm_get_vmspace(struct vm *vm)
+{
+
+	return (vm->vmspace);
+}
+
 int
 vm_apicid2vcpuid(struct vm *vm, int apicid)
 {
@@ -1772,6 +2530,54 @@ vm_apicid2vcpuid(struct vm *vm, int apicid)
 	return (apicid);
 }
 
+void
+vm_smp_rendezvous(struct vm *vm, int vcpuid, cpuset_t dest,
+    vm_rendezvous_func_t func, void *arg)
+{
+	int i;
+
+	/*
+	 * Enforce that this function is called without any locks
+	 */
+	WITNESS_WARN(WARN_PANIC, NULL, "vm_smp_rendezvous");
+	KASSERT(vcpuid == -1 || (vcpuid >= 0 && vcpuid < VM_MAXCPU),
+	    ("vm_smp_rendezvous: invalid vcpuid %d", vcpuid));
+
+restart:
+	mtx_lock(&vm->rendezvous_mtx);
+	if (vm->rendezvous_func != NULL) {
+		/*
+		 * If a rendezvous is already in progress then we need to
+		 * call the rendezvous handler in case this 'vcpuid' is one
+		 * of the targets of the rendezvous.
+		 */
+		RENDEZVOUS_CTR0(vm, vcpuid, "Rendezvous already in progress");
+		mtx_unlock(&vm->rendezvous_mtx);
+		vm_handle_rendezvous(vm, vcpuid);
+		goto restart;
+	}
+	KASSERT(vm->rendezvous_func == NULL, ("vm_smp_rendezvous: previous "
+	    "rendezvous is still in progress"));
+
+	RENDEZVOUS_CTR0(vm, vcpuid, "Initiating rendezvous");
+	vm->rendezvous_req_cpus = dest;
+	CPU_ZERO(&vm->rendezvous_done_cpus);
+	vm->rendezvous_arg = arg;
+	vm_set_rendezvous_func(vm, func);
+	mtx_unlock(&vm->rendezvous_mtx);
+
+	/*
+	 * Wake up any sleeping vcpus and trigger a VM-exit in any running
+	 * vcpus so they handle the rendezvous as soon as possible.
+	 */
+	for (i = 0; i < VM_MAXCPU; i++) {
+		if (CPU_ISSET(i, &dest))
+			vcpu_notify_event(vm, i, false);
+	}
+
+	vm_handle_rendezvous(vm, vcpuid);
+}
+
 struct vatpic *
 vm_atpic(struct vm *vm)
 {
@@ -1784,6 +2590,20 @@ vm_atpit(struct vm *vm)
 	return (vm->vatpit);
 }
 
+struct vpmtmr *
+vm_pmtmr(struct vm *vm)
+{
+
+	return (vm->vpmtmr);
+}
+
+struct vrtc *
+vm_rtc(struct vm *vm)
+{
+
+	return (vm->vrtc);
+}
+
 enum vm_reg_name
 vm_segment_name(int seg)
 {
@@ -1805,21 +2625,19 @@ void
 vm_copy_teardown(struct vm *vm, int vcpuid, struct vm_copyinfo *copyinfo,
     int num_copyinfo)
 {
-#ifdef	__FreeBSD__
 	int idx;
 
 	for (idx = 0; idx < num_copyinfo; idx++) {
 		if (copyinfo[idx].cookie != NULL)
 			vm_gpa_release(copyinfo[idx].cookie);
 	}
-#endif
 	bzero(copyinfo, num_copyinfo * sizeof(struct vm_copyinfo));
 }
 
 int
 vm_copy_setup(struct vm *vm, int vcpuid, struct vm_guest_paging *paging,
     uint64_t gla, size_t len, int prot, struct vm_copyinfo *copyinfo,
-    int num_copyinfo)
+    int num_copyinfo, int *fault)
 {
 	int error, idx, nused;
 	size_t n, off, remaining;
@@ -1832,8 +2650,8 @@ vm_copy_setup(struct vm *vm, int vcpuid, struct vm_guest_paging *paging,
 	remaining = len;
 	while (remaining > 0) {
 		KASSERT(nused < num_copyinfo, ("insufficient vm_copyinfo"));
-		error = vm_gla2gpa(vm, vcpuid, paging, gla, prot, &gpa);
-		if (error)
+		error = vm_gla2gpa(vm, vcpuid, paging, gla, prot, &gpa, fault);
+		if (error || *fault)
 			return (error);
 		off = gpa & PAGE_MASK;
 		n = min(remaining, PAGE_SIZE - off);
@@ -1845,8 +2663,8 @@ vm_copy_setup(struct vm *vm, int vcpuid, struct vm_guest_paging *paging,
 	}
 
 	for (idx = 0; idx < nused; idx++) {
-		hva = vm_gpa_hold(vm, copyinfo[idx].gpa, copyinfo[idx].len,
-		    prot, &cookie);
+		hva = vm_gpa_hold(vm, vcpuid, copyinfo[idx].gpa,
+		    copyinfo[idx].len, prot, &cookie);
 		if (hva == NULL)
 			break;
 		copyinfo[idx].hva = hva;
@@ -1855,8 +2673,9 @@ vm_copy_setup(struct vm *vm, int vcpuid, struct vm_guest_paging *paging,
 
 	if (idx != nused) {
 		vm_copy_teardown(vm, vcpuid, copyinfo, num_copyinfo);
-		return (-1);
+		return (EFAULT);
 	} else {
+		*fault = 0;
 		return (0);
 	}
 }
@@ -1894,3 +2713,136 @@ vm_copyout(struct vm *vm, int vcpuid, const void *kaddr,
 		idx++;
 	}
 }
+
+/*
+ * Return the amount of in-use and wired memory for the VM. Since
+ * these are global stats, only return the values with for vCPU 0
+ */
+VMM_STAT_DECLARE(VMM_MEM_RESIDENT);
+VMM_STAT_DECLARE(VMM_MEM_WIRED);
+
+static void
+vm_get_rescnt(struct vm *vm, int vcpu, struct vmm_stat_type *stat)
+{
+
+	if (vcpu == 0) {
+		vmm_stat_set(vm, vcpu, VMM_MEM_RESIDENT,
+	       	    PAGE_SIZE * vmspace_resident_count(vm->vmspace));
+	}	
+}
+
+static void
+vm_get_wiredcnt(struct vm *vm, int vcpu, struct vmm_stat_type *stat)
+{
+
+	if (vcpu == 0) {
+		vmm_stat_set(vm, vcpu, VMM_MEM_WIRED,
+	      	    PAGE_SIZE * pmap_wired_count(vmspace_pmap(vm->vmspace)));
+	}	
+}
+
+VMM_STAT_FUNC(VMM_MEM_RESIDENT, "Resident memory", vm_get_rescnt);
+VMM_STAT_FUNC(VMM_MEM_WIRED, "Wired memory", vm_get_wiredcnt);
+
+#ifndef __FreeBSD__
+int
+vm_ioport_hook(struct vm *vm, uint_t ioport, vmm_rmem_cb_t rfunc,
+    vmm_wmem_cb_t wfunc, void *arg, void **cookie)
+{
+	list_t *ih = &vm->ioport_hooks;
+	vm_ioport_hook_t *hook, *node;
+
+	if (ioport == 0) {
+		return (EINVAL);
+	}
+
+	rw_enter(&vm->ioport_rwlock, RW_WRITER);
+	/*
+	 * Find the node position in the list which this region should be
+	 * inserted behind to maintain sorted order.
+	 */
+	for (node = list_tail(ih); node != NULL; node = list_prev(ih, node)) {
+		if (ioport == node->vmih_ioport) {
+			/* Reject duplicate port hook  */
+			rw_exit(&vm->ioport_rwlock);
+			return (EEXIST);
+		} else if (ioport > node->vmih_ioport) {
+			break;
+		}
+	}
+
+	hook = kmem_alloc(sizeof (*hook), KM_SLEEP);
+	hook->vmih_ioport = ioport;
+	hook->vmih_arg = arg;
+	hook->vmih_rmem_cb = rfunc;
+	hook->vmih_wmem_cb = wfunc;
+	if (node == NULL) {
+		list_insert_head(ih, hook);
+	} else {
+		list_insert_after(ih, node, hook);
+	}
+
+	*cookie = (void *)hook;
+	rw_exit(&vm->ioport_rwlock);
+	return (0);
+}
+
+void
+vm_ioport_unhook(struct vm *vm, void **cookie)
+{
+	vm_ioport_hook_t *hook;
+	list_t *ih = &vm->ioport_hooks;
+
+	rw_enter(&vm->ioport_rwlock, RW_WRITER);
+	hook = *cookie;
+	list_remove(ih, hook);
+	kmem_free(hook, sizeof (*hook));
+	*cookie = NULL;
+	rw_exit(&vm->ioport_rwlock);
+}
+
+int
+vm_ioport_handle_hook(struct vm *vm, int cpuid, bool in, int port, int bytes,
+    uint32_t *val)
+{
+	vm_ioport_hook_t *hook;
+	list_t *ih = &vm->ioport_hooks;
+	int err = 0;
+
+	rw_enter(&vm->ioport_rwlock, RW_READER);
+	for (hook = list_head(ih); hook != NULL; hook = list_next(ih, hook)) {
+		if (hook->vmih_ioport == port) {
+			break;
+		}
+	}
+	if (hook == NULL) {
+		err = ENOENT;
+		goto bail;
+	}
+
+	if (in) {
+		uint64_t tval;
+
+		if (hook->vmih_rmem_cb == NULL) {
+			err = ENOENT;
+			goto bail;
+		}
+		err = hook->vmih_rmem_cb(hook->vmih_arg, (uintptr_t)port,
+		    (uint_t)bytes, &tval);
+		*val = (uint32_t)tval;
+	} else {
+		if (hook->vmih_wmem_cb == NULL) {
+			err = ENOENT;
+			goto bail;
+		}
+		err = hook->vmih_wmem_cb(hook->vmih_arg, (uintptr_t)port,
+		    (uint_t)bytes, (uint64_t)*val);
+	}
+
+bail:
+	rw_exit(&vm->ioport_rwlock);
+	return (err);
+}
+
+
+#endif /* __FreeBSD__ */
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_host.c b/usr/src/uts/i86pc/io/vmm/vmm_host.c
index b94caf4009..f590722bc2 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_host.c
+++ b/usr/src/uts/i86pc/io/vmm/vmm_host.c
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/vmm_host.c 242275 2012-10-29 01:51:24Z neel $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -39,7 +39,7 @@
  */
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/sys/amd64/vmm/vmm_host.c 242275 2012-10-29 01:51:24Z neel $");
+__FBSDID("$FreeBSD$");
 
 #include <sys/param.h>
 #include <sys/pcpu.h>
@@ -50,11 +50,14 @@ __FBSDID("$FreeBSD: head/sys/amd64/vmm/vmm_host.c 242275 2012-10-29 01:51:24Z ne
 
 #include "vmm_host.h"
 
-static uint64_t vmm_host_efer, vmm_host_pat, vmm_host_cr0, vmm_host_cr4;
+static uint64_t vmm_host_efer, vmm_host_pat, vmm_host_cr0, vmm_host_cr4,
+	vmm_host_xcr0;
+static struct xsave_limits vmm_xsave_limits;
 
 void
 vmm_host_state_init(void)
 {
+	unsigned int regs[4];
 
 	vmm_host_efer = rdmsr(MSR_EFER);
 	vmm_host_pat = rdmsr(MSR_PAT);
@@ -69,6 +72,26 @@ vmm_host_state_init(void)
 	vmm_host_cr0 = rcr0() | CR0_TS;
 
 	vmm_host_cr4 = rcr4();
+
+	/*
+	 * Only permit a guest to use XSAVE if the host is using
+	 * XSAVE.  Only permit a guest to use XSAVE features supported
+	 * by the host.  This ensures that the FPU state used by the
+	 * guest is always a subset of the saved guest FPU state.
+	 *
+	 * In addition, only permit known XSAVE features where the
+	 * rules for which features depend on other features is known
+	 * to properly emulate xsetbv.
+	 */
+	if (vmm_host_cr4 & CR4_XSAVE) {
+		vmm_xsave_limits.xsave_enabled = 1;
+		vmm_host_xcr0 = rxcr(0);
+		vmm_xsave_limits.xcr0_allowed = vmm_host_xcr0 &
+		    (XFEATURE_AVX | XFEATURE_MPX | XFEATURE_AVX512);
+
+		cpuid_count(0xd, 0x0, regs);
+		vmm_xsave_limits.xsave_max_size = regs[1];
+	}
 }
 
 uint64_t
@@ -99,6 +122,13 @@ vmm_get_host_cr4(void)
 	return (vmm_host_cr4);
 }
 
+uint64_t
+vmm_get_host_xcr0(void)
+{
+
+	return (vmm_host_xcr0);
+}
+
 uint64_t
 vmm_get_host_datasel(void)
 {
@@ -122,7 +152,6 @@ vmm_get_host_codesel(void)
 #endif
 }
 
-
 uint64_t
 vmm_get_host_tsssel(void)
 {
@@ -158,3 +187,10 @@ vmm_get_host_idtrbase(void)
 	return (idtr.dtr_base);
 #endif
 }
+
+const struct xsave_limits *
+vmm_get_xsave_limits(void)
+{
+
+	return (&vmm_xsave_limits);
+}
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_host.h b/usr/src/uts/i86pc/io/vmm/vmm_host.h
index c5984e8f99..f96a71ea09 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_host.h
+++ b/usr/src/uts/i86pc/io/vmm/vmm_host.h
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/vmm_host.h 242275 2012-10-29 01:51:24Z neel $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -50,17 +50,25 @@
 #error "no user-servicable parts inside"
 #endif
 
+struct xsave_limits {
+	int		xsave_enabled;
+	uint64_t	xcr0_allowed;
+	uint32_t	xsave_max_size;
+};
+
 void vmm_host_state_init(void);
 
 uint64_t vmm_get_host_pat(void);
 uint64_t vmm_get_host_efer(void);
 uint64_t vmm_get_host_cr0(void);
 uint64_t vmm_get_host_cr4(void);
+uint64_t vmm_get_host_xcr0(void);
 uint64_t vmm_get_host_datasel(void);
 uint64_t vmm_get_host_codesel(void);
 uint64_t vmm_get_host_tsssel(void);
 uint64_t vmm_get_host_fsbase(void);
 uint64_t vmm_get_host_idtrbase(void);
+const struct xsave_limits *vmm_get_xsave_limits(void);
 
 /*
  * Inline access to host state that is used on every VM entry
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_instruction_emul.c b/usr/src/uts/i86pc/io/vmm/vmm_instruction_emul.c
index d35a74b33c..1e05cfc8da 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_instruction_emul.c
+++ b/usr/src/uts/i86pc/io/vmm/vmm_instruction_emul.c
@@ -24,7 +24,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/vmm_instruction_emul.c 281987 2015-04-25 19:02:06Z tychon $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -41,12 +41,13 @@
  */
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/sys/amd64/vmm/vmm_instruction_emul.c 281987 2015-04-25 19:02:06Z tychon $");
+__FBSDID("$FreeBSD$");
 
 #ifdef _KERNEL
 #include <sys/param.h>
 #include <sys/pcpu.h>
 #include <sys/systm.h>
+#include <sys/proc.h>
 
 #include <vm/vm.h>
 #include <vm/pmap.h>
@@ -85,6 +86,7 @@ enum {
 	VIE_OP_TYPE_MOVS,
 	VIE_OP_TYPE_GROUP1,
 	VIE_OP_TYPE_STOS,
+	VIE_OP_TYPE_BITTEST,
 	VIE_OP_TYPE_LAST
 };
 
@@ -104,6 +106,11 @@ static const struct vie_op two_byte_opcodes[256] = {
 		.op_byte = 0xB7,
 		.op_type = VIE_OP_TYPE_MOVZX,
 	},
+	[0xBA] = {
+		.op_byte = 0xBA,
+		.op_type = VIE_OP_TYPE_BITTEST,
+		.op_flags = VIE_OP_F_IMM8,
+	},
 	[0xBE] = {
 		.op_byte = 0xBE,
 		.op_type = VIE_OP_TYPE_MOVSX,
@@ -119,6 +126,10 @@ static const struct vie_op one_byte_opcodes[256] = {
 		.op_byte = 0x2B,
 		.op_type = VIE_OP_TYPE_SUB,
 	},
+	[0x39] = {
+		.op_byte = 0x39,
+		.op_type = VIE_OP_TYPE_CMP,
+	},
 	[0x3B] = {
 		.op_byte = 0x3B,
 		.op_type = VIE_OP_TYPE_CMP,
@@ -184,14 +195,20 @@ static const struct vie_op one_byte_opcodes[256] = {
 		.op_byte = 0x23,
 		.op_type = VIE_OP_TYPE_AND,
 	},
+	[0x80] = {
+		/* Group 1 extended opcode */
+		.op_byte = 0x80,
+		.op_type = VIE_OP_TYPE_GROUP1,
+		.op_flags = VIE_OP_F_IMM8,
+	},
 	[0x81] = {
-		/* XXX Group 1 extended opcode */
+		/* Group 1 extended opcode */
 		.op_byte = 0x81,
 		.op_type = VIE_OP_TYPE_GROUP1,
 		.op_flags = VIE_OP_F_IMM,
 	},
 	[0x83] = {
-		/* XXX Group 1 extended opcode */
+		/* Group 1 extended opcode */
 		.op_byte = 0x83,
 		.op_type = VIE_OP_TYPE_GROUP1,
 		.op_flags = VIE_OP_F_IMM8,
@@ -597,13 +614,11 @@ emulate_movx(void *vm, int vcpuid, uint64_t gpa, struct vie *vie,
 
 /*
  * Helper function to calculate and validate a linear address.
- *
- * Returns 0 on success and 1 if an exception was injected into the guest.
  */
 static int
 get_gla(void *vm, int vcpuid, struct vie *vie, struct vm_guest_paging *paging,
     int opsize, int addrsize, int prot, enum vm_reg_name seg,
-    enum vm_reg_name gpr, uint64_t *gla)
+    enum vm_reg_name gpr, uint64_t *gla, int *fault)
 {
 	struct seg_desc desc;
 	uint64_t cr0, val, rflags;
@@ -629,7 +644,7 @@ get_gla(void *vm, int vcpuid, struct vie *vie, struct vm_guest_paging *paging,
 			vm_inject_ss(vm, vcpuid, 0);
 		else
 			vm_inject_gp(vm, vcpuid);
-		return (1);
+		goto guest_fault;
 	}
 
 	if (vie_canonical_check(paging->cpu_mode, *gla)) {
@@ -637,14 +652,19 @@ get_gla(void *vm, int vcpuid, struct vie *vie, struct vm_guest_paging *paging,
 			vm_inject_ss(vm, vcpuid, 0);
 		else
 			vm_inject_gp(vm, vcpuid);
-		return (1);
+		goto guest_fault;
 	}
 
 	if (vie_alignment_check(paging->cpl, opsize, cr0, rflags, *gla)) {
 		vm_inject_ac(vm, vcpuid, 0);
-		return (1);
+		goto guest_fault;
 	}
 
+	*fault = 0;
+	return (0);
+
+guest_fault:
+	*fault = 1;
 	return (0);
 }
 
@@ -660,7 +680,7 @@ emulate_movs(void *vm, int vcpuid, uint64_t gpa, struct vie *vie,
 #endif
 	uint64_t dstaddr, srcaddr, dstgpa, srcgpa, val;
 	uint64_t rcx, rdi, rsi, rflags;
-	int error, opsize, seg, repeat;
+	int error, fault, opsize, seg, repeat;
 
 	opsize = (vie->op.op_byte == 0xA4) ? 1 : vie->opsize;
 	val = 0;
@@ -683,8 +703,10 @@ emulate_movs(void *vm, int vcpuid, uint64_t gpa, struct vie *vie,
 		 * The count register is %rcx, %ecx or %cx depending on the
 		 * address size of the instruction.
 		 */
-		if ((rcx & vie_size2mask(vie->addrsize)) == 0)
-			return (0);
+		if ((rcx & vie_size2mask(vie->addrsize)) == 0) {
+			error = 0;
+			goto done;
+		}
 	}
 
 	/*
@@ -705,13 +727,16 @@ emulate_movs(void *vm, int vcpuid, uint64_t gpa, struct vie *vie,
 
 	seg = vie->segment_override ? vie->segment_register : VM_REG_GUEST_DS;
 	error = get_gla(vm, vcpuid, vie, paging, opsize, vie->addrsize,
-	    PROT_READ, seg, VM_REG_GUEST_RSI, &srcaddr);
-	if (error)
+	    PROT_READ, seg, VM_REG_GUEST_RSI, &srcaddr, &fault);
+	if (error || fault)
 		goto done;
 
 	error = vm_copy_setup(vm, vcpuid, paging, srcaddr, opsize, PROT_READ,
-	    copyinfo, nitems(copyinfo));
+	    copyinfo, nitems(copyinfo), &fault);
 	if (error == 0) {
+		if (fault)
+			goto done;	/* Resume guest to handle fault */
+
 		/*
 		 * case (2): read from system memory and write to mmio.
 		 */
@@ -720,11 +745,6 @@ emulate_movs(void *vm, int vcpuid, uint64_t gpa, struct vie *vie,
 		error = memwrite(vm, vcpuid, gpa, val, opsize, arg);
 		if (error)
 			goto done;
-	} else if (error > 0) {
-		/*
-		 * Resume guest execution to handle fault.
-		 */
-		goto done;
 	} else {
 		/*
 		 * 'vm_copy_setup()' is expected to fail for cases (3) and (4)
@@ -732,13 +752,17 @@ emulate_movs(void *vm, int vcpuid, uint64_t gpa, struct vie *vie,
 		 */
 
 		error = get_gla(vm, vcpuid, vie, paging, opsize, vie->addrsize,
-		    PROT_WRITE, VM_REG_GUEST_ES, VM_REG_GUEST_RDI, &dstaddr);
-		if (error)
+		    PROT_WRITE, VM_REG_GUEST_ES, VM_REG_GUEST_RDI, &dstaddr,
+		    &fault);
+		if (error || fault)
 			goto done;
 
 		error = vm_copy_setup(vm, vcpuid, paging, dstaddr, opsize,
-		    PROT_WRITE, copyinfo, nitems(copyinfo));
+		    PROT_WRITE, copyinfo, nitems(copyinfo), &fault);
 		if (error == 0) {
+			if (fault)
+				goto done;    /* Resume guest to handle fault */
+
 			/*
 			 * case (3): read from MMIO and write to system memory.
 			 *
@@ -754,27 +778,29 @@ emulate_movs(void *vm, int vcpuid, uint64_t gpa, struct vie *vie,
 
 			vm_copyout(vm, vcpuid, &val, copyinfo, opsize);
 			vm_copy_teardown(vm, vcpuid, copyinfo, nitems(copyinfo));
-		} else if (error > 0) {
-			/*
-			 * Resume guest execution to handle fault.
-			 */
-			goto done;
 		} else {
 			/*
 			 * Case (4): read from and write to mmio.
+			 *
+			 * Commit to the MMIO read/write (with potential
+			 * side-effects) only after we are sure that the
+			 * instruction is not going to be restarted due
+			 * to address translation faults.
 			 */
 			error = vm_gla2gpa(vm, vcpuid, paging, srcaddr,
-			    PROT_READ, &srcgpa);
-			if (error)
-				goto done;
-			error = memread(vm, vcpuid, srcgpa, &val, opsize, arg);
-			if (error)
+			    PROT_READ, &srcgpa, &fault);
+			if (error || fault)
 				goto done;
 
 			error = vm_gla2gpa(vm, vcpuid, paging, dstaddr,
-			   PROT_WRITE, &dstgpa);
+			   PROT_WRITE, &dstgpa, &fault);
+			if (error || fault)
+				goto done;
+
+			error = memread(vm, vcpuid, srcgpa, &val, opsize, arg);
 			if (error)
 				goto done;
+
 			error = memwrite(vm, vcpuid, dstgpa, val, opsize, arg);
 			if (error)
 				goto done;
@@ -819,10 +845,9 @@ emulate_movs(void *vm, int vcpuid, uint64_t gpa, struct vie *vie,
 			vm_restart_instruction(vm, vcpuid);
 	}
 done:
-	if (error < 0)
-		return (EFAULT);
-	else
-		return (0);
+	KASSERT(error == 0 || error == EFAULT, ("%s: unexpected error %d",
+	    __func__, error));
+	return (error);
 }
 
 static int
@@ -1042,39 +1067,55 @@ emulate_cmp(void *vm, int vcpuid, uint64_t gpa, struct vie *vie,
 	    mem_region_read_t memread, mem_region_write_t memwrite, void *arg)
 {
 	int error, size;
-	uint64_t op1, op2, rflags, rflags2;
+	uint64_t regop, memop, op1, op2, rflags, rflags2;
 	enum vm_reg_name reg;
 
 	size = vie->opsize;
 	switch (vie->op.op_byte) {
+	case 0x39:
 	case 0x3B:
 		/*
+		 * 39/r		CMP r/m16, r16
+		 * 39/r		CMP r/m32, r32
+		 * REX.W 39/r	CMP r/m64, r64
+		 *
 		 * 3B/r		CMP r16, r/m16
 		 * 3B/r		CMP r32, r/m32
 		 * REX.W + 3B/r	CMP r64, r/m64
 		 *
-		 * Compare first operand (reg) with second operand (r/m) and
+		 * Compare the first operand with the second operand and
 		 * set status flags in EFLAGS register. The comparison is
 		 * performed by subtracting the second operand from the first
 		 * operand and then setting the status flags.
 		 */
 
-		/* Get the first operand */
+		/* Get the register operand */
 		reg = gpr_map[vie->reg];
-		error = vie_read_register(vm, vcpuid, reg, &op1);
+		error = vie_read_register(vm, vcpuid, reg, &regop);
 		if (error)
 			return (error);
 
-		/* Get the second operand */
-		error = memread(vm, vcpuid, gpa, &op2, size, arg);
+		/* Get the memory operand */
+		error = memread(vm, vcpuid, gpa, &memop, size, arg);
 		if (error)
 			return (error);
 
+		if (vie->op.op_byte == 0x3B) {
+			op1 = regop;
+			op2 = memop;
+		} else {
+			op1 = memop;
+			op2 = regop;
+		}
 		rflags2 = getcc(size, op1, op2);
 		break;
+	case 0x80:
 	case 0x81:
 	case 0x83:
 		/*
+		 * 80 /7		cmp r/m8, imm8
+		 * REX + 80 /7		cmp r/m8, imm8
+		 *
 		 * 81 /7		cmp r/m16, imm16
 		 * 81 /7		cmp r/m32, imm32
 		 * REX.W + 81 /7	cmp r/m64, imm32 sign-extended to 64
@@ -1090,6 +1131,8 @@ emulate_cmp(void *vm, int vcpuid, uint64_t gpa, struct vie *vie,
 		 * the status flags.
 		 *
 		 */
+		if (vie->op.op_byte == 0x80)
+			size = 1;
 
 		/* get the first operand */
                 error = memread(vm, vcpuid, gpa, &op1, size, arg);
@@ -1179,7 +1222,7 @@ emulate_stack_op(void *vm, int vcpuid, uint64_t mmio_gpa, struct vie *vie,
 #endif
 	struct seg_desc ss_desc;
 	uint64_t cr0, rflags, rsp, stack_gla, val;
-	int error, size, stackaddrsize, pushop;
+	int error, fault, size, stackaddrsize, pushop;
 
 	val = 0;
 	size = vie->opsize;
@@ -1202,7 +1245,7 @@ emulate_stack_op(void *vm, int vcpuid, uint64_t mmio_gpa, struct vie *vie,
 		size = vie->opsize_override ? 2 : 8;
 	} else {
 		/*
-		 * In protected or compability mode the 'B' flag in the
+		 * In protected or compatibility mode the 'B' flag in the
 		 * stack-segment descriptor determines the size of the
 		 * stack pointer.
 		 */
@@ -1245,18 +1288,10 @@ emulate_stack_op(void *vm, int vcpuid, uint64_t mmio_gpa, struct vie *vie,
 	}
 
 	error = vm_copy_setup(vm, vcpuid, paging, stack_gla, size,
-	    pushop ? PROT_WRITE : PROT_READ, copyinfo, nitems(copyinfo));
-	if (error == -1) {
-		/*
-		 * XXX cannot return a negative error value here because it
-		 * ends up being the return value of the VM_RUN() ioctl and
-		 * is interpreted as a pseudo-error (for e.g. ERESTART).
-		 */
-		return (EFAULT);
-	} else if (error == 1) {
-		/* Resume guest execution to handle page fault */
-		return (0);
-	}
+	    pushop ? PROT_WRITE : PROT_READ, copyinfo, nitems(copyinfo),
+	    &fault);
+	if (error || fault)
+		return (error);
 
 	if (pushop) {
 		error = memread(vm, vcpuid, mmio_gpa, &val, size, arg);
@@ -1347,6 +1382,48 @@ emulate_group1(void *vm, int vcpuid, uint64_t gpa, struct vie *vie,
 	return (error);
 }
 
+static int
+emulate_bittest(void *vm, int vcpuid, uint64_t gpa, struct vie *vie,
+    mem_region_read_t memread, mem_region_write_t memwrite, void *memarg)
+{
+	uint64_t val, rflags;
+	int error, bitmask, bitoff;
+
+	/*
+	 * 0F BA is a Group 8 extended opcode.
+	 *
+	 * Currently we only emulate the 'Bit Test' instruction which is
+	 * identified by a ModR/M:reg encoding of 100b.
+	 */
+	if ((vie->reg & 7) != 4)
+		return (EINVAL);
+
+	error = vie_read_register(vm, vcpuid, VM_REG_GUEST_RFLAGS, &rflags);
+	KASSERT(error == 0, ("%s: error %d getting rflags", __func__, error));
+
+	error = memread(vm, vcpuid, gpa, &val, vie->opsize, memarg);
+	if (error)
+		return (error);
+
+	/*
+	 * Intel SDM, Vol 2, Table 3-2:
+	 * "Range of Bit Positions Specified by Bit Offset Operands"
+	 */
+	bitmask = vie->opsize * 8 - 1;
+	bitoff = vie->immediate & bitmask;
+
+	/* Copy the bit into the Carry flag in %rflags */
+	if (val & (1UL << bitoff))
+		rflags |= PSL_C;
+	else
+		rflags &= ~PSL_C;
+
+	error = vie_update_register(vm, vcpuid, VM_REG_GUEST_RFLAGS, rflags, 8);
+	KASSERT(error == 0, ("%s: error %d updating rflags", __func__, error));
+
+	return (0);
+}
+
 int
 vmm_emulate_instruction(void *vm, int vcpuid, uint64_t gpa, struct vie *vie,
     struct vm_guest_paging *paging, mem_region_read_t memread,
@@ -1403,6 +1480,10 @@ vmm_emulate_instruction(void *vm, int vcpuid, uint64_t gpa, struct vie *vie,
 		error = emulate_sub(vm, vcpuid, gpa, vie,
 				    memread, memwrite, memarg);
 		break;
+	case VIE_OP_TYPE_BITTEST:
+		error = emulate_bittest(vm, vcpuid, gpa, vie,
+		    memread, memwrite, memarg);
+		break;
 	default:
 		error = EINVAL;
 		break;
@@ -1624,29 +1705,31 @@ ptp_release(void **cookie)
 }
 
 static void *
-ptp_hold(struct vm *vm, vm_paddr_t ptpphys, size_t len, void **cookie)
+ptp_hold(struct vm *vm, int vcpu, vm_paddr_t ptpphys, size_t len, void **cookie)
 {
 	void *ptr;
 
 	ptp_release(cookie);
-	ptr = vm_gpa_hold(vm, ptpphys, len, VM_PROT_RW, cookie);
+	ptr = vm_gpa_hold(vm, vcpu, ptpphys, len, VM_PROT_RW, cookie);
 	return (ptr);
 }
 
 int
 vm_gla2gpa(struct vm *vm, int vcpuid, struct vm_guest_paging *paging,
-    uint64_t gla, int prot, uint64_t *gpa)
+    uint64_t gla, int prot, uint64_t *gpa, int *guest_fault)
 {
+	int nlevels, pfcode, retval, usermode, writable;
 	int ptpshift = 0, ptpindex = 0;
+	uint64_t ptpphys;
 	uint64_t *ptpbase = NULL, pte = 0, pgsize = 0;
-	int nlevels, pfcode, retval, usermode, writable;
 #ifdef	__FreeBSD__
 	u_int retries;
 #endif
-	uint64_t ptpphys;
 	uint32_t *ptpbase32, pte32;
 	void *cookie;
 
+	*guest_fault = 0;
+
 	usermode = (paging->cpl == 3 ? 1 : 0);
 	writable = prot & VM_PROT_WRITE;
 	cookie = NULL;
@@ -1682,7 +1765,8 @@ restart:
 			/* Zero out the lower 12 bits. */
 			ptpphys &= ~0xfff;
 
-			ptpbase32 = ptp_hold(vm, ptpphys, PAGE_SIZE, &cookie);
+			ptpbase32 = ptp_hold(vm, vcpuid, ptpphys, PAGE_SIZE,
+			    &cookie);
 
 			if (ptpbase32 == NULL)
 				goto error;
@@ -1741,7 +1825,8 @@ restart:
 		/* Zero out the lower 5 bits and the upper 32 bits */
 		ptpphys &= 0xffffffe0UL;
 
-		ptpbase = ptp_hold(vm, ptpphys, sizeof(*ptpbase) * 4, &cookie);
+		ptpbase = ptp_hold(vm, vcpuid, ptpphys, sizeof(*ptpbase) * 4,
+		    &cookie);
 		if (ptpbase == NULL)
 			goto error;
 
@@ -1764,7 +1849,7 @@ restart:
 		/* Zero out the lower 12 bits and the upper 12 bits */
 		ptpphys >>= 12; ptpphys <<= 24; ptpphys >>= 12;
 
-		ptpbase = ptp_hold(vm, ptpphys, PAGE_SIZE, &cookie);
+		ptpbase = ptp_hold(vm, vcpuid, ptpphys, PAGE_SIZE, &cookie);
 		if (ptpbase == NULL)
 			goto error;
 
@@ -1813,18 +1898,20 @@ restart:
 	*gpa = pte | (gla & (pgsize - 1));
 done:
 	ptp_release(&cookie);
+	KASSERT(retval == 0 || retval == EFAULT, ("%s: unexpected retval %d",
+	    __func__, retval));
 	return (retval);
 error:
-	retval = -1;
+	retval = EFAULT;
 	goto done;
 fault:
-	retval = 1;
+	*guest_fault = 1;
 	goto done;
 }
 
 int
 vmm_fetch_instruction(struct vm *vm, int vcpuid, struct vm_guest_paging *paging,
-    uint64_t rip, int inst_length, struct vie *vie)
+    uint64_t rip, int inst_length, struct vie *vie, int *faultptr)
 {
 	struct vm_copyinfo copyinfo[2];
 	int error, prot;
@@ -1834,13 +1921,14 @@ vmm_fetch_instruction(struct vm *vm, int vcpuid, struct vm_guest_paging *paging,
 
 	prot = PROT_READ | PROT_EXEC;
 	error = vm_copy_setup(vm, vcpuid, paging, rip, inst_length, prot,
-	    copyinfo, nitems(copyinfo));
-	if (error == 0) {
-		vm_copyin(vm, vcpuid, copyinfo, vie->inst, inst_length);
-		vm_copy_teardown(vm, vcpuid, copyinfo, nitems(copyinfo));
-		vie->num_valid = inst_length;
-	}
-	return (error);
+	    copyinfo, nitems(copyinfo), faultptr);
+	if (error || *faultptr)
+		return (error);
+
+	vm_copyin(vm, vcpuid, copyinfo, vie->inst, inst_length);
+	vm_copy_teardown(vm, vcpuid, copyinfo, nitems(copyinfo));
+	vie->num_valid = inst_length;
+	return (0);
 }
 
 static int
@@ -2264,28 +2352,18 @@ decode_moffset(struct vie *vie)
 	return (0);
 }
 
-/*
- * Verify that all the bytes in the instruction buffer were consumed.
- */
-static int
-verify_inst_length(struct vie *vie)
-{
-
-	if (vie->num_processed)
-		return (0);
-	else
-		return (-1);
-}
-
 /*
  * Verify that the 'guest linear address' provided as collateral of the nested
  * page table fault matches with our instruction decoding.
  */
 static int
-verify_gla(struct vm *vm, int cpuid, uint64_t gla, struct vie *vie)
+verify_gla(struct vm *vm, int cpuid, uint64_t gla, struct vie *vie,
+    enum vm_cpu_mode cpu_mode)
 {
 	int error;
-	uint64_t base, idx, gla2;
+	uint64_t base, segbase, idx, gla2;
+	enum vm_reg_name seg;
+	struct seg_desc desc;
 
 	/* Skip 'gla' verification */
 	if (gla == VIE_INVALID_GLA)
@@ -2305,7 +2383,7 @@ verify_gla(struct vm *vm, int cpuid, uint64_t gla, struct vie *vie)
 		 * instruction
 		 */
 		if (vie->base_register == VM_REG_GUEST_RIP)
-			base += vie->num_valid;
+			base += vie->num_processed;
 	}
 
 	idx = 0;
@@ -2318,14 +2396,48 @@ verify_gla(struct vm *vm, int cpuid, uint64_t gla, struct vie *vie)
 		}
 	}
 
-	/* XXX assuming that the base address of the segment is 0 */
-	gla2 = base + vie->scale * idx + vie->displacement;
+	/*
+	 * From "Specifying a Segment Selector", Intel SDM, Vol 1
+	 *
+	 * In 64-bit mode, segmentation is generally (but not
+	 * completely) disabled.  The exceptions are the FS and GS
+	 * segments.
+	 *
+	 * In legacy IA-32 mode, when the ESP or EBP register is used
+	 * as the base, the SS segment is the default segment.  For
+	 * other data references, except when relative to stack or
+	 * string destination the DS segment is the default.  These
+	 * can be overridden to allow other segments to be accessed.
+	 */
+	if (vie->segment_override)
+		seg = vie->segment_register;
+	else if (vie->base_register == VM_REG_GUEST_RSP ||
+	    vie->base_register == VM_REG_GUEST_RBP)
+		seg = VM_REG_GUEST_SS;
+	else
+		seg = VM_REG_GUEST_DS;
+	if (cpu_mode == CPU_MODE_64BIT && seg != VM_REG_GUEST_FS &&
+	    seg != VM_REG_GUEST_GS) {
+		segbase = 0;
+	} else {
+		error = vm_get_seg_desc(vm, cpuid, seg, &desc);
+		if (error) {
+			printf("verify_gla: error %d getting segment"
+			       " descriptor %d", error,
+			       vie->segment_register);
+			return (-1);
+		}
+		segbase = desc.base;
+	}
+
+	gla2 = segbase + base + vie->scale * idx + vie->displacement;
 	gla2 &= size2mask[vie->addrsize];
 	if (gla != gla2) {
-		printf("verify_gla mismatch: "
+		printf("verify_gla mismatch: segbase(0x%0lx)"
 		       "base(0x%0lx), scale(%d), index(0x%0lx), "
 		       "disp(0x%0lx), gla(0x%0lx), gla2(0x%0lx)\n",
-		       base, vie->scale, idx, vie->displacement, gla, gla2);
+		       segbase, base, vie->scale, idx, vie->displacement,
+		       gla, gla2);
 		return (-1);
 	}
 
@@ -2358,11 +2470,8 @@ vmm_decode_instruction(struct vm *vm, int cpuid, uint64_t gla,
 	if (decode_moffset(vie))
 		return (-1);
 
-	if (verify_inst_length(vie))
-		return (-1);
-
 	if ((vie->op.op_flags & VIE_OP_F_NO_GLA_VERIFICATION) == 0) {
-		if (verify_gla(vm, cpuid, gla, vie))
+		if (verify_gla(vm, cpuid, gla, vie, cpu_mode))
 			return (-1);
 	}
 
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_ioport.c b/usr/src/uts/i86pc/io/vmm/vmm_ioport.c
index bea750f162..934e01a38f 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_ioport.c
+++ b/usr/src/uts/i86pc/io/vmm/vmm_ioport.c
@@ -25,12 +25,9 @@
  */
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/sys/amd64/vmm/vmm_ioport.c 277168 2015-01-14 07:18:51Z neel $");
+__FBSDID("$FreeBSD$");
 
 #include <sys/param.h>
-#include <sys/types.h>
-#include <sys/queue.h>
-#include <sys/cpuset.h>
 #include <sys/systm.h>
 
 #include <machine/vmm.h>
@@ -38,6 +35,8 @@ __FBSDID("$FreeBSD: head/sys/amd64/vmm/vmm_ioport.c 277168 2015-01-14 07:18:51Z
 
 #include "vatpic.h"
 #include "vatpit.h"
+#include "vpmtmr.h"
+#include "vrtc.h"
 #include "vmm_ioport.h"
 #include "vmm_ktr.h"
 
@@ -55,6 +54,9 @@ ioport_handler_func_t ioport_handler[MAX_IOPORTS] = {
 	[IO_ICU2 + ICU_IMR_OFFSET] = vatpic_slave_handler,
 	[IO_ELCR1] = vatpic_elc_handler,
 	[IO_ELCR2] = vatpic_elc_handler,
+	[IO_PMTMR] = vpmtmr_handler,
+	[IO_RTC] = vrtc_addr_handler,
+	[IO_RTC + 1] = vrtc_data_handler,
 };
 
 #ifdef KTR
@@ -103,6 +105,7 @@ emulate_inout_port(struct vm *vm, int vcpuid, struct vm_exit *vmexit,
 	uint32_t mask, val;
 	int error;
 
+#ifdef __FreeBSD__
 	/*
 	 * If there is no handler for the I/O port then punt to userspace.
 	 */
@@ -111,6 +114,28 @@ emulate_inout_port(struct vm *vm, int vcpuid, struct vm_exit *vmexit,
 		*retu = true;
 		return (0);
 	}
+#else /* __FreeBSD__ */
+	handler = NULL;
+	if (vmexit->u.inout.port < MAX_IOPORTS) {
+		handler = ioport_handler[vmexit->u.inout.port];
+	}
+	/* Look for hooks, if a standard handler is not present */
+	if (handler == NULL) {
+		mask = vie_size2mask(vmexit->u.inout.bytes);
+		if (!vmexit->u.inout.in) {
+			val = vmexit->u.inout.eax & mask;
+		}
+		error = vm_ioport_handle_hook(vm, vcpuid, vmexit->u.inout.in,
+		    vmexit->u.inout.port, vmexit->u.inout.bytes, &val);
+		if (error == 0) {
+			goto finish;
+		}
+
+		*retu = true;
+		return (0);
+	}
+
+#endif /* __FreeBSD__ */
 
 	mask = vie_size2mask(vmexit->u.inout.bytes);
 
@@ -131,6 +156,9 @@ emulate_inout_port(struct vm *vm, int vcpuid, struct vm_exit *vmexit,
 		return (EIO);
 	}
 
+#ifndef __FreeBSD__
+finish:
+#endif /* __FreeBSD__ */
 	if (vmexit->u.inout.in) {
 		vmexit->u.inout.eax &= ~mask;
 		vmexit->u.inout.eax |= val & mask;
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_ioport.h b/usr/src/uts/i86pc/io/vmm/vmm_ioport.h
index 624dd8f1d8..ba51989b1a 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_ioport.h
+++ b/usr/src/uts/i86pc/io/vmm/vmm_ioport.h
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/vmm_ioport.h 273706 2014-10-26 19:03:06Z neel $
+ * $FreeBSD$
  */
 
 #ifndef	_VMM_IOPORT_H_
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_ktr.h b/usr/src/uts/i86pc/io/vmm/vmm_ktr.h
index 917c7f83a4..61ff53f47f 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_ktr.h
+++ b/usr/src/uts/i86pc/io/vmm/vmm_ktr.h
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/vmm_ktr.h 258699 2013-11-27 22:18:08Z neel $
+ * $FreeBSD$
  */
 
 #ifndef _VMM_KTR_H_
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_lapic.c b/usr/src/uts/i86pc/io/vmm/vmm_lapic.c
index 3215c74a44..3ff8e11418 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_lapic.c
+++ b/usr/src/uts/i86pc/io/vmm/vmm_lapic.c
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/vmm_lapic.c 264509 2014-04-15 17:06:26Z tychon $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -39,7 +39,7 @@
  */
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/sys/amd64/vmm/vmm_lapic.c 264509 2014-04-15 17:06:26Z tychon $");
+__FBSDID("$FreeBSD$");
 
 #include <sys/param.h>
 #include <sys/systm.h>
@@ -49,7 +49,6 @@ __FBSDID("$FreeBSD: head/sys/amd64/vmm/vmm_lapic.c 264509 2014-04-15 17:06:26Z t
 #include <x86/apicreg.h>
 
 #include <machine/vmm.h>
-#include "vmm_ipi.h"
 #include "vmm_ktr.h"
 #include "vmm_lapic.h"
 #include "vlapic.h"
@@ -70,7 +69,11 @@ lapic_set_intr(struct vm *vm, int cpu, int vector, bool level)
 	if (cpu < 0 || cpu >= VM_MAXCPU)
 		return (EINVAL);
 
-	if (vector < 32 || vector > 255)
+	/*
+	 * According to section "Maskable Hardware Interrupts" in Intel SDM
+	 * vectors 16 through 255 can be delivered through the local APIC.
+	 */
+	if (vector < 16 || vector > 255)
 		return (EINVAL);
 
 	vlapic = vm_lapic(vm, cpu);
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_lapic.h b/usr/src/uts/i86pc/io/vmm/vmm_lapic.h
index ee47ee7783..75f2d2aa58 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_lapic.h
+++ b/usr/src/uts/i86pc/io/vmm/vmm_lapic.h
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/vmm_lapic.h 259863 2013-12-25 06:46:31Z neel $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_mem.c b/usr/src/uts/i86pc/io/vmm/vmm_mem.c
new file mode 100644
index 0000000000..c9be6c9aff
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/vmm_mem.c
@@ -0,0 +1,122 @@
+/*-
+ * Copyright (c) 2011 NetApp, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY NETAPP, INC ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL NETAPP, INC OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/param.h>
+#include <sys/systm.h>
+#include <sys/malloc.h>
+#include <sys/sglist.h>
+#include <sys/lock.h>
+#include <sys/rwlock.h>
+
+#include <vm/vm.h>
+#include <vm/vm_param.h>
+#include <vm/pmap.h>
+#include <vm/vm_map.h>
+#include <vm/vm_object.h>
+#include <vm/vm_page.h>
+#include <vm/vm_pager.h>
+
+#include <machine/md_var.h>
+
+#include "vmm_mem.h"
+
+int
+vmm_mem_init(void)
+{
+
+	return (0);
+}
+
+vm_object_t
+vmm_mmio_alloc(struct vmspace *vmspace, vm_paddr_t gpa, size_t len,
+	       vm_paddr_t hpa)
+{
+	int error;
+	vm_object_t obj;
+	struct sglist *sg;
+
+	sg = sglist_alloc(1, M_WAITOK);
+	error = sglist_append_phys(sg, hpa, len);
+	KASSERT(error == 0, ("error %d appending physaddr to sglist", error));
+
+	obj = vm_pager_allocate(OBJT_SG, sg, len, VM_PROT_RW, 0, NULL);
+	if (obj != NULL) {
+		/*
+		 * VT-x ignores the MTRR settings when figuring out the
+		 * memory type for translations obtained through EPT.
+		 *
+		 * Therefore we explicitly force the pages provided by
+		 * this object to be mapped as uncacheable.
+		 */
+		VM_OBJECT_WLOCK(obj);
+		error = vm_object_set_memattr(obj, VM_MEMATTR_UNCACHEABLE);
+		VM_OBJECT_WUNLOCK(obj);
+		if (error != KERN_SUCCESS) {
+			panic("vmm_mmio_alloc: vm_object_set_memattr error %d",
+				error);
+		}
+		error = vm_map_find(&vmspace->vm_map, obj, 0, &gpa, len, 0,
+				    VMFS_NO_SPACE, VM_PROT_RW, VM_PROT_RW, 0);
+		if (error != KERN_SUCCESS) {
+			vm_object_deallocate(obj);
+			obj = NULL;
+		}
+	}
+
+	/*
+	 * Drop the reference on the sglist.
+	 *
+	 * If the scatter/gather object was successfully allocated then it
+	 * has incremented the reference count on the sglist. Dropping the
+	 * initial reference count ensures that the sglist will be freed
+	 * when the object is deallocated.
+	 * 
+	 * If the object could not be allocated then we end up freeing the
+	 * sglist.
+	 */
+	sglist_free(sg);
+
+	return (obj);
+}
+
+void
+vmm_mmio_free(struct vmspace *vmspace, vm_paddr_t gpa, size_t len)
+{
+
+	vm_map_remove(&vmspace->vm_map, gpa, gpa + len);
+}
+
+vm_paddr_t
+vmm_mem_maxaddr(void)
+{
+
+	return (ptoa(Maxmem));
+}
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_mem.h b/usr/src/uts/i86pc/io/vmm/vmm_mem.h
index 05dc37fb9a..165aabdde2 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_mem.h
+++ b/usr/src/uts/i86pc/io/vmm/vmm_mem.h
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/vmm_mem.h 245678 2013-01-20 03:42:49Z neel $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -41,9 +41,13 @@
 #ifndef	_VMM_MEM_H_
 #define	_VMM_MEM_H_
 
+struct vmspace;
+struct vm_object;
+
 int		vmm_mem_init(void);
-vm_paddr_t	vmm_mem_alloc(size_t size);
-void		vmm_mem_free(vm_paddr_t start, size_t size);
+struct vm_object *vmm_mmio_alloc(struct vmspace *, vm_paddr_t gpa, size_t len,
+				 vm_paddr_t hpa);
+void		vmm_mmio_free(struct vmspace *, vm_paddr_t gpa, size_t size);
 vm_paddr_t	vmm_mem_maxaddr(void);
 
 #endif
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c b/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c
index 3151e1d7ec..39f1a260c8 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c
+++ b/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c
@@ -21,30 +21,38 @@
 #include <sys/stat.h>
 #include <sys/vmsystm.h>
 #include <sys/ddi.h>
+#include <sys/mkdev.h>
 #include <sys/sunddi.h>
 #include <sys/fs/dv_node.h>
 #include <sys/pc_hvm.h>
+#include <sys/cpuset.h>
+#include <sys/id_space.h>
 
 #include <sys/vmm.h>
 #include <sys/vmm_instruction_emul.h>
 #include <sys/vmm_dev.h>
 #include <sys/vmm_impl.h>
+#include <sys/vmm_drv.h>
 
 #include <vm/vm.h>
 #include <vm/seg_dev.h>
 
 #include "io/vatpic.h"
 #include "io/vioapic.h"
+#include "io/vrtc.h"
+#include "io/vhpet.h"
 #include "vmm_lapic.h"
+#include "vmm_stat.h"
+#include "vm/vm_glue.h"
 
 static dev_info_t *vmm_dip;
 static void *vmm_statep;
 
-static SLIST_HEAD(, vmm_softc) head;
-
-static kmutex_t vmmdev_mtx;
-static uint_t vmmdev_inst_count = 0;
-static boolean_t vmmdev_load_failure;
+static kmutex_t		vmmdev_mtx;
+static list_t		vmmdev_list;
+static id_space_t	*vmmdev_minors;
+static uint_t		vmmdev_inst_count = 0;
+static boolean_t	vmmdev_load_failure;
 
 static const char *vmmdev_hvm_name = "bhyve";
 
@@ -60,6 +68,16 @@ static	void vmm_trace_rbuf_alloc(void);
 static	void vmm_trace_rbuf_free(void);
 #endif
 
+/* Holds and hooks from drivers external to vmm */
+struct vmm_hold {
+	list_node_t	vmh_node;
+	vmm_softc_t	*vmh_sc;
+	boolean_t	vmh_expired;
+	uint_t		vmh_ioport_hook_cnt;
+};
+
+static int vmm_drv_block_hook(vmm_softc_t *, boolean_t);
+
 /*
  * This routine is used to manage debug messages
  * on ring buffer.
@@ -214,35 +232,210 @@ vmmdev_init(void)
 	vmm_trace_rbuf_alloc();
 }
 
-void
+int
 vmmdev_cleanup(void)
 {
-	VERIFY(SLIST_EMPTY(&head));
+	VERIFY(list_is_empty(&vmmdev_list));
 
 	vmm_trace_dmsg_free();
+	return (0);
 }
 
-int
-vmmdev_do_ioctl(struct vmm_softc *sc, int cmd, intptr_t arg, int mode,
+static int
+vmmdev_get_memseg(vmm_softc_t *sc, struct vm_memseg *mseg)
+{
+	int error;
+	bool sysmem;
+
+	error = vm_get_memseg(sc->vmm_vm, mseg->segid, &mseg->len, &sysmem,
+	    NULL);
+	if (error || mseg->len == 0)
+		return (error);
+
+	if (!sysmem) {
+		vmm_devmem_entry_t *de;
+		list_t *dl = &sc->vmm_devmem_list;
+
+		for (de = list_head(dl); de != NULL; de = list_next(dl, de)) {
+			if (de->vde_segid == mseg->segid) {
+				break;
+			}
+		}
+		if (de != NULL) {
+			(void) strlcpy(mseg->name, de->vde_name,
+			    sizeof (mseg->name));
+		}
+	} else {
+		bzero(mseg->name, sizeof (mseg->name));
+	}
+
+	return (error);
+}
+
+/*
+ * The 'devmem' hack:
+ *
+ * On native FreeBSD, bhyve consumers are allowed to create 'devmem' segments
+ * in the vm which appear with their own name related to the vm under /dev.
+ * Since this would be a hassle from an sdev perspective and would require a
+ * new cdev interface (or complicate the existing one), we choose to implement
+ * this in a different manner.  When 'devmem' mappings are created, an
+ * identifying off_t is communicated back out to userspace.  That off_t,
+ * residing above the normal guest memory space, can be used to mmap the
+ * 'devmem' mapping from the already-open vm device.
+ */
+
+static int
+vmmdev_devmem_create(vmm_softc_t *sc, struct vm_memseg *mseg, const char *name)
+{
+	off_t map_offset;
+	vmm_devmem_entry_t *entry;
+
+	if (list_is_empty(&sc->vmm_devmem_list)) {
+		map_offset = VM_DEVMEM_START;
+	} else {
+		entry = list_tail(&sc->vmm_devmem_list);
+		map_offset = entry->vde_off + entry->vde_len;
+		if (map_offset < entry->vde_off) {
+			/* Do not tolerate overflow */
+			return (ERANGE);
+		}
+		/*
+		 * XXXJOY: We could choose to search the list for duplicate
+		 * names and toss an error.  Since we're using the offset
+		 * method for now, it does not make much of a difference.
+		 */
+	}
+
+	entry = kmem_zalloc(sizeof (*entry), KM_SLEEP);
+	entry->vde_segid = mseg->segid;
+	entry->vde_len = mseg->len;
+	entry->vde_off = map_offset;
+	(void) strlcpy(entry->vde_name, name, sizeof (entry->vde_name));
+	list_insert_tail(&sc->vmm_devmem_list, entry);
+
+	return (0);
+}
+
+static boolean_t
+vmmdev_devmem_segid(vmm_softc_t *sc, off_t off, off_t len, int *segidp)
+{
+	list_t *dl = &sc->vmm_devmem_list;
+	vmm_devmem_entry_t *de = NULL;
+
+	VERIFY(off >= VM_DEVMEM_START);
+
+	for (de = list_head(dl); de != NULL; de = list_next(dl, de)) {
+		/* XXX: Only hit on direct offset/length matches for now */
+		if (de->vde_off == off && de->vde_len == len) {
+			break;
+		}
+	}
+	if (de == NULL) {
+		return (B_FALSE);
+	}
+
+	*segidp = de->vde_segid;
+	return (B_TRUE);
+}
+
+static void
+vmmdev_devmem_purge(vmm_softc_t *sc)
+{
+	vmm_devmem_entry_t *entry;
+
+	while ((entry = list_remove_head(&sc->vmm_devmem_list)) != NULL) {
+		kmem_free(entry, sizeof (*entry));
+	}
+}
+
+static int
+vmmdev_alloc_memseg(vmm_softc_t *sc, struct vm_memseg *mseg)
+{
+	int error;
+	bool sysmem = true;
+
+	if (VM_MEMSEG_NAME(mseg)) {
+		sysmem = false;
+	}
+	error = vm_alloc_memseg(sc->vmm_vm, mseg->segid, mseg->len, sysmem);
+
+	if (error == 0 && VM_MEMSEG_NAME(mseg)) {
+		/*
+		 * Rather than create a whole fresh device from which userspace
+		 * can mmap this segment, instead make it available at an
+		 * offset above where the main guest memory resides.
+		 */
+		error = vmmdev_devmem_create(sc, mseg, mseg->name);
+		if (error != 0) {
+			vm_free_memseg(sc->vmm_vm, mseg->segid);
+		}
+	}
+	return (error);
+}
+
+
+static int
+vcpu_lock_one(vmm_softc_t *sc, int vcpu)
+{
+	int error;
+
+	if (vcpu < 0 || vcpu >= VM_MAXCPU)
+		return (EINVAL);
+
+	error = vcpu_set_state(sc->vmm_vm, vcpu, VCPU_FROZEN, true);
+	return (error);
+}
+
+static void
+vcpu_unlock_one(vmm_softc_t *sc, int vcpu)
+{
+	enum vcpu_state state;
+
+	state = vcpu_get_state(sc->vmm_vm, vcpu, NULL);
+	if (state != VCPU_FROZEN) {
+		panic("vcpu %s(%d) has invalid state %d", vm_name(sc->vmm_vm),
+		    vcpu, state);
+	}
+
+	vcpu_set_state(sc->vmm_vm, vcpu, VCPU_IDLE, false);
+}
+
+static int
+vcpu_lock_all(vmm_softc_t *sc)
+{
+	int error, vcpu;
+
+	for (vcpu = 0; vcpu < VM_MAXCPU; vcpu++) {
+		error = vcpu_lock_one(sc, vcpu);
+		if (error)
+			break;
+	}
+
+	if (error) {
+		while (--vcpu >= 0)
+			vcpu_unlock_one(sc, vcpu);
+	}
+
+	return (error);
+}
+
+static void
+vcpu_unlock_all(vmm_softc_t *sc)
+{
+	int vcpu;
+
+	for (vcpu = 0; vcpu < VM_MAXCPU; vcpu++)
+		vcpu_unlock_one(sc, vcpu);
+}
+
+static int
+vmmdev_do_ioctl(vmm_softc_t *sc, int cmd, intptr_t arg, int md,
     cred_t *credp, int *rvalp)
 {
-	int error, vcpu, state_changed;
-	struct vm_memory_segment seg;
-	struct vm_register vmreg;
-	struct vm_seg_desc vmsegdesc;
-	struct vm_run vmrun;
-	struct vm_lapic_irq vmirq;
-	struct vm_lapic_msi vmmsi;
-	struct vm_ioapic_irq ioapic_irq;
-	struct vm_isa_irq isa_irq;
-	struct vm_capability vmcap;
-	struct vm_x2apic x2apic;
-	struct vm_gla2gpa gg;
-	struct vm_activate_cpu vac;
-	int pincount;
-
-	vcpu = -1;
-	state_changed = 0;
+	int error = 0, vcpu = -1;
+	void *datap = (void *)arg;
+	boolean_t locked_one = B_FALSE, locked_all = B_FALSE;
 
 	/*
 	 * Some VMM ioctls can operate only on vcpus that are not running.
@@ -261,12 +454,17 @@ vmmdev_do_ioctl(struct vmm_softc *sc, int cmd, intptr_t arg, int mode,
 	case VM_SET_X2APIC_STATE:
 	case VM_GLA2GPA:
 	case VM_ACTIVATE_CPU:
+	case VM_SET_INTINFO:
+	case VM_GET_INTINFO:
 	case VM_RESTART_INSTRUCTION:
 		/*
-		 * XXX fragile, handle with care
-		 * Assumes that the first field of the ioctl data is the vcpu.
+		 * Copy in the ID of the vCPU chosen for this operation.
+		 * Since a nefarious caller could update their struct between
+		 * this locking and when the rest of the ioctl data is copied
+		 * in, it is _critical_ that this local 'vcpu' variable be used
+		 * rather than the in-struct one when performing the ioctl.
 		 */
-		if (ddi_copyin((void *)arg, &vcpu, sizeof (vcpu), mode)) {
+		if (ddi_copyin(datap, &vcpu, sizeof (vcpu), md)) {
 			return (EFAULT);
 		}
 		if (vcpu < 0 || vcpu >= VM_MAXCPU) {
@@ -274,31 +472,42 @@ vmmdev_do_ioctl(struct vmm_softc *sc, int cmd, intptr_t arg, int mode,
 			goto done;
 		}
 
-		error = vcpu_set_state(sc->vm, vcpu, VCPU_FROZEN, true);
+		error = vcpu_lock_one(sc, vcpu);
 		if (error)
 			goto done;
-
-		state_changed = 1;
+		locked_one = B_TRUE;
 		break;
-	case VM_MAP_MEMORY:
+
+	case VM_MAP_PPTDEV_MMIO:
+	case VM_BIND_PPTDEV:
+	case VM_UNBIND_PPTDEV:
+	case VM_ALLOC_MEMSEG:
+	case VM_MMAP_MEMSEG:
+	case VM_REINIT:
 		/*
-		 * ioctls that operate on the entire virtual machine must
-		 * prevent all vcpus from running.
+		 * All vCPUs must be prevented from running when performing
+		 * operations which act upon the entire VM.
 		 */
-		error = 0;
-		for (vcpu = 0; vcpu < VM_MAXCPU; vcpu++) {
-			error = vcpu_set_state(sc->vm, vcpu, VCPU_FROZEN, true);
-			if (error)
-				break;
-		}
-
-		if (error) {
-			while (--vcpu >= 0)
-				vcpu_set_state(sc->vm, vcpu, VCPU_IDLE, false);
+		error = vcpu_lock_all(sc);
+		if (error)
 			goto done;
-		}
+		locked_all = B_TRUE;
+		break;
 
-		state_changed = 2;
+	case VM_GET_MEMSEG:
+	case VM_MMAP_GETNEXT:
+#ifndef __FreeBSD__
+	case VM_DEVMEM_GETOFFSET:
+#endif
+		/*
+		 * Lock a vcpu to make sure that the memory map cannot be
+		 * modified while it is being inspected.
+		 */
+		vcpu = VM_MAXCPU - 1;
+		error = vcpu_lock_one(sc, vcpu);
+		if (error)
+			goto done;
+		locked_one = B_TRUE;
 		break;
 
 	default:
@@ -306,288 +515,630 @@ vmmdev_do_ioctl(struct vmm_softc *sc, int cmd, intptr_t arg, int mode,
 	}
 
 	switch (cmd) {
-	case VM_RUN:
-		if (ddi_copyin((void *)arg, &vmrun,
-		    sizeof (struct vm_run), mode)) {
-			return (EFAULT);
+	case VM_RUN: {
+		struct vm_run vmrun;
+
+		if (ddi_copyin(datap, &vmrun, sizeof (vmrun), md)) {
+			error = EFAULT;
+			break;
 		}
-		error = vm_run(sc->vm, &vmrun);
-		if (ddi_copyout(&vmrun, (void *)arg,
-		    sizeof (struct vm_run), mode)) {
-			return (EFAULT);
+		vmrun.cpuid = vcpu;
+		error = vm_run(sc->vmm_vm, &vmrun);
+		/*
+		 * XXXJOY: I think it's necessary to do copyout, even in the
+		 * face of errors, since the exit state is communicated out.
+		 */
+		if (ddi_copyout(&vmrun, datap, sizeof (vmrun), md)) {
+			error = EFAULT;
+			break;
 		}
 		break;
-	case VM_LAPIC_IRQ:
-		if (ddi_copyin((void *)arg, &vmirq,
-		    sizeof (struct vm_lapic_irq), mode)) {
-			return (EFAULT);
+	}
+	case VM_SUSPEND: {
+		struct vm_suspend vmsuspend;
+
+		if (ddi_copyin(datap, &vmsuspend, sizeof (vmsuspend), md)) {
+			error = EFAULT;
+			break;
 		}
-		error = lapic_intr_edge(sc->vm, vmirq.cpuid, vmirq.vector);
-		if (ddi_copyout(&vmirq, (void *)arg,
-		    sizeof (struct vm_lapic_irq), mode)) {
-			return (EFAULT);
+		error = vm_suspend(sc->vmm_vm, vmsuspend.how);
+		break;
+	}
+	case VM_REINIT:
+		if ((error = vmm_drv_block_hook(sc, B_TRUE)) != 0) {
+			/*
+			 * The VM instance should be free of driver-attached
+			 * hooks during the reinitialization process.
+			 */
+			break;
 		}
+		error = vm_reinit(sc->vmm_vm);
+		(void) vmm_drv_block_hook(sc, B_FALSE);
 		break;
-	case VM_LAPIC_LOCAL_IRQ:
-		if (ddi_copyin((void *)arg, &vmirq,
-		    sizeof (struct vm_lapic_irq), mode)) {
-			return (EFAULT);
+	case VM_STAT_DESC: {
+		struct vm_stat_desc statdesc;
+
+		if (ddi_copyin(datap, &statdesc, sizeof (statdesc), md)) {
+			error = EFAULT;
+			break;
 		}
-		error = lapic_set_local_intr(sc->vm, vmirq.cpuid,
-		    vmirq.vector);
-		if (ddi_copyout(&vmirq, (void *)arg,
-		    sizeof (struct vm_lapic_irq), mode)) {
-			return (EFAULT);
+		error = vmm_stat_desc_copy(statdesc.index, statdesc.desc,
+		    sizeof (statdesc.desc));
+		if (error == 0 &&
+		    ddi_copyout(&statdesc, datap, sizeof (statdesc), md)) {
+			error = EFAULT;
+			break;
 		}
 		break;
-	case VM_LAPIC_MSI:
-		if (ddi_copyin((void *)arg, &vmmsi,
-		    sizeof (struct vm_lapic_msi), mode)) {
-			return (EFAULT);
+	}
+	case VM_STATS_IOC: {
+		struct vm_stats vmstats;
+
+		CTASSERT(MAX_VM_STATS >= MAX_VMM_STAT_ELEMS);
+		if (ddi_copyin(datap, &vmstats, sizeof (vmstats), md)) {
+			error = EFAULT;
+			break;
 		}
-		error = lapic_intr_msi(sc->vm, vmmsi.addr, vmmsi.msg);
-		if (ddi_copyout(&vmmsi, (void *)arg,
-		    sizeof (struct vm_lapic_msi), mode)) {
-			return (EFAULT);
+		hrt2tv(gethrtime(), &vmstats.tv);
+		error = vmm_stat_copy(sc->vmm_vm, vmstats.cpuid,
+		    &vmstats.num_entries, vmstats.statbuf);
+		if (error == 0 &&
+		    ddi_copyout(&vmstats, datap, sizeof (vmstats), md)) {
+			error = EFAULT;
+			break;
 		}
-	case VM_IOAPIC_ASSERT_IRQ:
-		if (ddi_copyin((void *)arg, &ioapic_irq,
-		    sizeof (struct vm_ioapic_irq), mode)) {
-			return (EFAULT);
+		break;
+	}
+
+	/* XXXJOY: punt on these for now */
+	case VM_PPTDEV_MSI: {
+		struct vm_pptdev_msi pptmsi;
+
+		if (ddi_copyin(datap, &pptmsi, sizeof (pptmsi), md)) {
+			error = EFAULT;
+			break;
 		}
-		error = vioapic_assert_irq(sc->vm, ioapic_irq.irq);
-		if (ddi_copyout(&ioapic_irq, (void *)arg,
-		    sizeof (struct vm_ioapic_irq), mode)) {
-			return (EFAULT);
+		return (ENOTTY);
+	}
+	case VM_PPTDEV_MSIX: {
+		struct vm_pptdev_msix pptmsix;
+
+		if (ddi_copyin(datap, &pptmsix, sizeof (pptmsix), md)) {
+			error = EFAULT;
+			break;
+		}
+		return (ENOTTY);
+	}
+	case VM_MAP_PPTDEV_MMIO: {
+		struct vm_pptdev_mmio pptmmio;
+
+		if (ddi_copyin(datap, &pptmmio, sizeof (pptmmio), md)) {
+			error = EFAULT;
+			break;
+		}
+		return (ENOTTY);
+	}
+	case VM_BIND_PPTDEV:
+	case VM_UNBIND_PPTDEV: {
+		struct vm_pptdev pptdev;
+
+		if (ddi_copyin(datap, &pptdev, sizeof (pptdev), md)) {
+			error = EFAULT;
+			break;
+		}
+		return (ENOTTY);
+	}
+
+	case VM_INJECT_EXCEPTION: {
+		struct vm_exception vmexc;
+
+		if (ddi_copyin(datap, &vmexc, sizeof (vmexc), md)) {
+			error = EFAULT;
+			break;
 		}
+		error = vm_inject_exception(sc->vmm_vm, vcpu, vmexc.vector,
+		    vmexc.error_code_valid, vmexc.error_code,
+		    vmexc.restart_instruction);
 		break;
-	case VM_IOAPIC_DEASSERT_IRQ:
-		if (ddi_copyin((void *)arg, &ioapic_irq,
-		    sizeof (struct vm_ioapic_irq), mode)) {
-			return (EFAULT);
+	}
+	case VM_INJECT_NMI: {
+		struct vm_nmi vmnmi;
+
+		if (ddi_copyin(datap, &vmnmi, sizeof (vmnmi), md)) {
+			error = EFAULT;
+			break;
 		}
-		error = vioapic_deassert_irq(sc->vm, ioapic_irq.irq);
-		if (ddi_copyout(&ioapic_irq, (void *)arg,
-		    sizeof (struct vm_ioapic_irq), mode)) {
-			return (EFAULT);
+		error = vm_inject_nmi(sc->vmm_vm, vmnmi.cpuid);
+		break;
+	}
+	case VM_LAPIC_IRQ: {
+		struct vm_lapic_irq vmirq;
+
+		if (ddi_copyin(datap, &vmirq, sizeof (vmirq), md)) {
+			error = EFAULT;
+			break;
 		}
+		error = lapic_intr_edge(sc->vmm_vm, vmirq.cpuid, vmirq.vector);
 		break;
-	case VM_IOAPIC_PULSE_IRQ:
-		if (ddi_copyin((void *)arg, &ioapic_irq,
-		    sizeof (struct vm_ioapic_irq), mode)) {
-			return (EFAULT);
+	}
+	case VM_LAPIC_LOCAL_IRQ: {
+		struct vm_lapic_irq vmirq;
+
+		if (ddi_copyin(datap, &vmirq, sizeof (vmirq), md)) {
+			error = EFAULT;
+			break;
 		}
-		error = vioapic_pulse_irq(sc->vm, ioapic_irq.irq);
-		if (ddi_copyout(&ioapic_irq, (void *)arg,
-		    sizeof (struct vm_ioapic_irq), mode)) {
-			return (EFAULT);
+		error = lapic_set_local_intr(sc->vmm_vm, vmirq.cpuid,
+		    vmirq.vector);
+		break;
+	}
+	case VM_LAPIC_MSI: {
+		struct vm_lapic_msi vmmsi;
+
+		if (ddi_copyin(datap, &vmmsi, sizeof (vmmsi), md)) {
+			error = EFAULT;
+			break;
 		}
+		error = lapic_intr_msi(sc->vmm_vm, vmmsi.addr, vmmsi.msg);
 		break;
-	case VM_IOAPIC_PINCOUNT:
-		error = 0;
-		pincount = vioapic_pincount(sc->vm);
-		if (ddi_copyout(&pincount, (void *)arg, sizeof (int), mode)) {
-			return (EFAULT);
+	}
+
+	case VM_IOAPIC_ASSERT_IRQ: {
+		struct vm_ioapic_irq ioapic_irq;
+
+		if (ddi_copyin(datap, &ioapic_irq, sizeof (ioapic_irq), md)) {
+			error = EFAULT;
+			break;
 		}
+		error = vioapic_assert_irq(sc->vmm_vm, ioapic_irq.irq);
 		break;
-	case VM_ISA_ASSERT_IRQ:
-		if (ddi_copyin((void *)arg, &isa_irq,
-		    sizeof (struct vm_isa_irq), mode)) {
-			return (EFAULT);
+	}
+	case VM_IOAPIC_DEASSERT_IRQ: {
+		struct vm_ioapic_irq ioapic_irq;
+
+		if (ddi_copyin(datap, &ioapic_irq, sizeof (ioapic_irq), md)) {
+			error = EFAULT;
+			break;
+		}
+		error = vioapic_deassert_irq(sc->vmm_vm, ioapic_irq.irq);
+		break;
+	}
+	case VM_IOAPIC_PULSE_IRQ: {
+		struct vm_ioapic_irq ioapic_irq;
+
+		if (ddi_copyin(datap, &ioapic_irq, sizeof (ioapic_irq), md)) {
+			error = EFAULT;
+			break;
+		}
+		error = vioapic_pulse_irq(sc->vmm_vm, ioapic_irq.irq);
+		break;
+	}
+	case VM_IOAPIC_PINCOUNT: {
+		int pincount;
+
+		pincount = vioapic_pincount(sc->vmm_vm);
+		if (ddi_copyout(&pincount, datap, sizeof (int), md)) {
+			error = EFAULT;
+			break;
+		}
+		break;
+	}
+
+	case VM_ISA_ASSERT_IRQ: {
+		struct vm_isa_irq isa_irq;
+
+		if (ddi_copyin(datap, &isa_irq, sizeof (isa_irq), md)) {
+			error = EFAULT;
+			break;
 		}
-		error = vatpic_assert_irq(sc->vm, isa_irq.atpic_irq);
-		if (error == 0 && isa_irq.ioapic_irq != -1)
-			error = vioapic_assert_irq(sc->vm,
+		error = vatpic_assert_irq(sc->vmm_vm, isa_irq.atpic_irq);
+		if (error == 0 && isa_irq.ioapic_irq != -1) {
+			error = vioapic_assert_irq(sc->vmm_vm,
 			    isa_irq.ioapic_irq);
-		if (ddi_copyout(&isa_irq, (void *)arg,
-		    sizeof (struct vm_isa_irq), mode)) {
-			return (EFAULT);
 		}
 		break;
-	case VM_ISA_DEASSERT_IRQ:
-		if (ddi_copyin((void *)arg, &isa_irq,
-		    sizeof (struct vm_isa_irq), mode)) {
-			return (EFAULT);
+	}
+	case VM_ISA_DEASSERT_IRQ: {
+		struct vm_isa_irq isa_irq;
+
+		if (ddi_copyin(datap, &isa_irq, sizeof (isa_irq), md)) {
+			error = EFAULT;
+			break;
 		}
-		error = vatpic_deassert_irq(sc->vm, isa_irq.atpic_irq);
-		if (error == 0 && isa_irq.ioapic_irq != -1)
-			error = vioapic_deassert_irq(sc->vm,
+		error = vatpic_deassert_irq(sc->vmm_vm, isa_irq.atpic_irq);
+		if (error == 0 && isa_irq.ioapic_irq != -1) {
+			error = vioapic_deassert_irq(sc->vmm_vm,
 			    isa_irq.ioapic_irq);
-		if (ddi_copyout(&isa_irq, (void *)arg,
-		    sizeof (struct vm_isa_irq), mode)) {
-			return (EFAULT);
 		}
 		break;
-	case VM_ISA_PULSE_IRQ:
-		if (ddi_copyin((void *)arg, &isa_irq,
-		    sizeof (struct vm_isa_irq), mode)) {
-			return (EFAULT);
+	}
+	case VM_ISA_PULSE_IRQ: {
+		struct vm_isa_irq isa_irq;
+
+		if (ddi_copyin(datap, &isa_irq, sizeof (isa_irq), md)) {
+			error = EFAULT;
+			break;
 		}
-		error = vatpic_pulse_irq(sc->vm, isa_irq.atpic_irq);
-		if (error == 0 && isa_irq.ioapic_irq != -1)
-			error = vioapic_pulse_irq(sc->vm, isa_irq.ioapic_irq);
-		if (ddi_copyout(&isa_irq, (void *)arg,
-		    sizeof (struct vm_isa_irq), mode)) {
-			return (EFAULT);
+		error = vatpic_pulse_irq(sc->vmm_vm, isa_irq.atpic_irq);
+		if (error == 0 && isa_irq.ioapic_irq != -1) {
+			error = vioapic_pulse_irq(sc->vmm_vm,
+			    isa_irq.ioapic_irq);
+		}
+		break;
+	}
+	case VM_ISA_SET_IRQ_TRIGGER: {
+		struct vm_isa_irq_trigger isa_irq_trigger;
+
+		if (ddi_copyin(datap, &isa_irq_trigger,
+		    sizeof (isa_irq_trigger), md)) {
+			error = EFAULT;
+			break;
+		}
+		error = vatpic_set_irq_trigger(sc->vmm_vm,
+		    isa_irq_trigger.atpic_irq, isa_irq_trigger.trigger);
+		break;
+	}
+
+	case VM_MMAP_GETNEXT: {
+		struct vm_memmap mm;
+
+		if (ddi_copyin(datap, &mm, sizeof (mm), md)) {
+			error = EFAULT;
+			break;
+		}
+		error = vm_mmap_getnext(sc->vmm_vm, &mm.gpa, &mm.segid,
+		    &mm.segoff, &mm.len, &mm.prot, &mm.flags);
+		if (error == 0 && ddi_copyout(&mm, datap, sizeof (mm), md)) {
+			error = EFAULT;
+			break;
+		}
+		break;
+	}
+	case VM_MMAP_MEMSEG: {
+		struct vm_memmap mm;
+
+		if (ddi_copyin(datap, &mm, sizeof (mm), md)) {
+			error = EFAULT;
+			break;
+		}
+		error = vm_mmap_memseg(sc->vmm_vm, mm.gpa, mm.segid, mm.segoff,
+		    mm.len, mm.prot, mm.flags);
+		break;
+	}
+	case VM_ALLOC_MEMSEG: {
+		struct vm_memseg vmseg;
+
+		if (ddi_copyin(datap, &vmseg, sizeof (vmseg), md)) {
+			error = EFAULT;
+			break;
+		}
+		error = vmmdev_alloc_memseg(sc, &vmseg);
+		break;
+	}
+	case VM_GET_MEMSEG: {
+		struct vm_memseg vmseg;
+
+		if (ddi_copyin(datap, &vmseg, sizeof (vmseg), md)) {
+			error = EFAULT;
+			break;
+		}
+		error = vmmdev_get_memseg(sc, &vmseg);
+		if (error == 0 &&
+		    ddi_copyout(&vmseg, datap, sizeof (vmseg), md)) {
+			error = EFAULT;
+			break;
+		}
+		break;
+	}
+	case VM_GET_REGISTER: {
+		struct vm_register vmreg;
+
+		if (ddi_copyin(datap, &vmreg, sizeof (vmreg), md)) {
+			error = EFAULT;
+			break;
+		}
+		error = vm_get_register(sc->vmm_vm, vcpu, vmreg.regnum,
+		    &vmreg.regval);
+		if (error == 0 &&
+		    ddi_copyout(&vmreg, datap, sizeof (vmreg), md)) {
+			error = EFAULT;
+			break;
+		}
+		break;
+	}
+	case VM_SET_REGISTER: {
+		struct vm_register vmreg;
+
+		if (ddi_copyin(datap, &vmreg, sizeof (vmreg), md)) {
+			error = EFAULT;
+			break;
+		}
+		error = vm_set_register(sc->vmm_vm, vcpu, vmreg.regnum,
+		    vmreg.regval);
+		break;
+	}
+	case VM_SET_SEGMENT_DESCRIPTOR: {
+		struct vm_seg_desc vmsegd;
+
+		if (ddi_copyin(datap, &vmsegd, sizeof (vmsegd), md)) {
+			error = EFAULT;
+			break;
+		}
+		error = vm_set_seg_desc(sc->vmm_vm, vcpu, vmsegd.regnum,
+		    &vmsegd.desc);
+		break;
+	}
+	case VM_GET_SEGMENT_DESCRIPTOR: {
+		struct vm_seg_desc vmsegd;
+
+		if (ddi_copyin(datap, &vmsegd, sizeof (vmsegd), md)) {
+			error = EFAULT;
+			break;
+		}
+		error = vm_get_seg_desc(sc->vmm_vm, vcpu, vmsegd.regnum,
+		    &vmsegd.desc);
+		if (error == 0 &&
+		    ddi_copyout(&vmsegd, datap, sizeof (vmsegd), md)) {
+			error = EFAULT;
+			break;
+		}
+		break;
+	}
+	case VM_GET_CAPABILITY: {
+		struct vm_capability vmcap;
+
+		if (ddi_copyin(datap, &vmcap, sizeof (vmcap), md)) {
+			error = EFAULT;
+			break;
+		}
+		error = vm_get_capability(sc->vmm_vm, vcpu, vmcap.captype,
+		    &vmcap.capval);
+		if (error == 0 &&
+		    ddi_copyout(&vmcap, datap, sizeof (vmcap), md)) {
+			error = EFAULT;
+			break;
+		}
+		break;
+	}
+	case VM_SET_CAPABILITY: {
+		struct vm_capability vmcap;
+
+		if (ddi_copyin(datap, &vmcap, sizeof (vmcap), md)) {
+			error = EFAULT;
+			break;
+		}
+		error = vm_set_capability(sc->vmm_vm, vcpu, vmcap.captype,
+		    vmcap.capval);
+		break;
+	}
+	case VM_SET_X2APIC_STATE: {
+		struct vm_x2apic x2apic;
+
+		if (ddi_copyin(datap, &x2apic, sizeof (x2apic), md)) {
+			error = EFAULT;
+			break;
+		}
+		error = vm_set_x2apic_state(sc->vmm_vm, vcpu, x2apic.state);
+		break;
+	}
+	case VM_GET_X2APIC_STATE: {
+		struct vm_x2apic x2apic;
+
+		if (ddi_copyin(datap, &x2apic, sizeof (x2apic), md)) {
+			error = EFAULT;
+			break;
+		}
+		error = vm_get_x2apic_state(sc->vmm_vm, x2apic.cpuid,
+		    &x2apic.state);
+		if (error == 0 &&
+		    ddi_copyout(&x2apic, datap, sizeof (x2apic), md)) {
+			error = EFAULT;
+			break;
+		}
+		break;
+	}
+	case VM_GET_GPA_PMAP: {
+		struct vm_gpa_pte gpapte;
+
+		if (ddi_copyin(datap, &gpapte, sizeof (gpapte), md)) {
+			error = EFAULT;
+			break;
+		}
+#ifdef __FreeBSD__
+		/* XXXJOY: add function? */
+		pmap_get_mapping(vmspace_pmap(vm_get_vmspace(sc->vmm_vm)),
+		    gpapte.gpa, gpapte.pte, &gpapte.ptenum);
+#endif
+		error = 0;
+		break;
+	}
+	case VM_GET_HPET_CAPABILITIES: {
+		struct vm_hpet_cap hpetcap;
+
+		error = vhpet_getcap(&hpetcap);
+		if (error == 0 &&
+		    ddi_copyout(&hpetcap, datap, sizeof (hpetcap), md)) {
+			error = EFAULT;
+			break;
+		}
+		break;
+	}
+	case VM_GLA2GPA: {
+		struct vm_gla2gpa gg;
+
+		CTASSERT(PROT_READ == VM_PROT_READ);
+		CTASSERT(PROT_WRITE == VM_PROT_WRITE);
+		CTASSERT(PROT_EXEC == VM_PROT_EXECUTE);
+
+		if (ddi_copyin(datap, &gg, sizeof (gg), md)) {
+			error = EFAULT;
+			break;
 		}
+		gg.vcpuid = vcpu;
+		error = vm_gla2gpa(sc->vmm_vm, vcpu, &gg.paging, gg.gla,
+		    gg.prot, &gg.gpa, &gg.fault);
+		if (error == 0 && ddi_copyout(&gg, datap, sizeof (gg), md)) {
+			error = EFAULT;
+			break;
+		}
+		break;
+	}
+	case VM_ACTIVATE_CPU:
+		error = vm_activate_cpu(sc->vmm_vm, vcpu);
 		break;
-	case VM_MAP_MEMORY:
-		if (ddi_copyin((void *)arg, &seg,
-		    sizeof (struct vm_memory_segment), mode)) {
-			return (EFAULT);
+
+	case VM_GET_CPUS: {
+		struct vm_cpuset vm_cpuset;
+		cpuset_t tempset;
+		void *srcp = &tempset;
+		int size;
+
+		if (ddi_copyin(datap, &vm_cpuset, sizeof (vm_cpuset), md)) {
+			error = EFAULT;
+			break;
 		}
-		error = vm_malloc(sc->vm, seg.gpa, seg.len);
-		break;
-	case VM_GET_MEMORY_SEG:
-		if (ddi_copyin((void *)arg, &seg,
-		    sizeof (struct vm_memory_segment), mode)) {
-			return (EFAULT);
+
+		/* Be more generous about sizing since our cpuset_t is large. */
+		size = vm_cpuset.cpusetsize;
+		if (size <= 0 || size > sizeof (cpuset_t)) {
+			error = ERANGE;
 		}
-		seg.len = 0;
-		(void) vm_gpabase2memseg(sc->vm, seg.gpa, &seg);
-		if (ddi_copyout(&seg, (void *)arg,
-		    sizeof (struct vm_memory_segment), mode)) {
-			return (EFAULT);
+		/*
+		 * If they want a ulong_t or less, make sure they receive the
+		 * low bits with all the useful information.
+		 */
+		if (size <= tempset.cpub[0]) {
+			srcp = &tempset.cpub[0];
 		}
-		error = 0;
-		break;
-	case VM_GET_REGISTER:
-		if (ddi_copyin((void *)arg, &vmreg,
-		    sizeof (struct vm_register), mode)) {
-			return (EFAULT);
+
+		if (vm_cpuset.which == VM_ACTIVE_CPUS) {
+			tempset = vm_active_cpus(sc->vmm_vm);
+		} else if (vm_cpuset.which == VM_SUSPENDED_CPUS) {
+			tempset = vm_suspended_cpus(sc->vmm_vm);
+		} else {
+			error = EINVAL;
 		}
-		error = vm_get_register(sc->vm, vmreg.cpuid, vmreg.regnum,
-		    &vmreg.regval);
-		if (!error) {
-			if (ddi_copyout(&vmreg, (void *)arg,
-			    sizeof (struct vm_register), mode)) {
-				return (EFAULT);
-			}
+
+		ASSERT(size > 0 && size <= sizeof (tempset));
+		if (error == 0 &&
+		    ddi_copyout(&tempset, vm_cpuset.cpus, size, md)) {
+			error = EFAULT;
+			break;
 		}
 		break;
-	case VM_SET_REGISTER:
-		if (ddi_copyin((void *)arg, &vmreg,
-		    sizeof (struct vm_register), mode)) {
-			return (EFAULT);
+	}
+	case VM_SET_INTINFO: {
+		struct vm_intinfo vmii;
+
+		if (ddi_copyin(datap, &vmii, sizeof (vmii), md)) {
+			error = EFAULT;
+			break;
 		}
-		error = vm_set_register(sc->vm, vmreg.cpuid, vmreg.regnum,
-		    vmreg.regval);
+		error = vm_exit_intinfo(sc->vmm_vm, vcpu, vmii.info1);
 		break;
-	case VM_SET_SEGMENT_DESCRIPTOR:
-		if (ddi_copyin((void *)arg, &vmsegdesc,
-		    sizeof (struct vm_seg_desc), mode)) {
-			return (EFAULT);
+	}
+	case VM_GET_INTINFO: {
+		struct vm_intinfo vmii;
+
+		vmii.vcpuid = vcpu;
+		error = vm_get_intinfo(sc->vmm_vm, vcpu, &vmii.info1,
+		    &vmii.info2);
+		if (error == 0 &&
+		    ddi_copyout(&vmii, datap, sizeof (vmii), md)) {
+			error = EFAULT;
+			break;
 		}
-		error = vm_set_seg_desc(sc->vm, vmsegdesc.cpuid,
-		    vmsegdesc.regnum, &vmsegdesc.desc);
 		break;
-	case VM_GET_SEGMENT_DESCRIPTOR:
-		if (ddi_copyin((void *)arg, &vmsegdesc,
-		    sizeof (struct vm_seg_desc), mode)) {
-			return (EFAULT);
-		}
-		error = vm_get_seg_desc(sc->vm, vmsegdesc.cpuid,
-		    vmsegdesc.regnum, &vmsegdesc.desc);
-		if (!error) {
-			if (ddi_copyout(&vmsegdesc, (void *)arg,
-			    sizeof (struct vm_seg_desc), mode)) {
-				return (EFAULT);
-			}
+	}
+	case VM_RTC_WRITE: {
+		struct vm_rtc_data rtcdata;
+
+		if (ddi_copyin(datap, &rtcdata, sizeof (rtcdata), md)) {
+			error = EFAULT;
+			break;
 		}
+		error = vrtc_nvram_write(sc->vmm_vm, rtcdata.offset,
+		    rtcdata.value);
 		break;
-	case VM_GET_CAPABILITY:
-		if (ddi_copyin((void *)arg, &vmcap,
-		    sizeof (struct vm_capability), mode)) {
-			return (EFAULT);
+	}
+	case VM_RTC_READ: {
+		struct vm_rtc_data rtcdata;
+
+		if (ddi_copyin(datap, &rtcdata, sizeof (rtcdata), md)) {
+			error = EFAULT;
+			break;
 		}
-		error = vm_get_capability(sc->vm, vmcap.cpuid,
-		    vmcap.captype, &vmcap.capval);
-		if (!error) {
-			if (ddi_copyout(&vmcap, (void *)arg,
-			    sizeof (struct vm_capability), mode)) {
-				return (EFAULT);
-			}
+		error = vrtc_nvram_read(sc->vmm_vm, rtcdata.offset,
+		    &rtcdata.value);
+		if (error == 0 &&
+		    ddi_copyout(&rtcdata, datap, sizeof (rtcdata), md)) {
+			error = EFAULT;
+			break;
 		}
 		break;
-	case VM_SET_CAPABILITY:
-		if (ddi_copyin((void *)arg, &vmcap,
-		    sizeof (struct vm_capability), mode)) {
-			return (EFAULT);
+	}
+	case VM_RTC_SETTIME: {
+		struct vm_rtc_time rtctime;
+
+		if (ddi_copyin(datap, &rtctime, sizeof (rtctime), md)) {
+			error = EFAULT;
+			break;
 		}
-		error = vm_set_capability(sc->vm, vmcap.cpuid,
-		    vmcap.captype, vmcap.capval);
+		error = vrtc_set_time(sc->vmm_vm, rtctime.secs);
 		break;
-	case VM_SET_X2APIC_STATE:
-		if (ddi_copyin((void *)arg, &x2apic,
-		    sizeof (struct vm_x2apic), mode)) {
-			return (EFAULT);
+	}
+	case VM_RTC_GETTIME: {
+		struct vm_rtc_time rtctime;
+
+		rtctime.secs = vrtc_get_time(sc->vmm_vm);
+		if (ddi_copyout(&rtctime, datap, sizeof (rtctime), md)) {
+			error = EFAULT;
+			break;
 		}
-		error = vm_set_x2apic_state(sc->vm, x2apic.cpuid, x2apic.state);
 		break;
-	case VM_GET_X2APIC_STATE:
-		if (ddi_copyin((void *)arg, &x2apic,
-		    sizeof (struct vm_x2apic), mode)) {
-			return (EFAULT);
-		}
-		error = vm_get_x2apic_state(sc->vm, x2apic.cpuid,
-		    &x2apic.state);
-		if (!error) {
-			if (ddi_copyout(&x2apic, (void *)arg,
-			    sizeof (struct vm_x2apic), mode)) {
-				return (EFAULT);
-			}
-		}
+	}
+
+	case VM_RESTART_INSTRUCTION:
+		error = vm_restart_instruction(sc->vmm_vm, vcpu);
 		break;
-	case VM_GLA2GPA: {
-		CTASSERT(PROT_READ == VM_PROT_READ);
-		CTASSERT(PROT_WRITE == VM_PROT_WRITE);
-		CTASSERT(PROT_EXEC == VM_PROT_EXECUTE);
 
-		if (ddi_copyin((void *)arg, &gg,
-		    sizeof (struct vm_gla2gpa), mode)) {
-			return (EFAULT);
+#ifndef __FreeBSD__
+	case VM_DEVMEM_GETOFFSET: {
+		struct vm_devmem_offset vdo;
+		list_t *dl = &sc->vmm_devmem_list;
+		vmm_devmem_entry_t *de = NULL;
+
+		if (ddi_copyin(datap, &vdo, sizeof (vdo), md) != 0) {
+			error = EFAULT;
+			break;
 		}
-		error = vm_gla2gpa(sc->vm, gg.vcpuid, &gg.paging, gg.gla,
-		    gg.prot, &gg.gpa);
-		KASSERT(error == 0 || error == 1 || error == -1,
-		    ("%s: vm_gla2gpa unknown error %d", __func__, error));
-		if (error >= 0) {
-			/*
-			 * error = 0: the translation was successful
-			 * error = 1: a fault was injected into the guest
-			 */
-			gg.fault = error;
-			error = 0;
-			if (ddi_copyout(&gg, (void *)arg,
-			    sizeof (struct vm_gla2gpa), mode)) {
-				return (EFAULT);
+
+		for (de = list_head(dl); de != NULL; de = list_next(dl, de)) {
+			if (de->vde_segid == vdo.segid) {
+				break;
+			}
+		}
+		if (de != NULL) {
+			vdo.offset = de->vde_off;
+			if (ddi_copyout(&vdo, datap, sizeof (vdo), md) != 0) {
+				error = EFAULT;
 			}
 		} else {
-			error = EFAULT;
+			error = ENOENT;
 		}
 		break;
 	}
-	case VM_ACTIVATE_CPU:
-		if (ddi_copyin((void *)arg, &vac,
-		    sizeof (struct vm_activate_cpu), mode)) {
-			return (EFAULT);
-		}
-		error = vm_activate_cpu(sc->vm, vac.vcpuid);
-		break;
-	case VM_RESTART_INSTRUCTION:
-		error = vm_restart_instruction(sc->vm, vcpu);
-		break;
+#endif
 	default:
 		error = ENOTTY;
 		break;
 	}
 
-	if (state_changed == 1) {
-		vcpu_set_state(sc->vm, vcpu, VCPU_IDLE, false);
-	} else if (state_changed == 2) {
-		for (vcpu = 0; vcpu < VM_MAXCPU; vcpu++) {
-			vcpu_set_state(sc->vm, vcpu, VCPU_IDLE, false);
-		}
+	/* Release any vCPUs that were locked for the operation */
+	if (locked_one) {
+		vcpu_unlock_one(sc, vcpu);
+	} else if (locked_all) {
+		vcpu_unlock_all(sc);
 	}
 
 done:
@@ -596,20 +1147,6 @@ done:
 	return (error);
 }
 
-static minor_t
-vmm_find_free_minor(void)
-{
-	minor_t		minor;
-
-	for (minor = 1; ; minor++) {
-		if (ddi_get_soft_state(vmm_statep, minor) == NULL)
-			break;
-	}
-
-	return (minor);
-}
-
-
 static boolean_t
 vmmdev_mod_incr()
 {
@@ -653,14 +1190,31 @@ vmmdev_mod_decr(void)
 	}
 }
 
+static vmm_softc_t *
+vmm_lookup(const char *name)
+{
+	list_t *vml = &vmmdev_list;
+	vmm_softc_t *sc;
+
+	ASSERT(MUTEX_HELD(&vmmdev_mtx));
+
+	for (sc = list_head(vml); sc != NULL; sc = list_next(vml, sc)) {
+		if (strcmp(sc->vmm_name, name) == 0) {
+			break;
+		}
+	}
+
+	return (sc);
+}
+
 static int
 vmmdev_do_vm_create(dev_info_t *dip, char *name)
 {
-	struct vmm_softc	*sc = NULL;
-	minor_t			minor;
-	int			error = ENOMEM;
+	vmm_softc_t	*sc = NULL;
+	minor_t		minor;
+	int		error = ENOMEM;
 
-	if (strlen(name) >= VM_MAX_NAMELEN) {
+	if (strnlen(name, VM_MAX_NAMELEN) >= VM_MAX_NAMELEN) {
 		return (EINVAL);
 	}
 
@@ -670,7 +1224,14 @@ vmmdev_do_vm_create(dev_info_t *dip, char *name)
 		return (ENXIO);
 	}
 
-	minor = vmm_find_free_minor();
+	/* Look for duplicates names */
+	if (vmm_lookup(name) != NULL) {
+		vmmdev_mod_decr();
+		mutex_exit(&vmmdev_mtx);
+		return (EEXIST);
+	}
+
+	minor = id_alloc(vmmdev_minors);
 	if (ddi_soft_state_zalloc(vmm_statep, minor) != DDI_SUCCESS) {
 		goto fail;
 	} else if ((sc = ddi_get_soft_state(vmm_statep, minor)) == NULL) {
@@ -681,18 +1242,24 @@ vmmdev_do_vm_create(dev_info_t *dip, char *name)
 		goto fail;
 	}
 
-	error = vm_create(name, &sc->vm);
+	error = vm_create(name, &sc->vmm_vm);
 	if (error == 0) {
 		/* Complete VM intialization and report success. */
-		strcpy(sc->name, name);
-		sc->minor = minor;
-		SLIST_INSERT_HEAD(&head, sc, link);
+		(void) strlcpy(sc->vmm_name, name, sizeof (sc->vmm_name));
+		sc->vmm_minor = minor;
+		list_create(&sc->vmm_devmem_list, sizeof (vmm_devmem_entry_t),
+		    offsetof(vmm_devmem_entry_t, vde_node));
+		list_create(&sc->vmm_holds, sizeof (vmm_hold_t),
+		    offsetof(vmm_hold_t, vmh_node));
+		cv_init(&sc->vmm_cv, NULL, CV_DEFAULT, NULL);
+		list_insert_tail(&vmmdev_list, sc);
 		mutex_exit(&vmmdev_mtx);
 		return (0);
 	}
 
 	ddi_remove_minor_node(dip, name);
 fail:
+	id_free(vmmdev_minors, minor);
 	vmmdev_mod_decr();
 	if (sc != NULL) {
 		ddi_soft_state_free(vmm_statep, minor);
@@ -701,90 +1268,250 @@ fail:
 	return (error);
 }
 
-static struct vmm_softc *
-vmm_lookup(char *name)
+int
+vmm_drv_hold(file_t *fp, cred_t *cr, vmm_hold_t **holdp)
+{
+	vnode_t *vp = fp->f_vnode;
+	const dev_t dev = vp->v_rdev;
+	minor_t minor;
+	minor_t major;
+	vmm_softc_t *sc;
+	vmm_hold_t *hold;
+	int err = 0;
+
+	if (vp->v_type != VCHR) {
+		return (ENXIO);
+	}
+	major = getmajor(dev);
+	minor = getminor(dev);
+
+	mutex_enter(&vmmdev_mtx);
+	if (vmm_dip == NULL) {
+		err = ENOENT;
+		goto out;
+	}
+	if (major != ddi_driver_major(vmm_dip) ||
+	    (sc = ddi_get_soft_state(vmm_statep, minor)) == NULL) {
+		err = ENOENT;
+		goto out;
+	}
+	/* XXXJOY: check cred permissions against instance */
+
+	if ((sc->vmm_flags & (VMM_CLEANUP|VMM_PURGED)) != 0) {
+		err = EBUSY;
+		goto out;
+	}
+
+	hold = kmem_zalloc(sizeof (*hold), KM_SLEEP);
+	hold->vmh_sc = sc;
+	hold->vmh_expired = B_FALSE;
+	list_insert_tail(&sc->vmm_holds, hold);
+	sc->vmm_flags |= VMM_HELD;
+	*holdp = hold;
+
+out:
+	mutex_exit(&vmmdev_mtx);
+	return (err);
+}
+
+void
+vmm_drv_rele(vmm_hold_t *hold)
 {
-	struct vmm_softc	*sc;
+	vmm_softc_t *sc;
 
-	SLIST_FOREACH(sc, &head, link) {
-		if (strcmp(sc->name, name) == 0) {
-			break;
-		}
+	ASSERT(hold != NULL);
+	ASSERT(hold->vmh_sc != NULL);
+	VERIFY(hold->vmh_ioport_hook_cnt == 0);
+
+	mutex_enter(&vmmdev_mtx);
+	sc = hold->vmh_sc;
+	list_remove(&sc->vmm_holds, hold);
+	if (list_is_empty(&sc->vmm_holds)) {
+		sc->vmm_flags &= ~VMM_HELD;
+		cv_broadcast(&sc->vmm_cv);
 	}
+	mutex_exit(&vmmdev_mtx);
+	kmem_free(hold, sizeof (*hold));
+}
 
-	return (sc);
+boolean_t
+vmm_drv_expired(vmm_hold_t *hold)
+{
+	ASSERT(hold != NULL);
+
+	return (hold->vmh_expired);
+}
+
+void *
+vmm_drv_gpa2kva(vmm_hold_t *hold, uintptr_t gpa, size_t sz)
+{
+	struct vm *vm;
+	struct vmspace *vmspace;
+
+	ASSERT(hold != NULL);
 
+	vm = hold->vmh_sc->vmm_vm;
+	vmspace = vm_get_vmspace(vm);
+
+	return (vmspace_find_kva(vmspace, gpa, sz));
 }
 
-struct vm *
-vm_lookup_by_name(char *name)
+int
+vmm_drv_ioport_hook(vmm_hold_t *hold, uint_t ioport, vmm_drv_rmem_cb_t rfunc,
+    vmm_drv_wmem_cb_t wfunc, void *arg, void **cookie)
 {
-	struct vmm_softc	*sc;
+	vmm_softc_t *sc;
+	int err;
 
+	ASSERT(hold != NULL);
+	ASSERT(cookie != NULL);
+
+	sc = hold->vmh_sc;
 	mutex_enter(&vmmdev_mtx);
+	/* Confirm that hook installation is not blocked */
+	if ((sc->vmm_flags & VMM_BLOCK_HOOK) != 0) {
+		mutex_exit(&vmmdev_mtx);
+		return (EBUSY);
+	}
+	/*
+	 * Optimistically record an installed hook which will prevent a block
+	 * from being asserted while the mutex is dropped.
+	 */
+	hold->vmh_ioport_hook_cnt++;
+	mutex_exit(&vmmdev_mtx);
 
-	if ((sc = vmm_lookup(name)) == NULL) {
+	err = vm_ioport_hook(sc->vmm_vm, ioport, (vmm_rmem_cb_t)rfunc,
+	    (vmm_wmem_cb_t)wfunc, arg, cookie);
+
+	if (err != 0) {
+		mutex_enter(&vmmdev_mtx);
+		/* Walk back optimism about the hook installation */
+		hold->vmh_ioport_hook_cnt--;
 		mutex_exit(&vmmdev_mtx);
-		return (NULL);
 	}
+	return (err);
+}
+
+void
+vmm_drv_ioport_unhook(vmm_hold_t *hold, void **cookie)
+{
+	vmm_softc_t *sc;
+
+	ASSERT(hold != NULL);
+	ASSERT(cookie != NULL);
+	ASSERT(hold->vmh_ioport_hook_cnt != 0);
 
+	sc = hold->vmh_sc;
+	vm_ioport_unhook(sc->vmm_vm, cookie);
+
+	mutex_enter(&vmmdev_mtx);
+	hold->vmh_ioport_hook_cnt--;
 	mutex_exit(&vmmdev_mtx);
+}
+
+static int
+vmm_drv_purge(vmm_softc_t *sc)
+{
+	ASSERT(MUTEX_HELD(&vmmdev_mtx));
+
+	if ((sc->vmm_flags & VMM_HELD) != 0) {
+		vmm_hold_t *hold;
+
+		sc->vmm_flags |= VMM_CLEANUP;
+		for (hold = list_head(&sc->vmm_holds); hold != NULL;
+		    hold = list_next(&sc->vmm_holds, hold)) {
+			hold->vmh_expired = B_TRUE;
+		}
+		while ((sc->vmm_flags & VMM_HELD) != 0) {
+			if (cv_wait_sig(&sc->vmm_cv, &vmmdev_mtx) <= 0) {
+				return (EINTR);
+			}
+		}
+		sc->vmm_flags &= ~VMM_CLEANUP;
+	}
 
-	return (sc->vm);
+	VERIFY(list_is_empty(&sc->vmm_holds));
+	sc->vmm_flags |= VMM_PURGED;
+	return (0);
 }
 
 static int
-vmmdev_do_vm_destroy(dev_info_t *dip, char *name)
+vmm_drv_block_hook(vmm_softc_t *sc, boolean_t enable_block)
 {
-	struct vmm_softc	*sc;
-	dev_info_t		*pdip = ddi_get_parent(dip);
+	int err = 0;
 
 	mutex_enter(&vmmdev_mtx);
+	if (!enable_block) {
+		VERIFY((sc->vmm_flags & VMM_BLOCK_HOOK) != 0);
 
-	if ((sc = vmm_lookup(name)) == NULL) {
-		mutex_exit(&vmmdev_mtx);
-		return (ENOENT);
-	}
-	if (sc->open) {
-		mutex_exit(&vmmdev_mtx);
-		return (EBUSY);
+		sc->vmm_flags &= ~VMM_BLOCK_HOOK;
+		goto done;
 	}
 
-	vm_destroy(sc->vm);
-	SLIST_REMOVE(&head, sc, vmm_softc, link);
-	ddi_remove_minor_node(dip, name);
-	ddi_soft_state_free(vmm_statep, sc->minor);
-	(void) devfs_clean(pdip, NULL, DV_CLEAN_FORCE);
-	vmmdev_mod_decr();
+	/* If any holds have hooks installed, the block is a failure */
+	if (!list_is_empty(&sc->vmm_holds)) {
+		vmm_hold_t *hold;
 
-	mutex_exit(&vmmdev_mtx);
+		for (hold = list_head(&sc->vmm_holds); hold != NULL;
+		    hold = list_next(&sc->vmm_holds, hold)) {
+			if (hold->vmh_ioport_hook_cnt != 0) {
+				err = EBUSY;
+				goto done;
+			}
+		}
+	}
+	sc->vmm_flags |= VMM_BLOCK_HOOK;
 
-	return (0);
+done:
+	mutex_exit(&vmmdev_mtx);
+	return (err);
 }
 
-int
-vmmdev_do_vm_mmap(struct vmm_softc *vmm_sc, off_t off, int nprot)
+static int
+vmmdev_do_vm_destroy(dev_info_t *dip, const char *name)
 {
-	vm_paddr_t	paddr;
+	vmm_softc_t	*sc;
+	dev_info_t	*pdip = ddi_get_parent(dip);
+	minor_t		minor;
 
 	mutex_enter(&vmmdev_mtx);
 
-	paddr = vm_gpa2hpa(vmm_sc->vm, (vm_paddr_t)off, PAGE_SIZE);
-	if (paddr == -1) {
-		return (-1);
+	if ((sc = vmm_lookup(name)) == NULL) {
+		mutex_exit(&vmmdev_mtx);
+		return (ENOENT);
+	}
+	if (sc->vmm_is_open) {
+		mutex_exit(&vmmdev_mtx);
+		return (EBUSY);
+	}
+	if (vmm_drv_purge(sc) != 0) {
+		mutex_exit(&vmmdev_mtx);
+		return (EINTR);
 	}
 
+	/* Clean up devmem entries */
+	vmmdev_devmem_purge(sc);
+
+	vm_destroy(sc->vmm_vm);
+	list_remove(&vmmdev_list, sc);
+	ddi_remove_minor_node(dip, sc->vmm_name);
+	minor = sc->vmm_minor;
+	ddi_soft_state_free(vmm_statep, minor);
+	id_free(vmmdev_minors, minor);
+	(void) devfs_clean(pdip, NULL, DV_CLEAN_FORCE);
+	vmmdev_mod_decr();
+
 	mutex_exit(&vmmdev_mtx);
 
-	return (btop(paddr));
+	return (0);
 }
 
 
 static int
 vmm_open(dev_t *devp, int flag, int otyp, cred_t *credp)
 {
-	minor_t			minor;
-	struct vmm_softc	*sc;
+	minor_t		minor;
+	vmm_softc_t	*sc;
 
 	minor = getminor(*devp);
 	if (minor == VMM_CTL_MINOR) {
@@ -805,11 +1532,7 @@ vmm_open(dev_t *devp, int flag, int otyp, cred_t *credp)
 		return (ENXIO);
 	}
 
-	if (sc->open) {
-		mutex_exit(&vmmdev_mtx);
-		return (EBUSY);
-	}
-	sc->open = B_TRUE;
+	sc->vmm_is_open = B_TRUE;
 	mutex_exit(&vmmdev_mtx);
 
 	return (0);
@@ -818,8 +1541,8 @@ vmm_open(dev_t *devp, int flag, int otyp, cred_t *credp)
 static int
 vmm_close(dev_t dev, int flag, int otyp, cred_t *credp)
 {
-	minor_t			minor;
-	struct vmm_softc	*sc;
+	minor_t		minor;
+	vmm_softc_t	*sc;
 
 	minor = getminor(dev);
 	if (minor == VMM_CTL_MINOR)
@@ -832,7 +1555,8 @@ vmm_close(dev_t dev, int flag, int otyp, cred_t *credp)
 		return (ENXIO);
 	}
 
-	sc->open = B_FALSE;
+	VERIFY(sc->vmm_is_open);
+	sc->vmm_is_open = B_FALSE;
 	mutex_exit(&vmmdev_mtx);
 
 	return (0);
@@ -842,28 +1566,39 @@ static int
 vmm_ioctl(dev_t dev, int cmd, intptr_t arg, int mode, cred_t *credp,
     int *rvalp)
 {
-	struct vmm_softc	*sc;
-	struct vmm_ioctl	kvi;
-	minor_t			minor;
+	vmm_softc_t	*sc;
+	minor_t		minor;
 
 	minor = getminor(dev);
 
 	if (minor == VMM_CTL_MINOR) {
-		if (ddi_copyin((void *)arg, &kvi, sizeof (struct vmm_ioctl),
-		    mode)) {
-			return (EFAULT);
+		void *argp = (void *)arg;
+		char name[VM_MAX_NAMELEN] = { 0 };
+		size_t len = 0;
+
+		if ((mode & FKIOCTL) != 0) {
+			len = strlcpy(name, argp, sizeof (name));
+		} else {
+			if (copyinstr(argp, name, sizeof (name), &len) != 0) {
+				return (EFAULT);
+			}
+		}
+		if (len >= VM_MAX_NAMELEN) {
+			return (ENAMETOOLONG);
 		}
+
 		switch (cmd) {
 		case VMM_CREATE_VM:
 			if ((mode & FWRITE) == 0)
 				return (EPERM);
-			return (vmmdev_do_vm_create(vmm_dip, kvi.vmm_name));
+			return (vmmdev_do_vm_create(vmm_dip, name));
 		case VMM_DESTROY_VM:
 			if ((mode & FWRITE) == 0)
 				return (EPERM);
-			return (vmmdev_do_vm_destroy(vmm_dip, kvi.vmm_name));
+			return (vmmdev_do_vm_destroy(vmm_dip, name));
 		default:
-			break;
+			/* No other actions are legal on ctl device */
+			return (ENOTTY);
 		}
 	}
 
@@ -873,47 +1608,60 @@ vmm_ioctl(dev_t dev, int cmd, intptr_t arg, int mode, cred_t *credp,
 	return (vmmdev_do_ioctl(sc, cmd, arg, mode, credp, rvalp));
 }
 
-static int
-vmm_mmap(dev_t dev, off_t off, int prot)
-{
-	struct vmm_softc	*sc;
-
-	sc = ddi_get_soft_state(vmm_statep, getminor(dev));
-	ASSERT(sc);
-
-	return (vmmdev_do_vm_mmap(sc, off, prot));
-}
-
 static int
 vmm_segmap(dev_t dev, off_t off, struct as *as, caddr_t *addrp, off_t len,
     unsigned int prot, unsigned int maxprot, unsigned int flags, cred_t *credp)
 {
-	struct segdev_crargs	dev_a;
-	int			error;
+	vmm_softc_t *sc;
+	const minor_t minor = getminor(dev);
+	struct vm *vm;
+	int err;
+	vm_object_t vmo = NULL;
+	struct vmspace *vms;
+
+	if (minor == VMM_CTL_MINOR) {
+		return (ENODEV);
+	}
+	if (off < 0 || (off + len) <= 0) {
+		return (EINVAL);
+	}
+	if ((prot & PROT_USER) == 0) {
+		return (EACCES);
+	}
 
-	as_rangelock(as);
+	sc = ddi_get_soft_state(vmm_statep, minor);
+	ASSERT(sc);
 
-	error = choose_addr(as, addrp, len, off, ADDR_VACALIGN, flags);
-	if (error != 0) {
-		as_rangeunlock(as);
-		return (error);
+	/* Get a read lock on the guest memory map by freezing any vcpu. */
+	if ((err = vcpu_lock_all(sc)) != 0) {
+		return (err);
 	}
 
-	dev_a.mapfunc = vmm_mmap;
-	dev_a.dev = dev;
-	dev_a.offset = off;
-	dev_a.type = (flags & MAP_TYPE);
-	dev_a.prot = (uchar_t)prot;
-	dev_a.maxprot = (uchar_t)maxprot;
-	dev_a.hat_attr = 0;
-	dev_a.hat_flags = HAT_LOAD_NOCONSIST;
-	dev_a.devmap_data = NULL;
+	vm = sc->vmm_vm;
+	vms = vm_get_vmspace(vm);
+	if (off >= VM_DEVMEM_START) {
+		int segid;
 
-	error = as_map(as, *addrp, len, segdev_create, &dev_a);
+		/* Mapping a devmem "device" */
+		if (!vmmdev_devmem_segid(sc, off, len, &segid)) {
+			err = ENODEV;
+			goto out;
+		}
+		err = vm_get_memseg(vm, segid, NULL, NULL, &vmo);
+		if (err != 0) {
+			goto out;
+		}
+		err = vm_segmap_obj(vms, vmo, as, addrp, prot, maxprot, flags);
+	} else {
+		/* Mapping a part of the guest physical space */
+		err = vm_segmap_space(vms, off, as, addrp, len, prot, maxprot,
+		    flags);
+	}
 
-	as_rangeunlock(as);
 
-	return (error);
+out:
+	vcpu_unlock_all(sc);
+	return (err);
 }
 
 static int
@@ -940,6 +1688,9 @@ vmm_attach(dev_info_t *dip, ddi_attach_cmd_t cmd)
 
 	ddi_report_dev(dip);
 
+	/* XXX: This needs updating */
+	vmm_arena_init();
+
 	return (DDI_SUCCESS);
 }
 
@@ -955,7 +1706,13 @@ vmm_detach(dev_info_t *dip, ddi_detach_cmd_t cmd)
 
 	/* Ensure that all resources have been cleaned up */
 	mutex_enter(&vmmdev_mtx);
-	if (!SLIST_EMPTY(&head) || vmmdev_inst_count != 0) {
+	if (!list_is_empty(&vmmdev_list) || vmmdev_inst_count != 0) {
+		mutex_exit(&vmmdev_mtx);
+		return (DDI_FAILURE);
+	}
+
+	/* XXX: This needs updating */
+	if (!vmm_arena_fini()) {
 		mutex_exit(&vmmdev_mtx);
 		return (DDI_FAILURE);
 	}
@@ -979,7 +1736,7 @@ static struct cb_ops vmm_cb_ops = {
 	nodev,		/* write */
 	vmm_ioctl,
 	nodev,		/* devmap */
-	vmm_mmap,
+	nodev,		/* mmap */
 	vmm_segmap,
 	nochpoll,	/* poll */
 	ddi_prop_op,
@@ -1018,8 +1775,12 @@ _init(void)
 	int	error;
 
 	mutex_init(&vmmdev_mtx, NULL, MUTEX_DRIVER, NULL);
+	list_create(&vmmdev_list, sizeof (vmm_softc_t),
+	    offsetof(vmm_softc_t, vmm_node));
+	vmmdev_minors = id_space_create("vmm_minors", VMM_CTL_MINOR + 1,
+	    MAXMIN32);
 
-	error = ddi_soft_state_init(&vmm_statep, sizeof (struct vmm_softc), 0);
+	error = ddi_soft_state_init(&vmm_statep, sizeof (vmm_softc_t), 0);
 	if (error) {
 		return (error);
 	}
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_sol_glue.c b/usr/src/uts/i86pc/io/vmm/vmm_sol_glue.c
index 584a4978cc..0e32f0c47b 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_sol_glue.c
+++ b/usr/src/uts/i86pc/io/vmm/vmm_sol_glue.c
@@ -1,30 +1,3 @@
-/*
- * Copyright (c) 2004 John Baldwin <jhb@FreeBSD.org>
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *
- * THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS ``AS IS'' AND
- * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE
- * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
- * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
- * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
- * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
- * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
- * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
- * SUCH DAMAGE.
- *
- * $FreeBSD: head/sys/kern/subr_sleepqueue.c 261520 2014-02-05 18:13:27Z jhb $
- */
 /*
  * Copyright (c) 2004 Poul-Henning Kamp
  * All rights reserved.
@@ -74,6 +47,10 @@
 #include <sys/queue.h>
 #include <sys/spl.h>
 #include <sys/systm.h>
+#include <sys/ddidmareq.h>
+#include <sys/id_space.h>
+#include <sys/psm_defs.h>
+#include <sys/smp_impldefs.h>
 
 #include <machine/cpufunc.h>
 #include <machine/fpu.h>
@@ -85,11 +62,33 @@
 #include <vm/as.h>
 #include <vm/seg_kmem.h>
 
+
+u_char const bin2bcd_data[] = {
+	0x00, 0x01, 0x02, 0x03, 0x04, 0x05, 0x06, 0x07, 0x08, 0x09,
+	0x10, 0x11, 0x12, 0x13, 0x14, 0x15, 0x16, 0x17, 0x18, 0x19,
+	0x20, 0x21, 0x22, 0x23, 0x24, 0x25, 0x26, 0x27, 0x28, 0x29,
+	0x30, 0x31, 0x32, 0x33, 0x34, 0x35, 0x36, 0x37, 0x38, 0x39,
+	0x40, 0x41, 0x42, 0x43, 0x44, 0x45, 0x46, 0x47, 0x48, 0x49,
+	0x50, 0x51, 0x52, 0x53, 0x54, 0x55, 0x56, 0x57, 0x58, 0x59,
+	0x60, 0x61, 0x62, 0x63, 0x64, 0x65, 0x66, 0x67, 0x68, 0x69,
+	0x70, 0x71, 0x72, 0x73, 0x74, 0x75, 0x76, 0x77, 0x78, 0x79,
+	0x80, 0x81, 0x82, 0x83, 0x84, 0x85, 0x86, 0x87, 0x88, 0x89,
+	0x90, 0x91, 0x92, 0x93, 0x94, 0x95, 0x96, 0x97, 0x98, 0x99
+};
+
 vm_paddr_t
 pmap_kextract(vm_offset_t va)
 {
 	pfn_t	pfn;
 
+	/*
+	 * Since hat_getpfnum() may block on an htable mutex, this is not at
+	 * all safe to run from a critical_enter/kpreempt_disable context.
+	 * The FreeBSD analog does not have the same locking constraints, so
+	 * close attention must be paid wherever this is called.
+	 */
+	ASSERT(curthread->t_preempt == 0);
+
 	pfn = hat_getpfnum(kas.a_hat, (caddr_t)va);
 	ASSERT(pfn != PFN_INVALID);
 	return (pfn << PAGE_SHIFT) | ((uintptr_t)va & PAGE_MASK);
@@ -180,6 +179,48 @@ free(void *addr, struct malloc_type *mtp)
 	kmem_free(addr, i->size + sizeof (struct kmem_item));
 }
 
+extern void *contig_alloc(size_t, ddi_dma_attr_t *, uintptr_t, int);
+extern void contig_free(void *, size_t);
+
+void *
+contigmalloc(unsigned long size, struct malloc_type *type, int flags,
+    vm_paddr_t low, vm_paddr_t high, unsigned long alignment,
+    vm_paddr_t boundary)
+{
+	ddi_dma_attr_t attr = {
+		/* Using fastboot_dma_attr as a guide... */
+		DMA_ATTR_V0,
+		low,			/* dma_attr_addr_lo */
+		high,			/* dma_attr_addr_hi */
+		0x00000000FFFFFFFFULL,	/* dma_attr_count_max */
+		alignment,		/* dma_attr_align */
+		1,			/* dma_attr_burstsize */
+		1,			/* dma_attr_minxfer */
+		0x00000000FFFFFFFFULL,	/* dma_attr_maxxfer */
+		0x00000000FFFFFFFFULL,	/* dma_attr_seg: any */
+		1,			/* dma_attr_sgllen */
+		alignment,		/* dma_attr_granular */
+		0,			/* dma_attr_flags */
+	};
+	int cansleep = (flags & M_WAITOK);
+	void *result;
+
+	ASSERT(alignment == PAGESIZE);
+
+	result = contig_alloc((size_t)size, &attr, alignment, cansleep);
+
+	if (result != NULL && (flags & M_ZERO) != 0) {
+		bzero(result, size);
+	}
+	return (result);
+}
+
+void
+contigfree(void *addr, unsigned long size, struct malloc_type *type)
+{
+	contig_free(addr, size);
+}
+
 void
 mtx_init(struct mtx *mtx, char *name, const char *type_name, int opts)
 {
@@ -201,34 +242,17 @@ void
 critical_enter(void)
 {
 	kpreempt_disable();
-	thread_affinity_set(curthread, CPU_CURRENT);
 }
 
 void
 critical_exit(void)
 {
-	thread_affinity_clear(curthread);
 	kpreempt_enable();
 }
 
-struct unr {
-	u_int		item;
-	struct unr	*link;
-};
-
-#define	UNR_HASHSIZE	8
-
-struct unrhdr {
-	struct mtx	*mtx;
-	struct unr	*hash[UNR_HASHSIZE];
-	u_int		min;
-	u_int		max;
-	u_int		next;
-};
-
-#define	HASH_UNR(uh, i)	((uh)->hash[(i) & ((UNR_HASHSIZE) - 1)])
-
-static struct mtx unr_mtx;
+struct unrhdr;
+static kmutex_t unr_lock;
+static uint_t unr_idx;
 
 /*
  * Allocate a new unrheader set.
@@ -238,91 +262,44 @@ static struct mtx unr_mtx;
 struct unrhdr *
 new_unrhdr(int low, int high, struct mtx *mtx)
 {
-	struct unrhdr	*uh;
+	id_space_t *ids;
+	char name[] = "vmm_unr_00000000";
 
-	uh = kmem_zalloc(sizeof (struct unrhdr), KM_SLEEP);
-	if (mtx) {
-		uh->mtx = mtx;
-	} else {
-		uh->mtx = &unr_mtx;
-	}
-	uh->min = low;
-	uh->max = high;
-	uh->next = uh->min;
+	ASSERT(mtx == NULL);
+
+	mutex_enter(&unr_lock);
+	/* Get a unique name for the id space */
+	(void) snprintf(name, sizeof (name), "vmm_unr_%08X", unr_idx);
+	VERIFY(++unr_idx != UINT_MAX);
+	mutex_exit(&unr_lock);
 
-	return (uh);
+	ids = id_space_create(name, low, high);
+
+	return ((struct unrhdr *)ids);
 }
 
 void
 delete_unrhdr(struct unrhdr *uh)
 {
-	kmem_free(uh, sizeof (struct unrhdr));
-}
-
-static struct unr *
-unr_lookup(struct unrhdr *uh, int item)
-{
-	struct unr	*unr;
-
-	ASSERT(MUTEX_HELD(&uh->mtx->m));
+	id_space_t *ids = (id_space_t *)uh;
 
-	for (unr = HASH_UNR(uh, item); unr != NULL; unr = unr->link) {
-		if (unr->item == item)
-			break;
-	}
-
-	return (unr);
+	id_space_destroy(ids);
 }
 
 int
 alloc_unr(struct unrhdr *uh)
 {
-	struct unr	*unr;
-	int		item, start;
+	id_space_t *ids = (id_space_t *)uh;
 
-	mutex_enter(&uh->mtx->m);
-	start = uh->next;
-	for (;;) {
-		item = uh->next;
-		if (++uh->next == uh->max) {
-			uh->next = uh->min;
-		}
-
-		if (unr_lookup(uh, item) == NULL) {
-			unr = kmem_zalloc(sizeof (struct unr), KM_SLEEP);
-			unr->item = item;
-			unr->link = HASH_UNR(uh, item);
-			HASH_UNR(uh, item) = unr;
-			break;
-		}
-
-		if (item == start) {
-			item = -1;
-			break;
-		}
-	}
-	mutex_exit(&uh->mtx->m);
-
-	return (item);
+	return (id_alloc(ids));
 }
 
 void
 free_unr(struct unrhdr *uh, u_int item)
 {
-	struct unr	*unr, **unrp;
+	id_space_t *ids = (id_space_t *)uh;
 
-	mutex_enter(&uh->mtx->m);
-	unrp = &HASH_UNR(uh, item);
-	for (;;) {
-		ASSERT(*unrp != NULL);
-		if ((*unrp)->item == item)
-			break;
-		unrp = &(*unrp)->link;
-	}
-	unr = *unrp;
-	*unrp = unr->link;
-	mutex_exit(&uh->mtx->m);
-	kmem_free(unr, sizeof (struct unr));
+	id_free(ids, item);
 }
 
 
@@ -350,25 +327,43 @@ vmm_glue_callout_init(struct callout *c, int mpsafe)
 	when.cyt_interval = CY_INFINITY;
 
 	mutex_enter(&cpu_lock);
-	c->c_cyc_id = cyclic_add(&hdlr, &when);
+#if 0
+	/*
+	 * XXXJOY: according to the freebsd sources, callouts do not begin
+	 * their life in the ACTIVE state.
+	 */
 	c->c_flags |= CALLOUT_ACTIVE;
+#else
+	bzero(c, sizeof (*c));
+#endif
+	c->c_cyc_id = cyclic_add(&hdlr, &when);
 	mutex_exit(&cpu_lock);
 }
 
+static __inline hrtime_t
+sbttohrtime(sbintime_t sbt)
+{
+	return (((sbt >> 32) * NANOSEC) +
+	    (((uint64_t)NANOSEC * (uint32_t)sbt) >> 32));
+}
+
 int
 vmm_glue_callout_reset_sbt(struct callout *c, sbintime_t sbt, sbintime_t pr,
     void (*func)(void *), void *arg, int flags)
 {
+	hrtime_t target = sbttohrtime(sbt);
+
 	ASSERT(c->c_cyc_id != CYCLIC_NONE);
 
 	c->c_func = func;
 	c->c_arg = arg;
 	c->c_flags |= (CALLOUT_ACTIVE | CALLOUT_PENDING);
 
-	if (flags & C_ABSOLUTE)
-		cyclic_reprogram(c->c_cyc_id, sbt);
-	else
-		cyclic_reprogram(c->c_cyc_id, sbt + gethrtime());
+	if (flags & C_ABSOLUTE) {
+		cyclic_reprogram(c->c_cyc_id, target);
+	} else {
+		cyclic_reprogram(c->c_cyc_id, target + gethrtime());
+	}
 
 	return (0);
 }
@@ -396,200 +391,16 @@ vmm_glue_callout_drain(struct callout *c)
 	return (0);
 }
 
-static int
-ipi_cpu_justreturn(xc_arg_t a1, xc_arg_t a2, xc_arg_t a3)
-{
-	return (0);
-}
-
 void
 ipi_cpu(int cpu, u_int ipi)
 {
-	cpuset_t	set;
-
-	CPUSET_ONLY(set, cpu);
-	xc_call_nowait(NULL, NULL, NULL, CPUSET2BV(set), ipi_cpu_justreturn);
-}
-
-#define	SC_TABLESIZE	256			/* Must be power of 2. */
-#define	SC_MASK		(SC_TABLESIZE - 1)
-#define	SC_SHIFT	8
-#define	SC_HASH(wc)	((((uintptr_t)(wc) >> SC_SHIFT) ^ (uintptr_t)(wc)) & \
-			    SC_MASK)
-#define	SC_LOOKUP(wc)	&sleepq_chains[SC_HASH(wc)]
-
-struct sleepqueue {
-	u_int sq_blockedcnt;			/* Num. of blocked threads. */
-	LIST_ENTRY(sleepqueue) sq_hash;		/* Chain. */
-	void		*sq_wchan;		/* Wait channel. */
-	kcondvar_t	sq_cv;
-};
-
-struct sleepqueue_chain {
-	LIST_HEAD(, sleepqueue) sc_queues;	/* List of sleep queues. */
-	struct mtx	sc_lock;		/* Spin lock for this chain. */
-};
-
-static struct sleepqueue_chain	sleepq_chains[SC_TABLESIZE];
-
-#define	SLEEPQ_CACHE_SZ		(64)
-static kmem_cache_t		*vmm_sleepq_cache;
-
-static int
-vmm_sleepq_cache_init(void *buf, void *user_arg, int kmflags)
-{
-	struct sleepqueue *sq = (struct sleepqueue *)buf;
-
-	bzero(sq, sizeof (struct sleepqueue));
-	cv_init(&sq->sq_cv, NULL, CV_DRIVER, NULL);
-
-	return (0);
-}
-
-static void
-vmm_sleepq_cache_fini(void *buf, void *user_arg)
-{
-	struct sleepqueue *sq = (struct sleepqueue *)buf;
-	cv_destroy(&sq->sq_cv);
-}
-
-static void
-init_sleepqueues(void)
-{
-	int	i;
-
-        for (i = 0; i < SC_TABLESIZE; i++) {
-		LIST_INIT(&sleepq_chains[i].sc_queues);
-		mtx_init(&sleepq_chains[i].sc_lock, "sleepq chain", NULL,
-		    MTX_SPIN);
-	}
-
-	vmm_sleepq_cache = kmem_cache_create("vmm_sleepq_cache",
-	    sizeof (struct sleepqueue), SLEEPQ_CACHE_SZ, vmm_sleepq_cache_init,
-	    vmm_sleepq_cache_fini, NULL, NULL, NULL, 0);
-
-}
-
-/*
- * Lock the sleep queue chain associated with the specified wait channel.
- */
-static void
-sleepq_lock(void *wchan)
-{
-	struct sleepqueue_chain *sc;
-
-	sc = SC_LOOKUP(wchan);
-	mtx_lock_spin(&sc->sc_lock);
-}
-
-/*
- * Look up the sleep queue associated with a given wait channel in the hash
- * table locking the associated sleep queue chain.  If no queue is found in
- * the table, NULL is returned.
- */
-static struct sleepqueue *
-sleepq_lookup(void *wchan)
-{
-	struct sleepqueue_chain	*sc;
-	struct sleepqueue	*sq;
-
-	KASSERT(wchan != NULL, ("%s: invalid NULL wait channel", __func__));
-	sc = SC_LOOKUP(wchan);
-	mtx_assert(&sc->sc_lock, MA_OWNED);
-	LIST_FOREACH(sq, &sc->sc_queues, sq_hash)
-		if (sq->sq_wchan == wchan)
-			return (sq);
-	return (NULL);
-}
-
-/*
- * Unlock the sleep queue chain associated with a given wait channel.
- */
-static void
-sleepq_release(void *wchan)
-{
-	struct sleepqueue_chain *sc;
-
-	sc = SC_LOOKUP(wchan);
-	mtx_unlock_spin(&sc->sc_lock);
-}
-
-struct sleepqueue *
-sleepq_add(void *wchan)
-{
-	struct sleepqueue_chain	*sc;
-	struct sleepqueue	*sq;
-
-	sc = SC_LOOKUP(wchan);
-
-	/* Look up the sleep queue associated with the wait channel 'wchan'. */
-	sq = sleepq_lookup(wchan);
-
-	if (sq == NULL) {
-		sq = kmem_cache_alloc(vmm_sleepq_cache, KM_SLEEP);
-		LIST_INSERT_HEAD(&sc->sc_queues, sq, sq_hash);
-		sq->sq_wchan = wchan;
-	}
-
-	sq->sq_blockedcnt++;
-
-	return (sq);
-}
-
-void
-sleepq_remove(struct sleepqueue *sq)
-{
-	sq->sq_blockedcnt--;
-
-	if (sq->sq_blockedcnt == 0) {
-		LIST_REMOVE(sq, sq_hash);
-		kmem_cache_free(vmm_sleepq_cache, sq);
-	}
-}
-
-int
-msleep_spin(void *chan, struct mtx *mtx, const char *wmesg, int ticks)
-{
-	struct sleepqueue	*sq;
-	int			error = 0;
-
-	sleepq_lock(chan);
-	sq = sleepq_add(chan);
-	sleepq_release(chan);
-
-	cv_reltimedwait(&sq->sq_cv, &mtx->m, ticks, TR_CLOCK_TICK);
-
-	sleepq_lock(chan);
-	sleepq_remove(sq);
-	sleepq_release(chan);
-
-	return (error);
-}
-
-void
-wakeup(void *chan)
-{
-	struct sleepqueue	*sq;
-
-	sleepq_lock(chan);
-	sq = sleepq_lookup(chan);
-	if (sq != NULL) {
-		cv_broadcast(&sq->sq_cv);
-	}
-	sleepq_release(chan);
-}
-
-void
-wakeup_one(void *chan)
-{
-	struct sleepqueue	*sq;
-
-	sleepq_lock(chan);
-	sq = sleepq_lookup(chan);
-	if (sq != NULL) {
-		cv_signal(&sq->sq_cv);
-	}
-	sleepq_release(chan);
+	/*
+	 * This was previously implemented as an invocation of asynchronous
+	 * no-op crosscalls to interrupt the target CPU.  Since even nowait
+	 * crosscalls can block in certain circumstances, a direct poke_cpu()
+	 * is safer when called from delicate contexts.
+	 */
+	poke_cpu(cpu);
 }
 
 u_int	cpu_high;		/* Highest arg to CPUID */
@@ -774,12 +585,224 @@ vmm_sol_glue_init(void)
 {
 	vmm_cpuid_init();
 	fpu_save_area_init();
-	init_sleepqueues();
+	unr_idx = 0;
 }
 
 void
 vmm_sol_glue_cleanup(void)
 {
 	fpu_save_area_cleanup();
-	kmem_cache_destroy(vmm_sleepq_cache);
+}
+
+int idtvec_justreturn;
+
+int
+lapic_ipi_alloc(int *id)
+{
+	/* Only poke_cpu() equivalent is supported */
+	VERIFY(id == &idtvec_justreturn);
+
+	/*
+	 * This is only used by VMX to allocate a do-nothing vector for
+	 * interrupting other running CPUs.  The cached poke_cpu() vector
+	 * as an "allocation" is perfect for this.
+	 */
+	if (psm_cached_ipivect != NULL) {
+		return (psm_cached_ipivect(XC_CPUPOKE_PIL, PSM_INTR_POKE));
+	}
+
+	return (-1);
+}
+
+void
+lapic_ipi_free(int vec)
+{
+	VERIFY(vec > 0);
+
+	/*
+	 * A cached vector was used in the first place.
+	 * No deallocation is necessary
+	 */
+	return;
+}
+
+
+/* From FreeBSD's sys/kern/subr_clock.c */
+
+/*-
+ * Copyright (c) 1988 University of Utah.
+ * Copyright (c) 1982, 1990, 1993
+ *	The Regents of the University of California.  All rights reserved.
+ *
+ * This code is derived from software contributed to Berkeley by
+ * the Systems Programming Group of the University of Utah Computer
+ * Science Department.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ * 4. Neither the name of the University nor the names of its contributors
+ *    may be used to endorse or promote products derived from this software
+ *    without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ *	from: Utah $Hdr: clock.c 1.18 91/01/21$
+ *	from: @(#)clock.c	8.2 (Berkeley) 1/12/94
+ *	from: NetBSD: clock_subr.c,v 1.6 2001/07/07 17:04:02 thorpej Exp
+ *	and
+ *	from: src/sys/i386/isa/clock.c,v 1.176 2001/09/04
+ */
+
+#include <sys/clock.h>
+
+/*--------------------------------------------------------------------*
+ * Generic routines to convert between a POSIX date
+ * (seconds since 1/1/1970) and yr/mo/day/hr/min/sec
+ * Derived from NetBSD arch/hp300/hp300/clock.c
+ */
+
+#define	FEBRUARY	2
+#define	days_in_year(y) 	(leapyear(y) ? 366 : 365)
+#define	days_in_month(y, m) \
+	(month_days[(m) - 1] + (m == FEBRUARY ? leapyear(y) : 0))
+/* Day of week. Days are counted from 1/1/1970, which was a Thursday */
+#define	day_of_week(days)	(((days) + 4) % 7)
+
+static const int month_days[12] = {
+	31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31
+};
+
+
+/*
+ * This inline avoids some unnecessary modulo operations
+ * as compared with the usual macro:
+ *   ( ((year % 4) == 0 &&
+ *      (year % 100) != 0) ||
+ *     ((year % 400) == 0) )
+ * It is otherwise equivalent.
+ */
+static int
+leapyear(int year)
+{
+	int rv = 0;
+
+	if ((year & 3) == 0) {
+		rv = 1;
+		if ((year % 100) == 0) {
+			rv = 0;
+			if ((year % 400) == 0)
+				rv = 1;
+		}
+	}
+	return (rv);
+}
+
+int
+clock_ct_to_ts(struct clocktime *ct, struct timespec *ts)
+{
+	int i, year, days;
+
+	year = ct->year;
+
+#ifdef __FreeBSD__
+	if (ct_debug) {
+		printf("ct_to_ts(");
+		print_ct(ct);
+		printf(")");
+	}
+#endif
+
+	/* Sanity checks. */
+	if (ct->mon < 1 || ct->mon > 12 || ct->day < 1 ||
+	    ct->day > days_in_month(year, ct->mon) ||
+	    ct->hour > 23 ||  ct->min > 59 || ct->sec > 59 ||
+	    (sizeof(time_t) == 4 && year > 2037)) {	/* time_t overflow */
+#ifdef __FreeBSD__
+		if (ct_debug)
+			printf(" = EINVAL\n");
+#endif
+		return (EINVAL);
+	}
+
+	/*
+	 * Compute days since start of time
+	 * First from years, then from months.
+	 */
+	days = 0;
+	for (i = POSIX_BASE_YEAR; i < year; i++)
+		days += days_in_year(i);
+
+	/* Months */
+	for (i = 1; i < ct->mon; i++)
+	  	days += days_in_month(year, i);
+	days += (ct->day - 1);
+
+	ts->tv_sec = (((time_t)days * 24 + ct->hour) * 60 + ct->min) * 60 +
+	    ct->sec;
+	ts->tv_nsec = ct->nsec;
+
+#ifdef __FreeBSD__
+	if (ct_debug)
+		printf(" = %ld.%09ld\n", (long)ts->tv_sec, (long)ts->tv_nsec);
+#endif
+	return (0);
+}
+
+void
+clock_ts_to_ct(struct timespec *ts, struct clocktime *ct)
+{
+	int i, year, days;
+	time_t rsec;	/* remainder seconds */
+	time_t secs;
+
+	secs = ts->tv_sec;
+	days = secs / SECDAY;
+	rsec = secs % SECDAY;
+
+	ct->dow = day_of_week(days);
+
+	/* Subtract out whole years, counting them in i. */
+	for (year = POSIX_BASE_YEAR; days >= days_in_year(year); year++)
+		days -= days_in_year(year);
+	ct->year = year;
+
+	/* Subtract out whole months, counting them in i. */
+	for (i = 1; days >= days_in_month(year, i); i++)
+		days -= days_in_month(year, i);
+	ct->mon = i;
+
+	/* Days are what is left over (+1) from all that. */
+	ct->day = days + 1;
+
+	/* Hours, minutes, seconds are easy */
+	ct->hour = rsec / 3600;
+	rsec = rsec % 3600;
+	ct->min  = rsec / 60;
+	rsec = rsec % 60;
+	ct->sec  = rsec;
+	ct->nsec = ts->tv_nsec;
+#ifdef __FreeBSD__
+	if (ct_debug) {
+		printf("ts_to_ct(%ld.%09ld) = ",
+		    (long)ts->tv_sec, (long)ts->tv_nsec);
+		print_ct(ct);
+		printf("\n");
+	}
+#endif
 }
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_sol_mem.c b/usr/src/uts/i86pc/io/vmm/vmm_sol_mem.c
deleted file mode 100644
index cb97db106b..0000000000
--- a/usr/src/uts/i86pc/io/vmm/vmm_sol_mem.c
+++ /dev/null
@@ -1,109 +0,0 @@
-/*
- * Copyright (c) 2011 NetApp, Inc.
- * All rights reserved.
- *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions
- * are met:
- * 1. Redistributions of source code must retain the above copyright
- *    notice, this list of conditions and the following disclaimer.
- * 2. Redistributions in binary form must reproduce the above copyright
- *    notice, this list of conditions and the following disclaimer in the
- *    documentation and/or other materials provided with the distribution.
- *
- * THIS SOFTWARE IS PROVIDED BY NETAPP, INC ``AS IS'' AND
- * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
- * ARE DISCLAIMED.  IN NO EVENT SHALL NETAPP, INC OR CONTRIBUTORS BE LIABLE
- * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
- * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
- * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
- * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
- * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
- * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
- * SUCH DAMAGE.
- *
- * $FreeBSD: head/sys/amd64/vmm/vmm_mem.c 245678 2013-01-20 03:42:49Z neel $
- */
-/*
- * This file and its contents are supplied under the terms of the
- * Common Development and Distribution License ("CDDL"), version 1.0.
- * You may only use this file in accordance with the terms of version
- * 1.0 of the CDDL.
- *
- * A full copy of the text of the CDDL should have accompanied this
- * source.  A copy of the CDDL is also available via the Internet at
- * http://www.illumos.org/license/CDDL.
- *
- * Copyright 2013 Pluribus Networks Inc.
- */
-
-#include <sys/cdefs.h>
-
-#include <sys/param.h>
-#include <sys/lock.h>
-#include <sys/mutex.h>
-#include <sys/systm.h>
-#include <sys/malloc.h>
-#include <sys/kernel.h>
-
-#include <vm/vm.h>
-#include <machine/pmap.h>
-
-#include <sys/ddi.h>
-
-#include "vmm_util.h"
-#include "vmm_mem.h"
-
-int
-vmm_mem_init(void)
-{
-	return (0);
-}
-
-vm_paddr_t
-vmm_mem_alloc(size_t size)
-{
-	clock_t usec = 2 * 1000000;
-	vm_paddr_t pa;
-	caddr_t addr;
-
-	if (size != PAGE_SIZE)
-		panic("vmm_mem_alloc: invalid allocation size %lu", size);
-
-	while (usec > 0) {
-		if ((addr = kmem_zalloc(PAGE_SIZE, KM_NOSLEEP)) != NULL) {
-			ASSERT(((uintptr_t)addr & PAGE_MASK) == 0);
-			pa = vtophys((vm_offset_t)addr);
-			return (pa);
-		}
-		delay(drv_usectohz((clock_t)500000));
-		usec -= 500000;
-	}
-
-	return (NULL);
-}
-
-void
-vmm_mem_free(vm_paddr_t base, size_t length)
-{
-	page_t	*pp;
-
-	if (base & PAGE_MASK) {
-		panic("vmm_mem_free: base 0x%0lx must be aligned on a "
-		    "0x%0x boundary\n", base, PAGE_SIZE);
-	}
-
-	if (length != PAGE_SIZE) {
-		panic("vmm_mem_free: invalid length %lu", length);
-	}
-
-	pp = page_numtopp_nolock(btop(base));
-	kmem_free((void *)pp->p_offset, PAGE_SIZE);
-}
-
-vm_paddr_t
-vmm_mem_maxaddr(void)
-{
-	return (ptob(physmax + 1));
-}
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c b/usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c
new file mode 100644
index 0000000000..0ad4ee2284
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c
@@ -0,0 +1,1437 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2017 Joyent, Inc.
+ */
+
+#include <sys/param.h>
+#include <sys/kmem.h>
+#include <sys/thread.h>
+#include <sys/list.h>
+#include <sys/mman.h>
+#include <sys/types.h>
+#include <sys/sysmacros.h>
+#include <sys/machsystm.h>
+#include <sys/vmsystm.h>
+#include <sys/malloc.h>
+#include <vm/as.h>
+#include <vm/seg_vn.h>
+#include <vm/seg_kmem.h>
+#include <vm/seg_vmm.h>
+
+#include <vm/vm_extern.h>
+#include <vm/vm_map.h>
+#include "vm/vm_glue.h"
+
+#define	PMAP_TO_VMMAP(pm)	((vm_map_t)		\
+	((caddr_t)(pm) - offsetof(struct vmspace, vms_pmap)))
+#define	VMMAP_TO_VMSPACE(vmmap)	((struct vmspace *)		\
+	((caddr_t)(vmmap) - offsetof(struct vmspace, vm_map)))
+
+/* Similar to htable, but without the bells and whistles */
+struct eptable {
+	struct eptable	*ept_next;
+	uintptr_t	ept_vaddr;
+	pfn_t		ept_pfn;
+	int16_t		ept_level;
+	int16_t		ept_valid_cnt;
+	uint32_t	_ept_pad2;
+	struct eptable	*ept_prev;
+	struct eptable	*ept_parent;
+	void		*ept_kva;
+};
+typedef struct eptable eptable_t;
+
+struct eptable_map {
+	kmutex_t		em_lock;
+	eptable_t		*em_root;
+	eptable_t		**em_hash;
+	size_t			em_table_cnt;
+	size_t			em_wired;
+
+	/* Protected by eptable_map_lock */
+	struct eptable_map	*em_next;
+	struct eptable_map	*em_prev;
+};
+typedef struct eptable_map eptable_map_t;
+
+#define	EPTABLE_HASH(va, lvl, sz)				\
+	((((va) >> LEVEL_SHIFT(1)) + ((va) >> 28) + (lvl))	\
+	& ((sz) - 1))
+
+#define	EPTABLE_VA2IDX(tbl, va)					\
+	(((va) - (tbl)->ept_vaddr) >>				\
+	LEVEL_SHIFT((tbl)->ept_level))
+
+#define	EPTABLE_IDX2VA(tbl, idx)				\
+	((tbl)->ept_vaddr +					\
+	((idx) << LEVEL_SHIFT((tbl)->ept_level)))
+
+#define	EPT_R	(0x1 << 0)
+#define	EPT_W	(0x1 << 1)
+#define	EPT_X	(0x1 << 2)
+#define	EPT_RWX	(EPT_R|EPT_W|EPT_X)
+#define	EPT_MAP	(0x1 << 7)
+
+#define	EPT_PAT(attr)	(((attr) & 0x7) << 3)
+
+#define	EPT_PADDR	(0x000ffffffffff000ull)
+
+#define	EPT_IS_ABSENT(pte)	(((pte) & EPT_RWX) == 0)
+#define	EPT_IS_TABLE(pte)	(((pte) & EPT_MAP) == 0)
+#define	EPT_PTE_PFN(pte)	mmu_btop((pte) & EPT_PADDR)
+#define	EPT_PTE_PROT(pte)	((pte) & EPT_RWX)
+
+#define	EPT_PTE_ASSIGN_TABLE(pfn)			\
+	((pfn_to_pa(pfn) & EPT_PADDR) | EPT_RWX)
+#define	EPT_PTE_ASSIGN_PAGE(pfn, prot, attr)		\
+	((pfn_to_pa(pfn) & EPT_PADDR) | EPT_MAP |	\
+	((prot) & EPT_RWX) |				\
+	EPT_PAT(attr))
+
+struct vmspace_mapping {
+	list_node_t	vmsm_node;
+	vm_object_t	vmsm_object;
+	uintptr_t	vmsm_addr;
+	size_t		vmsm_len;
+	off_t		vmsm_offset;
+	uint_t		vmsm_prot;
+};
+typedef struct vmspace_mapping vmspace_mapping_t;
+
+#define VMSM_OFFSET(vmsm, addr)	(			\
+	    (vmsm)->vmsm_offset +			\
+	    ((addr) - (uintptr_t)(vmsm)->vmsm_addr))
+
+
+/* Private glue interfaces */
+static void pmap_free(pmap_t);
+static eptable_t *eptable_alloc(void);
+static void eptable_free(eptable_t *);
+static void eptable_init(eptable_map_t *);
+static void eptable_fini(eptable_map_t *);
+static eptable_t *eptable_hash_lookup(eptable_map_t *, uintptr_t, level_t);
+static void eptable_hash_insert(eptable_map_t *, eptable_t *);
+static void eptable_hash_remove(eptable_map_t *, eptable_t *);
+static eptable_t *eptable_walk(eptable_map_t *, uintptr_t, level_t, uint_t *,
+    boolean_t);
+static pfn_t eptable_mapin(eptable_map_t *, uintptr_t, pfn_t, uint_t, uint_t,
+    vm_memattr_t);
+static void eptable_mapout(eptable_map_t *, uintptr_t);
+static int eptable_find(eptable_map_t *, uintptr_t, pfn_t *, uint_t *);
+static vmspace_mapping_t *vm_mapping_find(struct vmspace *, uintptr_t, size_t);
+static void vm_mapping_remove(struct vmspace *, vmspace_mapping_t *);
+
+static kmutex_t eptable_map_lock;
+static struct eptable_map *eptable_map_head = NULL;
+
+static vmem_t	*vmm_arena = NULL;
+
+
+void
+vmm_arena_init(void)
+{
+	/*
+	 * XXXJOY: Hahaha, this is terrible, pls fix, prototype only
+	 */
+	vmm_arena = vmem_create("vmm_arena", NULL, 0, PAGESIZE,
+	    segkmem_zio_alloc, segkmem_zio_free, zio_arena, 0, VM_SLEEP);
+}
+
+boolean_t
+vmm_arena_fini(void)
+{
+	if (vmem_size(vmm_arena, VMEM_ALLOC) != 0) {
+		return (B_FALSE);
+	}
+	vmem_destroy(vmm_arena);
+	vmm_arena = NULL;
+	return (B_TRUE);
+}
+
+struct vmspace *
+vmspace_alloc(vm_offset_t start, vm_offset_t end, pmap_pinit_t pinit)
+{
+	struct vmspace *vms;
+	const uintptr_t size = end + 1;
+
+	/*
+	 * This whole mess is built on the assumption that a 64-bit address
+	 * space is available to work with for the various pagetable tricks.
+	 */
+	VERIFY(ttoproc(curthread)->p_model == DATAMODEL_LP64);
+	VERIFY(start == 0 && size > 0 && (size & PAGEOFFSET) == 0 &&
+	    size <= (uintptr_t)USERLIMIT);
+
+	vms = kmem_zalloc(sizeof (*vms), KM_SLEEP);
+	vms->vms_size = size;
+	list_create(&vms->vms_maplist, sizeof (vmspace_mapping_t),
+	    offsetof(vmspace_mapping_t, vmsm_node));
+
+	if (pinit(&vms->vms_pmap) == 0) {
+		kmem_free(vms, sizeof (*vms));
+		return (NULL);
+	}
+
+	return (vms);
+}
+
+void
+vmspace_free(struct vmspace *vms)
+{
+	VERIFY(list_is_empty(&vms->vms_maplist));
+
+	pmap_free(&vms->vms_pmap);
+	kmem_free(vms, sizeof (*vms));
+}
+
+pmap_t
+vmspace_pmap(struct vmspace *vms)
+{
+	return (&vms->vms_pmap);
+}
+
+long
+vmspace_resident_count(struct vmspace *vms)
+{
+	/* XXXJOY: finish */
+	return (0);
+}
+
+void *
+vmspace_find_kva(struct vmspace *vms, uintptr_t addr, size_t size)
+{
+	vmspace_mapping_t *vmsm;
+	void *result = NULL;
+
+	mutex_enter(&vms->vms_lock);
+	vmsm = vm_mapping_find(vms, addr, size);
+	if (vmsm != NULL) {
+		struct vm_object *vmo = vmsm->vmsm_object;
+
+		switch (vmo->vmo_type) {
+		case OBJT_DEFAULT:
+			result = (void *)((uintptr_t)vmo->vmo_data +
+			    VMSM_OFFSET(vmsm, addr));
+			break;
+		default:
+			break;
+		}
+	}
+	mutex_exit(&vms->vms_lock);
+
+	return (result);
+}
+
+static int
+vmspace_pmap_wire(struct vmspace *vms, uintptr_t addr, pfn_t pfn, uint_t lvl,
+    uint_t prot, vm_memattr_t attr)
+{
+	enum pmap_type type = vms->vms_pmap.pm_type;
+
+	ASSERT(MUTEX_HELD(&vms->vms_lock));
+
+	switch (type) {
+	case PT_EPT: {
+		eptable_map_t *map = (eptable_map_t *)vms->vms_pmap.pm_map;
+
+		(void) eptable_mapin(map, addr, pfn, lvl, prot, attr);
+
+		vms->vms_pmap.pm_eptgen++;
+		return (0);
+	}
+	case PT_RVI:
+		/* RVI support not yet implemented */
+	default:
+		panic("unsupported pmap type: %x", type);
+		/* NOTREACHED */
+		break;
+	}
+	return (0);
+}
+
+static int
+vmspace_pmap_iswired(struct vmspace *vms, uintptr_t addr, uint_t *prot)
+{
+	enum pmap_type type = vms->vms_pmap.pm_type;
+
+	ASSERT(MUTEX_HELD(&vms->vms_lock));
+
+	switch (type) {
+	case PT_EPT: {
+		eptable_map_t *map = (eptable_map_t *)vms->vms_pmap.pm_map;
+		pfn_t pfn;
+
+		return (eptable_find(map, addr, &pfn, prot));
+	}
+	case PT_RVI:
+		/* RVI support not yet implemented */
+	default:
+		panic("unsupported pmap type: %x", type);
+		/* NOTREACHED */
+		break;
+	}
+	return (-1);
+}
+
+static int
+vmspace_pmap_unmap(struct vmspace *vms, uintptr_t addr, size_t size)
+{
+	enum pmap_type type = vms->vms_pmap.pm_type;
+
+	ASSERT(MUTEX_HELD(&vms->vms_lock));
+
+	switch (type) {
+	case PT_EPT: {
+		eptable_map_t *map = (eptable_map_t *)vms->vms_pmap.pm_map;
+		uintptr_t maddr = (uintptr_t)addr;
+		const ulong_t npages = btop(size);
+		ulong_t idx;
+
+		/* XXXJOY: punt on large pages for now */
+		for (idx = 0; idx < npages; idx++, maddr += PAGESIZE) {
+			eptable_mapout(map, maddr);
+		}
+		vms->vms_pmap.pm_eptgen++;
+		return (0);
+	}
+		break;
+	case PT_RVI:
+		/* RVI support not yet implemented */
+	default:
+		panic("unsupported pmap type: %x", type);
+		/* NOTREACHED */
+		break;
+	}
+	return (0);
+}
+
+static void
+pmap_free(pmap_t pmap)
+{
+	switch (pmap->pm_type) {
+	case PT_EPT: {
+		eptable_map_t *map = (eptable_map_t *)pmap->pm_map;
+
+		pmap->pm_pml4 = NULL;
+		pmap->pm_map = NULL;
+
+		eptable_fini(map);
+		kmem_free(map, sizeof (*map));
+		return;
+	}
+	case PT_RVI:
+		/* RVI support not yet implemented */
+	default:
+		panic("unsupported pmap type: %x", pmap->pm_type);
+		/* NOTREACHED */
+		break;
+	}
+}
+
+int
+pmap_pinit_type(pmap_t pmap, enum pmap_type type, int flags)
+{
+	/* For use in vmm only */
+	pmap->pm_type = type;
+	switch (type) {
+	case PT_EPT: {
+		eptable_map_t *map;
+
+		map = kmem_zalloc(sizeof (*map), KM_SLEEP);
+		eptable_init(map);
+
+		pmap->pm_map = map;
+		pmap->pm_pml4 = map->em_root->ept_kva;
+		return (1);
+	}
+	case PT_RVI:
+		/* RVI support not yet implemented */
+		return (0);
+	default:
+		panic("unsupported pmap type: %x", type);
+		/* NOTREACHED */
+		break;
+	}
+
+	/* XXXJOY: finish */
+	return (1);
+}
+
+long
+pmap_wired_count(pmap_t pmap)
+{
+	enum pmap_type type = pmap->pm_type;
+	long val = 0L;
+
+	switch (type) {
+	case PT_EPT:
+		val = ((eptable_map_t *)pmap->pm_map)->em_wired;
+		break;
+	case PT_RVI:
+		/* RVI support not yet implemented */
+	default:
+		panic("unsupported pmap type: %x", type);
+		/* NOTREACHED */
+		break;
+	}
+	return (val);
+}
+
+int
+pmap_emulate_accessed_dirty(pmap_t pmap, vm_offset_t va, int ftype)
+{
+	/* Allow the fallback to vm_fault to handle this */
+	return (-1);
+}
+
+
+static eptable_t *
+eptable_alloc(void)
+{
+	eptable_t *ept;
+	caddr_t page;
+
+	ept = kmem_zalloc(sizeof (*ept), KM_SLEEP);
+	page = kmem_zalloc(PAGESIZE, KM_SLEEP);
+	ept->ept_kva = page;
+	ept->ept_pfn = hat_getpfnum(kas.a_hat, page);
+
+	return (ept);
+}
+
+static void
+eptable_free(eptable_t *ept)
+{
+	void *page = ept->ept_kva;
+
+	ASSERT(ept->ept_pfn != PFN_INVALID);
+	ASSERT(ept->ept_kva != NULL);
+
+	ept->ept_pfn = PFN_INVALID;
+	ept->ept_kva = NULL;
+
+	kmem_free(page, PAGESIZE);
+	kmem_free(ept, sizeof (*ept));
+}
+
+static void
+eptable_init(eptable_map_t *map)
+{
+	eptable_t *root;
+
+	VERIFY0(mmu.hash_cnt & (mmu.hash_cnt - 1));
+
+	map->em_table_cnt = mmu.hash_cnt;
+	map->em_hash = kmem_zalloc(sizeof (eptable_t *) * map->em_table_cnt,
+	    KM_SLEEP);
+
+	root = eptable_alloc();
+	root->ept_level = mmu.max_level;
+	map->em_root = root;
+
+	/* Insert into global tracking list of eptable maps */
+	mutex_enter(&eptable_map_lock);
+	map->em_next = eptable_map_head;
+	map->em_prev = NULL;
+	if (eptable_map_head != NULL) {
+		eptable_map_head->em_prev = map;
+	}
+	eptable_map_head = map;
+	mutex_exit(&eptable_map_lock);
+}
+
+static void
+eptable_fini(eptable_map_t *map)
+{
+	const uint_t cnt = map->em_table_cnt;
+
+	/* Remove from global tracking list of eptable maps */
+	mutex_enter(&eptable_map_lock);
+	if (map->em_next != NULL) {
+		map->em_next->em_prev = map->em_prev;
+	}
+	if (map->em_prev != NULL) {
+		map->em_prev->em_next = map->em_next;
+	} else {
+		eptable_map_head = map->em_next;
+	}
+	mutex_exit(&eptable_map_lock);
+
+	mutex_enter(&map->em_lock);
+	/* XXJOY: Should we expect to need this clean-up? */
+	for (uint_t i = 0; i < cnt; i++) {
+		eptable_t *ept = map->em_hash[i];
+
+		while (ept != NULL) {
+			eptable_t *next = ept->ept_next;
+
+			eptable_hash_remove(map, ept);
+			eptable_free(ept);
+			ept = next;
+		}
+	}
+
+	kmem_free(map->em_hash, sizeof (eptable_t *) * cnt);
+	eptable_free(map->em_root);
+
+	mutex_exit(&map->em_lock);
+	mutex_destroy(&map->em_lock);
+}
+
+static eptable_t *
+eptable_hash_lookup(eptable_map_t *map, uintptr_t va, level_t lvl)
+{
+	const uint_t hash = EPTABLE_HASH(va, lvl, map->em_table_cnt);
+	eptable_t *ept;
+
+	ASSERT(MUTEX_HELD(&map->em_lock));
+
+	for (ept = map->em_hash[hash]; ept != NULL; ept = ept->ept_next) {
+		if (ept->ept_vaddr == va && ept->ept_level == lvl)
+			break;
+	}
+	return (ept);
+}
+
+static void
+eptable_hash_insert(eptable_map_t *map, eptable_t *ept)
+{
+	const uintptr_t va = ept->ept_vaddr;
+	const uint_t lvl = ept->ept_level;
+	const uint_t hash = EPTABLE_HASH(va, lvl, map->em_table_cnt);
+
+	ASSERT(MUTEX_HELD(&map->em_lock));
+	ASSERT(eptable_hash_lookup(map, va, lvl) == NULL);
+
+	ept->ept_prev = NULL;
+	if (map->em_hash[hash] == NULL) {
+		ept->ept_next = NULL;
+	} else {
+		eptable_t *chain = map->em_hash[hash];
+
+		ept->ept_next = chain;
+		chain->ept_prev = ept;
+	}
+	map->em_hash[hash] = ept;
+}
+
+static void
+eptable_hash_remove(eptable_map_t *map, eptable_t *ept)
+{
+	const uintptr_t va = ept->ept_vaddr;
+	const uint_t lvl = ept->ept_level;
+	const uint_t hash = EPTABLE_HASH(va, lvl, map->em_table_cnt);
+
+	ASSERT(MUTEX_HELD(&map->em_lock));
+
+	if (ept->ept_prev == NULL) {
+		ASSERT(map->em_hash[hash] == ept);
+
+		map->em_hash[hash] = ept->ept_next;
+	} else {
+		ept->ept_prev->ept_next = ept->ept_next;
+	}
+	if (ept->ept_next != NULL) {
+		ept->ept_next->ept_prev = ept->ept_prev;
+	}
+	ept->ept_next = NULL;
+	ept->ept_prev = NULL;
+}
+
+static eptable_t *
+eptable_walk(eptable_map_t *map, uintptr_t va, level_t tgtlvl, uint_t *idxp,
+    boolean_t do_create)
+{
+	eptable_t *ept = map->em_root;
+	level_t lvl = ept->ept_level;
+	uint_t idx = UINT_MAX;
+
+	ASSERT(MUTEX_HELD(&map->em_lock));
+
+	while (lvl >= tgtlvl) {
+		x86pte_t *ptes, entry;
+		const uintptr_t masked_va = va & LEVEL_MASK((uint_t)lvl);
+		eptable_t *newept = NULL;
+
+		idx = EPTABLE_VA2IDX(ept, va);
+		if (lvl == tgtlvl || lvl == 0) {
+			break;
+		}
+
+		ptes = (x86pte_t *)ept->ept_kva;
+		entry = ptes[idx];
+		if (EPT_IS_ABSENT(entry)) {
+			if (!do_create) {
+				break;
+			}
+
+			newept = eptable_alloc();
+			newept->ept_level = lvl - 1;
+			newept->ept_vaddr = masked_va;
+			newept->ept_parent = ept;
+
+			eptable_hash_insert(map, newept);
+			entry = EPT_PTE_ASSIGN_TABLE(newept->ept_pfn);
+			ptes[idx] = entry;
+			ept->ept_valid_cnt++;
+		} else if (EPT_IS_TABLE(entry)) {
+			/* Do lookup in next level of page table */
+			newept = eptable_hash_lookup(map, masked_va, lvl - 1);
+
+			VERIFY(newept);
+			VERIFY3P(pfn_to_pa(newept->ept_pfn), ==,
+			    (entry & EPT_PADDR));
+		} else {
+			/*
+			 * There is a (large) page mapped here.  Since support
+			 * for non-PAGESIZE pages is not yet present, this is a
+			 * surprise.
+			 */
+			panic("unexpected parge page in pte %p", &ptes[idx]);
+		}
+		ept = newept;
+		lvl--;
+	}
+
+	VERIFY(lvl >= 0 && idx != UINT_MAX);
+	*idxp = idx;
+	return (ept);
+}
+
+static pfn_t
+eptable_mapin(eptable_map_t *map, uintptr_t va, pfn_t pfn, uint_t lvl,
+    uint_t prot, vm_memattr_t attr)
+{
+	uint_t idx;
+	eptable_t *ept;
+	x86pte_t *ptes, entry;
+	const size_t pgsize = (size_t)LEVEL_SIZE(lvl);
+	pfn_t oldpfn = PFN_INVALID;
+
+	CTASSERT(EPT_R == PROT_READ);
+	CTASSERT(EPT_W == PROT_WRITE);
+	CTASSERT(EPT_X == PROT_EXEC);
+	ASSERT((prot & EPT_RWX) != 0 && (prot & ~EPT_RWX) == 0);
+
+	/* XXXJOY: punt on large pages for now */
+	VERIFY(lvl == 0);
+
+	mutex_enter(&map->em_lock);
+	ept = eptable_walk(map, va, (level_t)lvl, &idx, B_TRUE);
+	ptes = (x86pte_t *)ept->ept_kva;
+	entry = ptes[idx];
+
+	if (!EPT_IS_ABSENT(entry)) {
+		if (EPT_IS_TABLE(entry)) {
+			panic("unexpected table entry %lx in %p[%d]",
+			    entry, ept, idx);
+		}
+		/*
+		 * XXXJOY: Just clean the entry for now. Assume(!) that
+		 * invalidation is going to occur anyways.
+		 */
+		oldpfn = EPT_PTE_PFN(ptes[idx]);
+		ept->ept_valid_cnt--;
+		ptes[idx] = (x86pte_t)0;
+		map->em_wired -= (pgsize >> PAGESHIFT);
+	}
+
+	entry = EPT_PTE_ASSIGN_PAGE(pfn, prot, attr);
+	ptes[idx] = entry;
+	ept->ept_valid_cnt++;
+	map->em_wired += (pgsize >> PAGESHIFT);
+	mutex_exit(&map->em_lock);
+
+	return (oldpfn);
+}
+
+static void
+eptable_mapout(eptable_map_t *map, uintptr_t va)
+{
+	eptable_t *ept;
+	uint_t idx;
+	x86pte_t *ptes, entry;
+
+	mutex_enter(&map->em_lock);
+	/* Find the lowest level entry at this VA */
+	ept = eptable_walk(map, va, -1, &idx, B_FALSE);
+
+	ptes = (x86pte_t *)ept->ept_kva;
+	entry = ptes[idx];
+
+	if (EPT_IS_ABSENT(entry)) {
+		/*
+		 * There is nothing here to free up.  If this was a sparsely
+		 * wired mapping, the absence is no concern.
+		 */
+		mutex_exit(&map->em_lock);
+		return;
+	} else {
+		pfn_t oldpfn;
+		const size_t pagesize = LEVEL_SIZE((uint_t)ept->ept_level);
+
+		if (EPT_IS_TABLE(entry)) {
+			panic("unexpected table entry %lx in %p[%d]",
+			    entry, ept, idx);
+		}
+		/*
+		 * XXXJOY: Just clean the entry for now. Assume(!) that
+		 * invalidation is going to occur anyways.
+		 */
+		oldpfn = EPT_PTE_PFN(ptes[idx]);
+		ept->ept_valid_cnt--;
+		ptes[idx] = (x86pte_t)0;
+		map->em_wired -= (pagesize >> PAGESHIFT);
+	}
+
+	while (ept->ept_valid_cnt == 0 && ept->ept_parent != NULL) {
+		eptable_t *next = ept->ept_parent;
+
+		idx = EPTABLE_VA2IDX(next, va);
+		ptes = (x86pte_t *)next->ept_kva;
+
+		entry = ptes[idx];
+		ASSERT(EPT_IS_TABLE(entry));
+		ASSERT(EPT_PTE_PFN(entry) == ept->ept_pfn);
+
+		ptes[idx] = (x86pte_t)0;
+		next->ept_valid_cnt--;
+		eptable_hash_remove(map, ept);
+		ept->ept_parent = NULL;
+		eptable_free(ept);
+
+		ept = next;
+	}
+	mutex_exit(&map->em_lock);
+}
+
+static int
+eptable_find(eptable_map_t *map, uintptr_t va, pfn_t *pfn, uint_t *prot)
+{
+	eptable_t *ept;
+	uint_t idx;
+	x86pte_t *ptes, entry;
+	int err = -1;
+
+	mutex_enter(&map->em_lock);
+	/* Find the lowest level entry at this VA */
+	ept = eptable_walk(map, va, -1, &idx, B_FALSE);
+
+	/* XXXJOY: Until large pages are supported, this check is easy */
+	if (ept->ept_level != 0) {
+		mutex_exit(&map->em_lock);
+		return (-1);
+	}
+
+	ptes = (x86pte_t *)ept->ept_kva;
+	entry = ptes[idx];
+
+	if (!EPT_IS_ABSENT(entry)) {
+		if (EPT_IS_TABLE(entry)) {
+			panic("unexpected table entry %lx in %p[%d]",
+			    entry, ept, idx);
+		}
+
+		*pfn = EPT_PTE_PFN(entry);
+		*prot = EPT_PTE_PROT(entry);
+		err = 0;
+	}
+
+	mutex_exit(&map->em_lock);
+	return (err);
+}
+
+struct sglist_ent {
+	vm_paddr_t	sge_pa;
+	size_t		sge_len;
+};
+struct sglist {
+	kmutex_t		sg_lock;
+	uint_t			sg_refcnt;
+	uint_t			sg_len;
+	uint_t			sg_next;
+	struct sglist_ent	sg_entries[];
+};
+
+#define	SG_SIZE(cnt)	(sizeof (struct sglist) + \
+	(sizeof (struct sglist_ent) * (cnt)))
+
+struct sglist *
+sglist_alloc(int nseg, int flags)
+{
+	const size_t sz = SG_SIZE(nseg);
+	const int flag = (flags & M_WAITOK) ? KM_SLEEP : KM_NOSLEEP;
+	struct sglist *sg;
+
+	ASSERT(nseg > 0);
+
+	sg = kmem_zalloc(sz, flag);
+	if (sg != NULL) {
+		sg->sg_len = nseg;
+		sg->sg_refcnt = 1;
+	}
+	return (sg);
+}
+
+void
+sglist_free(struct sglist *sg)
+{
+	size_t sz;
+
+	mutex_enter(&sg->sg_lock);
+	if (sg->sg_refcnt > 1) {
+		sg->sg_refcnt--;
+		mutex_exit(&sg->sg_lock);
+		return;
+	}
+
+	VERIFY(sg->sg_refcnt == 1);
+	sg->sg_refcnt = 0;
+	sz = SG_SIZE(sg->sg_len);
+	mutex_exit(&sg->sg_lock);
+	kmem_free(sg, sz);
+}
+
+int
+sglist_append_phys(struct sglist *sg, vm_paddr_t pa, size_t len)
+{
+	uint_t idx;
+	struct sglist_ent *ent;
+
+	/* Restrict to page-aligned entries */
+	if ((pa & PAGEOFFSET) != 0 || (len & PAGEOFFSET) != 0 || len == 0) {
+		return (EINVAL);
+	}
+
+	mutex_enter(&sg->sg_lock);
+	idx = sg->sg_next;
+	if (idx >= sg->sg_len) {
+		mutex_exit(&sg->sg_lock);
+		return (ENOSPC);
+	}
+
+	ent = &sg->sg_entries[idx];
+	ASSERT(ent->sge_pa == 0 && ent->sge_len == 0);
+	ent->sge_pa = pa;
+	ent->sge_len = len;
+	sg->sg_next++;
+
+	mutex_exit(&sg->sg_lock);
+	return (0);
+}
+
+
+static pfn_t
+vm_object_pager_none(vm_object_t vmo, uintptr_t off, pfn_t *lpfn, uint_t *lvl)
+{
+	panic("bad vm_object pager");
+	/* NOTREACHED */
+	return (PFN_INVALID);
+}
+
+static pfn_t
+vm_object_pager_heap(vm_object_t vmo, uintptr_t off, pfn_t *lpfn, uint_t *lvl)
+{
+	const uintptr_t kaddr = ALIGN2PAGE((uintptr_t)vmo->vmo_data + off);
+	uint_t idx, level;
+	htable_t *ht;
+	x86pte_t pte;
+	pfn_t top_pfn, pfn;
+
+	ASSERT(vmo->vmo_type == OBJT_DEFAULT);
+	ASSERT(off < vmo->vmo_size);
+
+	ht = htable_getpage(kas.a_hat, kaddr, &idx);
+	if (ht == NULL) {
+		return (PFN_INVALID);
+	}
+	pte = x86pte_get(ht, idx);
+	if (!PTE_ISPAGE(pte, ht->ht_level)) {
+		htable_release(ht);
+		return (PFN_INVALID);
+	}
+
+	pfn = top_pfn = PTE2PFN(pte, ht->ht_level);
+	level = ht->ht_level;
+	if (ht->ht_level > 0) {
+		pfn += mmu_btop(kaddr & LEVEL_OFFSET((uint_t)ht->ht_level));
+	}
+	htable_release(ht);
+
+	if (lpfn != NULL) {
+		*lpfn = top_pfn;
+	}
+	if (lvl != NULL) {
+		*lvl = level;
+	}
+	return (pfn);
+}
+
+static pfn_t
+vm_object_pager_sg(vm_object_t vmo, uintptr_t off, pfn_t *lpfn, uint_t *lvl)
+{
+	const uintptr_t aoff = ALIGN2PAGE(off);
+	uint_t level = 0;
+	uintptr_t pos = 0;
+	struct sglist *sg;
+	struct sglist_ent *ent;
+	pfn_t pfn = PFN_INVALID;
+
+	ASSERT(vmo->vmo_type == OBJT_SG);
+	ASSERT(off < vmo->vmo_size);
+
+	sg = vmo->vmo_data;
+	if (sg == NULL) {
+		return (PFN_INVALID);
+	}
+
+	ent = &sg->sg_entries[0];
+	for (uint_t i = 0; i < sg->sg_next; i++, ent++) {
+		if (aoff >= pos && aoff < (pos + ent->sge_len)) {
+			/* XXXJOY: Punt on large pages for now */
+			level = 0;
+			pfn = mmu_btop(ent->sge_pa + (aoff - pos));
+			break;
+		}
+		pos += ent->sge_len;
+	}
+
+	if (lpfn != 0) {
+		*lpfn = pfn;
+	}
+	if (lvl != 0) {
+		*lvl = level;
+	}
+	return (pfn);
+}
+
+vm_object_t
+vm_object_allocate(objtype_t type, vm_pindex_t psize)
+{
+	vm_object_t vmo;
+	const size_t size = ptob((size_t)psize);
+
+	vmo = kmem_alloc(sizeof (*vmo), KM_SLEEP);
+	mutex_init(&vmo->vmo_lock, NULL, MUTEX_DEFAULT, NULL);
+
+	/* For now, these are to stay fixed after allocation */
+	vmo->vmo_type = type;
+	vmo->vmo_size = size;
+	vmo->vmo_attr = VM_MEMATTR_DEFAULT;
+
+	switch (type) {
+	case OBJT_DEFAULT: {
+		/* XXXJOY: opt-in to larger pages? */
+		vmo->vmo_data = vmem_alloc(vmm_arena, size, KM_SLEEP);
+		/* XXXJOY: Better zeroing approach? */
+		bzero(vmo->vmo_data, size);
+		vmo->vmo_pager = vm_object_pager_heap;
+	}
+		break;
+	case OBJT_SG:
+		vmo->vmo_data = NULL;
+		vmo->vmo_pager = vm_object_pager_sg;
+		break;
+	default:
+		panic("Unsupported vm_object type");
+		break;
+	}
+
+	vmo->vmo_refcnt = 1;
+	return (vmo);
+}
+
+vm_object_t
+vm_pager_allocate(objtype_t type, void *handle, vm_ooffset_t size,
+    vm_prot_t prot, vm_ooffset_t off, void *cred)
+{
+	struct vm_object *vmo;
+	struct sglist *sg = (struct sglist *)handle;
+
+	/* XXXJOY: be very restrictive for now */
+	VERIFY(type == OBJT_SG);
+	VERIFY(off == 0);
+
+	vmo = vm_object_allocate(type, size);
+	vmo->vmo_data = sg;
+
+	mutex_enter(&sg->sg_lock);
+	VERIFY(sg->sg_refcnt++ >= 1);
+	mutex_exit(&sg->sg_lock);
+
+	return (vmo);
+}
+
+void
+vm_object_deallocate(vm_object_t vmo)
+{
+	ASSERT(vmo != NULL);
+
+	mutex_enter(&vmo->vmo_lock);
+	VERIFY(vmo->vmo_refcnt);
+	vmo->vmo_refcnt--;
+	if (vmo->vmo_refcnt != 0) {
+		mutex_exit(&vmo->vmo_lock);
+		return;
+	}
+
+	switch (vmo->vmo_type) {
+	case OBJT_DEFAULT:
+		vmem_free(vmm_arena, vmo->vmo_data, vmo->vmo_size);
+		break;
+	case OBJT_SG:
+		sglist_free((struct sglist *)vmo->vmo_data);
+		break;
+	default:
+		panic("Unsupported vm_object type");
+		break;
+	}
+
+	vmo->vmo_pager = vm_object_pager_none;
+	vmo->vmo_data = NULL;
+	vmo->vmo_size = 0;
+	mutex_exit(&vmo->vmo_lock);
+	mutex_destroy(&vmo->vmo_lock);
+	kmem_free(vmo, sizeof (*vmo));
+}
+
+int
+vm_object_set_memattr(vm_object_t vmo, vm_memattr_t attr)
+{
+	ASSERT(MUTEX_HELD(&vmo->vmo_lock));
+
+	switch (attr) {
+	case VM_MEMATTR_UNCACHEABLE:
+	case VM_MEMATTR_WRITE_BACK:
+		vmo->vmo_attr = attr;
+		return (0);
+	default:
+		break;
+	}
+	return (EINVAL);
+}
+
+void
+vm_object_reference(vm_object_t vmo)
+{
+	ASSERT(vmo != NULL);
+
+	mutex_enter(&vmo->vmo_lock);
+	vmo->vmo_refcnt++;
+	mutex_exit(&vmo->vmo_lock);
+}
+
+static vmspace_mapping_t *
+vm_mapping_find(struct vmspace *vms, uintptr_t addr, size_t size)
+{
+	vmspace_mapping_t *vmsm;
+	list_t *ml = &vms->vms_maplist;
+	const uintptr_t range_end = addr + size;
+
+	ASSERT(MUTEX_HELD(&vms->vms_lock));
+	ASSERT(addr <= range_end);
+
+	if (addr >= vms->vms_size) {
+		return (NULL);
+	}
+	for (vmsm = list_head(ml); vmsm != NULL; vmsm = list_next(ml, vmsm)) {
+		const uintptr_t seg_end = vmsm->vmsm_addr + vmsm->vmsm_len;
+
+		if (addr >= vmsm->vmsm_addr && addr < seg_end) {
+			if (range_end <= seg_end) {
+				return (vmsm);
+			} else {
+				return (NULL);
+			}
+		}
+	}
+	return (NULL);
+}
+
+static boolean_t
+vm_mapping_gap(struct vmspace *vms, uintptr_t addr, size_t size)
+{
+	vmspace_mapping_t *vmsm;
+	list_t *ml = &vms->vms_maplist;
+	const uintptr_t range_end = addr + size;
+
+	ASSERT(MUTEX_HELD(&vms->vms_lock));
+
+	for (vmsm = list_head(ml); vmsm != NULL; vmsm = list_next(ml, vmsm)) {
+		const uintptr_t seg_end = vmsm->vmsm_addr + vmsm->vmsm_len;
+
+		if ((vmsm->vmsm_addr >= addr && vmsm->vmsm_addr < range_end) ||
+		    (seg_end > addr && seg_end < range_end)) {
+			return (B_FALSE);
+		}
+	}
+	return (B_TRUE);
+}
+
+static void
+vm_mapping_remove(struct vmspace *vms, vmspace_mapping_t *vmsm)
+{
+	list_t *ml = &vms->vms_maplist;
+
+	ASSERT(MUTEX_HELD(&vms->vms_lock));
+
+	list_remove(ml, vmsm);
+	vm_object_deallocate(vmsm->vmsm_object);
+	kmem_free(vmsm, sizeof (*vmsm));
+}
+
+int
+vm_fault(vm_map_t map, vm_offset_t off, vm_prot_t type, int flag)
+{
+	struct vmspace *vms = VMMAP_TO_VMSPACE(map);
+	const uintptr_t addr = off;
+	vmspace_mapping_t *vmsm;
+	struct vm_object *vmo;
+	uint_t prot, map_lvl;
+	pfn_t pfn;
+	uintptr_t map_addr;
+
+	mutex_enter(&vms->vms_lock);
+	if (vmspace_pmap_iswired(vms, addr, &prot) == 0) {
+		int err = 0;
+
+		/*
+		 * It is possible that multiple will vCPUs race to fault-in a
+		 * given address.  In such cases, the race loser(s) will
+		 * encounter the already-mapped page, needing to do nothing
+		 * more than consider it a success.
+		 *
+		 * If the fault exceeds protection, it is an obvious error.
+		 */
+		if ((prot & type) != type) {
+			err = FC_PROT;
+		}
+
+		mutex_exit(&vms->vms_lock);
+		return (err);
+	}
+
+	/* Try to wire up the address */
+	if ((vmsm = vm_mapping_find(vms, addr, 0)) == NULL) {
+		mutex_exit(&vms->vms_lock);
+		return (FC_NOMAP);
+	}
+	vmo = vmsm->vmsm_object;
+	prot = vmsm->vmsm_prot;
+
+	/* XXXJOY: punt on large pages for now */
+	pfn = vmo->vmo_pager(vmo, VMSM_OFFSET(vmsm, addr), NULL, NULL);
+	map_lvl = 0;
+	map_addr = P2ALIGN((uintptr_t)addr, LEVEL_SIZE(map_lvl));
+	VERIFY(pfn != PFN_INVALID);
+
+	/*
+	 * If pmap failure is to be handled, the previously
+	 * acquired page locks would need to be released.
+	 */
+	VERIFY0(vmspace_pmap_wire(vms, map_addr, pfn, map_lvl, prot,
+	    vmo->vmo_attr));
+
+	mutex_exit(&vms->vms_lock);
+	return (0);
+}
+
+int
+vm_fault_quick_hold_pages(vm_map_t map, vm_offset_t addr, vm_size_t len,
+    vm_prot_t prot, vm_page_t *ma, int max_count)
+{
+	struct vmspace *vms = VMMAP_TO_VMSPACE(map);
+	const uintptr_t vaddr = addr;
+	vmspace_mapping_t *vmsm;
+	struct vm_object *vmo;
+	vm_page_t vmp;
+
+	ASSERT0(addr & PAGEOFFSET);
+	ASSERT(len == PAGESIZE);
+	ASSERT(max_count == 1);
+
+	mutex_enter(&vms->vms_lock);
+	if ((vmsm = vm_mapping_find(vms, vaddr, PAGESIZE)) == NULL ||
+	    (prot & ~vmsm->vmsm_prot) != 0) {
+		mutex_exit(&vms->vms_lock);
+		return (-1);
+	}
+
+	vmp = kmem_zalloc(sizeof (struct vm_page), KM_SLEEP);
+
+	vmo = vmsm->vmsm_object;
+	vm_object_reference(vmo);
+	vmp->vmp_obj_held = vmo;
+	vmp->vmp_pfn = vmo->vmo_pager(vmo, VMSM_OFFSET(vmsm, vaddr), NULL,
+	    NULL);
+
+	mutex_exit(&vms->vms_lock);
+	*ma = vmp;
+	return (1);
+}
+
+/*
+ * Find a suitable location for a mapping (and install it).
+ */
+int
+vm_map_find(vm_map_t map, vm_object_t vmo, vm_ooffset_t off, vm_offset_t *addr,
+    vm_size_t len, vm_offset_t max_addr, int find_flags, vm_prot_t prot,
+    vm_prot_t prot_max, int cow)
+{
+	struct vmspace *vms = VMMAP_TO_VMSPACE(map);
+	const size_t size = (size_t)len;
+	const uintptr_t uoff = (uintptr_t)off;
+	uintptr_t base = *addr;
+	vmspace_mapping_t *vmsm;
+	int res = 0;
+
+	/* For use in vmm only */
+	VERIFY(find_flags == VMFS_NO_SPACE); /* essentially MAP_FIXED */
+	VERIFY(max_addr == 0);
+
+	if (size == 0 || off < 0 ||
+	    uoff >= (uoff + size) || vmo->vmo_size < (uoff + size)) {
+		return (EINVAL);
+	}
+
+	if (*addr >= vms->vms_size) {
+		return (ENOMEM);
+	}
+
+	vmsm = kmem_alloc(sizeof (*vmsm), KM_SLEEP);
+
+	mutex_enter(&vms->vms_lock);
+	if (!vm_mapping_gap(vms, base, size)) {
+		res = ENOMEM;
+		goto out;
+	}
+
+	if (res == 0) {
+		vmsm->vmsm_object = vmo;
+		vmsm->vmsm_addr = base;
+		vmsm->vmsm_len = len;
+		vmsm->vmsm_offset = (off_t)uoff;
+		vmsm->vmsm_prot = prot;
+		list_insert_tail(&vms->vms_maplist, vmsm);
+
+		/* Communicate out the chosen address. */
+		*addr = (vm_offset_t)base;
+	}
+out:
+	mutex_exit(&vms->vms_lock);
+	if (res != 0) {
+		kmem_free(vmsm, sizeof (*vmsm));
+	}
+	return (res);
+}
+
+int
+vm_map_remove(vm_map_t map, vm_offset_t start, vm_offset_t end)
+{
+	struct vmspace *vms = VMMAP_TO_VMSPACE(map);
+	const uintptr_t addr = start;
+	const size_t size = (size_t)(end - start);
+	vmspace_mapping_t *vmsm;
+	objtype_t type;
+
+	ASSERT(start < end);
+
+	mutex_enter(&vms->vms_lock);
+	/* expect to match existing mapping exactly */
+	if ((vmsm = vm_mapping_find(vms, addr, size)) == NULL ||
+	    vmsm->vmsm_addr != addr || vmsm->vmsm_len != size) {
+		mutex_exit(&vms->vms_lock);
+		return (ENOENT);
+	}
+
+	type = vmsm->vmsm_object->vmo_type;
+	switch (type) {
+	case OBJT_DEFAULT:
+	case OBJT_SG:
+		VERIFY0(vmspace_pmap_unmap(vms, addr, size));
+		break;
+	default:
+		panic("unsupported object type: %x", type);
+		/* NOTREACHED */
+		break;
+	}
+
+	vm_mapping_remove(vms, vmsm);
+	mutex_exit(&vms->vms_lock);
+	return (0);
+}
+
+int
+vm_map_wire(vm_map_t map, vm_offset_t start, vm_offset_t end, int flags)
+{
+	struct vmspace *vms = VMMAP_TO_VMSPACE(map);
+	const uintptr_t addr = start;
+	const size_t size = end - start;
+	vmspace_mapping_t *vmsm;
+	struct vm_object *vmo;
+	uint_t prot;
+
+	mutex_enter(&vms->vms_lock);
+
+	/* For the time being, only exact-match mappings are expected */
+	if ((vmsm = vm_mapping_find(vms, addr, size)) == NULL) {
+		mutex_exit(&vms->vms_lock);
+		return (FC_NOMAP);
+	}
+	vmo = vmsm->vmsm_object;
+	prot = vmsm->vmsm_prot;
+
+	for (uintptr_t pos = addr; pos < end;) {
+		pfn_t pfn;
+		uintptr_t pg_size, map_addr;
+		uint_t map_lvl = 0;
+
+		/* XXXJOY: punt on large pages for now */
+		pfn = vmo->vmo_pager(vmo, VMSM_OFFSET(vmsm, pos), NULL, NULL);
+		pg_size = LEVEL_SIZE(map_lvl);
+		map_addr = P2ALIGN(pos, pg_size);
+		VERIFY(pfn != PFN_INVALID);
+
+		VERIFY0(vmspace_pmap_wire(vms, map_addr, pfn, map_lvl, prot,
+		    vmo->vmo_attr));
+		pos += pg_size;
+	}
+
+	mutex_exit(&vms->vms_lock);
+
+	return (0);
+}
+
+/* Provided custom for bhyve 'devmem' segment mapping */
+int
+vm_segmap_obj(struct vmspace *vms, vm_object_t vmo, struct as *as,
+    caddr_t *addrp, uint_t prot, uint_t maxprot, uint_t flags)
+{
+	const size_t size = vmo->vmo_size;
+	int err;
+
+	if (vmo->vmo_type != OBJT_DEFAULT) {
+		/* Only support default objects for now */
+		return (ENOTSUP);
+	}
+
+	as_rangelock(as);
+
+	err = choose_addr(as, addrp, size, 0, ADDR_VACALIGN, flags);
+	if (err == 0) {
+		segvmm_crargs_t svma;
+
+		svma.kaddr = vmo->vmo_data;
+		svma.prot = prot;
+		svma.cookie = vmo;
+		svma.hold = (segvmm_holdfn_t)vm_object_reference;
+		svma.rele = (segvmm_relefn_t)vm_object_deallocate;
+
+		err = as_map(as, *addrp, size, segvmm_create, &svma);
+	}
+
+	as_rangeunlock(as);
+	return (err);
+}
+
+int
+vm_segmap_space(struct vmspace *vms, off_t off, struct as *as, caddr_t *addrp,
+    off_t len, uint_t prot, uint_t maxprot, uint_t flags)
+{
+	const uintptr_t addr = (uintptr_t)off;
+	const size_t size = (uintptr_t)len;
+	vmspace_mapping_t *vmsm;
+	vm_object_t vmo;
+	int err;
+
+	if (off < 0 || len <= 0 ||
+	    (addr & PAGEOFFSET) != 0 || (size & PAGEOFFSET) != 0) {
+		return (EINVAL);
+	}
+
+	mutex_enter(&vms->vms_lock);
+	if ((vmsm = vm_mapping_find(vms, addr, size)) == NULL) {
+		mutex_exit(&vms->vms_lock);
+		return (ENXIO);
+	}
+	if ((prot & ~(vmsm->vmsm_prot | PROT_USER)) != 0) {
+		mutex_exit(&vms->vms_lock);
+		return (EACCES);
+	}
+	vmo = vmsm->vmsm_object;
+	if (vmo->vmo_type != OBJT_DEFAULT) {
+		/* Only support default objects for now */
+		mutex_exit(&vms->vms_lock);
+		return (ENOTSUP);
+	}
+
+	as_rangelock(as);
+
+	err = choose_addr(as, addrp, size, off, ADDR_VACALIGN, flags);
+	if (err == 0) {
+		segvmm_crargs_t svma;
+		const uintptr_t addroff = addr - vmsm->vmsm_addr;
+		const uintptr_t mapoff = addroff + vmsm->vmsm_offset;
+
+		VERIFY(addroff < vmsm->vmsm_len);
+		VERIFY((vmsm->vmsm_len - addroff) >= size);
+		VERIFY(mapoff < vmo->vmo_size);
+		VERIFY((mapoff + size) <= vmo->vmo_size);
+
+		svma.kaddr = (void *)((uintptr_t)vmo->vmo_data + mapoff);
+		svma.prot = prot;
+		svma.cookie = vmo;
+		svma.hold = (segvmm_holdfn_t)vm_object_reference;
+		svma.rele = (segvmm_relefn_t)vm_object_deallocate;
+
+		err = as_map(as, *addrp, len, segvmm_create, &svma);
+	}
+
+	as_rangeunlock(as);
+	mutex_exit(&vms->vms_lock);
+	return (err);
+}
+
+void
+vm_page_lock(vm_page_t vmp)
+{
+	ASSERT(!MUTEX_HELD(&vmp->vmp_lock));
+
+	mutex_enter(&vmp->vmp_lock);
+}
+
+void
+vm_page_unlock(vm_page_t vmp)
+{
+	boolean_t purge = (vmp->vmp_pfn == PFN_INVALID);
+
+	ASSERT(MUTEX_HELD(&vmp->vmp_lock));
+
+	mutex_exit(&vmp->vmp_lock);
+
+	if (purge) {
+		mutex_destroy(&vmp->vmp_lock);
+		kmem_free(vmp, sizeof (*vmp));
+	}
+}
+
+void
+vm_page_unhold(vm_page_t vmp)
+{
+	ASSERT(MUTEX_HELD(&vmp->vmp_lock));
+	VERIFY(vmp->vmp_pfn != PFN_INVALID);
+
+	vm_object_deallocate(vmp->vmp_obj_held);
+	vmp->vmp_obj_held = NULL;
+	vmp->vmp_pfn = PFN_INVALID;
+}
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_stat.c b/usr/src/uts/i86pc/io/vmm/vmm_stat.c
new file mode 100644
index 0000000000..62002d632e
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/vmm_stat.c
@@ -0,0 +1,170 @@
+/*-
+ * Copyright (c) 2011 NetApp, Inc.
+ * All rights reserved.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ * 1. Redistributions of source code must retain the above copyright
+ *    notice, this list of conditions and the following disclaimer.
+ * 2. Redistributions in binary form must reproduce the above copyright
+ *    notice, this list of conditions and the following disclaimer in the
+ *    documentation and/or other materials provided with the distribution.
+ *
+ * THIS SOFTWARE IS PROVIDED BY NETAPP, INC ``AS IS'' AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+ * ARE DISCLAIMED.  IN NO EVENT SHALL NETAPP, INC OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
+ * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+ * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
+ * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
+ * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
+ * SUCH DAMAGE.
+ *
+ * $FreeBSD$
+ */
+
+#include <sys/cdefs.h>
+__FBSDID("$FreeBSD$");
+
+#include <sys/param.h>
+#include <sys/kernel.h>
+#include <sys/systm.h>
+#include <sys/malloc.h>
+
+#include <machine/vmm.h>
+#include "vmm_util.h"
+#include "vmm_stat.h"
+
+/*
+ * 'vst_num_elems' is the total number of addressable statistic elements
+ * 'vst_num_types' is the number of unique statistic types
+ *
+ * It is always true that 'vst_num_elems' is greater than or equal to
+ * 'vst_num_types'. This is because a stat type may represent more than
+ * one element (for e.g. VMM_STAT_ARRAY).
+ */
+static int vst_num_elems, vst_num_types;
+static struct vmm_stat_type *vsttab[MAX_VMM_STAT_ELEMS];
+
+static MALLOC_DEFINE(M_VMM_STAT, "vmm stat", "vmm stat");
+
+#define	vst_size	((size_t)vst_num_elems * sizeof(uint64_t))
+
+void
+vmm_stat_register(void *arg)
+{
+	struct vmm_stat_type *vst = arg;
+
+	/* We require all stats to identify themselves with a description */
+	if (vst->desc == NULL)
+		return;
+
+	if (vst->scope == VMM_STAT_SCOPE_INTEL && !vmm_is_intel())
+		return;
+
+	if (vst->scope == VMM_STAT_SCOPE_AMD && !vmm_is_amd())
+		return;
+
+	if (vst_num_elems + vst->nelems >= MAX_VMM_STAT_ELEMS) {
+		printf("Cannot accommodate vmm stat type \"%s\"!\n", vst->desc);
+		return;
+	}
+
+	vst->index = vst_num_elems;
+	vst_num_elems += vst->nelems;
+
+	vsttab[vst_num_types++] = vst;
+}
+
+int
+vmm_stat_copy(struct vm *vm, int vcpu, int *num_stats, uint64_t *buf)
+{
+	struct vmm_stat_type *vst;
+	uint64_t *stats;
+	int i;
+
+	if (vcpu < 0 || vcpu >= VM_MAXCPU)
+		return (EINVAL);
+
+	/* Let stats functions update their counters */
+	for (i = 0; i < vst_num_types; i++) {
+		vst = vsttab[i];
+		if (vst->func != NULL)
+			(*vst->func)(vm, vcpu, vst);
+	}
+
+	/* Copy over the stats */
+	stats = vcpu_stats(vm, vcpu);
+	for (i = 0; i < vst_num_elems; i++)
+		buf[i] = stats[i];
+	*num_stats = vst_num_elems;
+	return (0);
+}
+
+void *
+vmm_stat_alloc(void)
+{
+
+	return (malloc(vst_size, M_VMM_STAT, M_WAITOK));
+}
+
+void
+vmm_stat_init(void *vp)
+{
+
+	bzero(vp, vst_size);
+}
+
+void
+vmm_stat_free(void *vp)
+{
+	free(vp, M_VMM_STAT);
+}
+
+int
+vmm_stat_desc_copy(int index, char *buf, int bufsize)
+{
+	int i;
+	struct vmm_stat_type *vst;
+
+	for (i = 0; i < vst_num_types; i++) {
+		vst = vsttab[i];
+		if (index >= vst->index && index < vst->index + vst->nelems) {
+			if (vst->nelems > 1) {
+				snprintf(buf, bufsize, "%s[%d]",
+					 vst->desc, index - vst->index);
+			} else {
+				strlcpy(buf, vst->desc, bufsize);
+			}
+			return (0);	/* found it */
+		}
+	}
+
+	return (EINVAL);
+}
+
+/* global statistics */
+VMM_STAT(VCPU_MIGRATIONS, "vcpu migration across host cpus");
+VMM_STAT(VMEXIT_COUNT, "total number of vm exits");
+VMM_STAT(VMEXIT_EXTINT, "vm exits due to external interrupt");
+VMM_STAT(VMEXIT_HLT, "number of times hlt was intercepted");
+VMM_STAT(VMEXIT_CR_ACCESS, "number of times %cr access was intercepted");
+VMM_STAT(VMEXIT_RDMSR, "number of times rdmsr was intercepted");
+VMM_STAT(VMEXIT_WRMSR, "number of times wrmsr was intercepted");
+VMM_STAT(VMEXIT_MTRAP, "number of monitor trap exits");
+VMM_STAT(VMEXIT_PAUSE, "number of times pause was intercepted");
+VMM_STAT(VMEXIT_INTR_WINDOW, "vm exits due to interrupt window opening");
+VMM_STAT(VMEXIT_NMI_WINDOW, "vm exits due to nmi window opening");
+VMM_STAT(VMEXIT_INOUT, "number of times in/out was intercepted");
+VMM_STAT(VMEXIT_CPUID, "number of times cpuid was intercepted");
+VMM_STAT(VMEXIT_NESTED_FAULT, "vm exits due to nested page fault");
+VMM_STAT(VMEXIT_INST_EMUL, "vm exits for instruction emulation");
+VMM_STAT(VMEXIT_UNKNOWN, "number of vm exits for unknown reason");
+VMM_STAT(VMEXIT_ASTPENDING, "number of times astpending at exit");
+VMM_STAT(VMEXIT_REQIDLE, "number of times idle requested at exit");
+VMM_STAT(VMEXIT_USERSPACE, "number of vm exits handled in userspace");
+VMM_STAT(VMEXIT_RENDEZVOUS, "number of times rendezvous pending at exit");
+VMM_STAT(VMEXIT_EXCEPTION, "number of vm exits due to exceptions");
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_stat.h b/usr/src/uts/i86pc/io/vmm/vmm_stat.h
index 448fbcdb1a..f3338c42b9 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_stat.h
+++ b/usr/src/uts/i86pc/io/vmm/vmm_stat.h
@@ -26,7 +26,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/vmm_stat.h 250427 2013-05-10 02:59:49Z neel $
+ * $FreeBSD$
  */
 /*
  * Copyright 2017 Joyent, Inc.
@@ -45,20 +45,28 @@ enum vmm_stat_scope {
 	VMM_STAT_SCOPE_AMD,		/* AMD SVM specific statistic */
 };
 
+struct vmm_stat_type;
+typedef void (*vmm_stat_func_t)(struct vm *vm, int vcpu,
+    struct vmm_stat_type *stat);
+
 struct vmm_stat_type {
 	int	index;			/* position in the stats buffer */
 	int	nelems;			/* standalone or array */
 	const char *desc;		/* description of statistic */
+	vmm_stat_func_t func;
 	enum vmm_stat_scope scope;
 };
 
-void	vmm_stat_init(void *arg);
+void	vmm_stat_register(void *arg);
 
-#define	VMM_STAT_DEFINE(type, nelems, desc, scope)			\
+#define	VMM_STAT_FDEFINE(type, nelems, desc, func, scope)		\
 	struct vmm_stat_type type[1] = {				\
-		{ -1, nelems, desc, scope }				\
+		{ -1, nelems, desc, func, scope }			\
 	};								\
-	SYSINIT(type##_stat, SI_SUB_KLD, SI_ORDER_ANY, vmm_stat_init, type)
+	SYSINIT(type##_stat, SI_SUB_KLD, SI_ORDER_ANY, vmm_stat_register, type)
+
+#define VMM_STAT_DEFINE(type, nelems, desc, scope) 			\
+	VMM_STAT_FDEFINE(type, nelems, desc, NULL, scope)
 
 #define	VMM_STAT_DECLARE(type)						\
 	extern struct vmm_stat_type type[1]
@@ -70,10 +78,14 @@ void	vmm_stat_init(void *arg);
 #define	VMM_STAT_AMD(type, desc)	\
 	VMM_STAT_DEFINE(type, 1, desc, VMM_STAT_SCOPE_AMD)
 
+#define	VMM_STAT_FUNC(type, desc, func)	\
+	VMM_STAT_FDEFINE(type, 1, desc, func, VMM_STAT_SCOPE_ANY)
+
 #define	VMM_STAT_ARRAY(type, nelems, desc)	\
 	VMM_STAT_DEFINE(type, nelems, desc, VMM_STAT_SCOPE_ANY)
 
 void	*vmm_stat_alloc(void);
+void	vmm_stat_init(void *vp);
 void 	vmm_stat_free(void *vp);
 
 /*
@@ -95,7 +107,20 @@ vmm_stat_array_incr(struct vm *vm, int vcpu, struct vmm_stat_type *vst,
 		stats[vst->index + statidx] += x;
 #endif
 }
-		   
+
+static __inline void
+vmm_stat_array_set(struct vm *vm, int vcpu, struct vmm_stat_type *vst,
+		   int statidx, uint64_t val)
+{
+#ifdef VMM_KEEP_STATS
+	uint64_t *stats;
+	
+	stats = vcpu_stats(vm, vcpu);
+
+	if (vst->index >= 0 && statidx < vst->nelems)
+		stats[vst->index + statidx] = val;
+#endif
+}
 
 static __inline void
 vmm_stat_incr(struct vm *vm, int vcpu, struct vmm_stat_type *vst, uint64_t x)
@@ -106,6 +131,15 @@ vmm_stat_incr(struct vm *vm, int vcpu, struct vmm_stat_type *vst, uint64_t x)
 #endif
 }
 
+static __inline void
+vmm_stat_set(struct vm *vm, int vcpu, struct vmm_stat_type *vst, uint64_t val)
+{
+
+#ifdef VMM_KEEP_STATS
+	vmm_stat_array_set(vm, vcpu, vst, 0, val);
+#endif
+}
+
 VMM_STAT_DECLARE(VCPU_MIGRATIONS);
 VMM_STAT_DECLARE(VMEXIT_COUNT);
 VMM_STAT_DECLARE(VMEXIT_EXTINT);
@@ -125,6 +159,6 @@ VMM_STAT_DECLARE(VMEXIT_UNKNOWN);
 VMM_STAT_DECLARE(VMEXIT_ASTPENDING);
 VMM_STAT_DECLARE(VMEXIT_USERSPACE);
 VMM_STAT_DECLARE(VMEXIT_RENDEZVOUS);
-VMM_STAT_DECLARE(VMEXIT_USERSPACE);
 VMM_STAT_DECLARE(VMEXIT_EXCEPTION);
+VMM_STAT_DECLARE(VMEXIT_REQIDLE);
 #endif
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_util.c b/usr/src/uts/i86pc/io/vmm/vmm_util.c
index fabd42e13c..e3a65dd085 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_util.c
+++ b/usr/src/uts/i86pc/io/vmm/vmm_util.c
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/vmm_util.c 245678 2013-01-20 03:42:49Z neel $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -39,7 +39,7 @@
  */
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/sys/amd64/vmm/vmm_util.c 245678 2013-01-20 03:42:49Z neel $");
+__FBSDID("$FreeBSD$");
 
 #include <sys/param.h>
 #include <sys/libkern.h>
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_util.h b/usr/src/uts/i86pc/io/vmm/vmm_util.h
index fe1c1c9449..7f82332923 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_util.h
+++ b/usr/src/uts/i86pc/io/vmm/vmm_util.h
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/vmm_util.h 245678 2013-01-20 03:42:49Z neel $
+ * $FreeBSD$
  */
 
 #ifndef _VMM_UTIL_H_
diff --git a/usr/src/uts/i86pc/io/vmm/vmx_assym.s b/usr/src/uts/i86pc/io/vmm/vmx_assym.s
deleted file mode 100644
index d84ca30275..0000000000
--- a/usr/src/uts/i86pc/io/vmm/vmx_assym.s
+++ /dev/null
@@ -1 +0,0 @@
-#include "vmx_assym.h"
diff --git a/usr/src/uts/i86pc/io/vmm/x86.c b/usr/src/uts/i86pc/io/vmm/x86.c
index 02222ef5e7..60ef248a82 100644
--- a/usr/src/uts/i86pc/io/vmm/x86.c
+++ b/usr/src/uts/i86pc/io/vmm/x86.c
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/x86.c 255645 2013-09-17 17:56:53Z grehan $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -39,36 +39,76 @@
  */
 
 #include <sys/cdefs.h>
-__FBSDID("$FreeBSD: head/sys/amd64/vmm/x86.c 255645 2013-09-17 17:56:53Z grehan $");
+__FBSDID("$FreeBSD$");
 
 #include <sys/param.h>
-#include <sys/types.h>
+#include <sys/pcpu.h>
 #include <sys/systm.h>
-#include <sys/cpuset.h>
+#include <sys/sysctl.h>
+#include <sys/x86_archext.h>
 
 #include <machine/clock.h>
 #include <machine/cpufunc.h>
 #include <machine/md_var.h>
+#include <machine/segments.h>
 #include <machine/specialreg.h>
 
 #include <machine/vmm.h>
 
+#include "vmm_host.h"
+#include "vmm_ktr.h"
+#include "vmm_util.h"
 #include "x86.h"
 
+SYSCTL_DECL(_hw_vmm);
+SYSCTL_NODE(_hw_vmm, OID_AUTO, topology, CTLFLAG_RD, 0, NULL);
+
 #define	CPUID_VM_HIGH		0x40000000
 
 static const char bhyve_id[12] = "bhyve bhyve ";
 
 static uint64_t bhyve_xcpuids;
+SYSCTL_ULONG(_hw_vmm, OID_AUTO, bhyve_xcpuids, CTLFLAG_RW, &bhyve_xcpuids, 0,
+    "Number of times an unknown cpuid leaf was accessed");
+
+/*
+ * The default CPU topology is a single thread per package.
+ */
+static u_int threads_per_core = 1;
+SYSCTL_UINT(_hw_vmm_topology, OID_AUTO, threads_per_core, CTLFLAG_RDTUN,
+    &threads_per_core, 0, NULL);
+
+static u_int cores_per_package = 1;
+SYSCTL_UINT(_hw_vmm_topology, OID_AUTO, cores_per_package, CTLFLAG_RDTUN,
+    &cores_per_package, 0, NULL);
+
+static int cpuid_leaf_b = 1;
+SYSCTL_INT(_hw_vmm_topology, OID_AUTO, cpuid_leaf_b, CTLFLAG_RDTUN,
+    &cpuid_leaf_b, 0, NULL);
+
+/*
+ * Round up to the next power of two, if necessary, and then take log2.
+ * Returns -1 if argument is zero.
+ */
+static __inline int
+log2(u_int x)
+{
+
+	return (fls(x << (1 - powerof2(x))) - 1);
+}
 
 int
 x86_emulate_cpuid(struct vm *vm, int vcpu_id,
 		  uint32_t *eax, uint32_t *ebx, uint32_t *ecx, uint32_t *edx)
 {
-	int error;
-	unsigned int 	func, regs[4];
+	const struct xsave_limits *limits;
+	uint64_t cr4;
+	int error, enable_invpcid, level, width = 0, x2apic_id = 0;
+	unsigned int func, regs[4], logical_cpus;
 	enum x2apic_state x2apic_state;
 
+	VCPU_CTR2(vm, vcpu_id, "cpuid %#x,%#x", *eax, *ecx);
+
 	/*
 	 * Requests for invalid CPUID levels should map to the highest
 	 * available level instead.
@@ -102,26 +142,83 @@ x86_emulate_cpuid(struct vm *vm, int vcpu_id,
 		case CPUID_8000_0003:
 		case CPUID_8000_0004:
 		case CPUID_8000_0006:
+			cpuid_count(*eax, *ecx, regs);
+			break;
 		case CPUID_8000_0008:
 			cpuid_count(*eax, *ecx, regs);
+			if (vmm_is_amd()) {
+				/*
+				 * XXX this might appear silly because AMD
+				 * cpus don't have threads.
+				 *
+				 * However this matches the logical cpus as
+				 * advertised by leaf 0x1 and will work even
+				 * if the 'threads_per_core' tunable is set
+				 * incorrectly on an AMD host.
+				 */
+				logical_cpus = threads_per_core *
+				    cores_per_package;
+				regs[2] = logical_cpus - 1;
+			}
 			break;
 
 		case CPUID_8000_0001:
+			cpuid_count(*eax, *ecx, regs);
+
+			/*
+			 * Hide SVM and Topology Extension features from guest.
+			 */
+			regs[2] &= ~(AMDID2_SVM | AMDID2_TOPOLOGY);
+
+			/*
+			 * Don't advertise extended performance counter MSRs
+			 * to the guest.
+			 */
+			regs[2] &= ~AMDID2_PCXC;
+			regs[2] &= ~AMDID2_PNXC;
+			regs[2] &= ~AMDID2_PTSCEL2I;
+
+			/*
+			 * Don't advertise Instruction Based Sampling feature.
+			 */
+			regs[2] &= ~AMDID2_IBS;
+
+			/* NodeID MSR not available */
+			regs[2] &= ~AMDID2_NODE_ID;
+
+			/* Don't advertise the OS visible workaround feature */
+			regs[2] &= ~AMDID2_OSVW;
+
+			/* Hide mwaitx/monitorx capability from the guest */
+			regs[2] &= ~AMDID2_MWAITX;
+
 			/*
 			 * Hide rdtscp/ia32_tsc_aux until we know how
 			 * to deal with them.
 			 */
-			cpuid_count(*eax, *ecx, regs);
 			regs[3] &= ~AMDID_RDTSCP;
 			break;
 
 		case CPUID_8000_0007:
-			cpuid_count(*eax, *ecx, regs);
-#ifdef	__FreeBSD__
 			/*
-			 * If the host TSCs are not synchronized across
-			 * physical cpus then we cannot advertise an
-			 * invariant tsc to a vcpu.
+			 * AMD uses this leaf to advertise the processor's
+			 * power monitoring and RAS capabilities. These
+			 * features are hardware-specific and exposing
+			 * them to a guest doesn't make a lot of sense.
+			 *
+			 * Intel uses this leaf only to advertise the
+			 * "Invariant TSC" feature with all other bits
+			 * being reserved (set to zero).
+			 */
+			regs[0] = 0;
+			regs[1] = 0;
+			regs[2] = 0;
+			regs[3] = 0;
+
+			/*
+			 * "Invariant TSC" can be advertised to the guest if:
+			 * - host TSC frequency is invariant
+			 * - host TSCs are synchronized across physical cpus
 			 *
 			 * XXX This still falls short because the vcpu
 			 * can observe the TSC moving backwards as it
@@ -129,10 +226,12 @@ x86_emulate_cpuid(struct vm *vm, int vcpu_id,
 			 * it should discourage the guest from using the
 			 * TSC to keep track of time.
 			 */
-			if (!smp_tsc)
-				regs[3] &= ~AMDPM_TSC_INVARIANT;
-#endif
+#ifdef __FreeBSD__
+			/* XXXJOY: Wire up with our own TSC logic */
+			if (tsc_is_invariant && smp_tsc)
+				regs[3] |= AMDPM_TSC_INVARIANT;
 			break;
+#endif /* __FreeBSD__ */
 
 		case CPUID_0000_0001:
 			do_cpuid(1, regs);
@@ -150,22 +249,41 @@ x86_emulate_cpuid(struct vm *vm, int vcpu_id,
 			regs[1] |= (vcpu_id << CPUID_0000_0001_APICID_SHIFT);
 
 			/*
-			 * Don't expose VMX, SpeedStep or TME capability.
+			 * Don't expose VMX, SpeedStep, TME or SMX capability.
 			 * Advertise x2APIC capability and Hypervisor guest.
 			 */
 			regs[2] &= ~(CPUID2_VMX | CPUID2_EST | CPUID2_TM2);
+			regs[2] &= ~(CPUID2_SMX);
 
 			regs[2] |= CPUID2_HV;
 
 			if (x2apic_state != X2APIC_DISABLED)
 				regs[2] |= CPUID2_X2APIC;
+			else
+				regs[2] &= ~CPUID2_X2APIC;
+
+			/*
+			 * Only advertise CPUID2_XSAVE in the guest if
+			 * the host is using XSAVE.
+			 */
+			if (!(regs[2] & CPUID2_OSXSAVE))
+				regs[2] &= ~CPUID2_XSAVE;
 
 			/*
-			 * Hide xsave/osxsave/avx until the FPU save/restore
-			 * issues are resolved
+			 * If CPUID2_XSAVE is being advertised and the
+			 * guest has set CR4_XSAVE, set
+			 * CPUID2_OSXSAVE.
 			 */
-			regs[2] &= ~(CPUID2_XSAVE | CPUID2_OSXSAVE |
-				     CPUID2_AVX);
+			regs[2] &= ~CPUID2_OSXSAVE;
+			if (regs[2] & CPUID2_XSAVE) {
+				error = vm_get_register(vm, vcpu_id,
+				    VM_REG_GUEST_CR4, &cr4);
+				if (error)
+					panic("x86_emulate_cpuid: error %d "
+					      "fetching %%cr4", error);
+				if (cr4 & CR4_XSAVE)
+					regs[2] |= CPUID2_OSXSAVE;
+			}
 
 			/*
 			 * Hide monitor/mwait until we know how to deal with
@@ -177,7 +295,7 @@ x86_emulate_cpuid(struct vm *vm, int vcpu_id,
 			 * Hide the performance and debug features.
 			 */
 			regs[2] &= ~CPUID2_PDCM;
-			
+
 			/*
 			 * No TSC deadline support in the APIC yet
 			 */
@@ -187,48 +305,90 @@ x86_emulate_cpuid(struct vm *vm, int vcpu_id,
 			 * Hide thermal monitoring
 			 */
 			regs[3] &= ~(CPUID_ACPI | CPUID_TM);
-			
+
 			/*
-			 * Machine check handling is done in the host.
+			 * Hide the debug store capability.
 			 */
-			regs[3] &= ~(CPUID_MCA | CPUID_MCE);
-
-                        /*
-                        * Hide the debug store capability.
-                        */
 			regs[3] &= ~CPUID_DS;
 
 			/*
-			 * Disable multi-core.
+			 * Advertise the Machine Check and MTRR capability.
+			 *
+			 * Some guest OSes (e.g. Windows) will not boot if
+			 * these features are absent.
 			 */
+			regs[3] |= (CPUID_MCA | CPUID_MCE | CPUID_MTRR);
+
+			logical_cpus = threads_per_core * cores_per_package;
 			regs[1] &= ~CPUID_HTT_CORES;
-			regs[3] &= ~CPUID_HTT;
+			regs[1] |= (logical_cpus & 0xff) << 16;
+			regs[3] |= CPUID_HTT;
 			break;
 
 		case CPUID_0000_0004:
-			do_cpuid(4, regs);
+			cpuid_count(*eax, *ecx, regs);
 
-			/*
-			 * Do not expose topology.
-			 */
-			regs[0] &= 0xffff8000;
-			/*
-			 * The maximum number of processor cores in
-			 * this physical processor package and the
-			 * maximum number of threads sharing this
-			 * cache are encoded with "plus 1" encoding.
-			 * Adding one to the value in this register
-			 * field to obtains the actual value.
-			 *
-			 * Therefore 0 for both indicates 1 core
-			 * per package and no cache sharing.
-			 */
+			if (regs[0] || regs[1] || regs[2] || regs[3]) {
+				regs[0] &= 0x3ff;
+				regs[0] |= (cores_per_package - 1) << 26;
+				/*
+				 * Cache topology:
+				 * - L1 and L2 are shared only by the logical
+				 *   processors in a single core.
+				 * - L3 and above are shared by all logical
+				 *   processors in the package.
+				 */
+				logical_cpus = threads_per_core;
+				level = (regs[0] >> 5) & 0x7;
+				if (level >= 3)
+					logical_cpus *= cores_per_package;
+				regs[0] |= (logical_cpus - 1) << 14;
+			}
 			break;
 
-		case CPUID_0000_0006:
 		case CPUID_0000_0007:
+			regs[0] = 0;
+			regs[1] = 0;
+			regs[2] = 0;
+			regs[3] = 0;
+
+			/* leaf 0 */
+			if (*ecx == 0) {
+				cpuid_count(*eax, *ecx, regs);
+
+				/* Only leaf 0 is supported */
+				regs[0] = 0;
+
+				/*
+				 * Expose known-safe features.
+				 */
+				regs[1] &= (CPUID_STDEXT_FSGSBASE |
+				    CPUID_STDEXT_BMI1 | CPUID_STDEXT_HLE |
+				    CPUID_STDEXT_AVX2 | CPUID_STDEXT_BMI2 |
+				    CPUID_STDEXT_ERMS | CPUID_STDEXT_RTM |
+				    CPUID_STDEXT_AVX512F |
+				    CPUID_STDEXT_AVX512PF |
+				    CPUID_STDEXT_AVX512ER |
+				    CPUID_STDEXT_AVX512CD);
+				regs[2] = 0;
+				regs[3] = 0;
+
+				/* Advertise INVPCID if it is enabled. */
+				error = vm_get_capability(vm, vcpu_id,
+				    VM_CAP_ENABLE_INVPCID, &enable_invpcid);
+				if (error == 0 && enable_invpcid)
+					regs[1] |= CPUID_STDEXT_INVPCID;
+			}
+			break;
+
+		case CPUID_0000_0006:
+			regs[0] = CPUTPM1_ARAT;
+			regs[1] = 0;
+			regs[2] = 0;
+			regs[3] = 0;
+			break;
+
 		case CPUID_0000_000A:
-		case CPUID_0000_000D:
 			/*
 			 * Handle the access, but report 0 for
 			 * all options
@@ -243,10 +403,83 @@ x86_emulate_cpuid(struct vm *vm, int vcpu_id,
 			/*
 			 * Processor topology enumeration
 			 */
-			regs[0] = 0;
-			regs[1] = 0;
-			regs[2] = *ecx & 0xff;
-			regs[3] = vcpu_id;
+			if (*ecx == 0) {
+				logical_cpus = threads_per_core;
+				width = log2(logical_cpus);
+				level = CPUID_TYPE_SMT;
+				x2apic_id = vcpu_id;
+			}
+
+			if (*ecx == 1) {
+				logical_cpus = threads_per_core *
+				    cores_per_package;
+				width = log2(logical_cpus);
+				level = CPUID_TYPE_CORE;
+				x2apic_id = vcpu_id;
+			}
+
+			if (!cpuid_leaf_b || *ecx >= 2) {
+				width = 0;
+				logical_cpus = 0;
+				level = 0;
+				x2apic_id = 0;
+			}
+
+			regs[0] = width & 0x1f;
+			regs[1] = logical_cpus & 0xffff;
+			regs[2] = (level << 8) | (*ecx & 0xff);
+			regs[3] = x2apic_id;
+			break;
+
+		case CPUID_0000_000D:
+			limits = vmm_get_xsave_limits();
+			if (!limits->xsave_enabled) {
+				regs[0] = 0;
+				regs[1] = 0;
+				regs[2] = 0;
+				regs[3] = 0;
+				break;
+			}
+
+			cpuid_count(*eax, *ecx, regs);
+			switch (*ecx) {
+			case 0:
+				/*
+				 * Only permit the guest to use bits
+				 * that are active in the host in
+				 * %xcr0.  Also, claim that the
+				 * maximum save area size is
+				 * equivalent to the host's current
+				 * save area size.  Since this runs
+				 * "inside" of vmrun(), it runs with
+				 * the guest's xcr0, so the current
+				 * save area size is correct as-is.
+				 */
+				regs[0] &= limits->xcr0_allowed;
+				regs[2] = limits->xsave_max_size;
+				regs[3] &= (limits->xcr0_allowed >> 32);
+				break;
+			case 1:
+				/* Only permit XSAVEOPT. */
+				regs[0] &= CPUID_EXTSTATE_XSAVEOPT;
+				regs[1] = 0;
+				regs[2] = 0;
+				regs[3] = 0;
+				break;
+			default:
+				/*
+				 * If the leaf is for a permitted feature,
+				 * pass through as-is, otherwise return
+				 * all zeroes.
+				 */
+				if (!(limits->xcr0_allowed & (1ul << *ecx))) {
+					regs[0] = 0;
+					regs[1] = 0;
+					regs[2] = 0;
+					regs[3] = 0;
+				}
+				break;
+			}
 			break;
 
 		case 0x40000000:
@@ -274,3 +507,45 @@ x86_emulate_cpuid(struct vm *vm, int vcpu_id,
 
 	return (1);
 }
+
+bool
+vm_cpuid_capability(struct vm *vm, int vcpuid, enum vm_cpuid_capability cap)
+{
+	bool rv;
+
+	KASSERT(cap > 0 && cap < VCC_LAST, ("%s: invalid vm_cpu_capability %d",
+	    __func__, cap));
+
+	/*
+	 * Simply passthrough the capabilities of the host cpu for now.
+	 */
+	rv = false;
+	switch (cap) {
+#ifdef __FreeBSD__
+	case VCC_NO_EXECUTE:
+		if (amd_feature & AMDID_NX)
+			rv = true;
+		break;
+	case VCC_FFXSR:
+		if (amd_feature & AMDID_FFXSR)
+			rv = true;
+		break;
+	case VCC_TCE:
+		if (amd_feature2 & AMDID2_TCE)
+			rv = true;
+		break;
+#else
+	case VCC_NO_EXECUTE:
+		if (is_x86_feature(x86_featureset, X86FSET_NX))
+			rv = true;
+		break;
+	/* XXXJOY: No kernel detection for FFXR or TCE at present, so ignore */
+	case VCC_FFXSR:
+	case VCC_TCE:
+		break;
+#endif
+	default:
+		panic("%s: unknown vm_cpu_capability %d", __func__, cap);
+	}
+	return (rv);
+}
diff --git a/usr/src/uts/i86pc/io/vmm/x86.h b/usr/src/uts/i86pc/io/vmm/x86.h
index db2340b37b..6f99d52931 100644
--- a/usr/src/uts/i86pc/io/vmm/x86.h
+++ b/usr/src/uts/i86pc/io/vmm/x86.h
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/vmm/x86.h 255287 2013-09-06 05:16:10Z grehan $
+ * $FreeBSD$
  */
 
 #ifndef _X86_H_
@@ -62,4 +62,17 @@
 int x86_emulate_cpuid(struct vm *vm, int vcpu_id, uint32_t *eax, uint32_t *ebx,
 		      uint32_t *ecx, uint32_t *edx);
 
+enum vm_cpuid_capability {
+	VCC_NONE,
+	VCC_NO_EXECUTE,
+	VCC_FFXSR,
+	VCC_TCE,
+	VCC_LAST
+};
+
+/*
+ * Return 'true' if the capability 'cap' is enabled in this virtual cpu
+ * and 'false' otherwise.
+ */
+bool vm_cpuid_capability(struct vm *vm, int vcpuid, enum vm_cpuid_capability);
 #endif
diff --git a/usr/src/uts/i86pc/sys/Makefile b/usr/src/uts/i86pc/sys/Makefile
index 80461a55c1..0e3cbbe243 100644
--- a/usr/src/uts/i86pc/sys/Makefile
+++ b/usr/src/uts/i86pc/sys/Makefile
@@ -69,6 +69,7 @@ CHKHDRS=  \
 
 NOCHKHDRS= \
 	vmm.h		\
+	vmm_dev.h	\
 	vmm_impl.h	\
 	vmm_instruction_emul.h
 
diff --git a/usr/src/uts/i86pc/sys/vmm.h b/usr/src/uts/i86pc/sys/vmm.h
index fd021ce876..d013f4f70a 100644
--- a/usr/src/uts/i86pc/sys/vmm.h
+++ b/usr/src/uts/i86pc/sys/vmm.h
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/include/vmm.h 273375 2014-10-21 07:10:43Z neel $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -49,6 +49,7 @@ enum vm_suspend_how {
 	VM_SUSPEND_RESET,
 	VM_SUSPEND_POWEROFF,
 	VM_SUSPEND_HALT,
+	VM_SUSPEND_TRIPLEFAULT,
 	VM_SUSPEND_LAST
 };
 
@@ -90,6 +91,11 @@ enum vm_reg_name {
 	VM_REG_GUEST_GDTR,
 	VM_REG_GUEST_EFER,
 	VM_REG_GUEST_CR2,
+	VM_REG_GUEST_PDPTE0,
+	VM_REG_GUEST_PDPTE1,
+	VM_REG_GUEST_PDPTE2,
+	VM_REG_GUEST_PDPTE3,
+	VM_REG_GUEST_INTR_SHADOW,
 	VM_REG_LAST
 };
 
@@ -109,31 +115,37 @@ enum x2apic_state {
 #define	VM_INTINFO_HWEXCEPTION	(3 << 8)
 #define	VM_INTINFO_SWINTR	(4 << 8)
 
+
 #define	VM_MAX_NAMELEN	32
 
 #ifdef _KERNEL
 
 struct vm;
 struct vm_exception;
-struct vm_memory_segment;
 struct seg_desc;
 struct vm_exit;
 struct vm_run;
 struct vhpet;
 struct vioapic;
 struct vlapic;
+struct vmspace;
+struct vm_object;
 struct vm_guest_paging;
+struct pmap;
+
+struct vm_eventinfo {
+	void	*rptr;		/* rendezvous cookie */
+	int	*sptr;		/* suspend cookie */
+	int	*iptr;		/* reqidle cookie */
+};
 
-typedef int	(*vmm_init_func_t)(void);
+typedef int	(*vmm_init_func_t)(int ipinum);
 typedef int	(*vmm_cleanup_func_t)(void);
-typedef void *	(*vmi_init_func_t)(struct vm *vm); /* instance specific apis */
-typedef int	(*vmi_run_func_t)(void *vmi, int vcpu, register_t rip);
+typedef void	(*vmm_resume_func_t)(void);
+typedef void *	(*vmi_init_func_t)(struct vm *vm, struct pmap *pmap);
+typedef int	(*vmi_run_func_t)(void *vmi, int vcpu, register_t rip,
+		    struct pmap *pmap, struct vm_eventinfo *info);
 typedef void	(*vmi_cleanup_func_t)(void *vmi);
-typedef int	(*vmi_mmap_set_func_t)(void *vmi, vm_paddr_t gpa,
-				       vm_paddr_t hpa, size_t length,
-				       vm_memattr_t attr, int prot,
-				       boolean_t superpages_ok);
-typedef vm_paddr_t (*vmi_mmap_get_func_t)(void *vmi, vm_paddr_t gpa);
 typedef int	(*vmi_get_register_t)(void *vmi, int vcpu, int num,
 				      uint64_t *retval);
 typedef int	(*vmi_set_register_t)(void *vmi, int vcpu, int num,
@@ -144,24 +156,27 @@ typedef int	(*vmi_set_desc_t)(void *vmi, int vcpu, int num,
 				  struct seg_desc *desc);
 typedef int	(*vmi_get_cap_t)(void *vmi, int vcpu, int num, int *retval);
 typedef int	(*vmi_set_cap_t)(void *vmi, int vcpu, int num, int val);
+typedef struct vmspace * (*vmi_vmspace_alloc)(vm_offset_t min, vm_offset_t max);
+typedef void	(*vmi_vmspace_free)(struct vmspace *vmspace);
 typedef struct vlapic * (*vmi_vlapic_init)(void *vmi, int vcpu);
 typedef void	(*vmi_vlapic_cleanup)(void *vmi, struct vlapic *vlapic);
 
 struct vmm_ops {
 	vmm_init_func_t		init;		/* module wide initialization */
 	vmm_cleanup_func_t	cleanup;
+	vmm_resume_func_t	resume;
 
 	vmi_init_func_t		vminit;		/* vm-specific initialization */
 	vmi_run_func_t		vmrun;
 	vmi_cleanup_func_t	vmcleanup;
-	vmi_mmap_set_func_t	vmmmap_set;
-	vmi_mmap_get_func_t	vmmmap_get;
 	vmi_get_register_t	vmgetreg;
 	vmi_set_register_t	vmsetreg;
 	vmi_get_desc_t		vmgetdesc;
 	vmi_set_desc_t		vmsetdesc;
 	vmi_get_cap_t		vmgetcap;
 	vmi_set_cap_t		vmsetcap;
+	vmi_vmspace_alloc	vmspace_alloc;
+	vmi_vmspace_free	vmspace_free;
 	vmi_vlapic_init		vlapic_init;
 	vmi_vlapic_cleanup	vlapic_cleanup;
 };
@@ -171,20 +186,35 @@ extern struct vmm_ops vmm_ops_amd;
 
 int vm_create(const char *name, struct vm **retvm);
 void vm_destroy(struct vm *vm);
+int vm_reinit(struct vm *vm);
 const char *vm_name(struct vm *vm);
-int vm_malloc(struct vm *vm, vm_paddr_t gpa, size_t len);
-#ifdef	__FreeBSD__
+
+/*
+ * APIs that modify the guest memory map require all vcpus to be frozen.
+ */
+int vm_mmap_memseg(struct vm *vm, vm_paddr_t gpa, int segid, vm_ooffset_t off,
+    size_t len, int prot, int flags);
+int vm_alloc_memseg(struct vm *vm, int ident, size_t len, bool sysmem);
+void vm_free_memseg(struct vm *vm, int ident);
 int vm_map_mmio(struct vm *vm, vm_paddr_t gpa, size_t len, vm_paddr_t hpa);
-#endif
 int vm_unmap_mmio(struct vm *vm, vm_paddr_t gpa, size_t len);
-#ifndef	__FreeBSD__
-vm_paddr_t vm_gpa2hpa(struct vm *vm, vm_paddr_t gpa, size_t size);
-#endif
-void *vm_gpa_hold(struct vm *, vm_paddr_t gpa, size_t len, int prot,
-		  void **cookie);
+int vm_assign_pptdev(struct vm *vm, int bus, int slot, int func);
+int vm_unassign_pptdev(struct vm *vm, int bus, int slot, int func);
+
+/*
+ * APIs that inspect the guest memory map require only a *single* vcpu to
+ * be frozen. This acts like a read lock on the guest memory map since any
+ * modification requires *all* vcpus to be frozen.
+ */
+int vm_mmap_getnext(struct vm *vm, vm_paddr_t *gpa, int *segid,
+    vm_ooffset_t *segoff, size_t *len, int *prot, int *flags);
+int vm_get_memseg(struct vm *vm, int ident, size_t *len, bool *sysmem,
+    struct vm_object **objptr);
+void *vm_gpa_hold(struct vm *, int vcpuid, vm_paddr_t gpa, size_t len,
+    int prot, void **cookie);
 void vm_gpa_release(void *cookie);
-int vm_gpabase2memseg(struct vm *vm, vm_paddr_t gpabase,
-	      struct vm_memory_segment *seg);
+bool vm_mem_allocated(struct vm *vm, int vcpuid, vm_paddr_t gpa);
+
 int vm_get_register(struct vm *vm, int vcpu, int reg, uint64_t *retval);
 int vm_set_register(struct vm *vm, int vcpu, int reg, uint64_t val);
 int vm_get_seg_desc(struct vm *vm, int vcpu, int reg,
@@ -192,6 +222,7 @@ int vm_get_seg_desc(struct vm *vm, int vcpu, int reg,
 int vm_set_seg_desc(struct vm *vm, int vcpu, int reg,
 		    struct seg_desc *desc);
 int vm_run(struct vm *vm, struct vm_run *vmrun);
+int vm_suspend(struct vm *vm, enum vm_suspend_how how);
 int vm_inject_nmi(struct vm *vm, int vcpu);
 int vm_nmi_pending(struct vm *vm, int vcpuid);
 void vm_nmi_clear(struct vm *vm, int vcpuid);
@@ -207,10 +238,54 @@ int vm_get_x2apic_state(struct vm *vm, int vcpu, enum x2apic_state *state);
 int vm_set_x2apic_state(struct vm *vm, int vcpu, enum x2apic_state state);
 int vm_apicid2vcpuid(struct vm *vm, int apicid);
 int vm_activate_cpu(struct vm *vm, int vcpu);
-cpuset_t vm_active_cpus(struct vm *vm);
 struct vm_exit *vm_exitinfo(struct vm *vm, int vcpuid);
+void vm_exit_suspended(struct vm *vm, int vcpuid, uint64_t rip);
+void vm_exit_rendezvous(struct vm *vm, int vcpuid, uint64_t rip);
+void vm_exit_astpending(struct vm *vm, int vcpuid, uint64_t rip);
+void vm_exit_reqidle(struct vm *vm, int vcpuid, uint64_t rip);
 
+#ifdef _SYS__CPUSET_H_
+/*
+ * Rendezvous all vcpus specified in 'dest' and execute 'func(arg)'.
+ * The rendezvous 'func(arg)' is not allowed to do anything that will
+ * cause the thread to be put to sleep.
+ *
+ * If the rendezvous is being initiated from a vcpu context then the
+ * 'vcpuid' must refer to that vcpu, otherwise it should be set to -1.
+ *
+ * The caller cannot hold any locks when initiating the rendezvous.
+ *
+ * The implementation of this API may cause vcpus other than those specified
+ * by 'dest' to be stalled. The caller should not rely on any vcpus making
+ * forward progress when the rendezvous is in progress.
+ */
 typedef void (*vm_rendezvous_func_t)(struct vm *vm, int vcpuid, void *arg);
+void vm_smp_rendezvous(struct vm *vm, int vcpuid, cpuset_t dest,
+    vm_rendezvous_func_t func, void *arg);
+cpuset_t vm_active_cpus(struct vm *vm);
+cpuset_t vm_suspended_cpus(struct vm *vm);
+#endif	/* _SYS__CPUSET_H_ */
+
+static __inline int
+vcpu_rendezvous_pending(struct vm_eventinfo *info)
+{
+
+	return (*((uintptr_t *)(info->rptr)) != 0);
+}
+
+static __inline int
+vcpu_suspended(struct vm_eventinfo *info)
+{
+
+	return (*info->sptr);
+}
+
+static __inline int
+vcpu_reqidle(struct vm_eventinfo *info)
+{
+
+	return (*info->iptr);
+}
 
 /*
  * Return 1 if device indicated by bus/slot/func is supposed to be a
@@ -239,14 +314,30 @@ vcpu_is_running(struct vm *vm, int vcpu, int *hostcpu)
 	return (vcpu_get_state(vm, vcpu, hostcpu) == VCPU_RUNNING);
 }
 
+#ifdef _SYS_THREAD_H
+static __inline int
+vcpu_should_yield(struct vm *vm, int vcpu)
+{
+
+	if (curthread->t_astflag)
+		return (1);
+	else if (CPU->cpu_runrun)
+		return (1);
+	else
+		return (0);
+}
+#endif /* _SYS_THREAD_H */
+
 void *vcpu_stats(struct vm *vm, int vcpu);
-void vm_interrupt_hostcpu(struct vm *vm, int vcpu);
 void vcpu_notify_event(struct vm *vm, int vcpuid, bool lapic_intr);
+struct vmspace *vm_get_vmspace(struct vm *vm);
 struct vatpic *vm_atpic(struct vm *vm);
 struct vatpit *vm_atpit(struct vm *vm);
+struct vpmtmr *vm_pmtmr(struct vm *vm);
+struct vrtc *vm_rtc(struct vm *vm);
 
 /*
- * Inject exception 'vme' into the guest vcpu. This function returns 0 on
+ * Inject exception 'vector' into the guest vcpu. This function returns 0 on
  * success and non-zero on failure.
  *
  * Wrapper functions like 'vm_inject_gp()' should be preferred to calling
@@ -256,7 +347,8 @@ struct vatpit *vm_atpit(struct vm *vm);
  * This function should only be called in the context of the thread that is
  * executing this vcpu.
  */
-int vm_inject_exception(struct vm *vm, int vcpuid, struct vm_exception *vme);
+int vm_inject_exception(struct vm *vm, int vcpuid, int vector, int err_valid,
+    uint32_t errcode, int restart_instruction);
 
 /*
  * This function is called after a VM-exit that occurred during exception or
@@ -299,9 +391,10 @@ struct vm_copyinfo {
  * at 'gla' and 'len' bytes long. The 'prot' should be set to PROT_READ for
  * a copyin or PROT_WRITE for a copyout. 
  *
- * Returns 0 on success.
- * Returns 1 if an exception was injected into the guest.
- * Returns -1 otherwise.
+ * retval	is_fault	Interpretation
+ *   0		   0		Success
+ *   0		   1		An exception was injected into the guest
+ * EFAULT	  N/A		Unrecoverable error
  *
  * The 'copyinfo[]' can be passed to 'vm_copyin()' or 'vm_copyout()' only if
  * the return value is 0. The 'copyinfo[]' resources should be freed by calling
@@ -309,13 +402,15 @@ struct vm_copyinfo {
  */
 int vm_copy_setup(struct vm *vm, int vcpuid, struct vm_guest_paging *paging,
     uint64_t gla, size_t len, int prot, struct vm_copyinfo *copyinfo,
-    int num_copyinfo);
+    int num_copyinfo, int *is_fault);
 void vm_copy_teardown(struct vm *vm, int vcpuid, struct vm_copyinfo *copyinfo,
     int num_copyinfo);
 void vm_copyin(struct vm *vm, int vcpuid, struct vm_copyinfo *copyinfo,
     void *kaddr, size_t len);
 void vm_copyout(struct vm *vm, int vcpuid, const void *kaddr,
     struct vm_copyinfo *copyinfo, size_t len);
+
+int vcpu_trace_exceptions(struct vm *vm, int vcpuid);
 #endif	/* KERNEL */
 
 #define	VM_MAXCPU	16			/* maximum virtual cpus */
@@ -349,7 +444,6 @@ struct seg_desc {
 	uint32_t	limit;
 	uint32_t	access;
 };
-
 #define	SEG_DESC_TYPE(access)		((access) & 0x001f)
 #define	SEG_DESC_DPL(access)		(((access) >> 5) & 0x3)
 #define	SEG_DESC_PRESENT(access)	(((access) & 0x0080) ? 1 : 0)
@@ -444,7 +538,15 @@ enum vm_exitcode {
 	VM_EXITCODE_INST_EMUL,
 	VM_EXITCODE_SPINUP_AP,
 	VM_EXITCODE_DEPRECATED1,	/* used to be SPINDOWN_CPU */
+	VM_EXITCODE_RENDEZVOUS,
+	VM_EXITCODE_IOAPIC_EOI,
+	VM_EXITCODE_SUSPENDED,
 	VM_EXITCODE_INOUT_STR,
+	VM_EXITCODE_TASK_SWITCH,
+	VM_EXITCODE_MONITOR,
+	VM_EXITCODE_MWAIT,
+	VM_EXITCODE_SVM,
+	VM_EXITCODE_REQIDLE,
 	VM_EXITCODE_MAX
 };
 
@@ -469,6 +571,22 @@ struct vm_inout_str {
 	struct seg_desc seg_desc;
 };
 
+enum task_switch_reason {
+	TSR_CALL,
+	TSR_IRET,
+	TSR_JMP,
+	TSR_IDT_GATE,	/* task gate in IDT */
+};
+
+struct vm_task_switch {
+	uint16_t	tsssel;		/* new TSS selector */
+	int		ext;		/* task switch due to external event */
+	uint32_t	errcode;
+	int		errcode_valid;	/* push 'errcode' on the new stack */
+	enum task_switch_reason reason;
+	struct vm_guest_paging paging;
+};
+
 struct vm_exit {
 	enum vm_exitcode	exitcode;
 	int			inst_length;	/* 0 means unknown */
@@ -507,6 +625,14 @@ struct vm_exit {
 			int		inst_type;
 			int		inst_error;
 		} vmx;
+		/*
+		 * SVM specific payload.
+		 */
+		struct {
+			uint64_t	exitcode;
+			uint64_t	exitinfo1;
+			uint64_t	exitinfo2;
+		} svm;
 		struct {
 			uint32_t	code;		/* ecx value */
 			uint64_t	wval;
@@ -518,6 +644,13 @@ struct vm_exit {
 		struct {
 			uint64_t	rflags;
 		} hlt;
+		struct {
+			int		vector;
+		} ioapic_eoi;
+		struct {
+			enum vm_suspend_how how;
+		} suspended;
+		struct vm_task_switch task_switch;
 	} u;
 };
 
@@ -555,12 +688,26 @@ int vm_restart_instruction(void *vm, int vcpuid);
 
 #ifndef	__FreeBSD__
 #ifdef	_KERNEL
-extern void vmm_sol_glue_init(void);
-extern void vmm_sol_glue_cleanup(void);
 
-extern int vmm_mod_load(void);
-extern int vmm_mod_unload(void);
-#endif
-#endif
+void vmm_sol_glue_init(void);
+void vmm_sol_glue_cleanup(void);
+
+int vmm_mod_load(void);
+int vmm_mod_unload(void);
+
+/*
+ * Because of tangled headers, these are mirrored by vmm_drv.h to present the
+ * interface to driver consumers.
+ */
+typedef int (*vmm_rmem_cb_t)(void *, uintptr_t, uint_t, uint64_t *);
+typedef int (*vmm_wmem_cb_t)(void *, uintptr_t, uint_t, uint64_t);
+
+int vm_ioport_hook(struct vm *, uint_t, vmm_rmem_cb_t, vmm_wmem_cb_t, void *,
+    void **);
+void vm_ioport_unhook(struct vm *, void **);
+int vm_ioport_handle_hook(struct vm *, int, bool, int, int, uint32_t *);
+
+#endif /* _KERNEL */
+#endif /* __FreeBSD */
 
 #endif	/* _VMM_H_ */
diff --git a/usr/src/uts/i86pc/sys/vmm_dev.h b/usr/src/uts/i86pc/sys/vmm_dev.h
index 86e2616ff1..dd5a2a800f 100644
--- a/usr/src/uts/i86pc/sys/vmm_dev.h
+++ b/usr/src/uts/i86pc/sys/vmm_dev.h
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/include/vmm_dev.h 268889 2014-07-19 20:59:08Z neel $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -44,13 +44,25 @@
 
 #ifdef _KERNEL
 void	vmmdev_init(void);
-void	vmmdev_cleanup(void);
+int	vmmdev_cleanup(void);
 #endif
 
-struct vm_memory_segment {
-	vm_paddr_t	gpa;	/* in */
+struct vm_memmap {
+	vm_paddr_t	gpa;
+	int		segid;		/* memory segment */
+	vm_ooffset_t	segoff;		/* offset into memory segment */
+	size_t		len;		/* mmap length */
+	int		prot;		/* RWX */
+	int		flags;
+};
+#define	VM_MEMMAP_F_WIRED	0x01
+#define	VM_MEMMAP_F_IOMMU	0x02
+
+#define	VM_MEMSEG_NAME(m)	((m)->name[0] != '\0' ? (m)->name : NULL)
+struct vm_memseg {
+	int		segid;
 	size_t		len;
-	int		wired;
+	char		name[SPECNAMELEN + 1];
 };
 
 struct vm_register {
@@ -130,7 +142,7 @@ struct vm_pptdev_msi {
 	int		slot;
 	int		func;
 	int		numvec;		/* 0 means disabled */
-	uint32_t	msg;
+	uint64_t	msg;
 	uint64_t	addr;
 };
 
@@ -140,7 +152,7 @@ struct vm_pptdev_msix {
 	int		slot;
 	int		func;
 	int		idx;
-	uint32_t	msg;
+	uint64_t	msg;
 	uint32_t	vector_control;
 	uint64_t	addr;
 };
@@ -177,8 +189,8 @@ struct vm_hpet_cap {
 	uint32_t	capabilities;	/* lower 32 bits of HPET capabilities */
 };
 
-struct vm_activate_cpu {
-	int		vcpuid;
+struct vm_suspend {
+	enum vm_suspend_how how;
 };
 
 struct vm_gla2gpa {
@@ -190,26 +202,62 @@ struct vm_gla2gpa {
 	uint64_t	gpa;
 };
 
+struct vm_activate_cpu {
+	int		vcpuid;
+};
+
 struct vm_cpuset {
 	int		which;
 	int		cpusetsize;
+#ifndef _KERNEL
 	cpuset_t	*cpus;
+#else
+	void		*cpus;
+#endif
 };
 #define	VM_ACTIVE_CPUS		0
 #define	VM_SUSPENDED_CPUS	1
 
+struct vm_intinfo {
+	int		vcpuid;
+	uint64_t	info1;
+	uint64_t	info2;
+};
+
+struct vm_rtc_time {
+	time_t		secs;
+};
+
+struct vm_rtc_data {
+	int		offset;
+	uint8_t		value;
+};
+
+#ifndef __FreeBSD__
+struct vm_devmem_offset {
+	int		segid;
+	off_t		offset;
+};
+#endif
+
 enum {
 	/* general routines */
 	IOCNUM_ABIVERS = 0,
 	IOCNUM_RUN = 1,
 	IOCNUM_SET_CAPABILITY = 2,
 	IOCNUM_GET_CAPABILITY = 3,
+	IOCNUM_SUSPEND = 4,
+	IOCNUM_REINIT = 5,
 
 	/* memory apis */
-	IOCNUM_MAP_MEMORY = 10,
-	IOCNUM_GET_MEMORY_SEG = 11,
+	IOCNUM_MAP_MEMORY = 10,			/* deprecated */
+	IOCNUM_GET_MEMORY_SEG = 11,		/* deprecated */
 	IOCNUM_GET_GPA_PMAP = 12,
 	IOCNUM_GLA2GPA = 13,
+	IOCNUM_ALLOC_MEMSEG = 14,
+	IOCNUM_GET_MEMSEG = 15,
+	IOCNUM_MMAP_MEMSEG = 16,
+	IOCNUM_MMAP_GETNEXT = 17,
 
 	/* register/state accessors */
 	IOCNUM_SET_REGISTER = 20,
@@ -218,6 +266,8 @@ enum {
 	IOCNUM_GET_SEGMENT_DESCRIPTOR = 23,
 
 	/* interrupt injection */
+	IOCNUM_GET_INTINFO = 28,
+	IOCNUM_SET_INTINFO = 29,
 	IOCNUM_INJECT_EXCEPTION = 30,
 	IOCNUM_LAPIC_IRQ = 31,
 	IOCNUM_INJECT_NMI = 32,
@@ -254,14 +304,33 @@ enum {
 	/* vm_cpuset */
 	IOCNUM_ACTIVATE_CPU = 90,
 	IOCNUM_GET_CPUSET = 91,
+
+	/* RTC */
+	IOCNUM_RTC_READ = 100,
+	IOCNUM_RTC_WRITE = 101,
+	IOCNUM_RTC_SETTIME = 102,
+	IOCNUM_RTC_GETTIME = 103,
+
+#ifndef __FreeBSD__
+	/* illumos-custom ioctls */
+	IOCNUM_DEVMEM_GETOFFSET = 256,
+#endif
 };
 
 #define	VM_RUN		\
 	_IOWR('v', IOCNUM_RUN, struct vm_run)
-#define	VM_MAP_MEMORY	\
-	_IOWR('v', IOCNUM_MAP_MEMORY, struct vm_memory_segment)
-#define	VM_GET_MEMORY_SEG \
-	_IOWR('v', IOCNUM_GET_MEMORY_SEG, struct vm_memory_segment)
+#define	VM_SUSPEND	\
+	_IOW('v', IOCNUM_SUSPEND, struct vm_suspend)
+#define	VM_REINIT	\
+	_IO('v', IOCNUM_REINIT)
+#define	VM_ALLOC_MEMSEG	\
+	_IOW('v', IOCNUM_ALLOC_MEMSEG, struct vm_memseg)
+#define	VM_GET_MEMSEG	\
+	_IOWR('v', IOCNUM_GET_MEMSEG, struct vm_memseg)
+#define	VM_MMAP_MEMSEG	\
+	_IOW('v', IOCNUM_MMAP_MEMSEG, struct vm_memmap)
+#define	VM_MMAP_GETNEXT	\
+	_IOWR('v', IOCNUM_MMAP_GETNEXT, struct vm_memmap)
 #define	VM_SET_REGISTER \
 	_IOW('v', IOCNUM_SET_REGISTER, struct vm_register)
 #define	VM_GET_REGISTER \
@@ -310,10 +379,8 @@ enum {
 	_IOW('v', IOCNUM_PPTDEV_MSIX, struct vm_pptdev_msix)
 #define VM_INJECT_NMI \
 	_IOW('v', IOCNUM_INJECT_NMI, struct vm_nmi)
-#ifdef	__FreeBSD__
-#define	VM_STATS \
+#define	VM_STATS_IOC \
 	_IOWR('v', IOCNUM_VM_STATS, struct vm_stats)
-#endif
 #define	VM_STAT_DESC \
 	_IOWR('v', IOCNUM_VM_STAT_DESC, struct vm_stat_desc)
 #define	VM_SET_X2APIC_STATE \
@@ -330,6 +397,30 @@ enum {
 	_IOW('v', IOCNUM_ACTIVATE_CPU, struct vm_activate_cpu)
 #define	VM_GET_CPUS	\
 	_IOW('v', IOCNUM_GET_CPUSET, struct vm_cpuset)
+#define	VM_SET_INTINFO	\
+	_IOW('v', IOCNUM_SET_INTINFO, struct vm_intinfo)
+#define	VM_GET_INTINFO	\
+	_IOWR('v', IOCNUM_GET_INTINFO, struct vm_intinfo)
+#define VM_RTC_WRITE \
+	_IOW('v', IOCNUM_RTC_WRITE, struct vm_rtc_data)
+#define VM_RTC_READ \
+	_IOWR('v', IOCNUM_RTC_READ, struct vm_rtc_data)
+#define VM_RTC_SETTIME	\
+	_IOW('v', IOCNUM_RTC_SETTIME, struct vm_rtc_time)
+#define VM_RTC_GETTIME	\
+	_IOR('v', IOCNUM_RTC_GETTIME, struct vm_rtc_time)
 #define	VM_RESTART_INSTRUCTION \
 	_IOW('v', IOCNUM_RESTART_INSTRUCTION, int)
+
+#ifndef __FreeBSD__
+#define	VM_DEVMEM_GETOFFSET \
+	_IOW('v', IOCNUM_DEVMEM_GETOFFSET, struct vm_devmem_offset)
+
+/* ioctls used against ctl device for vm create/destroy */
+#define	VMM_IOC_BASE		(('V' << 16) | ('M' << 8))
+#define	VMM_CREATE_VM		(VMM_IOC_BASE | 0x01)
+#define	VMM_DESTROY_VM		(VMM_IOC_BASE | 0x02)
+
+#endif
+
 #endif
diff --git a/usr/src/uts/i86pc/sys/vmm_drv.h b/usr/src/uts/i86pc/sys/vmm_drv.h
new file mode 100644
index 0000000000..4437f06f3a
--- /dev/null
+++ b/usr/src/uts/i86pc/sys/vmm_drv.h
@@ -0,0 +1,39 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2017 Joyent, Inc.
+ */
+
+#ifndef _VMM_DRV_H_
+#define	_VMM_DRV_H_
+
+#ifdef	_KERNEL
+struct vmm_hold;
+typedef struct vmm_hold vmm_hold_t;
+
+/*
+ * Because of tangled headers, these definitions mirror their vmm_[rw]mem_cb_t
+ * counterparts in vmm.h.
+ */
+typedef int (*vmm_drv_rmem_cb_t)(void *, uintptr_t, uint_t, uint64_t *);
+typedef int (*vmm_drv_wmem_cb_t)(void *, uintptr_t, uint_t, uint64_t);
+
+extern int vmm_drv_hold(file_t *, cred_t *, vmm_hold_t **);
+extern void vmm_drv_rele(vmm_hold_t *);
+extern boolean_t vmm_drv_expired(vmm_hold_t *);
+extern void *vmm_drv_gpa2kva(vmm_hold_t *, uintptr_t, size_t);
+extern int vmm_drv_ioport_hook(vmm_hold_t *, uint_t, vmm_drv_rmem_cb_t,
+    vmm_drv_wmem_cb_t, void *, void **);
+extern void vmm_drv_ioport_unhook(vmm_hold_t *, void **);
+#endif /* _KERNEL */
+
+#endif /* _VMM_DRV_H_ */
diff --git a/usr/src/uts/i86pc/sys/vmm_impl.h b/usr/src/uts/i86pc/sys/vmm_impl.h
index 0126993dc2..365567a5cd 100644
--- a/usr/src/uts/i86pc/sys/vmm_impl.h
+++ b/usr/src/uts/i86pc/sys/vmm_impl.h
@@ -32,23 +32,44 @@
 #define	VMM_CTL_MINOR_NAME	VMM_DRIVER_NAME VMM_CTL_MINOR_NODE
 #define	VMM_CTL_MINOR		0
 
-#define	VMM_IOC_BASE		(('V' << 16) | ('M' << 8))
+#ifdef	_KERNEL
+
+/*
+ * Rather than creating whole character devices for devmem mappings, they are
+ * available by mmap(2)ing the vmm handle at a specific offset.  These offsets
+ * begin just above the maximum allow guest physical address.
+ */
+#include <vm/vm_param.h>
+#define	VM_DEVMEM_START	(VM_MAXUSER_ADDRESS + 1)
 
-#define	VMM_CREATE_VM		(VMM_IOC_BASE | 0x01)
-#define	VMM_DESTROY_VM		(VMM_IOC_BASE | 0x02)
+struct vmm_devmem_entry {
+	list_node_t	vde_node;
+	int		vde_segid;
+	char		vde_name[SPECNAMELEN + 1];
+	size_t		vde_len;
+	off_t		vde_off;
+};
+typedef struct vmm_devmem_entry vmm_devmem_entry_t;
 
-struct vmm_ioctl {
-	char vmm_name[VM_MAX_NAMELEN];
+enum vmm_softc_state {
+	VMM_HELD	= 1,	/* external driver(s) possess hold on VM */
+	VMM_CLEANUP	= 2,	/* request that holds are released */
+	VMM_PURGED	= 4,	/* all hold have been released */
+	VMM_BLOCK_HOOK	= 8	/* mem hook install temporarily blocked */
 };
 
-#ifdef	_KERNEL
 struct vmm_softc {
-	boolean_t			open;
-	minor_t				minor;
-	struct vm			*vm;
-	char				name[VM_MAX_NAMELEN];
-	SLIST_ENTRY(vmm_softc)		link;
+	list_node_t	vmm_node;
+	struct vm	*vmm_vm;
+	minor_t		vmm_minor;
+	char		vmm_name[VM_MAX_NAMELEN];
+	uint_t		vmm_flags;
+	boolean_t	vmm_is_open;
+	list_t		vmm_devmem_list;
+	list_t		vmm_holds;
+	kcondvar_t	vmm_cv;
 };
+typedef struct vmm_softc vmm_softc_t;
 #endif
 
 /*
diff --git a/usr/src/uts/i86pc/sys/vmm_instruction_emul.h b/usr/src/uts/i86pc/sys/vmm_instruction_emul.h
index 8138890a2c..9b2f6dd592 100644
--- a/usr/src/uts/i86pc/sys/vmm_instruction_emul.h
+++ b/usr/src/uts/i86pc/sys/vmm_instruction_emul.h
@@ -23,7 +23,7 @@
  * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
  * SUCH DAMAGE.
  *
- * $FreeBSD: head/sys/amd64/include/vmm_instruction_emul.h 276479 2014-12-31 20:31:32Z dim $
+ * $FreeBSD$
  */
 /*
  * This file and its contents are supplied under the terms of the
@@ -93,17 +93,19 @@ int vie_calculate_gla(enum vm_cpu_mode cpu_mode, enum vm_reg_name seg,
  */
 int vmm_fetch_instruction(struct vm *vm, int cpuid,
 			  struct vm_guest_paging *guest_paging,
-			  uint64_t rip, int inst_length, struct vie *vie);
+			  uint64_t rip, int inst_length, struct vie *vie,
+			  int *is_fault);
 
 /*
  * Translate the guest linear address 'gla' to a guest physical address.
  *
- * Returns 0 on success and '*gpa' contains the result of the translation.
- * Returns 1 if an exception was injected into the guest.
- * Returns -1 otherwise.
+ * retval	is_fault	Interpretation
+ *   0		   0		'gpa' contains result of the translation
+ *   0		   1		An exception was injected into the guest
+ * EFAULT	  N/A		An unrecoverable hypervisor error occurred
  */
 int vm_gla2gpa(struct vm *vm, int vcpuid, struct vm_guest_paging *paging,
-    uint64_t gla, int prot, uint64_t *gpa);
+    uint64_t gla, int prot, uint64_t *gpa, int *is_fault);
 
 void vie_init(struct vie *vie, const char *inst_bytes, int inst_length);
 
diff --git a/usr/src/uts/i86pc/vmm/Makefile b/usr/src/uts/i86pc/vmm/Makefile
index f283d2f98f..914e3c61b6 100644
--- a/usr/src/uts/i86pc/vmm/Makefile
+++ b/usr/src/uts/i86pc/vmm/Makefile
@@ -72,6 +72,9 @@ LINTTAGS	+= -erroff=E_FUNC_DECL_VAR_ARG2
 LINTTAGS	+= -erroff=E_ASM_IMPOSSIBLE_CONSTRAINT
 LINTTAGS	+= -erroff=E_ASM_UNUSED_PARAM
 LINTTAGS	+= -erroff=E_NOP_IF_STMT
+LINTTAGS	+= -erroff=E_ZERO_OR_NEGATIVE_SUBSCRIPT
+
+CERRWARN	+= -_gcc=-Wno-empty-body
 
 # These sources only compile with gcc.  Workaround a confluence of cruft
 # regarding dmake and shadow compilation by neutering the sun compiler.
@@ -86,9 +89,17 @@ INC_PATH	+= -I$(UTSBASE)/i86pc/io/vmm -I$(UTSBASE)/i86pc/io/vmm/io
 AS_INC_PATH	+= -I$(UTSBASE)/i86pc/io/vmm -I$(OBJS_DIR)
 
 CFLAGS		+= -_gcc=-Wimplicit-function-declaration
+# The FreeBSD %# notation makes gcc gripe
+CFLAGS		+= -_gcc=-Wno-format
+
+$(OBJS_DIR)/vmm.o := CERRWARN += -_gcc=-Wno-pointer-sign -_gcc=-Wno-type-limits
+$(OBJS_DIR)/svm.o := CERRWARN += -_gcc=-Wno-pointer-sign -_gcc=-Wno-type-limits
 
-OFFSETS_SRC	= $(CONF_SRCDIR)/offsets.in
-ASSYM_H		= $(OBJS_DIR)/vmx_assym.h
+OFFSETS_VMX	= $(CONF_SRCDIR)/intel/offsets.in
+OFFSETS_SVM	= $(CONF_SRCDIR)/amd/offsets.in
+ASSYM_VMX	= $(OBJS_DIR)/vmx_assym.h
+ASSYM_SVM	= $(OBJS_DIR)/svm_assym.h
+ASSYM_H		= $(ASSYM_VMX) $(ASSYM_SVM)
 
 CLEANFILES	+= $(ASSYM_H)
 
@@ -118,7 +129,10 @@ install:	$(INSTALL_DEPS)
 #
 include $(UTSBASE)/i86pc/Makefile.targ
 
-$(OBJECTS): $(ASSYM_H)
+$(ASSYM_VMX): $(OFFSETS_VMX) $(GENASSYM)
+	$(OFFSETS_CREATE) -I../../i86pc/io/vmm < $(OFFSETS_VMX) >$@
+$(ASSYM_SVM): $(OFFSETS_SVM) $(GENASSYM)
+	$(OFFSETS_CREATE) -I../../i86pc/io/vmm < $(OFFSETS_SVM) >$@
 
-$(ASSYM_H): $(OFFSETS_SRC) $(GENASSYM)
-	$(OFFSETS_CREATE) -I../../i86pc/io/vmm < $(OFFSETS_SRC) >$@
+$(OBJS_DIR)/vmx_support.o:  $(ASSYM_VMX)
+$(OBJS_DIR)/svm_support.o:  $(ASSYM_SVM)
