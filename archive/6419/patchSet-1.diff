commit 9a8a9cb9777ce18fefdc18372c9da07f3fa17bf6
Author: Richard Bradley <richard.bradley@joyent.com>
Date:   2019-06-11T10:17:06+01:00 (4 months ago)
    
    MANTA-4212 Postgres 11 Replication Queries Fail for pgstatsmon

diff --git a/lib/dbinit.js b/lib/dbinit.js
index c90793a..1be4383 100644
--- a/lib/dbinit.js
+++ b/lib/dbinit.js
@@ -75,7 +75,7 @@ function connect_to_database(args, callback) {
 	}, connect_timeout);
 
 	client = create_client({
-		'name': 'testbackend',
+		'name': args.conf.name,
 		'address': args.conf.hostname,
 		'port': args.conf.port
 	});
@@ -127,7 +127,9 @@ function get_db_version(args, callback) {
 	var server_version_num;
 
 	server_version_num = 'server_version_num';
-	query = mod_util.format('SHOW %s', server_version_num);
+	query = mod_util.format(
+	    'select current_setting(\'%s\')::integer as %s',
+	    server_version_num, server_version_num);
 
 	res = args.client.query(query);
 	res.once('row', function (row) {
@@ -332,8 +334,8 @@ function setup_monitoring_user(args, callback) {
 	mod_vasync.pipeline({
 		'funcs': [
 			connect_to_database,
-			stop_if_standby,
 			get_db_version,
+			stop_if_standby,
 			create_user,
 			create_activity_function,
 		        create_replication_function,
@@ -352,7 +354,7 @@ function setup_monitoring_user(args, callback) {
 		if (arg.client) {
 			arg.client.destroy();
 		}
-		callback(err);
+		callback(err, arg.pg_version);
 	});
 }
 
diff --git a/lib/pgstatsmon.js b/lib/pgstatsmon.js
index 95d506f..fa58407 100644
--- a/lib/pgstatsmon.js
+++ b/lib/pgstatsmon.js
@@ -3,7 +3,7 @@
  * License, v. 2.0. If a copy of the MPL was not distributed with this
  * file, You can obtain one at http://mozilla.org/MPL/2.0/.
  *
- * Copyright (c) 2018, Joyent, Inc.
+ * Copyright (c) 2019, Joyent, Inc.
  */
 
 /*
@@ -174,8 +174,6 @@ function PgMon(config)
 	/* interval returned from setInterval */
 	this.pm_interval_object = null;
 
-	/* queries to run */
-	this.pm_queries = [];
 	/* current state of each instance's request */
 	this.pm_state =	[];
 	/* last-seen datapoints for each instance */
@@ -187,7 +185,7 @@ function PgMon(config)
 	/* cueball connection pools for each instance */
 	this.pm_pools = [];
 
-	this.initializeMetrics(queries.getQueries(config));
+	this.initializeMetrics();
 }
 
 /*
@@ -219,7 +217,7 @@ PgMon.prototype.getTarget = function ()
  * that the PgMon class tracks.  This isn't an issue unless this is used after
  * pgstatsmon has already collected some metrics.
  */
-PgMon.prototype.initializeMetrics = function (query_list)
+PgMon.prototype.initializeMetrics = function ()
 {
 	var mon = this;
 
@@ -232,9 +230,6 @@ PgMon.prototype.initializeMetrics = function (query_list)
 
 	this.pm_targets = [];
 
-	this.pm_queries = query_list.map(
-	    function (q) { return (new Query(q, mon.pm_log)); });
-
 	/* Prometheus target */
 	this.pm_targets.push(mon.createTarget(this.pm_targetconf));
 
@@ -317,13 +312,8 @@ PgMon.prototype.connect = function ()
 			'resolver': resolver,
 			'backend': backend,
 			'needs_setup': true,
-			'setting_up': false
-		});
-
-		/* make sure we have the data structures set up */
-		mon.add_connection_data({
-			'key': key,
-			'name': backend.name
+			'setting_up': false,
+			'queries': []
 		});
 
 		setImmediate(function () {
@@ -372,7 +362,7 @@ PgMon.prototype.connect = function ()
  * to do these things, and then disconnect. Nothing will be done if the backend
  * is identified as being in recovery (a sync or async replica).
  */
-PgMon.prototype.setup_backend = function setup_backend(pi, callback)
+PgMon.prototype.setup_backend = function setup_backend(pi)
 {
 	var mon = this;
 	if (mon.pm_pools[pi].needs_setup === false ||
@@ -388,12 +378,13 @@ PgMon.prototype.setup_backend = function setup_backend(pi, callback)
 	mod_dbinit.setup_monitoring_user({
 		'user': mon.pm_dbuser,
 		'targetdb': mon.pm_database,
+		'name': mon.pm_pools[pi].backend.name,
 		'hostname': mon.pm_pools[pi].backend.address,
 		'port': mon.pm_pools[pi].backend.port,
 		'query_timeout': mon.pm_query_timeout,
 		'connect_timeout': mon.pm_connect_timeout,
 		'log': mon.pm_log.child({ 'component': 'backend_setup' })
-	}, function (err) {
+	}, function (err, pg_version) {
 		mon.pm_pools[pi].setting_up = false;
 		if (err) {
 			mon.pm_pools[pi].needs_setup = true;
@@ -401,11 +392,18 @@ PgMon.prototype.setup_backend = function setup_backend(pi, callback)
 				'error': err,
 				'backend': mon.pm_pools[pi].name
 			}, 'error setting up backend');
-		} else {
-			mon.pm_pools[pi].needs_setup = false;
+			return;
 		}
-	});
 
+		mon.pm_pools[pi].queries = queries.getQueries({
+		    'interval': mon.pm_interval_rate,
+		    'pg_version': pg_version
+		});
+		mon.pm_pools[pi].needs_setup = false;
+
+		/* make sure we have the data structures set up */
+		mon.add_connection_data(mon.pm_pools[pi]);
+	});
 };
 
 /*
@@ -517,10 +515,11 @@ PgMon.prototype.remove_connection_data = function (pi)
  * This adds the backend to the list of discovered backends, and then reserves
  * a slot for the backend in the data and state data structures.
  */
-PgMon.prototype.add_connection_data = function (backend)
+PgMon.prototype.add_connection_data = function (pool)
 {
 	var mon = this;
-	var num_queries = mon.pm_queries.length;
+	var backend = pool.backend;
+	var num_queries = pool.queries.length;
 	var data_array = new Array(num_queries);
 	var old_data_array = new Array(num_queries);
 	var state_array = new Array(num_queries);
@@ -532,7 +531,7 @@ PgMon.prototype.add_connection_data = function (backend)
 	 *  - Add a 'state' array to identify in-flight queries
 	 */
 	mon.pm_pgs.push({
-		'key': backend.key,
+		'key': pool.key,
 		'name': backend.name,
 		'conn': null,
 		'handle': null
@@ -673,7 +672,7 @@ PgMon.prototype.tick = function (callback)
 			mon.pm_pgs[pi].conn = conn;
 			backend = mon.pm_pgs[pi].name;
 
-			mon.pm_queries.forEach(function
+			mon.pm_pools[pi].queries.forEach(function
 			    kick_off_queries(query, qi) {
 
 				/*
@@ -797,10 +796,12 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 
 	var mon = this;
 	var log = mon.pm_log;
-	var query = mon.pm_queries[qi];
+
+	var backend = mon.pm_pgs[pi];
+
+	var query = mon.pm_pools[pi].queries[qi];
 	var state = mon.pm_state[pi][qi];
-	var client = mon.pm_pgs[pi].conn;
-	var backend = mon.pm_pgs[pi].name; /* for logging */
+	var client = backend.conn;
 
 	var time;
 	var timer, errmetric, res;
@@ -815,7 +816,7 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 	 */
 	if (state !== null) {
 		log.warn({
-		    'backend': mon.pm_pgs[pi].name,
+		    'backend': backend.name,
 		    'query': query.q_name,
 		    'last': state
 		}, 'skipping check (still pending)');
@@ -826,13 +827,13 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 	time = process.hrtime();
 	mon.pm_state[pi][qi] = new Date().toISOString();
 	log.debug({
-	    'backend': backend,
+	    'backend': backend.name,
 	    'query': query.q_name
 	}, 'check: start');
 
 	dtrace['backend-query-start'].fire(function () {
 		return ([
-		    backend,
+		    backend.name,
 		    query.q_name
 		]);
 	});
@@ -841,13 +842,13 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 	res.on('row', function on_query_row(row) {
 		if (aborted) {
 			log.warn({
-				'backend': backend,
+				'backend': backend.name,
 				'query': query.q_name
 			}, 'query was aborted');
 			return;
 		}
 		log.debug({
-		    'backend': backend,
+		    'backend': backend.name,
 		    'query': query.q_name
 		}, 'check: done');
 
@@ -856,7 +857,7 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 
 	res.on('error', function on_query_error(err) {
 		dtrace['backend-query-done'].fire(function () {
-			return ([backend, query.q_name]);
+			return ([backend.name, query.q_name]);
 		});
 
 		mon.pm_state[pi][qi] = null;
@@ -871,7 +872,7 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 				'name': 'pg_query_timeout',
 				'help': 'PG query timed out',
 				'metadata': {
-					'backend': backend,
+					'backend': backend.name,
 					'query': query.q_name
 				}
 			};
@@ -889,12 +890,12 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 			'name': 'pg_query_error',
 			'help': 'error performing PG query',
 			'metadata': {
-				'backend': backend,
+				'backend': backend.name,
 				'query': query.q_name
 			}
 		};
 		log.warn(err, {
-		    'backend': backend,
+		    'backend': backend.name,
 		    'query': query.q_name
 		}, 'query failed');
 		mon.emitCounter(errmetric, 1);
@@ -904,12 +905,12 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 
 	res.once('end', function on_query_end() {
 		dtrace['backend-query-done'].fire(function () {
-			return ([backend, query.q_name]);
+			return ([backend.name, query.q_name]);
 		});
 		res.removeAllListeners();
 		if (client.isDestroyed()) {
 			mon.pm_log.info({
-				'backend': backend
+				'backend': backend.name
 			}, 'client removed during query');
 			setImmediate(cb);
 			return;
@@ -948,8 +949,9 @@ PgMon.prototype.record = function (pi, qi, datum)
 	mod_assertplus.object(datum, 'datum');
 
 	var mon = this;
-	var backend = mon.pm_pgs[pi].name;
-	var query = mon.pm_queries[qi];
+	var backend = mon.pm_pgs[pi];
+
+	var query = mon.pm_pools[pi].queries[qi];
 	var oldresult, oldrow;
 	var reset_time;
 	var last_reset_time;
@@ -968,7 +970,7 @@ PgMon.prototype.record = function (pi, qi, datum)
 				    + ' value from a SQL query',
 				'metadata': {
 					'name': met.name,
-					'backend': backend,
+					'backend': backend.name,
 					'query': query.q_name
 				}
 			}, 1);
@@ -1000,7 +1002,7 @@ PgMon.prototype.record = function (pi, qi, datum)
 			last_reset_time = Date.parse(oldrow.stats_reset);
 			if (reset_time > last_reset_time) {
 				mon.pm_log.info({
-					'backend': backend,
+					'backend': backend.name,
 					'query': query.q_name,
 					'stats_reset': row.stats_reset
 				}, 'stats reset detected');
@@ -1010,7 +1012,7 @@ PgMon.prototype.record = function (pi, qi, datum)
 
 		if (!oldrow) {
 			mon.pm_log.info({
-			    'backend': backend,
+			    'backend': backend.name,
 			    'query': query.q_name,
 			    'key': key
 			}, 'row detected');
@@ -1051,7 +1053,7 @@ PgMon.prototype.record = function (pi, qi, datum)
 			if (old_value > new_value) {
 				/* some relations don't advertise stat resets */
 				mon.pm_log.info({
-					'backend': backend,
+					'backend': backend.name,
 					'key': key,
 					'counter': c
 				}, 'old value greater than new value -'
@@ -1101,7 +1103,7 @@ PgMon.prototype.qstatname = function (pi, qi, row, field)
 	mod_assertplus.object(field, 'field');
 
 	var mon = this;
-	var query = mon.pm_queries[qi];
+	var query = mon.pm_pools[pi].queries[qi];
 	var fieldname = field.attr;
 	var help = field.help;
 	var metadata = query.q_metadata;
diff --git a/lib/queries.js b/lib/queries.js
index b6419ae..4136e12 100644
--- a/lib/queries.js
+++ b/lib/queries.js
@@ -7,6 +7,8 @@
  */
 
 var mod_ajv = require('ajv');
+var mod_assertplus = require('assert-plus');
+var mod_jsprim = require('jsprim');
 
 /*
  * queries.js: Exposes a function that returns a list of queries to be executed
@@ -19,8 +21,12 @@ var mod_ajv = require('ajv');
  *
  *        name         human-readable name of the resulting metric
  *
- *        sql          sql statement string that will be executed on each
- *                     Postgres instance
+ *        versionToSql an object whose values are a sql statement string, and
+ *                     whose keys are a string of the minimum supported postgres
+ *                     version that the query is expected to run against.  the
+ *                     special key "all" denotes that the query can be expected
+ *                     to run against all versions of postgres.  a mixture of
+ *                     "all" and specific versions are not allowed.
  *
  *        statkey      unique per-row attribute name that pgstatsmon will use as
  *                     an index to store metrics in memory
@@ -57,352 +63,525 @@ var mod_ajv = require('ajv');
  *                     (e.g. 'ms' for 'milliseconds')
  *
  *
- * The query schema is validated when pgstatsmon starts.
+ * The query schema is validated when a consumer requests the set of queries.
  */
 
-function getQueries(config) {
+var METRIC_PROPERTY = {
+    'type': 'array',
+    'items': {
+	'type': 'object',
+	'properties': {
+	    'attr': { 'type': 'string' },
+	    'help': { 'type': 'string' },
+	    'unit': { 'type': 'string' },
+	    'expires': { 'type': 'boolean' }
+	},
+	'required': [ 'attr', 'help' ]
+    }
+};
 
-	/*
-	 * The expiry period is the configured postgres query interval plus 30
-	 * seconds. This allows expired metrics to be quickly detected while
-	 * avoiding false detections of expired metrics regardless of the value
-	 * of the query interval.
-	 */
-	var expiryPeriod = config.interval + 30000;
+var QUERY_SCHEMA = {
+    'type': 'array',
+    'items': {
+	'type': 'object',
+	'properties': {
+	    'name': { 'type': 'string' },
+	    'versionToSql': {
+		'type': 'object',
+		'minProperties': 1,
+		'patternProperties': {
+		    '^.*$': { 'type': 'string' }
+		},
+		'oneOf': [ {
+		    'propertyNames': {
+			'pattern': '[0-9]+'
+		    }
+		}, {
+		    'propertyNames': {
+			'pattern': 'all'
+		    },
+		    'maxProperties': 1
+		} ]
+	    },
+	    'statkey': { 'type': 'string' },
+	    'metadata': {
+		'type': 'array',
+		'items': { 'type': 'string' }
+	    },
+	    'counters': METRIC_PROPERTY,
+	    'gauges': METRIC_PROPERTY
+	},
+	'required': [ 'name', 'statkey', 'versionToSql' ]
+    }
+};
 
-	var queries = [ {
-	    'name': 'pg_stat_user_tables',
-	     'sql': 'SELECT * FROM pg_stat_user_tables',
-	     'statkey': 'relname',
-	     'metadata': [ 'relname' ],
-	     'counters': [
-	         { 'attr': 'analyze_count',
-	           'help': 'manual anaylze operations' },
-	         { 'attr': 'autoanalyze_count',
-	           'help': 'autoanalyze operations' },
-	         { 'attr': 'autovacuum_count',
-	           'help': 'autovacuum operations' },
-	         { 'attr': 'idx_scan', 'help': 'index scans' },
-	         { 'attr': 'idx_tup_fetch', 'help': 'index tuples fetched' },
-	         { 'attr': 'n_tup_del', 'help': 'tuples deleted' },
-	         { 'attr': 'n_tup_hot_upd', 'help': 'tuples updated (hot)' },
-	         { 'attr': 'n_tup_ins', 'help': 'tuples inserted' },
-	         { 'attr': 'n_tup_upd', 'help': 'tuples updated' },
-	         { 'attr': 'seq_scan', 'help': 'sequential table scans' },
-	         { 'attr': 'seq_tup_read', 'help': 'sequential tuples read' },
-	         { 'attr': 'vacuum_count', 'help': 'manual vacuum operations' }
-	     ],
-	     'gauges': [
-	         { 'attr': 'n_live_tup', 'help': 'estimated live tuples' },
-	         { 'attr': 'n_dead_tup', 'help': 'estimated dead tuples' }
-	     ]
-	}, {
-	    'name': 'pg_statio_user_tables',
-	    'sql': 'SELECT * FROM pg_statio_user_tables',
-	    'statkey': 'relname',
-	    'metadata': [ 'relname' ],
-	    'counters': [
-	        { 'attr': 'heap_blks_read',
-	          'help': 'number of disk blocks read from this table' },
-	        { 'attr': 'heap_blks_hit',
-	          'help': 'number of buffer hits in this table' },
-	        { 'attr': 'idx_blks_read',
-	          'help': 'number of disk blocks read from all indexes on ' +
-	          'this table' },
-	        { 'attr': 'idx_blks_hit',
-	          'help': 'number of disk blocks hit in all indexes on this ' +
-	          'table' }
-	      ]
-	}, {
-	    'name': 'pg_statio_user_indexes',
-	    'sql': 'SELECT * FROM pg_statio_user_indexes',
-	    'statkey': 'indexrelname',
-	    'metadata': [ 'indexrelname', 'relname' ],
-	    'counters': [
-	        { 'attr': 'idx_blks_read',
-	          'help': 'number of disk blocks read from this index' },
-	        { 'attr': 'idx_blks_hit',
-	          'help': 'number of buffer hits in this index' }
-	    ]
-	}, {
-	     'name': 'pg_stat_replication',
-	     'statkey': 'application_name',
-	     'metadata': [ 'sync_state' ],
-	     'sql': [ /* this only works on Postgres 9.4+ */
-	         'SELECT ',
-	         'sync_state, ',
-	         'sent_location - CAST (\'0/0\' AS pg_lsn) AS wal_sent, ',
-	         'write_location - CAST (\'0/0\' AS pg_lsn) ',
-	         'AS replica_wal_written, ',
-	         'flush_location - CAST (\'0/0\' AS pg_lsn) ',
-	         'AS replica_wal_flushed, ',
-	         'replay_location - CAST (\'0/0\' AS pg_lsn) AS ',
-	         'replica_wal_replayed ',
-	         'FROM get_stat_replication();'
-	     ].join('\n'),
-	     'counters': [
-	         { 'attr': 'wal_sent',
-	           'help': 'wal bytes sent to replica', 'unit': 'bytes' },
-	         { 'attr': 'replica_wal_written',
-	           'help': 'wal bytes written by replica', 'unit': 'bytes' },
-	         { 'attr': 'replica_wal_flushed',
-	           'help': 'wal bytes flushed by replica', 'unit': 'bytes' },
-	         { 'attr': 'replica_wal_replayed',
-	           'help': 'wal bytes replayed into database by replica',
-	           'unit': 'bytes' }
-	     ]
-	}, {
-	     'name': 'pg_recovery',
-	     'statkey': 'recovery',
-	     'metadata': [],
-	     'sql': [
-	         'SELECT \'recovery\' as recovery, ',
-	         'pg_last_xlog_replay_location() - CAST (\'0/0\' AS pg_lsn) ',
-	         '		AS wal_replayed_bytes, ',
-	         '',
-	         'CASE pg_is_in_recovery() WHEN \'t\' ',
-	         'THEN (SELECT pg_last_xlog_receive_location() - ',
-	         '		CAST (\'0/0\' AS pg_lsn))',
-	         'ELSE (NULL) END AS wal_received_bytes, ',
-	         '',
-	         'CASE pg_is_in_recovery() WHEN \'t\' ',
-	         'THEN (NULL) ',
-	         'ELSE (SELECT pg_current_xlog_flush_location() - ',
-	         '		CAST (\'0/0\' AS pg_lsn)) END ',
-	         '              AS wal_flushed_bytes, ',
-	         '',
-	         'CASE pg_is_in_recovery() WHEN \'t\' ',
-	         'THEN (NULL) ',
-	         'ELSE (SELECT pg_current_xlog_insert_location() - ',
-	         '		CAST (\'0/0\' AS pg_lsn)) END ',
-	         '              AS wal_inserted_bytes;'
-	     ].join('\n'),
-	     'counters': [
-	         { 'attr': 'wal_inserted_bytes', 'help': 'WAL bytes inserted' },
-	         { 'attr': 'wal_replayed_bytes',
-	           'help': 'WAL bytes replayed into DB' },
-	         { 'attr': 'wal_received_bytes',
-	           'help': 'WAL bytes received from upstream server' },
-	         { 'attr': 'wal_flushed_bytes', 'help': 'WAL bytes flushed ' +
-	           'to disk' }
-	     ]
-	}, {
-	    'name': 'pg_stat_activity',
-	    'statkey': 'datname',
-	    'metadata': [ 'datname', 'state' ],
-	    'sql': [
-	        'SELECT ',
-	        'pg_database.datname, states.state, ',
-	        'COALESCE(connections, 0) as connections ',
-	        'FROM ( ',
-	        '		VALUES ',
-	        '		(\'active\'), ',
-	        '		(\'idle\'), ',
-	        '		(\'idle in transaction\'), ',
-	        '		(\'idle in transaction (aborted)\'), ',
-	        '		(\'fastpath function call\'), ',
-	        '		(\'disabled\') ',
-	        ') AS states(state) CROSS JOIN pg_database ',
-	        'LEFT JOIN ( ',
-	        '		SELECT ',
-	        '		datname, state, count(*) AS connections ',
-	        '		FROM get_stat_activity() ',
-	        '               GROUP BY datname,state) AS active ',
-	        'ON states.state = active.state ',
-	        'AND pg_database.datname = active.datname ',
-	        'WHERE pg_database.datname NOT LIKE \'template%\';'
-	    ].join('\n'),
-	    'gauges': [ { 'attr': 'connections', 'help': 'worker process' +
-	        ' state' } ]
-	}, {
-	     'name': 'pg_stat_database',
-	     'statkey': 'datname',
-	     'metadata': [ 'datname' ],
-	     'sql': [
-	         'SELECT * ',
-	         'FROM pg_stat_database ',
-	         'WHERE datname NOT LIKE \'postgres\' AND ',
-	         'datname NOT LIKE \'template%\';'
-	     ].join('\n'),
-	     'gauges': [ { 'attr': 'numbackends',
-	         'help': 'number of connections' } ],
-	     'counters': [
-	         { 'attr': 'tup_returned', 'help': 'tuples returned' },
-	         { 'attr': 'tup_fetched', 'help': 'tuples fetched' },
-	         { 'attr': 'tup_inserted', 'help': 'tuples inserted' },
-	         { 'attr': 'tup_updated', 'help': 'tuples updated' },
-	         { 'attr': 'tup_deleted', 'help': 'tuples deleted' },
-	         { 'attr': 'blks_read', 'help': 'blocks read from disk' },
-	         { 'attr': 'blks_hit', 'help': 'blocks read from buffercache' },
-	         { 'attr': 'xact_commit', 'help': 'transactions committed' },
-	         { 'attr': 'xact_rollback', 'help': 'transactions rolled' +
-	           ' back' },
-	         { 'attr': 'blk_read_time', 'help': 'time spent reading blocks',
-	           'unit': 'ms' },
-	         { 'attr': 'blk_write_time', 'help': 'time spent writing' +
-	           ' blocks', 'unit': 'ms' }
-	     ]
-	}, {
-	    'name': 'pg_relation_size',
-	    'statkey': 'relname',
-	    'metadata': [ 'relname' ],
-	    'sql': [
-	        'SELECT relname, ',
-	        '		c.reltuples AS row_estimate,',
-	        '		pg_total_relation_size(c.oid) AS total_bytes,',
-	        '		pg_indexes_size(c.oid) AS index_bytes,',
-	        '		pg_total_relation_size(reltoastrelid) AS',
-		'               toast_bytes ',
-	        'FROM pg_class c ',
-	        'LEFT JOIN pg_namespace n ON n.oid = c.relnamespace ',
-	        'WHERE relkind = \'r\' AND nspname LIKE \'public\';'
-	    ].join('\n'),
-	    'gauges': [
-	        { 'attr': 'row_estimate', 'help': 'estimated number of' +
-	          ' tuples' },
-	        { 'attr': 'total_bytes', 'help': 'total bytes used' },
-	        { 'attr': 'index_bytes', 'help': 'bytes used by indexes' },
-	        { 'attr': 'toast_bytes', 'help': 'bytes used by toast files' }
-	    ]
-	}, {
-	     'name': 'pg_stat_bgwriter',
-	     'statkey': 'bgwriter',
-	     'metadata': [],
-	     'sql': [
-	         'SELECT * ',
-	         'FROM pg_stat_bgwriter;'
-	     ].join('\n'),
-	     'counters': [
-	         { 'attr': 'checkpoints_timed', 'help': 'scheduled' +
-	           ' checkpoints' },
-	         { 'attr': 'checkpoints_req', 'help': 'requested checkpoints' },
-	         { 'attr': 'checkpoint_write_time', 'help': 'time spent' +
-	           ' writing checkpoints to disk', 'unit': 'ms' },
-	         { 'attr': 'checkpoint_sync_time', 'help': 'time spent' +
-	           ' synchronizing checkpoints to disk', 'unit': 'ms' },
-	         { 'attr': 'buffers_checkpoint', 'help': 'buffers written' +
-	           ' during checkpoints' },
-	         { 'attr': 'buffers_clean', 'help': 'buffers written by' +
-	           ' bgwriter' },
-	         { 'attr': 'maxwritten_clean', 'help': 'number of times' +
-	           ' bgwriter stopped a cleaning scan because too many' +
-	           ' buffers were written' },
-	         { 'attr': 'buffers_backend', 'help': 'buffers written by a' +
-	           ' backend' },
-	         { 'attr': 'buffers_backend_fsync', 'help': 'number of fsync' +
-	           ' calls by backends' },
-	         { 'attr': 'buffers_alloc',
-	           'help': 'number of buffers allocated' }
-	     ]
-	}, {
-	    'name': 'pg_vacuum',
-	    'statkey': 'relname',
-	    'metadata': ['relname'],
-	    'sql': [ // relowner 10 is hard-coded to be the 'postgres' superuser
-	        'SELECT ',
-	        '	     relname, age(relfrozenxid) AS xid_age, ',
-	        '	     (SELECT ',
-	        '		 setting::int FROM pg_settings ',
-	        '		 WHERE',
-	        '		 name = \'autovacuum_freeze_max_age\') - ',
-	        '            age(relfrozenxid)',
-	        '	     AS tx_until_wraparound_autovacuum ',
-	        'FROM pg_class WHERE relowner != 10 AND relkind = \'r\';'
-	    ].join('\n'),
-	    'gauges': [
-	        { 'attr': 'xid_age', 'help': 'transactions since last ' +
-	          'wraparound autovacuum' },
-	        { 'attr': 'tx_until_wraparound_autovacuum', 'help':
-	          'transactions until the next wraparound autovacuum' }
-	    ]
-	}, {
-	     'name': 'pg_stat_progress_vacuum',
-	     'statkey': 'relname',
-	     'metadata': [ 'relname', 'vacuum_mode' ],
-	     'sql': [
-	         'SELECT * FROM get_stat_progress_vacuum()'
-	     ].join('\n'),
-	     'gauges': [
-	         { 'attr': 'phase', 'help': 'current processing phase of ' +
-	           'vacuum', 'expires': true, 'expiryPeriod': expiryPeriod },
-	         { 'attr': 'query_start', 'help': 'unix epoch timestamp of ' +
-	           'the vacuum began', 'expires': true,
-	           'expiryPeriod': expiryPeriod },
-	         { 'attr': 'heap_blks_total', 'help': 'total number of heap ' +
-	           'blocks in the table as of the beginning of the scan',
-	           'expires': true, 'expiryPeriod': expiryPeriod },
-	         { 'attr': 'heap_blks_scanned', 'help': 'number of heap ' +
-	           'blocks scanned', 'expires': true,
-	           'expiryPeriod': expiryPeriod },
-	         { 'attr': 'heap_blks_vacuumed', 'help': 'number of heap ' +
-	           'blocks vacuumed', 'expires': true,
-	           'expiryPeriod': expiryPeriod },
-	         { 'attr': 'index_vacuum_count', 'help': 'number of ' +
-	           'completed index vacuum cycles', 'expires': true,
-	           'expiryPeriod': expiryPeriod },
-	         { 'attr': 'max_dead_tuples', 'help': 'number of dead tuples ' +
-	           'that we can store before needing to perform an index ' +
-	           'vacuum cycle', 'expires': true,
-	           'expiryPeriod': expiryPeriod },
-	         { 'attr': 'num_dead_tuples', 'help': 'number of dead tuples ' +
-	           'collected since the last index vacuum cycle',
-	           'expires': true, 'expiryPeriod': expiryPeriod }
-	     ]
-	}];
+var QUERIES = [ {
+    'name': 'pg_stat_user_tables',
+    'statkey': 'relname',
+    'metadata': [ 'relname' ],
+    'versionToSql': { 'all': 'SELECT * FROM pg_stat_user_tables;' },
+    'counters': [ {
+	'attr': 'analyze_count',
+	'help': 'manual anaylze operations'
+    }, {
+	'attr': 'autoanalyze_count',
+	'help': 'autoanalyze operations'
+    }, {
+	'attr': 'autovacuum_count',
+	'help': 'autovacuum operations'
+    }, {
+	'attr': 'idx_scan',
+	'help': 'index scans'
+    }, {
+	'attr': 'idx_tup_fetch',
+	'help': 'index tuples fetched'
+    }, {
+	'attr': 'n_tup_del',
+	'help': 'tuples deleted'
+    }, {
+	'attr': 'n_tup_hot_upd',
+	'help': 'tuples updated (hot)'
+    }, {
+	'attr': 'n_tup_ins',
+	'help': 'tuples inserted'
+    }, {
+	'attr': 'n_tup_upd',
+	'help': 'tuples updated'
+    }, {
+	'attr': 'seq_scan',
+	'help': 'sequential table scans'
+    }, {
+	'attr': 'seq_tup_read',
+	'help': 'sequential tuples read'
+    }, {
+	'attr': 'vacuum_count',
+	'help': 'manual vacuum operations'
+    } ],
+    'gauges': [ {
+	'attr': 'n_live_tup',
+	'help': 'estimated live tuples'
+    }, {
+	'attr': 'n_dead_tup',
+	'help': 'estimated dead tuples'
+    } ]
+}, {
+    'name': 'pg_statio_user_tables',
+    'statkey': 'relname',
+    'metadata': [ 'relname' ],
+    'versionToSql': { 'all': 'SELECT * FROM pg_statio_user_tables;' },
+    'counters': [ {
+	'attr': 'heap_blks_read',
+	'help': 'number of disk blocks read from this table'
+    }, {
+	'attr': 'heap_blks_hit',
+	'help': 'number of buffer hits in this table'
+    }, {
+	'attr': 'idx_blks_read',
+	'help': 'number of disk blocks read from all indexes on this table'
+    }, {
+	'attr': 'idx_blks_hit',
+	'help': 'number of disk blocks hit in all indexes on this table'
+    } ]
+}, {
+    'name': 'pg_statio_user_indexes',
+    'statkey': 'indexrelname',
+    'metadata': [ 'indexrelname', 'relname' ],
+    'versionToSql': { 'all': 'SELECT * FROM pg_statio_user_indexes;' },
+    'counters': [ {
+	'attr': 'idx_blks_read',
+	'help': 'number of disk blocks read from this index'
+    }, {
+	'attr': 'idx_blks_hit',
+	'help': 'number of buffer hits in this index'
+    } ]
+}, {
+    'name': 'pg_stat_replication',
+    'statkey': 'application_name',
+    'metadata': [ 'sync_state' ],
+    'versionToSql': {
+	'90400': [
+	    'SELECT ',
+	    'sync_state, ',
+	    'sent_location - CAST (\'0/0\' AS pg_lsn) AS wal_sent, ',
+	    'write_location - CAST (\'0/0\' AS pg_lsn) ',
+	    'AS replica_wal_written, ',
+	    'flush_location - CAST (\'0/0\' AS pg_lsn) ',
+	    'AS replica_wal_flushed, ',
+	    'replay_location - CAST (\'0/0\' AS pg_lsn) AS ',
+	    'replica_wal_replayed ',
+	    'FROM get_stat_replication();' ].join('\n'),
+	'100000': [
+	    'SELECT ',
+	    'sync_state, ',
+	    'sent_lsn - CAST (\'0/0\' AS pg_lsn) AS wal_sent, ',
+	    'write_lsn - CAST (\'0/0\' AS pg_lsn) ',
+	    'AS replica_wal_written, ',
+	    'flush_lsn - CAST (\'0/0\' AS pg_lsn) ',
+	    'AS replica_wal_flushed, ',
+	    'replay_lsn - CAST (\'0/0\' AS pg_lsn) AS ',
+	    'replica_wal_replayed ',
+	    'FROM get_stat_replication();' ].join('\n')
+    },
+    'counters': [ {
+	'attr': 'wal_sent',
+	'help': 'wal bytes sent to replica', 'unit': 'bytes'
+    }, {
+	'attr': 'replica_wal_written',
+	'help': 'wal bytes written by replica', 'unit': 'bytes'
+    }, {
+	'attr': 'replica_wal_flushed',
+	'help': 'wal bytes flushed by replica', 'unit': 'bytes'
+    }, {
+	'attr': 'replica_wal_replayed',
+	'help': 'wal bytes replayed into database by replica',
+	'unit': 'bytes'
+    } ]
+}, {
+    'name': 'pg_recovery',
+    'statkey': 'recovery',
+    'metadata': [],
+    'versionToSql': {
+	'90400': [
+	    'SELECT \'recovery\' as recovery, ',
+	    'pg_last_xlog_replay_location() - CAST (\'0/0\' AS pg_lsn) ',
+	    '		AS wal_replayed_bytes, ',
+	    '',
+	    'CASE pg_is_in_recovery() WHEN \'t\' ',
+	    'THEN (SELECT pg_last_xlog_receive_location() - ',
+	    '		CAST (\'0/0\' AS pg_lsn))',
+	    'ELSE (NULL) END AS wal_received_bytes, ',
+	    '',
+	    'CASE pg_is_in_recovery() WHEN \'t\' ',
+	    'THEN (NULL) ',
+	    'ELSE (SELECT pg_current_xlog_flush_location() - ',
+	    '		CAST (\'0/0\' AS pg_lsn)) END ',
+	    '              AS wal_flushed_bytes, ',
+	    '',
+	    'CASE pg_is_in_recovery() WHEN \'t\' ',
+	    'THEN (NULL) ',
+	    'ELSE (SELECT pg_current_xlog_insert_location() - ',
+	    '		CAST (\'0/0\' AS pg_lsn)) END ',
+	    '              AS wal_inserted_bytes;' ].join('\n'),
+	'100000': [
+	    'SELECT \'recovery\' as recovery, ',
+	    'pg_last_wal_replay_lsn() - CAST (\'0/0\' AS pg_lsn) ',
+	    '		AS wal_replayed_bytes, ',
+	    '',
+	    'CASE pg_is_in_recovery() WHEN \'t\' ',
+	    'THEN (SELECT pg_last_wal_receive_lsn() - ',
+	    '		CAST (\'0/0\' AS pg_lsn))',
+	    'ELSE (NULL) END AS wal_received_bytes, ',
+	    '',
+	    'CASE pg_is_in_recovery() WHEN \'t\' ',
+	    'THEN (NULL) ',
+	    'ELSE (SELECT pg_current_wal_flush_lsn() - ',
+	    '		CAST (\'0/0\' AS pg_lsn)) END ',
+	    '              AS wal_flushed_bytes, ',
+	    '',
+	    'CASE pg_is_in_recovery() WHEN \'t\' ',
+	    'THEN (NULL) ',
+	    'ELSE (SELECT pg_current_wal_insert_lsn() - ',
+	    '		CAST (\'0/0\' AS pg_lsn)) END ',
+	    '              AS wal_inserted_bytes;' ].join('\n')
+    },
+    'counters': [ {
+	'attr': 'wal_inserted_bytes',
+	'help': 'WAL bytes inserted'
+    }, {
+	'attr': 'wal_replayed_bytes',
+	'help': 'WAL bytes replayed into DB'
+    }, {
+	'attr': 'wal_received_bytes',
+	'help': 'WAL bytes received from upstream server'
+    }, {
+	'attr': 'wal_flushed_bytes',
+	'help': 'WAL bytes flushed to disk'
+    } ]
+}, {
+    'name': 'pg_stat_activity',
+    'statkey': 'datname',
+    'metadata': [ 'datname', 'state' ],
+    'versionToSql': { 'all': [
+	'SELECT ',
+	'pg_database.datname, states.state, ',
+	'COALESCE(connections, 0) as connections ',
+	'FROM ( ',
+	'		VALUES ',
+	'		(\'active\'), ',
+	'		(\'idle\'), ',
+	'		(\'idle in transaction\'), ',
+	'		(\'idle in transaction (aborted)\'), ',
+	'		(\'fastpath function call\'), ',
+	'		(\'disabled\') ',
+	') AS states(state) CROSS JOIN pg_database ',
+	'LEFT JOIN ( ',
+	'		SELECT ',
+	'		datname, state, count(*) AS connections ',
+	'		FROM get_stat_activity() ',
+	'               GROUP BY datname,state) AS active ',
+	'ON states.state = active.state ',
+	'AND pg_database.datname = active.datname ',
+	'WHERE pg_database.datname NOT LIKE \'template%\';' ].join('\n') },
+    'gauges': [ {
+	'attr': 'connections',
+	'help': 'worker process state'
+    } ]
+}, {
+    'name': 'pg_stat_database',
+    'statkey': 'datname',
+    'metadata': [ 'datname' ],
+    'versionToSql': { 'all': [
+	'SELECT * ',
+	'FROM pg_stat_database ',
+	'WHERE datname NOT LIKE \'postgres\' AND ',
+	'datname NOT LIKE \'template%\';' ].join('\n') },
+    'gauges': [ {
+	'attr': 'numbackends',
+	'help': 'number of connections'
+    } ],
+    'counters': [ {
+	'attr': 'tup_returned',
+	'help': 'tuples returned'
+    }, {
+	'attr': 'tup_fetched',
+	'help': 'tuples fetched'
+    }, {
+	'attr': 'tup_inserted',
+	'help': 'tuples inserted'
+    }, {
+	'attr': 'tup_updated',
+	'help': 'tuples updated'
+    }, {
+	'attr': 'tup_deleted',
+	'help': 'tuples deleted'
+    }, {
+	'attr': 'blks_read',
+	'help': 'blocks read from disk'
+    }, {
+	'attr': 'blks_hit',
+	'help': 'blocks read from buffercache'
+    }, {
+	'attr': 'xact_commit',
+	'help': 'transactions committed'
+    }, {
+	'attr': 'xact_rollback',
+	'help': 'transactions rolled back'
+    }, {
+	'attr': 'blk_read_time',
+	'help': 'time spent reading blocks',
+	'unit': 'ms'
+    }, {
+	'attr': 'blk_write_time',
+	'help': 'time spent writing blocks',
+	'unit': 'ms'
+    } ]
+}, {
+    'name': 'pg_relation_size',
+    'statkey': 'relname',
+    'metadata': [ 'relname' ],
+    'versionToSql': { 'all': [
+	'SELECT relname, ',
+	'		c.reltuples AS row_estimate,',
+	'		pg_total_relation_size(c.oid) AS total_bytes,',
+	'		pg_indexes_size(c.oid) AS index_bytes,',
+	'		pg_total_relation_size(reltoastrelid) AS',
+	'               toast_bytes ',
+	'FROM pg_class c ',
+	'LEFT JOIN pg_namespace n ON n.oid = c.relnamespace ',
+	'WHERE relkind = \'r\' AND nspname LIKE \'public\';' ].join('\n') },
+    'gauges': [ {
+	'attr': 'row_estimate',
+	'help': 'estimated number of tuples'
+    }, {
+	'attr': 'total_bytes',
+	'help': 'total bytes used'
+    }, {
+	'attr': 'index_bytes',
+	'help': 'bytes used by indexes'
+    }, {
+	'attr': 'toast_bytes',
+	'help': 'bytes used by toast files'
+    } ]
+}, {
+    'name': 'pg_stat_bgwriter',
+    'statkey': 'bgwriter',
+    'metadata': [],
+    'versionToSql': { 'all': 'SELECT * FROM pg_stat_bgwriter;' },
+    'counters': [ {
+	'attr': 'checkpoints_timed',
+	'help': 'scheduled checkpoints'
+    }, {
+	'attr': 'checkpoints_req',
+	'help': 'requested checkpoints'
+    }, {
+	'attr': 'checkpoint_write_time',
+	'help': 'time spent writing checkpoints to disk',
+	'unit': 'ms'
+    }, {
+	'attr': 'checkpoint_sync_time',
+	'help': 'time spent synchronizing checkpoints to disk',
+	'unit': 'ms'
+    }, {
+	'attr': 'buffers_checkpoint',
+	'help': 'buffers written during checkpoints'
+    }, {
+	'attr': 'buffers_clean',
+	'help': 'buffers written by bgwriter'
+    }, {
+	'attr': 'maxwritten_clean',
+	'help': 'number of times bgwriter stopped a cleaning scan because ' +
+	    'too many buffers were written'
+    }, {
+	'attr': 'buffers_backend',
+	'help': 'buffers written by a backend'
+    }, {
+	'attr': 'buffers_backend_fsync',
+	'help': 'number of fsync calls by backends'
+    }, {
+	'attr': 'buffers_alloc',
+	'help': 'number of buffers allocated'
+    } ]
+}, {
+    'name': 'pg_vacuum',
+    'statkey': 'relname',
+    'metadata': [ 'relname' ],
+    'versionToSql': { 'all': [
+	// relowner 10 is hard-coded to be the 'postgres' superuser
+	'SELECT ',
+	'	     relname, age(relfrozenxid) AS xid_age, ',
+	'	     (SELECT ',
+	'		 setting::int FROM pg_settings ',
+	'		 WHERE',
+	'		 name = \'autovacuum_freeze_max_age\') - ',
+	'            age(relfrozenxid)',
+	'	     AS tx_until_wraparound_autovacuum ',
+	'FROM pg_class WHERE relowner != 10 AND relkind = \'r\';' ].join('\n')
+    },
+    'gauges': [ {
+	'attr': 'xid_age',
+	'help': 'transactions since last wraparound autovacuum'
+    }, {
+	'attr': 'tx_until_wraparound_autovacuum',
+	'help': 'transactions until the next wraparound autovacuum'
+    } ]
+}, {
+    'name': 'pg_stat_progress_vacuum',
+    'statkey': 'relname',
+    'metadata': [ 'relname', 'vacuum_mode' ],
+    'versionToSql': {
+	'90600': 'SELECT * FROM get_stat_progress_vacuum();'
+    },
+    'gauges': [ {
+	'attr': 'phase',
+	'help': 'current processing phase of vacuum',
+	'expires': true
+    }, {
+	'attr': 'query_start',
+	'help': 'unix epoch timestamp of the vacuum began',
+	'expires': true
+    }, {
+	'attr': 'heap_blks_total',
+	'help': 'total number of heap blocks in the table as of the ' +
+	    'beginning of the scan',
+	'expires': true
+    }, {
+	'attr': 'heap_blks_scanned',
+	'help': 'number of heap blocks scanned',
+	'expires': true
+    }, {
+	'attr': 'heap_blks_vacuumed',
+	'help': 'number of heap blocks vacuumed',
+	'expires': true
+    }, {
+	'attr': 'index_vacuum_count',
+	'help': 'number of completed index vacuum cycles',
+	'expires': true
+    }, {
+	'attr': 'max_dead_tuples',
+	'help': 'number of dead tuples that we can store before needing ' +
+	    'to perform an index vacuum cycle',
+	'expires': true
+    }, {
+	'attr': 'num_dead_tuples',
+	'help': 'number of dead tuples collected since the last index ' +
+	    'vacuum cycle',
+	'expires': true
+    } ]
+} ];
 
-	var validationResult = validateQueries(queries);
-	if (validationResult === true) {
-		return (queries);
-	} else {
-		/* if validation fails, try to print a decent message */
-		throw new Error(validationResult.error);
-	}
+function Query(args, sql) {
+	this.q_name = args.name;
+	this.q_statkey = args.statkey || null;
+	this.q_gauges = (args.gauges || []).slice(0);
+	this.q_counters = (args.counters || []).slice(0);
+	this.q_metadata = (args.metadata || []).slice(0);
+
+	this.q_sql = sql;
 }
 
-/*
- * Validate the query schema. Returns a boolean to indicate validation success
- * or failure.
- */
-function validateQueries(queries)
-{
-	var ajv = new mod_ajv();
-	var metric = {
-	    'type': 'object',
-	    'properties': {
-	        'attr': { 'type': 'string' },
-	        'help': { 'type': 'string' },
-	        'unit': { 'type': 'string' }
-	    },
-	    'required': [ 'attr', 'help' ]
-	};
-	var query = {
-	    'type': 'object',
-	    'properties': {
-	        'name': { 'type': 'string' },
-	        'sql': { 'type': 'string' },
-	        'statkey': { 'type': 'string' },
-	        'metadata': { 'type': 'array',
-	            'items': { 'type': 'string' } },
-	        'counters': { 'type': 'array',
-	            'items': metric },
-	         'gauges': { 'type': 'array',
-	            'items': metric }
-	    },
-	    'required': [ 'name', 'sql', 'statkey' ]
-	};
-	var queryArray = {
-	    'type': 'array',
-	    'items': query
-	};
+function getQueries(args) {
+	mod_assertplus.object(args, 'args');
+	mod_assertplus.number(args.interval, 'args.interval');
+	mod_assertplus.number(args.pg_version, 'args.pg_version');
 
-	/* check the 'query' object against the queryArray schema */
-	if (ajv.validate(queryArray, queries) === true) {
-	    return (true);
-	} else {
-	    /* if validation fails, return an error string */
-	    var errStr = JSON.stringify(ajv.errors, null, 4);
-	    return ({error: errStr});
+	var validator = mod_ajv({ 'allErrors': true });
+	if (!validator.validate(QUERY_SCHEMA, QUERIES)) {
+		var errStr = JSON.stringify(validator.errors, null, 4);
+		throw new Error('Query validation has failed: ' + errStr);
 	}
+
+	var applicableQueries = [];
+
+	QUERIES.forEach(function (query) {
+		if (query.hasOwnProperty('gauges')) {
+			query.gauges.forEach(function (gauge) {
+				if (gauge.expires) {
+					gauge.expiryPeriod =
+					    args.interval + 30000;
+				}
+			});
+		}
+
+		/*
+		 * It's possible for a query to not be applicable to the
+		 * provided version of postgres, so we want to be sure that
+		 * we're only passing back queries that we know will run
+		 * against the provided version.
+		 */
+		var applicableSql = null;
+		mod_jsprim.forEachKey(query.versionToSql,
+		    function (min_pg_version, sql) {
+			if (min_pg_version === 'all') {
+				applicableSql = sql;
+				return;
+			}
+
+			var min_pg_version_num =
+			    mod_jsprim.parseInteger(min_pg_version);
+
+			/*
+			 * The query schema has already validated that our keys
+			 * are strings of numbers by this point, but we make
+			 * this assertion in case values that fit the schema get
+			 * here but jsprim doesn't like them.
+			 */
+			mod_assertplus.ok(
+			    !(min_pg_version_num instanceof Error),
+			    min_pg_version_num);
+
+			if (args.pg_version >= min_pg_version_num) {
+				applicableSql = sql;
+			}
+		});
+		if (applicableSql !== null) {
+			mod_assertplus.string(applicableSql);
+			applicableQueries.push(new Query(
+			    query, applicableSql));
+		}
+	});
+
+	return (applicableQueries);
 }
 
 module.exports.getQueries = getQueries;
+module.exports._schema = QUERY_SCHEMA;
diff --git a/test/badquery.tst.js b/test/badquery.tst.js
index be61b21..4523175 100644
--- a/test/badquery.tst.js
+++ b/test/badquery.tst.js
@@ -3,7 +3,7 @@
  * License, v. 2.0. If a copy of the MPL was not distributed with this
  * file, You can obtain one at http://mozilla.org/MPL/2.0/.
  *
- * Copyright (c) 2018, Joyent, Inc.
+ * Copyright (c) 2019, Joyent, Inc.
  */
 
 var helper = require('./helper');
@@ -106,9 +106,6 @@ function BadQuery(callback)
 				 */
 				setTimeout(cb, 500);
 			},
-			function (_, cb) {
-				self.mon.tick(cb);
-			},
 			function (_, cb) {
 				clearInterval(self.mon.pm_intervalObj);
 				cb();
@@ -144,20 +141,19 @@ BadQuery.prototype.run_invalid_query = function (callback)
 
 	/* bogus query that causes Postgres to return an error */
 	queries = [ {
-		'name': 'test_bad_query',
-		'sql': 'SELECT *',
-		'statkey': 'non_existent',
-		'metadata': [ 'no_metadata' ],
-		'counters': [],
-		'gauges': []
+		'q_name': 'test_bad_query',
+		'q_sql': 'SELECT *',
+		'q_statkey': 'non_existent',
+		'q_metadata': [ 'no_metadata' ],
+		'q_counters': [],
+		'q_gauges': []
 	} ];
 
 	var labels = {
-		'query': queries[0].name,
+		'query': queries[0].q_name,
 		'backend': self.mon.pm_pgs[0]['name']
 	};
 
-	this.mon.initializeMetrics(queries);
 	/*
 	 * since mon.initializeMetrics() drops all of the data, we need to get
 	 * a pointer to the new PrometheusTarget
@@ -166,6 +162,10 @@ BadQuery.prototype.run_invalid_query = function (callback)
 
 	mod_vasync.pipeline({
 		'funcs': [
+			function (_, cb) {
+				self.mon.pm_pools[0].queries = queries;
+				cb();
+			},
 			/* make sure counters are created */
 			function (_, cb) {
 				self.mon.tick(cb);
diff --git a/test/get_queries.tst.js b/test/get_queries.tst.js
new file mode 100644
index 0000000..8957692
--- /dev/null
+++ b/test/get_queries.tst.js
@@ -0,0 +1,36 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ *
+ * Copyright (c) 2019, Joyent, Inc.
+ */
+
+var mod_assertplus = require('assert-plus');
+
+var lib_queries = require('../lib/queries');
+
+var tests = [ {
+    'args': {
+	'interval': 100,
+	'pg_version': 90200
+    },
+    'expected': {
+	'nqueries': 8
+    }
+}, {
+    'args': {
+	'interval': 100,
+	'pg_version': 90500
+    },
+    'expected': {
+	'nqueries': 10
+    }
+} ];
+
+
+tests.forEach(function (t) {
+	var q = lib_queries.getQueries(t.args);
+
+	mod_assertplus.equal(q.length, t.expected.nqueries);
+});
diff --git a/test/validate_queries.tst.js b/test/validate_queries.tst.js
new file mode 100644
index 0000000..44d1c63
--- /dev/null
+++ b/test/validate_queries.tst.js
@@ -0,0 +1,122 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ *
+ * Copyright (c) 2019, Joyent, Inc.
+ */
+
+/*
+ * test/validate_queries.tst.js
+ *
+ * Tests to ensure the query schema defined in lib/queries.js validates the type
+ * of query structure that's documented.
+ *
+ * This test suite only confirms that the expected number of errors are returned
+ * from attempted validation, not that the errors themselves are of the correct
+ * type.
+ */
+
+var mod_assertplus = require('assert-plus');
+var mod_ajv = require('ajv');
+
+var lib_queries = require('../lib/queries');
+
+var tests = [ {
+    'queries': [ {} ],
+    'expected': {
+	'nerrors': 3
+    }
+}, {
+    'queries': {},
+    'expected': {
+	'nerrors': 1
+    }
+}, {
+    'queries': 'select 1;',
+    'expected': {
+	'nerrors': 1
+    }
+}, {
+    'queries': [ {
+	'name': 'test_query',
+	'versionToSql': { 'all': 'select 1;' },
+	'statkey': 'testing'
+    } ],
+    'expected': {
+	'nerrors': 0
+    }
+}, {
+    'queries': [ {
+	'name': 'test_query',
+	'versionToSql': { 'not_a_valid_key': 'select 1;' },
+	'statkey': 'testing'
+    } ],
+    'expected': {
+	'nerrors': 5
+    }
+}, {
+    'queries': [ {
+	'name': 'test_query',
+	'statkey': 'testing',
+	'versionToSql': { '123': 'select 123;' }
+    } ],
+    'expected': {
+	'nerrors': 0
+    }
+}, {
+    'queries': [ {
+	'name': 'test_query',
+	'versionToSql': {
+	    '900': 'select 900;',
+	    '1000': 'select 1000;'
+	},
+	'statkey': 'testing'
+    } ],
+    'expected': {
+	'nerrors': 0
+    }
+}, {
+    'queries': [ {
+	'name': 'test_query',
+	'versionToSql': {
+	    'all': 'select 1;',
+	    'all': 'select 1;'
+	},
+	'statkey': 'testing'
+    } ],
+    'expected': {
+	'nerrors': 0
+    }
+}, {
+    'queries': [ {
+	'name': 'test_query',
+	'versionToSql': {
+	    'all': 'select 1;',
+	    '123': 'select 1;'
+	},
+	'statkey': 'testing'
+    } ],
+    'expected': {
+	'nerrors': 6
+    }
+}, {
+    'queries': [ {
+	'name': 'test_query',
+	'versionToSql': {
+	    '5786327846932794236475326596927': 'select 1;'
+	},
+	'statkey': 'testing'
+    } ],
+    'expected': {
+	'nerrors': 0
+    }
+} ];
+
+tests.forEach(function (t) {
+	var validator = mod_ajv({ 'allErrors': true });
+	validator.validate(lib_queries._schema, t.queries);
+
+	mod_assertplus.equal((validator.errors || []).length,
+	    t.expected.nerrors);
+});
