From f1985d97f330b8ddf3404eb32db7d51070090fcb Mon Sep 17 00:00:00 2001
From: Mohamed Khalfella <mohamed.khalfella@joyent.com>
Date: Tue, 16 Oct 2018 23:06:07 +0000
Subject: [PATCH] MANTA-4003 Want msplit to support gzip compression

---
 bin/msplit                              |  84 ++++++++++++++++--------
 docs/man/msplit.md                      |  11 +++-
 man/msplit.1                            |  13 +++-
 test/data/testBasicGzCompress_output_00 | Bin 0 -> 24 bytes
 test/data/testBasicGzCompress_output_01 | Bin 0 -> 24 bytes
 test/msplit.test.js                     |  70 ++++++++++++++------
 6 files changed, 126 insertions(+), 52 deletions(-)
 create mode 100644 test/data/testBasicGzCompress_output_00
 create mode 100644 test/data/testBasicGzCompress_output_01

diff --git a/bin/msplit b/bin/msplit
index c8ad497..6f5c508 100755
--- a/bin/msplit
+++ b/bin/msplit
@@ -30,13 +30,14 @@ var mod_lib = require('../lib');
 var mod_manta = require('manta');
 var mod_uuid = require('node-uuid');
 var mod_vasync = require('vasync');
+var mod_zlib = require('zlib');
 
 /*
  * msplit: demux streams to send to multiple reducers
  */
 var msUsageMessage = [
 	'usage: msplit [-n number_of_reducers] [-d delimiter] [-f field_list]',
-	'              [-i] [-j] [-e javascript]',
+	'              [-i] [-j] [-e javascript] [-z]',
 	'',
 	'msplit is used to split a stream into many mpipes, one per the number',
 	'of reducers for your job.  The -n option specifies the number of',
@@ -95,7 +96,13 @@ var msUsageMessage = [
 	'Using the (-i) option:',
 	'The -i option turns off partition key hashing and instead treats ',
 	'the kay as the integer index of the reducer.  If you use this option ',
-	'the integer must be between 0 and number_of_reducers - 1.'
+	'the integer must be between 0 and number_of_reducers - 1.',
+	'',
+	'When -z is specified, msplit will compress the output streams in gzip',
+	'format. The next phase in the job needs to expect this and decompress',
+	'its input objects as needed. Note, concatenating gzip streams should ',
+	'not be an issue as long as the next phase treats its input as a ',
+	'stream of compressed data.'
 ].join('\n');
 
 var msHashAlgo = 'md5';
@@ -116,9 +123,10 @@ function main()
 	opts.nReducers = null;
 	opts.parseJson = false;
 	opts.reducerSelector = selectReducerByHash;
+	opts.gzCompress = false;
 	opts.allowedTypes = [ 'string' ];
 
-	parser = new mod_getopt.BasicParser('d:f:n:e:ijt', process.argv);
+	parser = new mod_getopt.BasicParser('d:f:n:e:ijtz', process.argv);
 
 	while ((option = parser.getopt()) !== undefined) {
 		switch (option.option) {
@@ -155,6 +163,9 @@ function main()
 				usage('invalid number of reducers');
 			}
 			break;
+		case 'z':
+			opts.gzCompress = true;
+			break;
 
 		default:
 			/* error message already emitted by getopt */
@@ -197,21 +208,7 @@ function main()
 
 	// Streams
 	var waitingForDrain = [];
-	var ended = false;
-
-	function tryEnd() {
-		if (ended) {
-			return;
-		}
-		var endNow = !process.stdin.readable;
-		for (var j = 0; j < msStreams.length; ++j) {
-			endNow = endNow && !msStreams[j].writable;
-		}
-		if (endNow) {
-			ended = true;
-			msUploadFiles(opts);
-		}
-	}
+	var nClosedStreams = 0;
 
 	lstream = new mod_lstream();
 
@@ -219,6 +216,10 @@ function main()
 		var ws = msStreams[opts.reducerSelector(line, opts)];
 		var flushed = ws.write(line);
 		flushed = ws.write('\n') && flushed;
+		// If not flushed, add the stream to waitForDrain list
+		// and pause stdin. This is not optimal because it could
+		// become ready again by the time we write to it next time.
+		// TODO: Fix this.
 		if (!flushed) {
 			if (waitingForDrain.indexOf(ws) === -1) {
 				waitingForDrain.push(ws);
@@ -232,53 +233,82 @@ function main()
 	});
 
 	lstream.once('end', function () {
+		// When done processing input, close all the reducer streams.
+		// This will end StringBuffer stream and all the underlying
+		// streams. Ultimately, this will trigger 'close' event on
+		// file stream when all the data have been flushed to disk.
 		for (i = 0; i < opts.nReducers; ++i) {
 			msStreams[i].end();
 		}
-		tryEnd();
 	});
 
 	process.stdin.pause();
 	process.stdin.pipe(lstream);
 	lstream.resume();
 
+	var onStreamError = function (err) {
+		msFatal('File: ' + this.filename + ': ' + err.message);
+	};
+
 	// Set up file write streams
 	for (i = 0; i < opts.nReducers; ++i) {
 		var filename = msTmpFilePrefix + i;
+		var fileStream = mod_fs.createWriteStream(filename);
+		var outputStream = fileStream;
+
+		// Compress the output if '-z' is specified.
+		if (opts.gzCompress) {
+			// When gzip compression is specified, redirect
+			// output to gzip stream and pipe it to the reducer
+			// file.
+			outputStream = mod_zlib.createGzip();
+			outputStream.pipe(fileStream);
+		}
+
 		var stream = mod_lib.createStringWriteBuffer({
-			stream: mod_fs.createWriteStream(filename)
+			stream: outputStream	// Write output to outputStream
 		});
 		stream.filename = filename;
 
-		stream.once('open', function (index) {
+		fileStream.once('open', function (index) {
 			msStreams[index] = this;
+			// Resume stdin when all the reducer files are open.
 			if (Object.keys(msStreams).length === opts.nReducers) {
 				process.stdin.resume();
 			}
 		}.bind(stream, i));
 
 		stream.on('drain', function () {
+			// Remove the stream object from waitingForDrain []
 			var index = waitingForDrain.indexOf(this);
 			if (index !== -1) {
 				waitingForDrain.splice(index, 1);
 			}
+			// Resume stdin when all the reducer streams are ready
+			// to take input.
 			if (waitingForDrain.length === 0) {
 				process.stdin.resume();
 			}
 		}.bind(stream));
 
-		stream.once('error', function (err) {
-			msFatal('File: ' + this.filename + ': ' +
-				err.message);
-		}.bind(stream));
+		// Capture errors on both outputStream and fileStream.
+		// When gzip compression is specified, outputStream is
+		// different from fileStream. Otherwise, outputStream is
+		// fileStream. We register the error event handlers for both
+		// streams in case they are different.
+		outputStream.once('error', onStreamError.bind(stream));
+		fileStream.once('error', onStreamError.bind(stream));
 
-		stream.once('close', function () {
+		fileStream.once('close', function () {
 			if (process.stdin.readable) {
 				msFatal('File: ' + this.filename + ': ' +
 					'write stream closed prematurely');
 				return;
 			}
-			tryEnd();
+			// Upload the files when all the reducer streams
+			// have been closed.
+			if (++nClosedStreams == opts.nReducers)
+				msUploadFiles(opts);
 		}.bind(stream));
 	}
 }
diff --git a/docs/man/msplit.md b/docs/man/msplit.md
index bbb5e2e..f1fdd57 100644
--- a/docs/man/msplit.md
+++ b/docs/man/msplit.md
@@ -1,4 +1,4 @@
-msplit 1 "May 2013" Manta "Manta Compute Bin"
+msplit 1 "Jan 2019" Manta "Manta Compute Bin"
 ============================================
 
 NAME
@@ -10,7 +10,7 @@ SYNOPSIS
 --------
 
 `msplit` [-d delimiter] [-e javascript] [-f field_list] [-j] [-i]
-         [-n number_of_reducers]
+         [-n number_of_reducers] [-z]
 
 
 DESCRIPTION
@@ -99,6 +99,7 @@ EXAMPLES
 
     $ msplit -d ',' -f 5,3 -n 4
     $ msplit -j -f id,type -n 4
+    $ msplit -j -f group_uuid,type -n 3 -z
     $ msplit -e "return line.substring(0,16)" -n 4
     $ msplit -j -e "this.id.substring(0,9)" -n 4
     $ msplit -i -j -e "this.latency % 4" -n 4
@@ -130,6 +131,12 @@ OPTIONS
 `-n`
   Number of reducers.  Should match the number of reducers for your job.
 
+`-z`
+  Compress the ouput in gzip format. When the intermediate objects are compressible,
+  using this option reduces the time it takes to upload the task outputs to manta.
+  Note, concatenating gzip streams should not be an issue as long as the next phase
+  treats its input as a single stream of compressed data.
+
 BUGS
 ----
 
diff --git a/man/msplit.1 b/man/msplit.1
index f1623c0..8605db8 100644
--- a/man/msplit.1
+++ b/man/msplit.1
@@ -1,11 +1,11 @@
-.TH msplit 1 "May 2013" Manta "Manta Compute Bin"
+.TH msplit 1 "Jan 2019" Manta "Manta Compute Bin"
 .SH NAME
 .PP
 msplit \- split the output stream for the current task to many reducers.
 .SH SYNOPSIS
 .PP
-\fB\fCmsplit\fR [\-d delimiter] [\-e javascript] [\-f field\fIlist] [\-j] [\-i]
-         [\-n number\fPof_reducers]
+\fB\fCmsplit\fR [\-d delimiter] [\-e javascript] [\-f field_list] [\-j] [\-i]
+         [\-n number\fIof\fPreducers] [\-z]
 .SH DESCRIPTION
 .PP
 Reads content from stdin and partitions data across the number of reducers that
@@ -123,6 +123,7 @@ $ ... | msplit -j -e "this.id.substring(0,9)" -n 4
 .nf
 $ msplit -d ',' -f 5,3 -n 4
 $ msplit -j -f id,type -n 4
+$ msplit -j -f group_uuid,type -n 3 -z
 $ msplit -e "return line.substring(0,16)" -n 4
 $ msplit -j -e "this.id.substring(0,9)" -n 4
 $ msplit -i -j -e "this.latency % 4" -n 4
@@ -153,6 +154,12 @@ Process the input as newline\-separated json objects.
 .TP
 \fB\fC-n\fR
 Number of reducers.  Should match the number of reducers for your job.
+.TP
+\fB\fC-z\fR
+Compress the ouput in gzip format. When the intermediate objects are compressible,
+using this option reduces the time it takes to upload the task outputs to manta.
+Note, concatenating gzip streams should not be an issue as long as the next phase
+treats its input as a single stream of compressed data.
 .SH BUGS
 .PP
 Report bugs at Github
diff --git a/test/data/testBasicGzCompress_output_00 b/test/data/testBasicGzCompress_output_00
new file mode 100644
index 0000000..efa72b5
Binary files /dev/null and b/test/data/testBasicGzCompress_output_00 differ
diff --git a/test/data/testBasicGzCompress_output_01 b/test/data/testBasicGzCompress_output_01
new file mode 100644
index 0000000..6bc7639
Binary files /dev/null and b/test/data/testBasicGzCompress_output_01 differ
diff --git a/test/msplit.test.js b/test/msplit.test.js
index 64d9cdc..fd80572 100644
--- a/test/msplit.test.js
+++ b/test/msplit.test.js
@@ -4,6 +4,7 @@
 
 var mod_child_process = require('child_process');
 var mod_fs = require('fs');
+var mod_path = require('path');
 var mod_http = require('http');
 
 var mod_bunyan = require('bunyan');
@@ -79,12 +80,12 @@ function runTest(opts, callback)
 
 before(function (cb) {
 	SERVER = mod_http.createServer(function (req, res) {
-		var body = '';
+		var body = [];
 		req.on('data', function (data) {
-			body += data;
+			body.push(data);
 		});
 		req.on('end', function () {
-			req.body = body;
+			req.body = Buffer.concat(body);
 			SERVER.requests.push(req);
 			res.writeHead(204);
 			res.end();
@@ -139,8 +140,8 @@ test('testBasic', function (t)
 		t.equal(2, SERVER.requests.length);
 		var reqs = transformRequests(t, SERVER.requests);
 		t.deepEqual({
-			'0': '1\n3\n',
-			'1': '2\n4\n'
+			'0': new Buffer('1\n3\n'),
+			'1': new Buffer('2\n4\n')
 		}, reqs);
 		t.ok(result.error === null);
 		t.equal('', result.stdout);
@@ -166,8 +167,8 @@ test('testBasicAltField', function (t)
 		t.equal(2, SERVER.requests.length);
 		var reqs = transformRequests(t, SERVER.requests);
 		t.deepEqual({
-			'0': '1,2,3\n1,1,1\n',
-			'1': '1,2,2\n1,3,4\n'
+			'0': new Buffer('1,2,3\n1,1,1\n'),
+			'1': new Buffer('1,2,2\n1,3,4\n')
 		}, reqs);
 		t.ok(result.error === null);
 		t.equal('', result.stdout);
@@ -188,8 +189,8 @@ test('testBasicJson', function (t)
 		t.equal(2, SERVER.requests.length);
 		var reqs = transformRequests(t, SERVER.requests);
 		t.deepEqual({
-			'0': '{"x":1}\n{"x":3}\n',
-			'1': '{"x":2}\n{"x":4}\n'
+			'0': new Buffer('{"x":1}\n{"x":3}\n'),
+			'1': new Buffer('{"x":2}\n{"x":4}\n')
 		}, reqs);
 		t.ok(result.error === null);
 		t.equal('', result.stdout);
@@ -211,8 +212,8 @@ test('testBasicExec', function (t)
 		t.equal(2, SERVER.requests.length);
 		var reqs = transformRequests(t, SERVER.requests);
 		t.deepEqual({
-			'0': '1\n2\n',
-			'1': '3\n4\n'
+			'0': new Buffer('1\n2\n'),
+			'1': new Buffer('3\n4\n')
 		}, reqs);
 		t.ok(result.error === null);
 		t.equal('', result.stdout);
@@ -234,8 +235,8 @@ test('testBasicExecJson', function (t)
 		t.equal(2, SERVER.requests.length);
 		var reqs = transformRequests(t, SERVER.requests);
 		t.deepEqual({
-			'0': '{"x":1}\n{"x":2}\n',
-			'1': '{"x":3}\n{"x":4}\n'
+			'0': new Buffer('{"x":1}\n{"x":2}\n'),
+			'1': new Buffer('{"x":3}\n{"x":4}\n')
 		}, reqs);
 		t.ok(result.error === null);
 		t.equal('', result.stdout);
@@ -257,8 +258,8 @@ test('testExecWithoutReturn', function (t)
 		t.equal(2, SERVER.requests.length);
 		var reqs = transformRequests(t, SERVER.requests);
 		t.deepEqual({
-			'0': '1\n2\n',
-			'1': '3\n4\n'
+			'0': new Buffer('1\n2\n'),
+			'1': new Buffer('3\n4\n')
 		}, reqs);
 		t.ok(result.error === null);
 		t.equal('', result.stdout);
@@ -280,8 +281,8 @@ test('testExecWithTrailingSemicolon', function (t)
 		t.equal(2, SERVER.requests.length);
 		var reqs = transformRequests(t, SERVER.requests);
 		t.deepEqual({
-			'0': '1\n2\n',
-			'1': '3\n4\n'
+			'0': new Buffer('1\n2\n'),
+			'1': new Buffer('3\n4\n')
 		}, reqs);
 		t.ok(result.error === null);
 		t.equal('', result.stdout);
@@ -303,9 +304,9 @@ test('testReducerTargetingWithDashI', function (t)
 		t.equal(3, SERVER.requests.length);
 		var reqs = transformRequests(t, SERVER.requests);
 		t.deepEqual({
-			'0': '3\n6\n9\n',
-			'1': '1\n4\n7\n10\n',
-			'2': '2\n5\n8\n'
+			'0': new Buffer('3\n6\n9\n'),
+			'1': new Buffer('1\n4\n7\n10\n'),
+			'2': new Buffer('2\n5\n8\n')
 		}, reqs);
 		t.ok(result.error === null);
 		t.equal('', result.stdout);
@@ -328,3 +329,32 @@ test('testDashIOutOfRange', function (t)
 		t.done();
 	});
 });
+
+test('testBasicGzCompress', function (t)
+{
+	var testDataDir = mod_path.dirname(__filename) + '/data';
+	var obj0Path = testDataDir + '/testBasicGzCompress_output_00';
+	var obj1Path = testDataDir + '/testBasicGzCompress_output_01';
+
+	var obj0 = mod_fs.readFileSync(obj0Path);
+	var obj1 = mod_fs.readFileSync(obj1Path);
+
+	var sin = '1\n2\n3\n4\n';
+	runTest({
+		stdin: sin,
+		opts: ['-n', 2, '-z'],
+		nReducers: 2
+	}, function (result) {
+		t.equal(0, result.code);
+		t.equal(2, SERVER.requests.length);
+		var reqs = transformRequests(t, SERVER.requests);
+		t.deepEqual({
+			'0': obj0,
+			'1': obj1
+		}, reqs);
+		t.ok(result.error === null);
+		t.equal('', result.stdout);
+		t.equal('', result.stderr);
+		t.done();
+	});
+});
-- 
2.21.0

