commit afcb2b2c515ea39e8abf23a57183a475a9bb0204
Author: Tim Foster <tim.foster@joyent.com>
Date:   2019-05-23T14:15:46+01:00 (5 months ago)
    
    TRITON-1376 update build documentation to account for engbld changes

diff --git a/docs/developer-guide/building.md b/docs/developer-guide/building.md
index cc2cf04..e42bc27 100644
--- a/docs/developer-guide/building.md
+++ b/docs/developer-guide/building.md
@@ -1,530 +1,806 @@
-# Building SDC
+---
+Building Triton and Manta
+---
+
+## Introduction
+
+[Triton](https://github.com/joyent/triton/#triton-datacenter) and
+[Manta](https://github.com/joyent/manta#manta-tritons-object-storage-and-converged-analytics-solution)
+are comprised of an operating system, a series of components which run the
+services that make up the system, and a set of administrative tools.
+Many of the components are deployed inside dedicated Illumos zones.
+You may find the [architecture diagram](https://github.com/joyent/triton/blob/master/docs/developer-guide/architecture.md) useful to refer to.
+
+All of that software can be built directly from
+[the sources on github](https://github.com/joyent/triton/blob/master/docs/developer-guide/repos.md)
+and the resulting components can be assembled into an image that you can install
+on your own hardware.
+
+This guide tells you how to get started.
+
+If you're working on Manta, or are interested about other aspects of the build
+system, see the [Manta dev notes](https://github.com/joyent/manta/blob/master/docs/dev-notes.md).
 
 ## Prerequisites
 
  * we assume you have npm/node installed on your workstation
  * we assume you have git installed on your workstation
  * we assume you have json (npm install -g json) installed on your workstation
- * we assume you understand the basics of SDC, if not please start with [the
-   SmartDataCenter README](https://github.com/joyent/sdc#readme)
+ * we assume you understand the basics of Triton, if not please start with [the
+   Triton README](https://github.com/joyent/triton#readme)
  * we assume you have your SSH keys loaded in your ssh-agent when connecting
    to build zones via SSH.
 
 ## Decisions to Make
 
-To build SDC components you to decide first make a couple choices:
+To build Triton and Manta components you first need to make some choices:
 
  * which components are you going to build?
- * where are you going to build components?
-
-### Components
-
-If you are building any of the following components:
-
- * manta-manatee
- * sdc-manatee
- * electric-moray
- * platform
-
-you will need a sdc-multiarch 13.3.1 build zone. For any other components you
-will need an sdc-smartos 1.6.3. If you want to build *all* components, you'll
-need both.
-
-### Where to build
-
-If you have an account in the Joyent Public Cloud you can build all components
-(except "platform", see "Building the Platform") required to create a working
-SDC headnode in the JPC and have the outputs pushed to Joyent's Manta. This is
-the easiest method for building, but will create several zones (one for each
-zone image built) and store results in Manta, both of which will have billing
-consequences.
-
-If instead you would like to build components in a local SDC install or in a
-downloaded CoaL image, you will have some additional setup to do before you can
-build. In this case, see the section "Setting up an SDC for builds".
+ * where are you going to build those components?
+
+## Components
+
+Each component that makes up part of Triton or Manta is formed from a
+`base image` installed with a specfic set of packages from a given pkgsrc
+release, along with the software that implements the services delivered by
+that component.
+
+The different components have different build zone requirements, since
+different components deploy with different base images. We require the
+build zone to be running the same pkgsrc release that the component will
+ultimately be deployed with.
+
+The top level `Makefile` in each component repository includes metadata to
+declare what build zone it needs. The `make validate-buildenv` target in each
+component `Makefile` will determine whether the build zone and environment is
+correctly configured. The `make show-buildenv` will give a short summary of the
+expected build zone.
+
+A few components have quite tailored build systems, such as
+[`smartos-live`](https://github.com/joyent/smartos-live#smartos-live-smartos-platform),
+which builds the operating system and associated services (collectively known
+as the `platform`) or
+[`sdc-headnode`](https://github.com/joyent/sdc-headnode/#sdc-headnode),
+which assembles components into a bootable USB, iso or vmware image.
+
+For the most part though, components share the same `Makefile` rules to make
+it easier for developers to build any component.
+
+## Where to build
+
+Most Triton/Manta components can build on either of two platforms:
+
+ * Triton - [a full Triton install](https://github.com/joyent/triton#getting-started)
+   contains everything you need to install the build zones for building
+   components.
+   Triton is a little more resource-intensive than SmartOS, but can be scaled
+   out beyond a single system. If you're developing for Triton, you may find it
+   educational to also run your development environments on it.
+
+ * SmartOS - since Triton itself is based on SmartOS, it's also possible
+   to build on [a SmartOS install](https://wiki.smartos.org/display/DOC/Download+SmartOS)
+   that hosts those build zones. SmartOS is less resource-intensive than
+   Triton and can be easier to setup.
+
+The tooling to create build zones differs across these platforms, so in this
+guide, we'll explain both.
+
+If you're not developing the operating system itself (the `platform` component)
+it is possible to partially develop and test on other platforms (e.g OS X or
+Linux) However, doing complete builds of components requires ZFS tooling that
+is only supported on Triton/SmartOS at the time of writing. One exception is
+the `sdc-headnode` build which can be built on OS X in addition to Triton
+or SmartOS.
+
+It is also possible to build on virtual machines (vmware, kvm, bhyve, etc.)
+hosting SmartOS, or a
+[Triton "CoaL" ("Cloud on a Laptop")](https://github.com/joyent/triton/blob/master/docs/developer-guide/coal-setup.md) instance, though your virtualization platform will need to allow nested
+virtualization in order to host the 'retro' build zones that we talk about later
+in this document.
+
+## More about build zones
+
+A build zone contains a set of base pkgsrc packages, some additional
+development pkgsrc packages, and a set of build tools. In Triton and SmartOS,
+zones are created from `images`, which are templates that contain filesystem
+data, and some metadata.
+
+Joyent's production builds impose a further restriction that the Platform Image
+(that is, the kernel and userland bits running on the bare metal, either
+SmartOS or Triton) needs to be at a specific minimum version, defined via
+`min_platform` in `deps/eng/tools/mk/Makefile.defs`.
+
+At the time of writing, most components will build on more modern platform
+images, so for now, we'll leave aside the platform image restriction, other than
+to say that you should set `$ENGBLD_SKIP_VALIDATE_BUILD_PLATFORM` to `true`
+in your environment. We'll talk more about this in the
+[Going retro](#retro) section later in this document, so please read that.
+
+For convenience, we maintain a set of build zone images that already include
+the required pkgsrc packages and build tools. The table below lists those image
+names and their corresponding uuids:
+
+  |pkgsrc version | base image name                                               | build zone image uuid
+  ----------------|---------------------------------------------------------------|-----------------------------
+  |2011Q4         | sdc-smartos@1.6.3                                             | 956f365d-2444-4163-ad48-af2f377726e0
+  |2014Q2         | sdc-base@14.2.0                                               | 83708aad-20a8-45ff-bfc0-e53de610e418
+  |2015Q4         | triton-origin-multiarch-15.4.1@1.0.1                          | 1356e735-456e-4886-aebd-d6677921694c
+  |2018Q1         | minimal-multiarch@18.1.0                                      | 8b297456-1619-4583-8a5a-727082323f77
+  |2018Q4         | triton-origin-x86_64-18.4.0@master-20190410T193647Z-g982b0cea | 29b70133-1e97-47d9-a4c1-e4b2ee1a1451
+  |2019Q1         | triton-origin-x86_64-19.1.0@master-20190417T143547Z-g119675b  | fb751f94-3202-461d-b98d-4465560945ec
+
+These build zone image uuids are exactly what we build components on in Joyent's
+Jenkins infrastructure (when you examine the images, you'll find that they're
+named with `jenkins-agent-..` prefixes)
+
+For any component, you can find the suggested image_uuid that the component
+should build on by running the `make show-buildenv` command from the top-level
+of the component git repository. For example:
+
+```
+$ cd /home/timf/projects/sdc-manatee.git
+$ make show-buildenv
+2015Q4 triton-origin-multiarch-15.4.1@1.0.1 1356e735-456e-4886-aebd-d6677921694c
+$
+```
+
+## Creating a new build zone
+
+Taking one of the image uuids above, you need to download or import the
+image, then create a build zone from it. The mechanism to do that will depend on
+whether you're using a Triton instance or a SmartOS instance to run your build
+zone on.
+
+We'll describe both.
+
+### Triton-based build zones
 
-NOTE: Even if you do not use JPC and output your builds to Manta, you will still
-need an account in JPC in order to do a build. This is because dependent
-components will still need to be downloaded from Manta.
+This section assumes that you have a local Triton/CoaL setup and have access to
+the global zone.
 
-## Setting up your workspace for driving builds
+#### Setting up Triton for development
 
-### Clone MG (mountain-gorilla)
+CloudAPI allows us to run remote commands against a Triton instance in order
+to manage and create VMs, in our case, build zones.
 
-On your local workstation (tested with OS X, but should work elsewhere) you can
-run:
+If you're using CoaL, a cloudapi zone is not created by default,
+so run the following to create one:
 
 ```
-git clone git@github.com:joyent/mountain-gorilla.git MG && cd MG
+[root@headnode (coal-1) ~]# sdcadm post-setup cloudapi
+cloudapi0 zone created
 ```
 
-Most of the rest of these instructions can be performed from within this
-directory.
-
-After cloning, you should run:
+We also want to add external access to the adminui and imgapi services
+so that we can access the adminui remotely and so that the imgapi service
+can download the build zone images from updates.joyent.com:
 
 ```
-npm install
+[root@headnode (coal-1) ~]# sdcadm post-setup common-external-nics
+Added external nic to adminui
+Added external nic to imgapi
 ```
 
-in this directory to setup the dependencies. Once you've done that, you can also
-run:
+As we don't have a dedicated compute node (CN) on this Triton installation,
+we configure it to allow the creation of zones on the headnode itself,
+something that's normally not done in production:
 
 ```
-export PATH=${PATH}:$(pwd)/node_modules/smartdc/bin
+[root@headnode (coal-1) ~]# sdcadm post-setup dev-headnode-prov
+Configuring CNAPI to allow headnode provisioning and over-provisioning
+Refreshing instance cnapi0 config-agent
+[root@headnode (coal-1) ~]#
 ```
 
-### Setup your environment variables
-
-This is one of the most critical steps. The environment variables define which
-SDC/manta target/credentials will be used for the rest of setup. You must set
-at least:
+Next we should find the external address for adminui, cloudapi and imgapi
+so that we can interact with those services remotely:
 
 ```
-export MANTA_USER=<USER>
-export MANTA_KEY_ID=<KEY>
-export MANTA_URL=https://us-east.manta.joyent.com
-export SDC_ACCOUNT=<USER>
-export SDC_KEY_ID=<KEY>
-export SDC_URL=https://us-east-1.api.joyentcloud.com
+[root@headnode (coal-1) ~]# sdc-vmapi /vms/$(vmadm lookup alias=~imgapi) | \
+    json -H nics | json -ac 'this.nic_tag === "external"' ip
+10.88.88.5
 ```
 
-where <USER> is the name of the JPC/SDC user you want to build with, and <KEY>
-is the SSH fingerprint of the SSH key that you've added for your user.
-
-If you're using CoaL and using the default self-signed certificates for cloudapi
-you will also want to:
-
 ```
-export SDC_TESTING=1
+[root@headnode (coal-1) ~]# sdc-vmapi /vms/$(vmadm lookup alias=~cloudapi)| \
+     json -H nics | json -ac 'this.nic_tag === "external"' ip
+10.88.88.3
 ```
 
-otherwise you'll get errors like:
-
 ```
-sdc-listpackages: error: undefined
+[root@headnode (coal-1) ~]# sdc-vmapi /vms/$(vmadm lookup alias=~adminui )| \
+      json -H nics | json -ac 'this.nic_tag === "external"' ip
+10.88.88.4
 ```
 
-It's possible to use different <USER> values for MANTA_USER and SDC_ACCOUNT if
-you've pointed these at different SDC standups. In that case zones will be
-created using the SDC_ACCOUNT credentials and any files pulled from / pushed to
-Manta will be done using the MANTA_USER's credentials.
-
-If you're *not* using JPC here, you'll want to change the SDC_URL and MANTA_URL
-above to match your local cloudapi and manta respectively.
-
-NOTE: if your SDC is not yet setup, you need to set SDC_URL *after* setting up
-cloudapi in the next section.
-
-
-## Setting up an SDC for builds
-
-NOTE: skip this section (move on to "Setting up the build environment") if
-you're going to build in the JPC.
-
-This section assumes that you have a local SDC/CoaL setup and have access to
-the global zone.
-
-### Setting up cloudapi
-
-If you are using CoaL you won't have cloudapi by default. To add it you can
-run the following from within the MG directory on your workstation:
+Finally, we'll create a user account on the Triton instance ensuring the user
+is allowed to provision virtual machines:
 
 ```
-./tools/setup-cloudapi.sh root@<HN_GZ_IP>
+[root@headnode (coal-1) ~]#  echo '{
+    "approved_for_provisioning": "true",
+    "company": "Clavius",
+    "email": "email@domain.net",
+    "givenname": "Build",
+    "cn": "Build User",
+    "sn": "User",
+    "login": "builder",
+    "phone": "555 1212",
+    "userpassword": "1password"
+}' | sdc-useradm create
+User 92372c7f-d5a2-4223-888f-8e7982891b8e (login "builder") created
+[root@headnode (coal-1) ~]#
 ```
 
-where <HN_GZ_IP> is the IP of the GZ of your headnode. This script will login
-and create the cloudapi zone for you.
-
-If you're using CoaL and don't have any CNs attached to your headnode, you
-will also want to login to the GZ of your CoaL headnode and run:
+We'll add our SSH public key to that user:
 
 ```
-/zones/$(vmadm lookup -1 tags.smartdc_role=cloudapi)/root/opt/smartdc/cloudapi/tools/coal-setup.sh
+[root@headnode (coal-1) ~]# sdc-useradm add-key builder /tmp/id_rsa.pub
+Key "c3:eb:d4:e3:33:ca:9a:a1:54:b5:d6:40:f0:b2:93:68" added to user "builder"
 ```
 
-in order that you can provision using cloudapi on your headnode.
-
-
-### Setting up imgapi
-
-In order to perform builds you need to add an external interface to the imgapi
-zone. You also need to setup firewall rules (default rules do not allow
-connections on the external interface) that allow your build zone(s) to connect
-to the external interface of imgapi.
-
-Refer to the SDC documentation for details on how to perform the required steps
-here.
-
-Firewall rules can also be setup after the build zone(s) are created, but before
-the first build.
-
-Record the external IP address for imgapi. You'll need this later to set
-SDC_IMGAPI_URL.
-
-For your convienence, here are commands for the previous steps if you are
-running in COAL:
+and allow that user to create images using one of the default packages by
+adding our user to the `owner_uuids` array for that package.
+(the package is owned by the `admin` user by default)
 
 ```
-# Add an external nic
-$ /usbkey/scripts/add_external_nic.sh $(vmadm lookup alias=~imgapi)
-# Make sure the job finished successfully
-$ sdc-workflow /jobs/405b26f1-0f6a-4118-aacb-0d89fd777a36 | json -Ha execution
-succeeded
-# Find the external imgapi ip address
-$ sdc-vmapi /vms/$(vmadm lookup alias=~imgapi) | json -H nics | \
-    json -ac 'this.nic_tag === "external"' ip
-10.88.88.3
+[root@headnode (coal-1) ~]# sdc-papi /packages | json -ac 'name === "sdc_8192"' owner_uuids
+.
+.
+["930896af-bf8c-48d4-885c-6573a94b1853"]
+[root@headnode (coal-1) ~]# sdc-papi /packages/16425b42-b818-11e2-90d4-87477f18a688 -X PUT -d '{"owner_uuids": ["930896af-bf8c-48d4-885c-6573a94b1853", "92372c7f-d5a2-4223-888f-8e7982891b8e"]}'
+    [output omitted for brevity]
 ```
 
-If you're using your own SDC and do not have imgapi connected to a Manta (eg.
-you're using CoaL) you'll also need to run the following from the GZ of the
-headnode:
+We should now be able to use `triton` commands to interact with our Triton
+instance using the cloudapi IP address we determined earlier:
 
 ```
-echo '{"metadata": {"IMGAPI_ALLOW_LOCAL_CREATE_IMAGE_FROM_VM": true}}' \
-  | sapiadm update $(sdc-sapi /services?name=imgapi | json -H 0.uuid)
+$ npm install triton
+.
+.
+$ export SDC_URL=https://10.88.88.3
+$ export SDC_ACCOUNT=builder
+$ export SDC_TESTING=true
+$ export SDC_KEY_ID=c3:eb:d4:e3:33:ca:9a:a1:54:b5:d6:40:f0:b2:93:68
+$ triton info
+login: builder
+name: Build User
+email: email@domain.net
+url: https://10.88.88.3
+totalDisk: 0 B
+totalMemory: 0 B
+instances: 0
+$
 ```
 
-### Importing the images
-
-If you are using CoaL the sdc-smartos and sdc-multiarch images should already
-be imported. If for some reason your setup does *not* have these images, you'll
-need to import them. Follow the SDC documentation on importing images. The
-images you need are:
-
- * fd2cc906-8938-11e3-beab-4359c665ac99 / sdc-smartos 1.6.3
- * b4bdc598-8939-11e3-bea4-8341f6861379 / sdc-multiarch 13.3.1
-
+#### Retrieving an image on Triton
 
-## Setting up the build environment(s)
-
-Based on the choices you made earlier (see "Decisions to Make" section) you
-should know which build zones you will need. This section will guide you through
-the creation of the required zones.
-
-### Common steps to creating any build zone
-
-Before you continue, ensure that whatever user you're going to use (whether your
-personal account in JPC or 'admin' or other user in your local SDC/CoaL) has
-your SSH keys added to it. This is important as these instructions will have
-you running sdc-* commands and manta commands which will need these credentials.
-
-If you don't do this you'll see errors like:
+To retrieve an image on Triton, connect to the headnode and use
+`sdc-imgadm import`. For example:
 
 ```
-sdc-listpackages: error (InvalidCredentials): Invalid key d5:19:78:bb:d8:f5:ba:cd:6b:40:96:3f:5a:23:59:a9
+[root@headnode (coal-1) ~]# sdc-imgadm import -S 'https://updates.joyent.com?channel=experimental' 1356e735-456e-4886-aebd-d6677921694c
 ```
+(note that this requires that your imgapi instance has a nic on the external
+network. This can be achieved with `sdcadm post-setup common-external-nics`)
 
-### Find the package uuid for your build package
-
-Whether you're building in JPC or a local SDC you need to find the UUID of the
-package you're going to use to build. To do this (assuming you've setup all the
-variables listed in the previous section correctly) you can run:
+or if you have the manifest and gz files already downloaded, use the `-m`
+and `-f` flags to import the image directly. For example:
 
 ```
-sdc-listpackages | json -c "this.name == 'g3-standard-2-smartos'" 0.id
+[root@headnode (coal-1) ~]# sdc-imgadm import -m 1356e735-456e-4886-aebd-d6677921694c.manifest -f 1356e735-456e-4886-aebd-d6677921694c-file.gz
 ```
 
-replacing 'g3-standard-2-smartos' with the name of your package if you're not
-using JPC. For CoaL you can use package 'sdc_2048' if you haven't changed the
-default packages. The output of this command will be a UUID which you should
-substitute in commands below. In my case the value was
-'486bb054-6a97-4ba3-97b7-2413d5f8e849' so substitute your own value where you
-see that.  If your SDC_ACCOUNT isn't an administrator, you may not be able
-to find the `sdc_2048` package.  If you are using COAL this is because the
-package's owner_uuid is admin.  To make images public for you ruser to see, run
-this from the global zone:
+You should then modify the image so that it's owned by your `builder` user:
 
 ```
-sdc-papi /packages | json -Ha uuid | while read l; do \
-    echo '{ "owner_uuids": null }' | sdc-papi /packages/$l -X PUT -d@-; done
+[root@headnode (coal-1) ~]# sdc-imgadm update 1356e735-456e-4886-aebd-d6677921694c owner=92372c7f-d5a2-4223-888f-8e7982891b8e
 ```
 
-Note that you probably do *not* want to do this for a public SDC.  You are
-better off creating a new, public package.
-
-### Creating a sdc-smartos 1.6.3 build zone
-
-To create a sdc-smartos 1.6.3 zone you'll want to run:
+You should then see that image appearing in the `triton image list` command:
 
 ```
-sdc-createmachine \
-    --dataset fd2cc906-8938-11e3-beab-4359c665ac99 \
-    --package 486bb054-6a97-4ba3-97b7-2413d5f8e849 \
-    --name "build-1.6.3"
+$ triton image list
+SHORTID   NAME                            VERSION  FLAGS  OS       TYPE          PUBDATE
+1356e735  jenkins-agent-multiarch-15.4.1  2.1.0    -      smartos  zone-dataset  2018-12-19
+$
 ```
 
-changing "486bb054-6a97-4ba3-97b7-2413d5f8e849" to match the UUID you got in
-the previous step. The output should be a JSON object. The only field from
-that which you need to keep track of right now is the 'id' field. This is the
-UUID of the new build VM.
-
-You can run:
-
-```
-sdc-getmachine 721182fa-d4f1-61f6-8fae-9875512356e2 | json state
-```
+Repeat these steps for any other images that are needed to build your
+component.
 
-substituting your own UUID for '721182fa-d4f1-61f6-8fae-9875512356e2' until the
-result is 'running'. Once the VM goes running, you can find its IP using:
+#### Creating a build zone on Triton
 
-```
-sdc-getmachine 721182fa-d4f1-61f6-8fae-9875512356e2 | json ips
-```
+Normally, build zones can be created using the `triton` command line tool,
+however cloudapi doesn't currently have support for creating zones with a
+`delegated dataset` - where we assign the zone a separate zfs dataset which
+can be manipulated within the zone. Delegated datasets are needed by the
+`buildimage` tool to assemble component images.
 
-(again substituting your own UUID for '721182fa-d4f1-61f6-8fae-9875512356e2').
+So, the two ways of creating build zones are:
 
-At this point you'll want to take the IP address which is public (in the case
-there are more than one) and fill that in as <BUILD_ZONE_IP> in the section
-"Preparing build zones for builds" below.
+ * remotely, using the admin web interface
+ * directly on the headnode itself
 
 
-### Creating an sdc-multiarch 13.3.1 build zone
+##### To create a zone using the web interface
 
-To create a sdc-multiarch 13.3.1 build zone, you should follow the steps for
-creating a sdc-smartos 1.6.3 build zone with the exception that instead of
-dataset "fd2cc906-8938-11e3-beab-4359c665ac99" you should use dataset
-"b4bdc598-8939-11e3-bea4-8341f6861379" and you'll want to use a different name.
-For example:
+ * Visit the admin web UI using the `adminui` IP address we obtained earlier
+ * Login as `admin` and navigate to the "Provision a virtual machine" page,
+   e.g https://10.88.88.4/provision
+ * Select `builder` as the user, choose a descriptive build zone alias.
+   If you're the only user of the Triton instance, naming the build zone
+   after the image name can be convenient, e.g. "jenkins-agent-multiarch-15.4.1"
+ * Select a fairly large package, e.g. "sdc_8192 1.0.0", one of the default
+   packages that ships with Triton.
+ * Select one of the images we imported earlier, for example,
+   "1356e735-456e-4886-aebd-d6677921694c"
+ * Select the "Delegate Dataset" option
+ * Use the default brand, "joyent"
+ * Choose a CN to provision to (selecting "headnode" will ensure the build zone
+   is provisioned on that headnode)
+ * Choose the "external" network
+ * Add the following Customer metadata, `{"user-script": "/usr/sbin/mdata-get root_authorized_keys > ~root/.ssh/authorized_keys ; /usr/sbin/mdata-get root_authorized_keys > ~admin/.ssh/authorized_keys; svcadm enable manifest-import"}`
+ * Click "Provision machine"
 
-```
-sdc-createmachine \
-  --dataset b4bdc598-8939-11e3-bea4-8341f6861379 \
-  --package 486bb054-6a97-4ba3-97b7-2413d5f8e849 \
-  --name "build-13.3.1"
-```
+##### To create a zone directly from the headnode
 
-### Preparing build zone(s) for builds
+First we need to create a json file to pass to `sdc-vmadm`.
+There are several ways to do this, but we'll describe some simple approaches
+below.
 
-For each build zone (1.6.3 or 13.3.1) you want to follow the same set of
-instructions. First you want to do:
+When hosting dev zones on Triton, first we need to get some details to construct
+our json image manifest. Here, we're on a newly setup coal instance, so we're
+just looking for the builder user uuid, the external network uuid and the uuid
+of the headnode to provision to:
 
 ```
-./tools/setup-remote-build-zone.sh root@<BUILD_ZONE_IP>
+[root@headnode (coal-1) ~]# sdc-useradm search login=builder
+UUID                                  LOGIN    EMAIL             CREATED
+92372c7f-d5a2-4223-888f-8e7982891b8e  builder  email@domain.net  2019-05-22
+[root@headnode (coal-1) ~]# sdc-network list
+NAME         UUID                                  VLAN           SUBNET          GATEWAY
+admin        c5bb76da-4b19-434f-9c8c-63ef1a45ce41     0    10.99.99.0/24                -
+external     1bc9e7a0-3607-42a3-ba44-373924b6c9a6     0    10.88.88.0/24       10.88.88.2
+[root@headnode (coal-1) ~]# sdc-server list
+HOSTNAME             UUID                                 VERSION    SETUP    STATUS      RAM  ADMIN_IP
+headnode             564d8bfa-7076-6ccc-5072-7112ddf32acc     7.0     true   running     8191  10.99.99.7
 ```
 
-This will produce some output as it logs into your zone, installs some packages
-and generally gets it ready for you to login and start some builds.
-
-### Cloning MG in your build zone
-
-For each build zone (1.6.3 or 13.3.1) you want to clone MG before you start
-building. So SSH to the build zone, then run:
+Now use this json to create the VM, or use the adminui. The key parts are
+to specify `delegate_dataset`, required to use the new image construction
+tooling, and to give yourself enough RAM to have a useful development
+environment.
 
 ```
-ssh -A root@<BUILD_ZONE_IP> # Use the -A to forward your SSH agent
-git clone git@github.com:joyent/mountain-gorilla.git MG && cd MG
+{
+  "brand": "joyent",
+  "image_uuid": "1356e735-456e-4886-aebd-d6677921694c",
+  "alias": "jenkins-agent-multiarch-15.4.1",
+  "owner_uuid": "92372c7f-d5a2-4223-888f-8e7982891b8e",
+  "server_uuid": "564d8bfa-7076-6ccc-5072-7112ddf32acc",
+  "hostname": "jenkins-agent-multiarch-15.4.1",
+  "ram": 8192,
+  "quota": 100,
+  "delegate_dataset": true,
+  "resolvers": [
+    "10.0.0.29",
+    "208.67.220.220"
+  ],
+  "networks": [{"uuid": "1bc9e7a0-3607-42a3-ba44-373924b6c9a6"}],
+  "customer_metadata": {
+    "root_authorized_keys": "ssh-rsa AAAAB3NzaC1y... me@myselfandi",
+  "user-script": "/usr/sbin/mdata-get root_authorized_keys > ~root/.ssh/authorized_keys ; /usr/sbin/mdata-get root_authorized_keys > ~admin/.ssh/authorized_keys; svcadm enable manifest-import" }
+}
 ```
 
-### Add additional environment variables
-
-If you're building in JPC, you can skip this step. If you're building in a local
-SDC/CoaL setup, you'll probably also need to also set the following at this
-point:
+Then create the VM:
 
 ```
-export SDC_LOCAL_BUILD=1
-export SDC_IMAGE_PACKAGE=sdc_2048
-export SDC_TESTING=1
-export SDC_IMGAPI_URL=https://10.88.0.15
+[root@headnode (uk-1) ~]# sdc-vmadm create -f json
+Creating VM 60802c6d-e458-612b-bcc5-b472fffad1a2 (job "db55fe3e-a631-4df5-a5e7-18ab9ed11afa")
+[root@headnode (uk-1) ~]#
 ```
 
-You'll also need to ensure these variables are set at the time of each build.
-
-where:
-
- * SDC_LOCAL_BUILD tells MG that you don't want the build creating zones in JPC
-   or pushing files to JPC's Manta as part of the build process.
- * SDC_IMAGE_PACKAGE is the name of the package you want to use for the build
-   zones. CoaL ships with an sdc_2048 package which should work.
- * SDC_TESTING allows the node-smartdc tools to work even when you've got a
-   self-signed SSL certificate.
- * SDC_IMGAPI_URL should be set to https://<IP> where <IP> is the external IP
-   you added to imgapi (remembering to add the firewall rules if you have not
-   already)
-
-## Building
-
-The following commands should be run in the MG directory in the appropriate
-build zone for the target you're building. They should also be run with all the
-environment variables described earlier set.
-
-### Option 1: build a single target, taking dependencies from joyager
+##### Verify you can connect to the VM
 
-Ensure you've set the appropriate environment variables, especially:
+Having done this, you should be able to see that vm from `triton` and should
+be able to connect to it:
 
- * SDC_LOCAL_BUILD if you're building against your own SDC/CoaL
- * SDC_URL set to the proper cloudapi
-
-Then to build, run the following in your MG directory in your build zone:
-
-```
-TARG=<build>; ./configure -t ${TARG} -d joyager -D /stor/builds \
-    -O /stor/whatever/builds && make ${TARG}_upload_manta
 ```
+$ triton instance list
+SHORTID   NAME                            IMG                                   STATE    FLAGS  AGE
+64512d5b  jenkins-agent-multiarch-15.4.1  jenkins-agent-multiarch-15.4.1@2.1.0  running  -      1m
+$ triton instance ssh jenkins-agent-multiarch-15.4.1
+   __        .                   .
+ _|  |_      | .-. .  . .-. :--. |-
+|_    _|     ;|   ||  |(.-' |  | |
+  |__|   `--'  `-' `;-| `-' '  ' `-'
+                   /  ; Instance (minimal-multiarch-lts 15.4.1)
+                   `-'  https://docs.joyent.com/images/smartos/minimal
 
-if we use 'assets' for the build for example:
-
-```
-TARG=assets; ./configure -t ${TARG} -d joyager -D /stor/builds \
-    -O /stor/whatever/builds && make ${TARG}_upload_manta
+[root@64512d5b-f4f0-e070-9e1a-b076df72be6b ~]#
 ```
 
-which will:
+#### SmartOS-based build zones
 
- * download dependencies from /joyager/stor/builds
- * create a tarball of the assets bits + dependencies
- * create a SmartOS VM in JPC (using cloudapi)
- * install the tarball of bits into the JPC VM
- * create an image from the VM, sending to Manta
- * download the image from Manta modify the manifest
- * push the build back to manta in ${MANTA_USER}/stor/whatever/builds/assets
-
-
-### Option 2: build a single target, taking dependencies from joyager but not uploading results
-
-To *not* upload results to Manta, follow the same procedure as in "Option 1" but
-change the make target from:
+If you're developing on a SmartOS system, use the following to download
+the image:
 
 ```
-make ${TARG}_upload_manta
+imgadm import -S 'https://updates.joyent.com?channel=experimental' 1356e735-456e-4886-aebd-d6677921694c
 ```
 
-to:
+or download the image manifest and image file and import by hand, with:
 
 ```
-make ${TARG}
+curl -k -o img.manifest 'https://updates.joyent.com/images/1356e735-456e-4886-aebd-d6677921694c?channel=experimental'
+curl -k -o img.gz 'https://updates.joyent.com/images/1356e735-456e-4886-aebd-d6677921694c/file?channel=experimental'
 ```
 
-The result will then be in the bits/ directory instead of going to Manta.
-
-
-### Option 3: build all targets from scratch
-
-If you want to ensure you've built every bit that you're using, you'll want to
-do your builds in order and send them to a fresh Manta area. There's a tool
-in MG's tools directory that will help you build in the correct order. For
-example, assuming I'm running as Manta user "joyager" and I want to create a
-full set of builds and then use that to build a new headnode image, I'd start
-in my 1.6.3 zone and run:
-
-```
-(set -o errexit
-    for TARG in $(./tools/targets-1.6.3.sh); do
-        ./configure -t ${TARG} -d joyager -D /stor/whatever/builds \
-            -O /stor/whatever/builds && make ${TARG}_upload_manta
-    done
-)
-```
-
-which will build all the dependencies first then the 1.6.3-built images.
-Uploading to /joyager/stor/whatever/builds (which was empty when we started) and
-taking dependencies only from that directory.
-
-Once this is complete, we can run the same command just with:
+then do the following to add the image to your SmartOS instance:
 
 ```
-./tools/targets-13.3.1.sh
+imgadm install -m img.manifest -f img.gz
 ```
 
-instead of:
+To create the VM, use a json manifest similar to the one listed below.
+Note that there are syntax differences between Triton `sdc-vmadm` manifests
+and SmartOS `vmadm` manifests!
 
 ```
-./tools/targets-1.6.3.sh
+{
+  "brand": "joyent",
+  "image_uuid": "1356e735-456e-4886-aebd-d6677921694c",
+  "alias": "jenkins-agent-multiarch-15.4.1",
+  "hostname": "jenkins-agent-multiarch-15.4.1",
+  "max_physical_memory": 4096,
+  "quota": 100,
+  "delegate_dataset": true,
+  "fs_allowed": ["ufs", "pcfs"],
+  "resolvers": [
+    "10.0.0.29",
+    "208.67.220.220"
+  ],
+  "nics": [
+    {
+      "nic_tag": "admin",
+      "ip": "dhcp"
+    }
+  ],
+  "customer_metadata": {
+    "root_authorized_keys": "ssh-rsa AAAAB3NzaC1y... me@myselfandi",
+  "user-script": "/usr/sbin/mdata-get root_authorized_keys > ~root/.ssh/authorized_keys ; /usr/sbin/mdata-get root_authorized_keys > ~admin/.ssh/authorized_keys; svcadm enable manifest-import " }
+}
 ```
 
-generating the target list. The only target that currently cannot be built
-this way which is required for building a new headnode image is the platform
-target. The next section will deal with that.
-
-### Building sdc-headnode without using manta at all
-
-Assuming you've set all the environment variables and setup both build zones
-you need (including the modifications in the "Building the platform image"
-section) and setup your cloudapi and so forth, you can use the instructions in
-this section to build everything locally without Manta.
-
-Start on the 1.6.3 build zone and run:
+Then create the VM using `vmadm`:
 
 ```
-mkdir /root/MY_BITS
-export LOCAL_BITS_DIR=/root/MY_BITS
+[root@kura ~]# vmadm create -f json
+Successfully created VM c1f04dfb-63c6-ca69-b04b-d68e5b4ffadc
+[root@kura ~]#
 ```
 
-Then cd to your MG workspace on this zone and run:
+### Building a component
 
-```
-(set -o errexit; for TARG in $(tools/targets-1.6.3.sh); do \
-    ./configure -t ${TARG} -d joyager -D /stor/donotuse/builds \
-        -O /stor/donotuse/builds && make ${TARG}_local_bits_dir; done)
-```
+You should now be able to clone any of the Manta or Triton repositories.
 
-This will take a while. Once it completes, create the /root/MY_BITS directory
-on your 13.3.1 build zone and set the LOCAL_BITS_DIR variable:
+The following Makefile targets are conventions used in most Manta/Triton
+components:
 
-```
-mkdir /root/MY_BITS
-export LOCAL_BITS_DIR=/root/MY_BITS
-```
+  | target               | description
+  |----------------------|----------------------------------------------------------------------------------
+  | show-buildenv        |  show the build environment and build zone image uuid for building this component
+  | validate-buildenv    |  check that the build machine is capable of building this component
+  | all                  |  build all sources for this component
+  | release              |  build a tarball containing the bits for this component
+  | publish              |  publish a tarball containing the bits for this component
+  | buildimage           |  assemble a Triton/Manta image for this component
+  | bits-upload          |  post bits to Manta, and optionally updates.joyent.com for this component
+  | bits-upload-latest   |  just post the most recently built bits, useful in case bit
+  | check                |  run build tests (e.g. xml validation, linting)
+  | prepush              |  additional testing that should occur before pushing to github
 
-Now transfer all the bits from your 1.6.3 build zone to this 13.3.1 build zone.
-One way to do this is using rsync (make sure you preserve the directory
-structure):
 
-```
-rsync -va root@<1.6.3-zone-IP>:/root/MY_BITS/* /root/MY_BITS/
-```
+For more details on the specifics of these targets, we do have commentary in
+[eng.git:/Makefile](https://github.com/joyent/eng/blob/master/Makefile#L11) and
+[eng.git:/tools/mk/Makefile.defs](https://github.com/joyent/eng/blob/master/tools/mk/Makefile.defs#L29).
 
-Once that's complete (still logged into the 13.3.1 build zone with the
-LOCAL_BITS_DIR set) you can go to your MG directory and run:
+Typically, the following can be used to build any component, and will leave
+a component image (a compressed zfs send stream and image manifest) in `./bits`
+along with some additional metadata about the build:
 
 ```
-(set -o errexit; for TARG in $(tools/targets-13.3.1.sh) platform; do \
-    ./configure -t ${TARG} -d joyager -D /stor/donotuse/builds \
-        -O /stor/donotuse/builds && make ${TARG}_local_bits_dir; done)
+$ export ENGBLD_SKIP_VALIDATE_BUILD_PLATFORM=true
+$ make all release publish buildimage
 ```
 
-This will take quite a while (3-4 hours most likely) but once it's complete,
-/root/MY_BITS will contain all the bits required to build the usb headnode
-image. To do this, you will want to:
+The build will happily run as a non-root user (recommended!), however some
+parts of the build do need additional privileges. To add those to your non-root
+user inside your build zone, do:
 
 ```
-git clone git@github.com:joyent/sdc-headnode.git
-cd sdc-headnode
-make BITS_DIR=/root/MY_BITS usb
+# usermod -P 'Primary Administrator' youruser
 ```
 
-You can also change 'usb' to 'coal' if you want to build the CoaL image instead.
-
-
-## Building the platform image
+<a name="retro"></a>
 
-The platform image can be built in a 13.3.1 build zone just like any other
-MG target. However there are some additional changes required to these build
-zones before you can build platform.
+### Going retro
 
-You need to:
+We mentioned before that most components will build on modern platform images.
 
- * set fs_allowed="ufs,pcfs,tmpfs"
- * ensure you've got plenty of quota for your zone
- * ensure you've got enough DRAM allocated for your zone
+However, Joyent production builds always build on the earliest possible platform
+we support, defined by `min_platform`. We do this because of the binary
+compatibility guarantee that comes with Illumos (the kernel and userland
+software used by SmartOS): binaries compiled on older platforms are
+guaranteed to run on newer platforms, but the converse is not true.
+
+In addition, when compiling binaries, constant values from the platform headers
+may be included in those binaries at build-time. If those constants change
+across platform images (which several have) then the binary will have different
+behaviour depending on which platform the source was built on.
+
+For these reasons, when assembling the Manta/Triton images via the 'buildimage'
+target, we set the `min_platform` of the resulting image to be the version
+of the platform running on the build machine. Code in `vmadm` checks at
+provisioning-time that the platform hosting the VM is greater than, or equal to
+the `min_platform` value baked into the Manta/Triton image.
+
+As mentioned previously, the build system itself will report an error if
+your build platform is not equal to the `min_platform` image, via the
+`validate-buildenv` make target.
+
+In order to exactly replicate the build environment used for our production
+builds, and produce images that can be installed on any supported platform,
+we install build zones on `joyent-retro` VMs, which are bhyve (or KVM) SmartOS
+instances that boot that old platform image.
+(See [https://github.com/joyent/joyent-retro/blob/master/README.md](https://github.com/joyent/joyent-retro/blob/master/README.md))
+
+At the time of writing, our `min_platform` is set to `20151126T062538Z`
 
-One option for performing all of these at once would be do something like:
+That image is available as `joyent-retro-20151126T062538Z`,
+uuid `bd83a9b3-65cd-4160-be2e-f7c4c56e0606`
 
- * vmadm update <uuid> fs_allowed="ufs,pcfs,tmpfs" ram=8192 quota=200
+See: [https://updates.joyent.com/images/bd83a9b3-65cd-4160-be2e-f7c4c56e0606?channel=experimental](https://updates.joyent.com/images/bd83a9b3-65cd-4160-be2e-f7c4c56e0606?channel=experimental)
 
-from the GZ. This would work fine on hardware but is unlikely to work with CoaL
-unless you've bumped the default amount of DRAM for CoaL significantly.
+The retro image does not itself contain any devzone images, so those will have
+to be imported by hand.
 
-Once you have a properly setup 13.3.1 build zone you can build the platform
-with the same command as you'd use for other targets:
+The following example json would then be used to deploy it on a SmartOS
+instance. Note here we're adding a 64gb data disk which will then host our
+dev zones.
 
 ```
-TARG=platform; ./configure -t ${TARG} -d joyager -D /stor/whatever/builds \
-    -O /stor/whatever/builds && make ${TARG}_upload_manta
+{
+  "brand": "bhyve",
+  "alias": "retro-20151126T062538Z",
+  "hostname": "retro-20151126T062538Z",
+  "ram": 4096,
+  "vcpus": 6,
+  "quota": 100,
+  "delegate_dataset": true,
+  "fs_allowed": ["ufs", "pcfs"],
+  "resolvers": [
+    "10.0.0.29",
+    "208.67.220.220"
+  ],
+  "nics": [
+    {
+      "nic_tag": "admin",
+      "ip": "dhcp",
+      "netmask": "255.255.255.0",
+      "gateway": "10.0.0.1",
+      "model": "virtio",
+      "primary": "true"
+    }
+  ],
+  "disks": [
+            {
+                 "boot": true,
+                 "model": "virtio",
+                 "image_uuid": "bd83a9b3-65cd-4160-be2e-f7c4c56e0606",
+                 "image_name": "joyent-retro-20151126T062747Z"
+            },
+            {
+                "boot": false,
+                 "model": "virtio",
+                 "size": 65536,
+                 "media": "disk"
+            }
+],
+  "customer_metadata": {
+    "root_authorized_keys": "ssh-rsa AAAAB3Nz... me@myselfandi"
+  }
+}
 ```
 
-which will build and upload to Manta. Alternatively you can omit the
-_upload_manta and just have the platform build to the local bits/ directory.
+Having deployed a VM using this image on your Triton or SmartOS host, you can
+then ssh into the retro VM and proceed with creating build zones as mentioned in
+the earlier section and do **not** need to set
+`$ENGBLD_SKIP_VALIDATE_BUILD_PLATFORM` in your environment.
+
+Note that this retro image is itself a SmartOS instance rather than a Triton
+host, so you'll need to use SmartOS-formatted json to create build zones.
+
+To allow you to ssh directly into the build zones running in a retro VM,
+a simple ipnat configuration works fine. For example, this retro VM has the
+external IP address `10.0.0.180` and our build zones are all on the `172.16.9.0`
+network. We create a file `/etc/ipf/ipnat.conf`:
+
+```
+[root@27882aaa /etc/ipf]# cat ipnat.conf
+map vioif0 172.16.9.0/24 -> 0/32 portmap tcp/udp auto
+map vioif0 172.16.9.0/24 -> 0/32
+
+rdr vioif0 10.0.0.180 port 2222 -> 172.16.9.2 port 22 tcp
+rdr vioif0 10.0.0.180 port 2223 -> 172.16.9.3 port 22 tcp
+rdr vioif0 10.0.0.180 port 2224 -> 172.16.9.4 port 22 tcp
+rdr vioif0 10.0.0.180 port 2225 -> 172.16.9.5 port 22 tcp
+rdr vioif0 10.0.0.180 port 2226 -> 172.16.9.6 port 22 tcp
+```
+
+and enable ip forwarding and the ipfilter service in the retro VM:
+
+```
+[root@27882aaa ~]# routeadm -e ipv4-forwarding
+[root@27882aaa ~]# svcadm enable ipfilter
+```
+
+We can then ssh into our individual build zones with the following changes added
+to `~/.ssh/config`. Note that we manually added 'jenkins' non-root users to our
+zones here, and chose a simple alphabetic pattern to name build zones, each
+corresponding to a different build zone image.
+
+```
+Host retro-kabuild2
+        User jenkins
+        Hostname 10.0.0.180
+        Port    2222
+
+Host retro-kbbuild2
+        User jenkins
+        Hostname 10.0.0.180
+        Port    2223
+
+Host retro-kcbuild2
+        User jenkins
+        Hostname 10.0.0.180
+        Port    2224
+
+Host retro-kdbuild2
+        User jenkins
+        Hostname 10.0.0.180
+        Port    2225
+
+Host retro-kebuild2
+        User jenkins
+        Hostname 10.0.0.180
+        Port 2226
+```
+
+Here's us logging in:
+
+```
+timf@iorangi-eth0 (master) ssh retro-kabuild2
+-bash-4.1$ ifconfig
+lo0: flags=2001000849<UP,LOOPBACK,RUNNING,MULTICAST,IPv4,VIRTUAL> mtu 8232 index 1
+        inet 127.0.0.1 netmask ff000000
+net0: flags=40001000843<UP,BROADCAST,RUNNING,MULTICAST,IPv4,L3PROTECT> mtu 1500 index 2
+        inet 172.16.9.2 netmask ffffff00 broadcast 172.16.9.255
+lo0: flags=2002000849<UP,LOOPBACK,RUNNING,MULTICAST,IPv6,VIRTUAL> mtu 8252 index 1
+        inet6 ::1/128
+-bash-4.1$ id
+uid=103(jenkins) gid=1(other) groups=1(other)
+-bash-4.1$
+```
+
+As you can see, we connected to port 222 of 10.0.0.180, which brought us to
+the build zone that has the local IP address 172.16.9.2.
+
+### Additional notes on build artifacts
+
+The `bits-upload` or `bits-upload-latest` Makefile targets will upload built
+components from `./bits` using the `./deps/eng/tools/bits-upload.sh` script.
+
+   * `bits-upload` will publish bits to `$MANTA_USER/publics/builds/<component>`
+     by default, and will use `$MANTA_USER`, `$MANTA_KEY_ID` and `$MANTA_URL`
+     to determine the Manta address to post to.
+
+   * `bits-upload-latest` will attempt to retry the last upload, in case of
+      network interruption, but will otherwise not re-create any of the build
+      artifacts.
+
+   * publishing bits to the imgapi service on https://updates.joyent.com from
+     `bits-upload` requires you to have credentials configured there to allow
+     you to upload.
+
+   * By default, publishing to https://updates.joyent.com is disabled and will
+     only happen if `$ENGBLD_BITS_UPLOAD_IMAPI=true` in your shell environment.
+     You can also publish bits to a local (or NFS) path instead of Manta and
+     imgapi.
+     To do that, set `$ENGBLD_DEST_OUT_PATH` and `$ENGBLD_BITS_UPLOAD_LOCAL`
+
+     For example:
+     ```
+     export ENGBLD_DEST_OUT_PATH=/home/timf/projects/bits
+     export ENGBLD_BITS_UPLOAD_LOCAL=true
+     ```
+     This can be useful if doing a local `sdc-headnode` build.
+
+   * You can change which imgapi instance your build posts to by setting
+     `$UPDATES_IMGADM_USER`, `$UPDATES_IMG_URL` and `$UPDATES_IMGADM_IDENTITY`
+     in your environment. If not set, `$UPDATES_IMGADM_CHANNEL` is computed
+     automatically. See `./deps/eng/tools/bits-upload.sh`
+
+### Building from Gerrit
+
+When reviewing proposed changes from Gerrit, it can be useful to build and
+deploy those directly.
+
+Here we build patch set 2 of the `sdc-sapi.git` component, which has the
+gerrit id `5538`:
+
+```
+-bash-4.1$ cd /tmp
+-bash-4.1$ git clone https://cr.joyent.us/joyent/sdc-sapi.git
+Cloning into 'sdc-sapi'...
+remote: Counting objects: 2180, done
+remote: Finding sources: 100% (2180/2180)
+remote: Total 2180 (delta 1464), reused 2175 (delta 1464)
+Receiving objects: 100% (2180/2180), 540.64 KiB | 251.00 KiB/s, done.
+Resolving deltas: 100% (1464/1464), done.
+Checking connectivity... done.
+-bash-4.1$ cd sdc-sapi
+-bash-4.1$ git ls-remote | grep 5538
+From https://cr.joyent.us/joyent/sdc-sapi.git
+d2daf78578e3854069cbe194f5f9cf4c96571d22        refs/changes/38/5538/1
+53f51b8e1b4e6088b22757ee230edc9b6974e46e        refs/changes/38/5538/2
+-bash-4.1$ git fetch origin refs/changes/38/5538/2
+remote: Counting objects: 13, done
+remote: Finding sources: 100% (7/7)
+remote: Total 7 (delta 6), reused 7 (delta 6)
+Unpacking objects: 100% (7/7), done.
+From https://cr.joyent.us/joyent/sdc-sapi
+ * branch            refs/changes/38/5538/2 -> FETCH_HEAD
+ -bash-4.1$ git checkout FETCH_HEAD
+Note: checking out 'FETCH_HEAD'.
+
+You are in 'detached HEAD' state. You can look around, make experimental
+changes and commit them, and you can discard any commits you make in this
+state without impacting any branches by performing another checkout.
+
+If you want to create a new branch to retain commits you create, you may
+do so (now or later) by using -b with the checkout command again. Example:
+
+  git checkout -b <new-branch-name>
+
+  HEAD is now at 53f51b8... TRITON-1131 convert sdc-sapi to engbld framework
+  -bash-4.1$ git describe --all --long
+  heads/master-1-g53f51b8
+-bash-4.1$ make all release publish buildimage
+fatal: ref HEAD is not a symbolic ref
+/tmp/space/sdc-sapi/deps/eng/tools/validate-buildenv.sh
+.
+.
+[ 29.00080643] Saving manifest to "/tmp/sapi-zfs--20190215T144650Z-g53f51b8.imgmanifest"
+[ 30.24198958] Destroyed zones/3923c435-8688-47bb-a5f1-b213b010f826/data/b9f703a4-52e1-4c3d-b862-29b8dd047669
+[ 30.29018650] Deleted /zoneproto-49345
+[ 30.29080095] Build complete
+cp /tmp/sapi-zfs--20190215T144650Z-g53f51b8.zfs.gz /tmp/space/sdc-sapi/bits/sapi
+cp /tmp/sapi-zfs--20190215T144650Z-g53f51b8.imgmanifest /tmp/space/sdc-sapi/bits/sapi
+pfexec rm /tmp/sapi-zfs--20190215T144650Z-g53f51b8.zfs.gz
+pfexec rm /tmp/sapi-zfs--20190215T144650Z-g53f51b8.imgmanifest
+pfexec rm -rf /tmp/buildimage-sapi--20190215T144650Z-g53f51b8
+-bash-4.1$
+```
+
+Note that the first build of components on a new dev zone will likely take
+a little longer than usual as the `agent-cache` framework has to build each
+agent to be included in the image, and the `buildimage` tool has to download
+and cache the base images for the component. See TOOLS-2063 and TOOLS-2066.
+
+Also note in the above, that the `$(BRANCH)` used for the build artifacts looks
+a little unusual due to the fact we checked out a gerrit branch that doesn't
+follow the same naming format as most git branches.
diff --git a/docs/developer-guide/repos.md b/docs/developer-guide/repos.md
index 2fedf72..756ec54 100644
--- a/docs/developer-guide/repos.md
+++ b/docs/developer-guide/repos.md
@@ -109,7 +109,6 @@ Build tools are used for creating the zone images:
 * [sdcnode](https://github.com/joyent/sdcnode): Tools for creation of prebuilt node tarballs for SDC components.
 * [sdc-headnode](https://github.com/joyent/sdc-headnode): Repository for building headnode images for SDC, and the intial setup and configuration of the headnode itself
 * [sdc-scripts](https://github.com/joyent/sdc-scripts): Common scripts for configuring and setting up SDC zones.
-* [mountain-gorilla](https://github.com/joyent/mountain-gorilla): Builder of all the SDC bits.
 * [ipxe](https://github.com/joyent/ipxe): SDC iPXE implementation
 
 
@@ -147,4 +146,3 @@ The other repos are used by one of the other repos above:
 * [sdc-wf-shared](https://github.com/joyent/sdc-wf-shared): SmartDataCenter workflow shared code.
 * [cloud-tycoon](https://github.com/joyent/cloud-tycoon): DC simulation package
 * [node-triton-netconfig](https://github.com/joyent/node-triton-netconfig): A module to help manage the network configuration of a trition datacenter
-
diff --git a/etc/repos.json b/etc/repos.json
index 8cf37d8..aeaa2c1 100644
--- a/etc/repos.json
+++ b/etc/repos.json
@@ -495,13 +495,6 @@
     "git": "git@github.com:joyent/sdc-wf-shared.git",
     "desc": "SmartDataCenter workflow shared code."
   },
-  {
-    "git": "git@github.com:joyent/mountain-gorilla.git",
-    "desc": "Builder of all the SDC bits.",
-    "tags": [
-      "build"
-    ]
-  },
   {
     "git": "git@github.com:joyent/ipxe.git",
     "desc": "SDC iPXE implementation",
@@ -547,5 +540,9 @@
     "tags": [
       "docs"
     ]
+  },
+  {
+    "git": "git@github.com:joyent/node-triton-netconfig.git",
+    "desc": "A module to help manage the network configuration of a trition datacenter"
   }
 ]
diff --git a/repos.json b/repos.json
index 27f823f..8f058e8 100644
--- a/repos.json
+++ b/repos.json
@@ -149,9 +149,6 @@
                 "vm": true
             }
         },
-        {
-            "name": "mountain-gorilla"
-        },
         {
             "name": "node-amqp-joyent"
         },
