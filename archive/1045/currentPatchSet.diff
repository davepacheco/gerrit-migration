commit c3a74d5742528a00460cd90853f05d041e9015ef (refs/changes/45/1045/6)
Author: David Pacheco <dap@joyent.com>
Date:   2016-12-07T09:25:47-08:00 (2 years, 10 months ago)
    
    MANTA-3037 manta-adm genconfig should support multiple DCs
    MANTA-3041 manta-adm genconfig policy with uneven racks could use work
    MANTA-3044 manta-adm genconfig could print summary
    Reviewed by: Richard Bradley <richard.bradley@joyent.com>
    Approved by: Richard Bradley <richard.bradley@joyent.com>

diff --git a/cmd/manta-adm.js b/cmd/manta-adm.js
index 4e96735..b4bc330 100755
--- a/cmd/manta-adm.js
+++ b/cmd/manta-adm.js
@@ -204,23 +204,32 @@ MantaAdm.prototype.do_genconfig = function (subcmd, opts, args, callback)
 		callback(new Error(
 		    'expected "lab", "coal", or --from-file option'));
 		return;
+	} else if (opts.directory) {
+		callback(new Error(
+		    '--directory can only be used with --from-file'));
+		return;
 	}
 
 	this.initAdm(opts, function () {
 		var adm = self.madm_adm;
 		var func;
-		var options = {
-		    'outstream': process.stdout
-		};
+		var options = {};
 
 		if (args[0] == 'lab') {
 			func = adm.dumpConfigLab;
+			options['outstream'] = process.stdout;
 		} else if (args[0] == 'coal') {
 			func = adm.dumpConfigCoal;
+			options['outstream'] = process.stdout;
 		} else {
 			assertplus.string(fromfile);
 			func = adm.genconfigFromFile;
 			options['filename'] = fromfile;
+			if (opts.directory) {
+				options['outDirectory'] = opts.directory;
+			} else {
+				options['outstream'] = process.stdout;
+			}
 			options['errstream'] = process.stderr;
 		}
 
@@ -228,15 +237,16 @@ MantaAdm.prototype.do_genconfig = function (subcmd, opts, args, callback)
 			if (err)
 				fatal(err.message);
 
-			func.call(adm, options, function (serr, nwarnings) {
+			func.call(adm, options, function (serr, nissues) {
 				if (serr)
 					fatal(serr.message);
 
-				if (nwarnings !== 0) {
+				if (nissues !== 0) {
 					console.error('error: bailing out ' +
 					    'because of at least one issue');
 					process.exit(1);
 				}
+
 				self.finiAdm();
 			});
 		});
@@ -251,13 +261,18 @@ MantaAdm.prototype.do_genconfig.help =
     '\n' +
     '    manta-adm genconfig lab\n' +
     ' or manta-adm genconfig coal\n' +
-    ' or manta-adm genconfig --from-file=FILE\n';
+    ' or manta-adm genconfig [--directory DIR] --from-file=FILE\n';
 
 MantaAdm.prototype.do_genconfig.options = [ {
     'names': [ 'from-file' ],
     'type': 'string',
     'helpArg': 'FILE',
     'help': 'Use server descriptions in FILE'
+}, {
+    'names': [ 'directory', 'd' ],
+    'type': 'string',
+    'helpArg': 'DIR',
+    'help': 'Output directory for generated configs'
 } ];
 
 /*
diff --git a/docs/man/man1/manta-adm.md b/docs/man/man1/manta-adm.md
index e4207d0..0f8546b 100644
--- a/docs/man/man1/manta-adm.md
+++ b/docs/man/man1/manta-adm.md
@@ -10,7 +10,7 @@ manta-adm - administer a Manta deployment
 
 `manta-adm genconfig "lab" | "coal"`
 
-`manta-adm genconfig --from-file=FILE`
+`manta-adm genconfig [--directory=DIR] --from-file=FILE`
 
 `manta-adm show [-l LOG_FILE] [-a] [-c] [-H] [-o FIELD...] [-s] SERVICE`
 
@@ -179,7 +179,7 @@ Example: list hostnames in form suitable for "sdc-oneachnode -n":
 
 `manta-adm genconfig "lab" | "coal"`
 
-`manta-adm genconfig --from-file=FILE`
+`manta-adm genconfig [--directory=DIR] --from-file=FILE`
 
 The `manta-adm genconfig` subcommand generates a JSON configuration file
 suitable for use with `manta-adm update`.  The images used for each service are
@@ -188,21 +188,36 @@ manta-init(1), so this command is sometimes used as a shortcut for identifying
 the latest images that have been fetched for each service.
 
 When the first argument is `"coal"`, the command produces a configuration
-suitable for a small VM-in-a-laptop deployment.
+suitable for a small VM-in-a-laptop deployment.  The configuration is always
+emitted to stdout.
 
 When the first argument is `"lab"`, the command produces a configuration
-suitable for a larger single-server install.
-
-The `--from-file=FILE` option can be used to generate a configuration suitable
-for a much larger deployment.  This form is somewhat experimental, but attempts
-to create a deployment that will survive failures of any component, server, or
-rack.  `FILE` is a JSON file describing the parameters of the deployment,
-including the number of metadata shards and the set of availability zones,
-racks, and servers.  You can omit availability zone and rack information.  The
-JSON file should represent a single object with properties:
+suitable for a larger single-server install.  The configuration is always
+emitted to stdout.
+
+The `--from-file=FILE` form can be used to generate a configuration suitable for
+a much larger, production-style deployment.  `FILE` is a JSON file in the format
+specified below that describes the parameters of the deployment, including the
+number of metadata shards and the set of availability zones, racks, and servers.
+This form attempts to create a deployment that will survive failures of any
+component, server, rack, or availability zone as long as sufficient servers,
+racks, and availability zones are included in the input file.  Availability zone
+and rack information can be omitted from the file, in which case the tool will
+generate a configuration ignoring rack-level and AZ-level considerations.  This
+tool uses a number of heuristics, and the output should be verified.
+
+By default, the generated configuration is emitted to stdout.  With the
+`--directory` option, the configuration will be written to files in the
+specified directory named by availability zone.  This option must be used if the
+servers in `FILE` span more than one availability zone.
+
+The input JSON file `FILE` should contain a single object with properties:
 
 `nshards` (positive integer)
-  the number of metadata shards to create
+  the number of database shards to create, which is usually one more than the
+  number of shards that are intended to store object metadata (in order to
+  accommodate jobs and low-volume system metadata that's typically stored in
+  shard 1)
 
 `servers` (array of objects)
   the list of servers available for deployment
@@ -224,10 +239,8 @@ Each element of `servers` is an object with properties:
   storage servers to determine the appropriate number of compute zones.
 
 `az` (string)
-  (optional) availability zone.  This version of the tool only works for a
-  single-availability-zone deployment, so all servers must have the same value
-  here.  If the value is omitted from any server, that server is placed into a
-  default availablity zone.
+  (optional) availability zone.  If the value is omitted from any server, that
+  server is placed into a default availablity zone.
 
 `rack` (string)
   (optional) arbitrary identifier for the rack this server is part of.  Racks
diff --git a/lib/adm.js b/lib/adm.js
index 39aa96c..f0703c5 100644
--- a/lib/adm.js
+++ b/lib/adm.js
@@ -17,11 +17,13 @@ var assertplus = require('assert-plus');
 var fs = require('fs');
 var jsprim = require('jsprim');
 var net = require('net');
+var path = require('path');
 var sprintf = require('sprintf-js').sprintf;
 var tab = require('tab');
 var vasync = require('vasync');
 var VError = require('verror').VError;
 var MultiError = require('verror').MultiError;
+var fprintf = require('extsprintf').fprintf;
 var common = require('../lib/common');
 var deploy = require('../lib/deploy');
 var layout = require('./layout');
@@ -202,10 +204,10 @@ function zkColumnNames()
  * it doesn't matter what specific servers they're on.  This is mainly used in
  * development and testing.
  *
- * There are three supported use cases:
+ * There are four supported use cases:
  *
  *     o "genconfig" operation: call loadSdcConfig, then fetchDeployed, then
- *       dumpConfigCoal or dumpConfigLab.
+ *       one of dumpConfigCoal, dumpConfigLab, or genconfigFromFile.
  *
  *     o "show" operation: call loadSdcConfig, then fetchDeployed, then then one
  *       or more of the dumpDeployed* family of functions.
@@ -679,21 +681,32 @@ maAdm.prototype.dumpConfigCommonFini = function (conf, sout, serverUuid)
 
 /*
  * Given a server configuration file, suggest a layout of Manta services across
- * that file.  See manta-adm(1).  This function behaves like dumpConfigCoal()
- * and dumpConfigLab() in that it dumps a JSON form of the configuration to
- * the given stream and invokes the callback.
+ * that file.  See manta-adm(1).  The callback is invoked with an explicit
+ * error for problems unrelated to the layout itself, and a count of errors
+ * related to the layout.  These errors have already been written to
+ * "errstream".
+ *
+ * Either "outstream" (a stream) or "outDirectory" (a string) should be
+ * specified to indicate where the generated output should go.
  */
 maAdm.prototype.genconfigFromFile = function (args, callback)
 {
-	var images, filename, outstream, errstream;
+	var images, filename, outdir, outstream, errstream;
+	var svclayout;
 
 	assertplus.object(args, 'args');
 	assertplus.string(args.filename, 'args.filename');
-	assertplus.object(args.outstream, 'args.outstream');
+	assertplus.optionalString(args.outDirectory, 'args.outDirectory');
+	assertplus.optionalObject(args.outstream, 'args.outstream');
 	assertplus.object(args.errstream, 'args.errstream');
 
+	assertplus.ok(typeof (args.outDirectory) == 'string' ||
+	    (typeof (args.outstream) == 'object' && args.outstream !== null),
+	    'at least one of "outDirectory" or "outstream" must be specified');
+
 	images = this.latestImagesByService();
 	filename = args.filename;
+	outdir = args.outDirectory;
 	outstream = args.outstream;
 	errstream = args.errstream;
 
@@ -706,21 +719,100 @@ maAdm.prototype.genconfigFromFile = function (args, callback)
 	    },
 
 	    function generate(dcconfig, subcallback) {
-		var svclayout;
+		var azs, generated;
 
 		svclayout = layout.generateLayout({
 		    'dcconfig': dcconfig,
 		    'images': images
 		});
 
-		svclayout.serialize(outstream, errstream);
-		subcallback(null, svclayout.nerrors());
+		/*
+		 * We don't need to propagate errors directly because
+		 * printIssues() will print a human-readable description of them
+		 * and we'll pass the count of errors back to the caller below.
+		 */
+		if (svclayout.nerrors() > 0) {
+			subcallback();
+			return;
+		}
+
+		/*
+		 * The input validator ensures that there's at least one server,
+		 * so by the time we get here there must be at least one
+		 * availability zone.
+		 */
+		azs = svclayout.azs();
+		assertplus.ok(azs.length > 0);
+		if (typeof (args.outDirectory) != 'string') {
+			/*
+			 * The user specified an output stream, not a directory.
+			 * This can only work when there's one AZ specified in
+			 * the input.
+			 */
+			if (azs.length != 1) {
+				subcallback(new VError('output directory ' +
+				    'must be specified when generating ' +
+				    'a configuration with more than one ' +
+				    'availability zone'));
+				return;
+			}
+
+			/*
+			 * We've already checked for the ways this can fail when
+			 * we checked nerrors() above.
+			 */
+			generated = svclayout.serialize(azs[0]);
+			assertplus.string(generated);
+			outstream.write(generated);
+			subcallback();
+			return;
+		}
+
+		/*
+		 * The user specified an output directory.  We'll write one
+		 * output file for each availability zone that we find.
+		 */
+		vasync.forEachPipeline({
+		    'inputs': azs,
+		    'func': function writeOneConfig(azname, pipecb) {
+			var rv, outpath;
+
+			/*
+			 * As above, we've already checked the conditions that
+			 * would ever cause this to fail.
+			 */
+			rv = svclayout.serialize(azname);
+			assertplus.string(rv);
+			outpath = path.join(outdir, azname);
+			fs.writeFile(outpath, rv, function (err) {
+				if (err) {
+					err = new VError(
+					    err, 'write "%s"', outpath);
+				} else {
+					fprintf(errstream,
+					    'wrote config for "%s"\n', azname);
+				}
+
+				pipecb(err);
+			});
+		    }
+		}, function (err) {
+			if (!err && svclayout.nerrors() === 0) {
+				fprintf(errstream, '\nSummary of generated ' +
+				    'configuration:\n\n');
+				svclayout.printSummary(errstream);
+				fprintf(errstream, '\n');
+			}
+
+			subcallback(err);
+		});
 	    }
-	], function (err, nissues) {
+	], function (err) {
 		if (err) {
 			callback(err);
 		} else {
-			callback(null, nissues);
+			svclayout.printIssues(errstream);
+			callback(null, svclayout.nerrors());
 		}
 	}));
 };
@@ -2313,13 +2405,6 @@ maAdm.prototype.auditZkServers = function ()
  * Utility functions (which could be moved to a common file)
  */
 
-function fprintf(stream)
-{
-	var args = Array.prototype.slice.call(arguments, 1);
-	var msg = sprintf.apply(null, args);
-	stream.write(msg);
-}
-
 function sortObjectsByProps(rows, comparators)
 {
 	return (rows.sort(function (i1, i2) {
diff --git a/lib/common.js b/lib/common.js
index b14fe20..0ab83b2 100644
--- a/lib/common.js
+++ b/lib/common.js
@@ -597,6 +597,47 @@ function insert(obj, value, keys)
 	insert(obj[keys[0]], value, keys.slice(1));
 }
 
+/*
+ * Given a list of arrays, produce a single array with the elements from each
+ * array taken in round-robin fashion.  So given arrays A, B, and C, this
+ * produces [ A[0], B[0], C[0], A[1], B[1], C[1], A[2], ... ].  If any array is
+ * shorter than the others, that array is skipped once all of its elements have
+ * been used.  For example:
+ *
+ *     stripe([ [ 1, 2, 3 ], [ 4, 5, 6, 7, 8 ] ])
+ *
+ * returns
+ *
+ *     [ 1, 4, 2, 5, 3, 6, 7, 8 ]
+ */
+function stripe(lists)
+{
+	var nextelt, rv, li, rvlength;
+
+	assert.arrayOfArray(lists, 'lists');
+	nextelt = new Array(lists.length);
+	rvlength = 0;
+	for (li = 0; li < lists.length; li++) {
+		nextelt[li] = 0;
+		rvlength += lists[li].length;
+	}
+
+	if (lists.length === 0) {
+		return ([]);
+	}
+
+	rv = [];
+	for (li = 0; rv.length < rvlength; li = (li + 1) % lists.length) {
+		if (nextelt[li] >= lists[li].length) {
+			continue;
+		}
+
+		rv.push(lists[li][nextelt[li]++]);
+	}
+
+	return (rv);
+}
+
 exports.shuffle = shuffle;
 exports.domainToPath = domainToPath;
 exports.initLogger = initLogger;
@@ -609,3 +650,4 @@ exports.findServerUuid = findServerUuid;
 exports.getOrCreateComputeId = getOrCreateComputeId;
 exports.commandExecute = commandExecute;
 exports.insert = insert;
+exports.stripe = stripe;
diff --git a/lib/layout.js b/lib/layout.js
index 88a0358..d50d342 100644
--- a/lib/layout.js
+++ b/lib/layout.js
@@ -36,9 +36,11 @@
 var assertplus = require('assert-plus');
 var fs = require('fs');
 var jsprim = require('jsprim');
+var tab = require('tab');
 var vasync = require('vasync');
 var VError = require('verror').VError;
 
+var common = require('./common');
 var services = require('./services');
 
 /* Public interface */
@@ -47,10 +49,7 @@ exports.generateLayout = generateLayout;
 
 /*
  * The parameters below configure broadly how we design a layout of Manta
- * services for a given set of availability zones, racks, and servers.  Right
- * now, we only support one availability zone, but we could extend the basic
- * approach used here to stripe across availability zones similar to the way it
- * currently stripes across racks.
+ * services for a given set of availability zones, racks, and servers.
  */
 
 /*
@@ -218,7 +217,7 @@ var ML_SCHEMA = {
  */
 function generateLayout(args)
 {
-	var dcconfig, images, layout;
+	var dcconfig, images, layout, extrametadata, extrastorage;
 
 	assertplus.object(args, 'args');
 	assertplus.object(args.dcconfig, 'args.dcconfig');
@@ -241,8 +240,7 @@ function generateLayout(args)
 		images[svcname] = image;
 	});
 
-	layout = new Layout();
-	layout.ml_dcconfig = dcconfig;
+	layout = new Layout(dcconfig);
 
 	if (dcconfig.dc_servers_metadata.length === 0 ||
 	    dcconfig.dc_servers_storage.length === 0) {
@@ -251,13 +249,38 @@ function generateLayout(args)
 		return (layout);
 	}
 
-	if (dcconfig.dc_az_names.length > 1) {
-		layout.ml_errors.push(new VError('multi-datacenter ' +
-		    'deployments are not yet supported by this tool'));
+	if (dcconfig.dc_az_names.length != 1 &&
+	    dcconfig.dc_az_names.length != 3) {
+		layout.ml_errors.push(new VError('only one- and three-' +
+		    'datacenter deployments are supported'));
 		return (layout);
 	}
 
-	if (dcconfig.dc_nshards > dcconfig.dc_servers_metadata.length) {
+	extrametadata = null;
+	extrastorage = null;
+	jsprim.forEachKey(dcconfig.dc_azs, function (azname, az) {
+		if (az.rsaz_nmetadata !== dcconfig.dc_min_nmetadata_perdc) {
+			extrametadata = azname;
+		}
+
+		if (az.rsaz_nstorage !== dcconfig.dc_min_nstorage_perdc) {
+			extrastorage = azname;
+		}
+	});
+
+	if (extrametadata !== null) {
+		layout.ml_warnings.push(new VError('datacenters have ' +
+		    'different numbers of metadata servers.  The impact of a ' +
+		    'datacenter failure will differ depending on which ' +
+		    'datacenter fails.'));
+	}
+
+	if (extrastorage !== null) {
+		layout.ml_warnings.push(new VError('datacenters have ' +
+		    'different numbers of storage servers.'));
+	}
+
+	if (dcconfig.dc_nshards > dcconfig.dc_min_nmetadata_perdc) {
 		/*
 		 * It doesn't make much sense to have more shards than metadata
 		 * servers.  If you know at least two Manatee primaries will
@@ -270,14 +293,15 @@ function generateLayout(args)
 		 * result, this is not a fatal error.
 		 */
 		layout.ml_warnings.push(new VError(
-		    'requested %d shards with only %d metadata server%s.  ' +
+		    'requested %d shards with only %d metadata server%s in ' +
+		    'at least one datacenter.  ' +
 		    'Multiple primary databases will wind up running on the ' +
 		    'same servers, and this configuration may not survive ' +
 		    'server failure.  This is not recommended.',
-		    dcconfig.dc_nshards, dcconfig.dc_servers_metadata.length,
-		    dcconfig.dc_servers_metadata.length == 1 ? '' : 's'));
+		    dcconfig.dc_nshards, dcconfig.dc_min_nmetadata_perdc,
+		    dcconfig.dc_min_nmetadata_perdc == 1 ? '' : 's'));
 	} else if (ML_NPERSHARD_INSTANCES * dcconfig.dc_nshards >
-	    dcconfig.dc_servers_metadata.length) {
+	    dcconfig.dc_az_names.length * dcconfig.dc_min_nmetadata_perdc) {
 		/*
 		 * Strictly speaking, this case is just as bad as the previous
 		 * one because you can wind up in the same state, with multiple
@@ -287,11 +311,12 @@ function generateLayout(args)
 		 * message is a little softer, but basically the same.
 		 */
 		layout.ml_warnings.push(new VError(
-		    'requested %d shards with only %d metadata server%s.  ' +
-		    'Under some conditions, multiple databases may wind up ' +
+		    'requested %d shards with only %d metadata server%s in ' +
+		    'at least one datacenter.  Under some conditions, ' +
+		    'multiple databases may wind up ' +
 		    'running on the same servers.  This is not recommended.',
-		    dcconfig.dc_nshards, dcconfig.dc_servers_metadata.length,
-		    dcconfig.dc_servers_metadata.length == 1 ? '' : 's'));
+		    dcconfig.dc_nshards, dcconfig.dc_min_nmetadata_perdc,
+		    dcconfig.dc_min_nmetadata_perdc == 1 ? '' : 's'));
 	}
 
 	if (dcconfig.dc_rack_names.length < ML_NPERSHARD_INSTANCES) {
@@ -522,6 +547,7 @@ DcConfigLoader.prototype.parseFromJson = function ()
 DcConfigLoader.prototype.parse = function ()
 {
 	var dcconfig, err, svcname;
+	var racknames;
 	var self = this;
 
 	if (this.dcl_errors.length === 0) {
@@ -596,12 +622,15 @@ DcConfigLoader.prototype.parse = function ()
 			dcconfig.dc_az_names.push(az);
 			dcconfig.dc_azs[az] = {
 			    'rsaz_name': az,
-			    'rsaz_rack_names': []
+			    'rsaz_rack_names': [],
+			    'rsaz_nstorage': 0,
+			    'rsaz_nmetadata': 0
 			};
 		}
 
 		if (!dcconfig.dc_racks.hasOwnProperty(rackname)) {
 			dcconfig.dc_rack_names.push(rackname);
+			dcconfig.dc_azs[az].rsaz_rack_names.push(rackname);
 			rack = dcconfig.dc_racks[rackname] = {
 			    'rsrack_az': az,
 			    'rsrack_name': rackname,
@@ -634,10 +663,12 @@ DcConfigLoader.prototype.parse = function ()
 		if (type == 'metadata') {
 			rack.rsrack_servers_metadata.push(cn);
 			dcconfig.dc_servers_metadata.push(cn);
+			dcconfig.dc_azs[az].rsaz_nmetadata++;
 		} else {
 			assertplus.equal(type, 'storage');
 			rack.rsrack_servers_storage.push(cn);
 			dcconfig.dc_servers_storage.push(cn);
+			dcconfig.dc_azs[az].rsaz_nstorage++;
 		}
 	});
 
@@ -646,6 +677,38 @@ DcConfigLoader.prototype.parse = function ()
 		return;
 	}
 
+	jsprim.forEachKey(dcconfig.dc_azs, function (_, az) {
+		if (dcconfig.dc_min_nmetadata_perdc === null ||
+		    dcconfig.dc_min_nmetadata_perdc > az.rsaz_nmetadata) {
+			dcconfig.dc_min_nmetadata_perdc = az.rsaz_nmetadata;
+		}
+
+		if (dcconfig.dc_min_nstorage_perdc === null ||
+		    dcconfig.dc_min_nstorage_perdc > az.rsaz_nstorage) {
+			dcconfig.dc_min_nstorage_perdc = az.rsaz_nstorage;
+		}
+	});
+
+	assertplus.number(dcconfig.dc_min_nmetadata_perdc);
+	assertplus.number(dcconfig.dc_min_nstorage_perdc);
+
+	/*
+	 * Construct the list of rack names by sorting the rack names within
+	 * each AZ and then selecting racks from each AZ's list.  By doing this,
+	 * spreading zones across racks is enough to also spread them across
+	 * AZs.
+	 */
+	jsprim.forEachKey(dcconfig.dc_azs, function (_, az) {
+		az.rsaz_rack_names.sort();
+	});
+	racknames = common.stripe.call(null,
+	    dcconfig.dc_az_names.map(function (azname) {
+		return (dcconfig.dc_azs[azname].rsaz_rack_names);
+	    }));
+	assertplus.deepEqual(racknames.slice(0).sort(),
+	    dcconfig.dc_rack_names.sort());
+	dcconfig.dc_rack_names = racknames;
+
 	assertplus.deepEqual(Object.keys(dcconfig.dc_azs).sort(),
 	    dcconfig.dc_az_names.slice(0).sort());
 	assertplus.deepEqual(Object.keys(dcconfig.dc_racks).sort(),
@@ -689,7 +752,9 @@ function DatacenterConfig()
 	 * properties:
 	 *
 	 *    rsaz_name       (string) name of this availability zone
-	 *    rsaz_rack_names (array)  list of rack identifiers in this AZ.
+	 *    rsaz_rack_names (array)  list of rack identifiers in this AZ
+	 *    rsaz_nmetadata  (num)    count of metadata servers in this AZ
+	 *    rsaz_nstorage   (num)    count of storage servers in this AZ
 	 */
 	/* list of az names in this region */
 	this.dc_az_names = [];
@@ -730,6 +795,11 @@ function DatacenterConfig()
 	/* list of storage server names */
 	this.dc_servers_storage = [];
 
+	/* minimum count of metadata servers across all DCs */
+	this.dc_min_nmetadata_perdc = null;
+	/* minimum count of storage servers across all DCs */
+	this.dc_min_nstorage_perdc = null;
+
 	/* Number of metadata shards */
 	this.dc_nshards = null;
 	/* Image overrides */
@@ -742,16 +812,18 @@ function DatacenterConfig()
  * all services should be deployed on each server within a region.  Private
  * interfaces are provided here to build up the layout, but once it's
  * constructed, it's immutable.  The only methods exposed to other files in this
- * module are serialize() and nerrors().
+ * module are azs(), serialize(), printIssues(), printSummary(), and nerrors().
  *
  * This is effectively a programmatic representation of the "manta-adm update"
  * data structure.  Some code is shared between them, but more functionality
  * could be commonized.
  */
-function Layout()
+function Layout(dcconfig)
 {
+	var self = this;
+
 	/* reference to dc config containing detailed metadata */
-	this.ml_dcconfig = null;
+	this.ml_dcconfig = dcconfig;
 
 	/* non-fatal problems with this layout */
 	this.ml_warnings = [];
@@ -767,6 +839,13 @@ function Layout()
 	 */
 	this.ml_configs_byserver = {};
 
+	/*
+	 * mapping of svcname -> az -> ServiceConfiguration
+	 *
+	 * Describes the instances deployed in each AZ.
+	 */
+	this.ml_configs_bysvcname_az = {};
+
 	/*
 	 * mapping of service name -> ServiceConfiguration
 	 *
@@ -775,27 +854,30 @@ function Layout()
 	this.ml_configs_bysvcname = {};
 
 	/*
-	 * Mutable state used for allocation.  We keep track of the rack and
-	 * server to use for the next allocation, and we track this separately
-	 * for each allocation class.  See allocateMetadataCn() for details.
+	 * Mutable state used for allocation.
+	 *
+	 * For metadata services, we construct a list of servers up front that
+	 * stripes across racks, which are already sorted so that they stripe
+	 * across AZs.  Then we just allocate from this list in order, cycling
+	 * back when we run out.  See the comment in allocateMetadataCn().
 	 */
-	this.ml_racki = {};
-	this.ml_serveri = {};
+	this.ml_metadata_i = {};
+	this.ml_metadata_striped = common.stripe(
+	    this.ml_dcconfig.dc_rack_names.map(function (rackname) {
+		return (self.ml_dcconfig.dc_racks[
+		    rackname].rsrack_servers_metadata);
+	    }));
 }
 
 /*
  * Returns the uuid of the server that should be used for the next allocation of
- * class "alloc_class".  Internal state is modified so that subsequent
- * server allocations take this allocation into account, but this method does
- * not actually assign an instance to this server.  You have to call
+ * class "alloc_class".  Internal state is modified so that subsequent server
+ * allocations take this allocation into account, but this method does not
+ * actually assign an instance to this server.  You have to call
  * allocateInstance() with the returned server uuid in order to do that.
  *
- * This allocator stripes metadata services across all of the metadata servers.
- * By that, we mean that we'll put one instance on a metadata server, then
- * another on the next server, and so on until we need no more instances.  In
- * order to maximize the likelihood of surviving rack failure, we actually
- * stripe across _racks_, picking one server from each rack.  When we come to
- * this rack again, we'll pick the next server in the rack.  This process is
+ * This allocator stripes services across all of the metadata servers,
+ * preferring to use servers in different racks when possible.  This process is
  * deterministic.  Allocations look like this:
  *
  *     rack 0, server 0
@@ -809,55 +891,38 @@ function Layout()
  *     rack 0, server 2
  *     ...
  *
- * We also support some racks being smaller than other racks.  In that case, we
- * still put the same number of services in each rack to maximize availability,
- * so they will be more densely packed in smaller racks.
+ * If some racks have fewer servers than others, those racks will wind up with
+ * fewer total instances.  This likely means that per-rack network utilization
+ * and the percentage of total capacity lost after a rack failure will vary
+ * across racks.
  *
  * The "alloc_class" is an arbitrary string token used to support orthogonal
  * groups of allocations.  The above describes what happens for sequential
  * allocations from the same "alloc_class".  Allocations for a new "alloc_class"
- * start over at rack 0, server 0.
+ * start over at rack 0, server 0, and these can be mixed with allocations for
+ * other values of "alloc_class".
  */
 Layout.prototype.allocateMetadataCn = function (alloc_class)
 {
-	var ri, rackname, rack, rackservers, si, cnid;
+	var which;
 
-	if (!this.ml_racki.hasOwnProperty(alloc_class)) {
-		this.ml_racki[alloc_class] = 0;
-		this.ml_serveri[alloc_class] = 0;
+	if (!this.ml_metadata_i.hasOwnProperty(alloc_class)) {
+		this.ml_metadata_i[alloc_class] = 0;
 	}
 
-	ri = this.ml_racki[alloc_class];
-	si = this.ml_serveri[alloc_class];
-
-	for (;;) {
-		assertplus.ok(ri < this.ml_dcconfig.dc_rack_names.length);
-		rackname = this.ml_dcconfig.dc_rack_names[ri];
-		assertplus.string(rackname);
-		rack = this.ml_dcconfig.dc_racks[rackname];
-		rackservers = rack.rsrack_servers_metadata;
-		if (rackservers.length !== 0) {
-			break;
-		}
-
-		if (++ri == this.ml_dcconfig.dc_rack_names.length) {
-			ri = 0;
-			si++;
-		}
-	}
-
-	cnid = rackservers[si % rackservers.length];
-	assertplus.string(cnid);
-
-	if (++ri == this.ml_dcconfig.dc_rack_names.length) {
-		ri = 0;
-		si++;
-	}
-
-	this.ml_racki[alloc_class] = ri;
-	this.ml_serveri[alloc_class] = si;
+	which = (this.ml_metadata_i[alloc_class]++) %
+	    this.ml_metadata_striped.length;
+	return (this.ml_metadata_striped[which]);
+};
 
-	return (cnid);
+Layout.prototype.cnidToAzName = function (cnid)
+{
+	var server, rack;
+	server = this.ml_dcconfig.dc_servers[cnid];
+	assertplus.ok(server);
+	rack = this.ml_dcconfig.dc_racks[server.rscn_rack];
+	assertplus.ok(rack);
+	return (rack.rsrack_az);
 };
 
 /*
@@ -867,6 +932,8 @@ Layout.prototype.allocateMetadataCn = function (alloc_class)
  */
 Layout.prototype.allocateInstance = function (cnid, svcname, config)
 {
+	var azname;
+
 	assertplus.string(cnid);
 	assertplus.string(svcname);
 
@@ -885,27 +952,57 @@ Layout.prototype.allocateInstance = function (cnid, svcname, config)
 		this.ml_configs_bysvcname[svcname] =
 		    new services.ServiceConfiguration(
 		    services.serviceConfigProperties(svcname));
+		this.ml_configs_bysvcname_az[svcname] = {};
 	}
 
 	this.ml_configs_bysvcname[svcname].incr(config);
+
+	azname = this.cnidToAzName(cnid);
+	if (!this.ml_configs_bysvcname_az[svcname].hasOwnProperty(azname)) {
+		this.ml_configs_bysvcname_az[svcname][azname] =
+		    new services.ServiceConfiguration(
+		    services.serviceConfigProperties(svcname));
+	}
+
+	this.ml_configs_bysvcname_az[svcname][azname].incr(config);
 };
 
 /*
- * Writes a JSON description of the generated layout to "outstream".  This
- * description is suitable for use by "manta-adm update".  A human-readable
- * summary of problems with the generated configuration are written to
- * "errstream".
+ * Returns the list of datacenter names laid out in this configuration.
  */
-Layout.prototype.serialize = function (outstream, errstream)
+Layout.prototype.azs = function ()
 {
-	var config, cnids;
-	var self = this;
+	return (this.ml_dcconfig.dc_az_names.slice(0));
+};
 
+/*
+ * Prints to "errstream" a list of problems encountered generating this
+ * configuration.
+ */
+Layout.prototype.printIssues = function (errstream)
+{
 	if (this.ml_errors.length > 0) {
 		this.ml_errors.forEach(function (err) {
 			errstream.write('error: ' + err.message + '\n');
 		});
-		return;
+	} else if (this.ml_warnings.length > 0) {
+		this.ml_warnings.forEach(function (err) {
+			errstream.write('warning: ' + err.message + '\n');
+		});
+	}
+};
+
+/*
+ * Returns a JSON description of the generated layout for datacenter "azname".
+ * This description is suitable for use by "manta-adm update".
+ */
+Layout.prototype.serialize = function (azname)
+{
+	var config, cnids;
+	var self = this;
+
+	if (this.nerrors() > 0) {
+		return (null);
 	}
 
 	/*
@@ -918,6 +1015,10 @@ Layout.prototype.serialize = function (outstream, errstream)
 	cnids.forEach(function (cnid) {
 		var cfgs, svcnames;
 
+		if (self.cnidToAzName(cnid) != azname) {
+			return;
+		}
+
 		config[cnid] = {};
 		assertplus.ok(self.ml_configs_byserver.hasOwnProperty(cnid));
 		cfgs = self.ml_configs_byserver[cnid];
@@ -928,13 +1029,7 @@ Layout.prototype.serialize = function (outstream, errstream)
 		});
 	});
 
-	outstream.write(JSON.stringify(config, null, '    ') + '\n');
-
-	if (this.ml_warnings.length > 0) {
-		this.ml_warnings.forEach(function (err) {
-			errstream.write('warning: ' + err.message + '\n');
-		});
-	}
+	return (JSON.stringify(config, null, '    ') + '\n');
 };
 
 /*
@@ -944,3 +1039,126 @@ Layout.prototype.nerrors = function ()
 {
 	return (this.ml_errors.length);
 };
+
+/*
+ * Prints to "outstream" a summary of the configuration for sanity-checking.
+ */
+Layout.prototype.printSummary = function (outstream)
+{
+	var columns, out, cfgs;
+	var svcnames;
+
+	if (this.nerrors() > 0) {
+		return (null);
+	}
+
+	columns = [ {
+	    'label': '',
+	    'width': 4
+	}, {
+	    'label': 'SERVICE',
+	    'width': 16
+	}, {
+	    'label': 'SHARD',
+	    'align': 'right',
+	    'width': 5
+	} ];
+
+	this.azs().forEach(function (azname) {
+		columns.push({
+		    'label': azname,
+		    'align': 'right',
+		    'width': 16
+		});
+	});
+
+	out = new tab.TableOutputStream({
+	    'stream': outstream,
+	    'columns': columns
+	});
+
+	/*
+	 * We'll print the services in this table in the order they normally
+	 * appear in "manta-adm" output, except that we'll put the sharded ones
+	 * last.
+	 *
+	 * Each non-sharded service has one row in the table showing counts of
+	 * all instances of that service in each AZ.
+	 */
+	cfgs = this.ml_configs_bysvcname_az;
+	svcnames = services.mSvcNames.filter(
+	    function (a) { return (!services.serviceIsSharded(a)); });
+	svcnames.forEach(function (svcname) {
+		var row;
+
+		row = {};
+		row['SERVICE'] = svcname;
+		row['SHARD'] = '-';
+		jsprim.forEachKey(cfgs[svcname], function (azname, svccfg) {
+			svccfg.each(function (cfg) {
+				/*
+				 * There should be only one config per service
+				 * per AZ because we would have used the same
+				 * image for all services.
+				 */
+				assertplus.ok(!row.hasOwnProperty(azname));
+				assertplus.number(cfg['count']);
+				row[azname] = cfg['count'];
+			});
+		});
+
+		out.writeRow(row);
+	});
+
+	/*
+	 * Sharded services have one row per shard showing the counts of
+	 * instances of that service for that shard in each AZ.
+	 */
+	svcnames = services.mSvcNames.filter(services.serviceIsSharded);
+	svcnames.forEach(function (svcname) {
+		/*
+		 * The only way we have to iterate shards is within a single AZ,
+		 * so we collect information in "rowsbyshard" first and then
+		 * emit all of the rows when we're done.
+		 */
+		var rowsbyshard = {};
+		jsprim.forEachKey(cfgs[svcname], function (azname, svccfg) {
+			svccfg.each(function (cfg) {
+				var shard, shardrow;
+
+				shard = cfg['SH'];
+				assertplus.number(shard);
+				if (rowsbyshard.hasOwnProperty(shard)) {
+					shardrow = rowsbyshard[shard];
+					/*
+					 * Like the case above, we should not
+					 * have already seen this
+					 * service/AZ/shard combination because
+					 * that would imply that we were using
+					 * more than one image for this service.
+					 */
+					assertplus.ok(!shardrow.hasOwnProperty(
+					    azname));
+				} else {
+					shardrow = {};
+					shardrow['SERVICE'] = svcname;
+					shardrow['SHARD'] = shard;
+					rowsbyshard[shard] = shardrow;
+				}
+
+				shardrow[azname] = cfg['count'];
+			});
+		});
+
+		/*
+		 * Shards are identified by numbers stored as strings.  To sort
+		 * them the way a person would like, we need to parse them as
+		 * integers and compare those.
+		 */
+		Object.keys(rowsbyshard).sort(function (a, b) {
+			return (parseInt(a, 10) - parseInt(b, 10));
+		}).forEach(function (shard) {
+			out.writeRow(rowsbyshard[shard]);
+		});
+	});
+};
diff --git a/man/man1/manta-adm.1 b/man/man1/manta-adm.1
index 978f165..ace8a97 100644
--- a/man/man1/manta-adm.1
+++ b/man/man1/manta-adm.1
@@ -8,7 +8,7 @@ manta\-adm \- administer a Manta deployment
 .PP
 \fB\fCmanta\-adm genconfig "lab" | "coal"\fR
 .PP
-\fB\fCmanta\-adm genconfig \-\-from\-file=FILE\fR
+\fB\fCmanta\-adm genconfig [\-\-directory=DIR] \-\-from\-file=FILE\fR
 .PP
 \fB\fCmanta\-adm show [\-l LOG_FILE] [\-a] [\-c] [\-H] [\-o FIELD...] [\-s] SERVICE\fR
 .PP
@@ -191,7 +191,7 @@ Example: list hostnames in form suitable for "sdc\-oneachnode \-n":
 .PP
 \fB\fCmanta\-adm genconfig "lab" | "coal"\fR
 .PP
-\fB\fCmanta\-adm genconfig \-\-from\-file=FILE\fR
+\fB\fCmanta\-adm genconfig [\-\-directory=DIR] \-\-from\-file=FILE\fR
 .PP
 The \fB\fCmanta\-adm genconfig\fR subcommand generates a JSON configuration file
 suitable for use with \fB\fCmanta\-adm update\fR\&.  The images used for each service are
@@ -201,21 +201,36 @@ so this command is sometimes used as a shortcut for identifying
 the latest images that have been fetched for each service.
 .PP
 When the first argument is \fB\fC"coal"\fR, the command produces a configuration
-suitable for a small VM\-in\-a\-laptop deployment.
+suitable for a small VM\-in\-a\-laptop deployment.  The configuration is always
+emitted to stdout.
 .PP
 When the first argument is \fB\fC"lab"\fR, the command produces a configuration
-suitable for a larger single\-server install.
-.PP
-The \fB\fC\-\-from\-file=FILE\fR option can be used to generate a configuration suitable
-for a much larger deployment.  This form is somewhat experimental, but attempts
-to create a deployment that will survive failures of any component, server, or
-rack.  \fB\fCFILE\fR is a JSON file describing the parameters of the deployment,
-including the number of metadata shards and the set of availability zones,
-racks, and servers.  You can omit availability zone and rack information.  The
-JSON file should represent a single object with properties:
+suitable for a larger single\-server install.  The configuration is always
+emitted to stdout.
+.PP
+The \fB\fC\-\-from\-file=FILE\fR form can be used to generate a configuration suitable for
+a much larger, production\-style deployment.  \fB\fCFILE\fR is a JSON file in the format
+specified below that describes the parameters of the deployment, including the
+number of metadata shards and the set of availability zones, racks, and servers.
+This form attempts to create a deployment that will survive failures of any
+component, server, rack, or availability zone as long as sufficient servers,
+racks, and availability zones are included in the input file.  Availability zone
+and rack information can be omitted from the file, in which case the tool will
+generate a configuration ignoring rack\-level and AZ\-level considerations.  This
+tool uses a number of heuristics, and the output should be verified.
+.PP
+By default, the generated configuration is emitted to stdout.  With the
+\fB\fC\-\-directory\fR option, the configuration will be written to files in the
+specified directory named by availability zone.  This option must be used if the
+servers in \fB\fCFILE\fR span more than one availability zone.
+.PP
+The input JSON file \fB\fCFILE\fR should contain a single object with properties:
 .TP
 \fB\fCnshards\fR (positive integer)
-the number of metadata shards to create
+the number of database shards to create, which is usually one more than the
+number of shards that are intended to store object metadata (in order to
+accommodate jobs and low\-volume system metadata that's typically stored in
+shard 1)
 .TP
 \fB\fCservers\fR (array of objects)
 the list of servers available for deployment
@@ -237,10 +252,8 @@ gigabytes of memory available on this server.  This is currently only used for
 storage servers to determine the appropriate number of compute zones.
 .TP
 \fB\fCaz\fR (string)
-(optional) availability zone.  This version of the tool only works for a
-single\-availability\-zone deployment, so all servers must have the same value
-here.  If the value is omitted from any server, that server is placed into a
-default availablity zone.
+(optional) availability zone.  If the value is omitted from any server, that
+server is placed into a default availablity zone.
 .TP
 \fB\fCrack\fR (string)
 (optional) arbitrary identifier for the rack this server is part of.  Racks
diff --git a/test/tst.layout.js b/test/tst.layout.js
index c156cc1..70b0161 100644
--- a/test/tst.layout.js
+++ b/test/tst.layout.js
@@ -222,6 +222,26 @@ var testcases = [ {
 	    mkserver('storage',  0, 0)
 	]
     }
+}, {
+    'name': 'one rack with fewer servers',
+    'config': {
+	'nshards': 2,
+	'servers': [
+	    mkserver('metadata', 0, 0),
+	    mkserver('metadata', 0, 1),
+	    mkserver('metadata', 0, 2),
+	    mkserver('storage',  0, 0),
+
+	    mkserver('metadata', 1, 0),
+	    mkserver('metadata', 1, 1),
+	    mkserver('metadata', 1, 2),
+	    mkserver('storage',  1, 0),
+
+	    mkserver('metadata', 2, 0),
+	    mkserver('metadata', 2, 2),
+	    mkserver('storage',  2, 0)
+	]
+    }
 }, {
     'name': '3-rack, 4-shard deployment',
     'config': {
@@ -249,6 +269,63 @@ var testcases = [ {
 	    mkserver('storage',  2, 1)
 	]
     }
+}, {
+    'name': '3-AZ, 1-rack-per-AZ, 1-shard deployment',
+    'config': {
+	'nshards': 1,
+	'servers': [
+	    mkserver('metadata', 0, 0, 0),
+	    mkserver('storage',  0, 0, 0),
+
+	    mkserver('metadata', 0, 0, 1),
+	    mkserver('storage',  0, 0, 1),
+
+	    mkserver('metadata', 0, 0, 2),
+	    mkserver('storage',  0, 0, 2)
+	]
+    }
+}, {
+    'name': '3-AZ, 1-rack-per-AZ, 2-shard deployment',
+    'config': {
+	'nshards': 2,
+	'servers': [
+	    mkserver('metadata', 0, 0, 0),
+	    mkserver('storage',  0, 0, 0),
+
+	    mkserver('metadata', 0, 0, 1),
+	    mkserver('storage',  0, 0, 1),
+
+	    mkserver('metadata', 0, 0, 2),
+	    mkserver('storage',  0, 0, 2)
+	]
+    }
+}, {
+    'name': '3-AZ, 2-racks-per-AZ, 3-shard deployment',
+    'config': {
+	'nshards': 3,
+	'servers': [
+	    mkserver('metadata', 0, 0, 0),
+	    mkserver('storage',  0, 0, 0),
+	    mkserver('metadata', 0, 1, 0),
+	    mkserver('storage',  0, 1, 0),
+	    mkserver('metadata', 0, 2, 0),
+	    mkserver('storage',  0, 2, 0),
+
+	    mkserver('metadata', 0, 0, 1),
+	    mkserver('storage',  0, 0, 1),
+	    mkserver('metadata', 0, 1, 1),
+	    mkserver('storage',  0, 1, 1),
+	    mkserver('metadata', 0, 2, 1),
+	    mkserver('storage',  0, 2, 1),
+
+	    mkserver('metadata', 0, 0, 2),
+	    mkserver('storage',  0, 0, 2),
+	    mkserver('metadata', 0, 1, 2),
+	    mkserver('storage',  0, 1, 2),
+	    mkserver('metadata', 0, 2, 2),
+	    mkserver('storage',  0, 2, 2)
+	]
+    }
 } ];
 
 /*
@@ -289,12 +366,13 @@ function main()
 }
 
 /*
- * Generate an object representing a server of type "role" in rack "racknum".
- * This will be server "role" + "servernum" within this rack.
+ * Generate an object representing a server of type "role" in rack "racknum",
+ * and optionally in availability zone "aznum".  This will be server "role" +
+ * "servernum" within this rack.
  */
-function mkserver(role, racknum, servernum)
+function mkserver(role, racknum, servernum, aznum)
 {
-	var rack, cnid;
+	var rack, cnid, server, azname;
 
 	assertplus.ok([ 'metadata', 'storage' ].indexOf(role) != -1);
 	assertplus.number(racknum);
@@ -303,16 +381,32 @@ function mkserver(role, racknum, servernum)
 	assertplus.number(servernum);
 	assertplus.ok(servernum >= 0);
 	assertplus.ok(servernum < 100);
+	assertplus.optionalNumber(aznum);
+
+	if (typeof (aznum) == 'number') {
+		azname = 'az' + aznum;
+		rack = azname + '_';
+		cnid = azname + '_';
+	} else {
+		rack = '';
+		cnid = '';
+	}
 
-	rack = sprintf('rack_r%02d', racknum);
-	cnid = sprintf('server_r%02d_%s%02d', racknum, role, servernum);
+	rack += sprintf('rack_r%02d', racknum);
+	cnid += sprintf('server_r%02d_%s%02d', racknum, role, servernum);
 
-	return ({
+	server = {
 	    'type': role,
 	    'uuid': cnid,
 	    'memory': 64,
 	    'rack': rack
-	});
+	};
+
+	if (azname !== undefined) {
+		server['az'] = azname;
+	}
+
+	return (server);
 }
 
 /*
@@ -448,7 +542,7 @@ function runTestCaseLoadDirectly(tcstate, callback)
  */
 function runTestCaseGenerate(tcstate, callback)
 {
-	var svclayout;
+	var svclayout, rv, azs;
 
 	/* See notes about error handling above. */
 	if (tcstate.tc_dcconfig !== null) {
@@ -460,7 +554,19 @@ function runTestCaseGenerate(tcstate, callback)
 		});
 
 		console.log('\ngenerated config:');
-		svclayout.serialize(process.stdout, process.stdout);
+		azs = svclayout.azs();
+		azs.forEach(function (azname, i) {
+			rv = svclayout.serialize(azname);
+			if (rv !== null) {
+				process.stdout.write(rv);
+			}
+		});
+
+		svclayout.printIssues(process.stdout);
+		if (svclayout.nerrors() === 0) {
+			console.log('\nsummary:');
+			svclayout.printSummary(process.stdout);
+		}
 	}
 
 	console.log(separator);
diff --git a/test/tst.layout.js.out b/test/tst.layout.js.out
index 6fb70af..d50b725 100644
--- a/test/tst.layout.js.out
+++ b/test/tst.layout.js.out
@@ -211,7 +211,29 @@ generated config:
         }
     }
 }
-warning: requested 3 shards with only 3 metadata servers.  Under some conditions, multiple databases may wind up running on the same servers.  This is not recommended.
+warning: requested 3 shards with only 3 metadata servers in at least one datacenter.  Under some conditions, multiple databases may wind up running on the same servers.  This is not recommended.
+
+summary:
+     SERVICE          SHARD       default_az
+     nameservice          -                3
+     electric-moray       -                3
+     storage              -                2
+     authcache            -                2
+     webapi               -                3
+     loadbalancer         -                3
+     jobsupervisor        -                2
+     jobpuller            -                2
+     medusa               -                2
+     ops                  -                1
+     madtom               -                1
+     marlin-dashboard     -                1
+     marlin               -               32
+     postgres             1                3
+     postgres             2                3
+     postgres             3                3
+     moray                1                3
+     moray                2                3
+     moray                3                3
 --------------------------------------------------
 --------------------------------------------------
 test case: invalid config: missing value (nshards)
@@ -490,7 +512,7 @@ input: {
 }
 
 generated config:
-error: multi-datacenter deployments are not yet supported by this tool
+error: only one- and three-datacenter deployments are supported
 --------------------------------------------------
 --------------------------------------------------
 test case: trivial two-system case, 1 shard
@@ -568,8 +590,26 @@ generated config:
         }
     }
 }
-warning: requested 1 shards with only 1 metadata server.  Under some conditions, multiple databases may wind up running on the same servers.  This is not recommended.
+warning: requested 1 shards with only 1 metadata server in at least one datacenter.  Under some conditions, multiple databases may wind up running on the same servers.  This is not recommended.
 warning: configuration has only 1 rack.  This configuration may not survive rack failure.
+
+summary:
+     SERVICE          SHARD       default_az
+     nameservice          -                3
+     electric-moray       -                2
+     storage              -                1
+     authcache            -                2
+     webapi               -                2
+     loadbalancer         -                2
+     jobsupervisor        -                2
+     jobpuller            -                2
+     medusa               -                2
+     ops                  -                1
+     madtom               -                1
+     marlin-dashboard     -                1
+     marlin               -               16
+     postgres             1                3
+     moray                1                3
 --------------------------------------------------
 --------------------------------------------------
 test case: trivial two-system case, 3 shards
@@ -659,13 +699,35 @@ generated config:
         }
     }
 }
-warning: requested 3 shards with only 1 metadata server.  Multiple primary databases will wind up running on the same servers, and this configuration may not survive server failure.  This is not recommended.
+warning: requested 3 shards with only 1 metadata server in at least one datacenter.  Multiple primary databases will wind up running on the same servers, and this configuration may not survive server failure.  This is not recommended.
 warning: configuration has only 1 rack.  This configuration may not survive rack failure.
+
+summary:
+     SERVICE          SHARD       default_az
+     nameservice          -                3
+     electric-moray       -                2
+     storage              -                1
+     authcache            -                2
+     webapi               -                2
+     loadbalancer         -                2
+     jobsupervisor        -                2
+     jobpuller            -                2
+     medusa               -                2
+     ops                  -                1
+     madtom               -                1
+     marlin-dashboard     -                1
+     marlin               -               16
+     postgres             1                3
+     postgres             2                3
+     postgres             3                3
+     moray                1                3
+     moray                2                3
+     moray                3                3
 --------------------------------------------------
 --------------------------------------------------
-test case: 3-rack, 4-shard deployment
+test case: one rack with fewer servers
 input: {
-    "nshards": 4,
+    "nshards": 2,
     "servers": [
         {
             "type": "metadata",
@@ -685,24 +747,12 @@ input: {
             "memory": 64,
             "rack": "rack_r00"
         },
-        {
-            "type": "metadata",
-            "uuid": "server_r00_metadata03",
-            "memory": 64,
-            "rack": "rack_r00"
-        },
         {
             "type": "storage",
             "uuid": "server_r00_storage00",
             "memory": 64,
             "rack": "rack_r00"
         },
-        {
-            "type": "storage",
-            "uuid": "server_r00_storage01",
-            "memory": 64,
-            "rack": "rack_r00"
-        },
         {
             "type": "metadata",
             "uuid": "server_r01_metadata00",
@@ -721,59 +771,29 @@ input: {
             "memory": 64,
             "rack": "rack_r01"
         },
-        {
-            "type": "metadata",
-            "uuid": "server_r01_metadata03",
-            "memory": 64,
-            "rack": "rack_r01"
-        },
         {
             "type": "storage",
             "uuid": "server_r01_storage00",
             "memory": 64,
             "rack": "rack_r01"
         },
-        {
-            "type": "storage",
-            "uuid": "server_r01_storage01",
-            "memory": 64,
-            "rack": "rack_r01"
-        },
         {
             "type": "metadata",
             "uuid": "server_r02_metadata00",
             "memory": 64,
             "rack": "rack_r02"
         },
-        {
-            "type": "metadata",
-            "uuid": "server_r02_metadata01",
-            "memory": 64,
-            "rack": "rack_r02"
-        },
         {
             "type": "metadata",
             "uuid": "server_r02_metadata02",
             "memory": 64,
             "rack": "rack_r02"
         },
-        {
-            "type": "metadata",
-            "uuid": "server_r02_metadata03",
-            "memory": 64,
-            "rack": "rack_r02"
-        },
         {
             "type": "storage",
             "uuid": "server_r02_storage00",
             "memory": 64,
             "rack": "rack_r02"
-        },
-        {
-            "type": "storage",
-            "uuid": "server_r02_storage01",
-            "memory": 64,
-            "rack": "rack_r02"
         }
     ]
 }
@@ -790,6 +810,9 @@ generated config:
         "loadbalancer": {
             "LOADBALANCER_IMAGE0": 1
         },
+        "medusa": {
+            "MEDUSA_IMAGE0": 1
+        },
         "moray": {
             "1": {
                 "MORAY_IMAGE0": 1
@@ -817,6 +840,9 @@ generated config:
         "loadbalancer": {
             "LOADBALANCER_IMAGE0": 1
         },
+        "marlin-dashboard": {
+            "DASHBOARD_IMAGE0": 1
+        },
         "moray": {
             "2": {
                 "MORAY_IMAGE0": 1
@@ -841,40 +867,6 @@ generated config:
         "loadbalancer": {
             "LOADBALANCER_IMAGE0": 1
         },
-        "moray": {
-            "3": {
-                "MORAY_IMAGE0": 1
-            }
-        },
-        "postgres": {
-            "3": {
-                "POSTGRES_IMAGE0": 1
-            }
-        },
-        "webapi": {
-            "WEBAPI_IMAGE0": 1
-        }
-    },
-    "server_r00_metadata03": {
-        "electric-moray": {
-            "ELECTRIC_MORAY_IMAGE0": 1
-        },
-        "loadbalancer": {
-            "LOADBALANCER_IMAGE0": 1
-        },
-        "moray": {
-            "4": {
-                "MORAY_IMAGE0": 1
-            }
-        },
-        "ops": {
-            "OPS_IMAGE0": 1
-        },
-        "postgres": {
-            "4": {
-                "POSTGRES_IMAGE0": 1
-            }
-        },
         "webapi": {
             "WEBAPI_IMAGE0": 1
         }
@@ -897,6 +889,9 @@ generated config:
         "nameservice": {
             "NAMESERVICE_IMAGE0": 1
         },
+        "ops": {
+            "OPS_IMAGE0": 1
+        },
         "postgres": {
             "1": {
                 "POSTGRES_IMAGE0": 1
@@ -940,21 +935,11 @@ generated config:
         "medusa": {
             "MEDUSA_IMAGE0": 1
         },
-        "moray": {
-            "3": {
-                "MORAY_IMAGE0": 1
-            }
-        },
-        "postgres": {
-            "3": {
-                "POSTGRES_IMAGE0": 1
-            }
-        },
         "webapi": {
             "WEBAPI_IMAGE0": 1
         }
     },
-    "server_r01_metadata03": {
+    "server_r02_metadata00": {
         "electric-moray": {
             "ELECTRIC_MORAY_IMAGE0": 1
         },
@@ -964,27 +949,6 @@ generated config:
         "madtom": {
             "MADTOM_IMAGE0": 1
         },
-        "moray": {
-            "4": {
-                "MORAY_IMAGE0": 1
-            }
-        },
-        "postgres": {
-            "4": {
-                "POSTGRES_IMAGE0": 1
-            }
-        },
-        "webapi": {
-            "WEBAPI_IMAGE0": 1
-        }
-    },
-    "server_r02_metadata00": {
-        "electric-moray": {
-            "ELECTRIC_MORAY_IMAGE0": 1
-        },
-        "loadbalancer": {
-            "LOADBALANCER_IMAGE0": 1
-        },
         "moray": {
             "1": {
                 "MORAY_IMAGE0": 1
@@ -1002,7 +966,7 @@ generated config:
             "WEBAPI_IMAGE0": 1
         }
     },
-    "server_r02_metadata01": {
+    "server_r02_metadata02": {
         "electric-moray": {
             "ELECTRIC_MORAY_IMAGE0": 1
         },
@@ -1026,54 +990,6 @@ generated config:
             "WEBAPI_IMAGE0": 1
         }
     },
-    "server_r02_metadata02": {
-        "electric-moray": {
-            "ELECTRIC_MORAY_IMAGE0": 1
-        },
-        "loadbalancer": {
-            "LOADBALANCER_IMAGE0": 1
-        },
-        "medusa": {
-            "MEDUSA_IMAGE0": 1
-        },
-        "moray": {
-            "3": {
-                "MORAY_IMAGE0": 1
-            }
-        },
-        "postgres": {
-            "3": {
-                "POSTGRES_IMAGE0": 1
-            }
-        },
-        "webapi": {
-            "WEBAPI_IMAGE0": 1
-        }
-    },
-    "server_r02_metadata03": {
-        "electric-moray": {
-            "ELECTRIC_MORAY_IMAGE0": 1
-        },
-        "loadbalancer": {
-            "LOADBALANCER_IMAGE0": 1
-        },
-        "marlin-dashboard": {
-            "DASHBOARD_IMAGE0": 1
-        },
-        "moray": {
-            "4": {
-                "MORAY_IMAGE0": 1
-            }
-        },
-        "postgres": {
-            "4": {
-                "POSTGRES_IMAGE0": 1
-            }
-        },
-        "webapi": {
-            "WEBAPI_IMAGE0": 1
-        }
-    },
     "server_r00_storage00": {
         "marlin": {
             "MARLIN_IMAGE0": 16
@@ -1082,14 +998,6 @@ generated config:
             "STORAGE_IMAGE0": 1
         }
     },
-    "server_r00_storage01": {
-        "marlin": {
-            "MARLIN_IMAGE0": 16
-        },
-        "storage": {
-            "STORAGE_IMAGE0": 1
-        }
-    },
     "server_r01_storage00": {
         "marlin": {
             "MARLIN_IMAGE0": 16
@@ -1098,14 +1006,6 @@ generated config:
             "STORAGE_IMAGE0": 1
         }
     },
-    "server_r01_storage01": {
-        "marlin": {
-            "MARLIN_IMAGE0": 16
-        },
-        "storage": {
-            "STORAGE_IMAGE0": 1
-        }
-    },
     "server_r02_storage00": {
         "marlin": {
             "MARLIN_IMAGE0": 16
@@ -1113,14 +1013,1410 @@ generated config:
         "storage": {
             "STORAGE_IMAGE0": 1
         }
-    },
-    "server_r02_storage01": {
-        "marlin": {
-            "MARLIN_IMAGE0": 16
-        },
-        "storage": {
-            "STORAGE_IMAGE0": 1
-        }
     }
 }
+
+summary:
+     SERVICE          SHARD       default_az
+     nameservice          -                3
+     electric-moray       -                8
+     storage              -                3
+     authcache            -                2
+     webapi               -                8
+     loadbalancer         -                8
+     jobsupervisor        -                2
+     jobpuller            -                2
+     medusa               -                2
+     ops                  -                1
+     madtom               -                1
+     marlin-dashboard     -                1
+     marlin               -               48
+     postgres             1                3
+     postgres             2                3
+     moray                1                3
+     moray                2                3
+--------------------------------------------------
+--------------------------------------------------
+test case: 3-rack, 4-shard deployment
+input: {
+    "nshards": 4,
+    "servers": [
+        {
+            "type": "metadata",
+            "uuid": "server_r00_metadata00",
+            "memory": 64,
+            "rack": "rack_r00"
+        },
+        {
+            "type": "metadata",
+            "uuid": "server_r00_metadata01",
+            "memory": 64,
+            "rack": "rack_r00"
+        },
+        {
+            "type": "metadata",
+            "uuid": "server_r00_metadata02",
+            "memory": 64,
+            "rack": "rack_r00"
+        },
+        {
+            "type": "metadata",
+            "uuid": "server_r00_metadata03",
+            "memory": 64,
+            "rack": "rack_r00"
+        },
+        {
+            "type": "storage",
+            "uuid": "server_r00_storage00",
+            "memory": 64,
+            "rack": "rack_r00"
+        },
+        {
+            "type": "storage",
+            "uuid": "server_r00_storage01",
+            "memory": 64,
+            "rack": "rack_r00"
+        },
+        {
+            "type": "metadata",
+            "uuid": "server_r01_metadata00",
+            "memory": 64,
+            "rack": "rack_r01"
+        },
+        {
+            "type": "metadata",
+            "uuid": "server_r01_metadata01",
+            "memory": 64,
+            "rack": "rack_r01"
+        },
+        {
+            "type": "metadata",
+            "uuid": "server_r01_metadata02",
+            "memory": 64,
+            "rack": "rack_r01"
+        },
+        {
+            "type": "metadata",
+            "uuid": "server_r01_metadata03",
+            "memory": 64,
+            "rack": "rack_r01"
+        },
+        {
+            "type": "storage",
+            "uuid": "server_r01_storage00",
+            "memory": 64,
+            "rack": "rack_r01"
+        },
+        {
+            "type": "storage",
+            "uuid": "server_r01_storage01",
+            "memory": 64,
+            "rack": "rack_r01"
+        },
+        {
+            "type": "metadata",
+            "uuid": "server_r02_metadata00",
+            "memory": 64,
+            "rack": "rack_r02"
+        },
+        {
+            "type": "metadata",
+            "uuid": "server_r02_metadata01",
+            "memory": 64,
+            "rack": "rack_r02"
+        },
+        {
+            "type": "metadata",
+            "uuid": "server_r02_metadata02",
+            "memory": 64,
+            "rack": "rack_r02"
+        },
+        {
+            "type": "metadata",
+            "uuid": "server_r02_metadata03",
+            "memory": 64,
+            "rack": "rack_r02"
+        },
+        {
+            "type": "storage",
+            "uuid": "server_r02_storage00",
+            "memory": 64,
+            "rack": "rack_r02"
+        },
+        {
+            "type": "storage",
+            "uuid": "server_r02_storage01",
+            "memory": 64,
+            "rack": "rack_r02"
+        }
+    ]
+}
+
+generated config:
+{
+    "server_r00_metadata00": {
+        "authcache": {
+            "AUTHCACHE_IMAGE0": 1
+        },
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "moray": {
+            "1": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "nameservice": {
+            "NAMESERVICE_IMAGE0": 1
+        },
+        "postgres": {
+            "1": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "server_r00_metadata01": {
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "jobsupervisor": {
+            "JOBSUPERVISOR_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "moray": {
+            "2": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "postgres": {
+            "2": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "server_r00_metadata02": {
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "jobpuller": {
+            "JOBPULLER_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "moray": {
+            "3": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "postgres": {
+            "3": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "server_r00_metadata03": {
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "moray": {
+            "4": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "ops": {
+            "OPS_IMAGE0": 1
+        },
+        "postgres": {
+            "4": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "server_r01_metadata00": {
+        "authcache": {
+            "AUTHCACHE_IMAGE0": 1
+        },
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "moray": {
+            "1": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "nameservice": {
+            "NAMESERVICE_IMAGE0": 1
+        },
+        "postgres": {
+            "1": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "server_r01_metadata01": {
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "jobsupervisor": {
+            "JOBSUPERVISOR_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "moray": {
+            "2": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "postgres": {
+            "2": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "server_r01_metadata02": {
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "medusa": {
+            "MEDUSA_IMAGE0": 1
+        },
+        "moray": {
+            "3": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "postgres": {
+            "3": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "server_r01_metadata03": {
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "madtom": {
+            "MADTOM_IMAGE0": 1
+        },
+        "moray": {
+            "4": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "postgres": {
+            "4": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "server_r02_metadata00": {
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "moray": {
+            "1": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "nameservice": {
+            "NAMESERVICE_IMAGE0": 1
+        },
+        "postgres": {
+            "1": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "server_r02_metadata01": {
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "jobpuller": {
+            "JOBPULLER_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "moray": {
+            "2": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "postgres": {
+            "2": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "server_r02_metadata02": {
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "medusa": {
+            "MEDUSA_IMAGE0": 1
+        },
+        "moray": {
+            "3": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "postgres": {
+            "3": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "server_r02_metadata03": {
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "marlin-dashboard": {
+            "DASHBOARD_IMAGE0": 1
+        },
+        "moray": {
+            "4": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "postgres": {
+            "4": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "server_r00_storage00": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    },
+    "server_r00_storage01": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    },
+    "server_r01_storage00": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    },
+    "server_r01_storage01": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    },
+    "server_r02_storage00": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    },
+    "server_r02_storage01": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    }
+}
+
+summary:
+     SERVICE          SHARD       default_az
+     nameservice          -                3
+     electric-moray       -               12
+     storage              -                6
+     authcache            -                2
+     webapi               -               12
+     loadbalancer         -               12
+     jobsupervisor        -                2
+     jobpuller            -                2
+     medusa               -                2
+     ops                  -                1
+     madtom               -                1
+     marlin-dashboard     -                1
+     marlin               -               96
+     postgres             1                3
+     postgres             2                3
+     postgres             3                3
+     postgres             4                3
+     moray                1                3
+     moray                2                3
+     moray                3                3
+     moray                4                3
+--------------------------------------------------
+--------------------------------------------------
+test case: 3-AZ, 1-rack-per-AZ, 1-shard deployment
+input: {
+    "nshards": 1,
+    "servers": [
+        {
+            "type": "metadata",
+            "uuid": "az0_server_r00_metadata00",
+            "memory": 64,
+            "rack": "az0_rack_r00",
+            "az": "az0"
+        },
+        {
+            "type": "storage",
+            "uuid": "az0_server_r00_storage00",
+            "memory": 64,
+            "rack": "az0_rack_r00",
+            "az": "az0"
+        },
+        {
+            "type": "metadata",
+            "uuid": "az1_server_r00_metadata00",
+            "memory": 64,
+            "rack": "az1_rack_r00",
+            "az": "az1"
+        },
+        {
+            "type": "storage",
+            "uuid": "az1_server_r00_storage00",
+            "memory": 64,
+            "rack": "az1_rack_r00",
+            "az": "az1"
+        },
+        {
+            "type": "metadata",
+            "uuid": "az2_server_r00_metadata00",
+            "memory": 64,
+            "rack": "az2_rack_r00",
+            "az": "az2"
+        },
+        {
+            "type": "storage",
+            "uuid": "az2_server_r00_storage00",
+            "memory": 64,
+            "rack": "az2_rack_r00",
+            "az": "az2"
+        }
+    ]
+}
+
+generated config:
+{
+    "az0_server_r00_metadata00": {
+        "authcache": {
+            "AUTHCACHE_IMAGE0": 1
+        },
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "jobpuller": {
+            "JOBPULLER_IMAGE0": 1
+        },
+        "jobsupervisor": {
+            "JOBSUPERVISOR_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "moray": {
+            "1": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "nameservice": {
+            "NAMESERVICE_IMAGE0": 1
+        },
+        "ops": {
+            "OPS_IMAGE0": 1
+        },
+        "postgres": {
+            "1": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "az0_server_r00_storage00": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    }
+}
+{
+    "az1_server_r00_metadata00": {
+        "authcache": {
+            "AUTHCACHE_IMAGE0": 1
+        },
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "jobsupervisor": {
+            "JOBSUPERVISOR_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "madtom": {
+            "MADTOM_IMAGE0": 1
+        },
+        "medusa": {
+            "MEDUSA_IMAGE0": 1
+        },
+        "moray": {
+            "1": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "nameservice": {
+            "NAMESERVICE_IMAGE0": 1
+        },
+        "postgres": {
+            "1": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "az1_server_r00_storage00": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    }
+}
+{
+    "az2_server_r00_metadata00": {
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "jobpuller": {
+            "JOBPULLER_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "marlin-dashboard": {
+            "DASHBOARD_IMAGE0": 1
+        },
+        "medusa": {
+            "MEDUSA_IMAGE0": 1
+        },
+        "moray": {
+            "1": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "nameservice": {
+            "NAMESERVICE_IMAGE0": 1
+        },
+        "postgres": {
+            "1": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "az2_server_r00_storage00": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    }
+}
+
+summary:
+     SERVICE          SHARD              az0              az1              az2
+     nameservice          -                1                1                1
+     electric-moray       -                1                1                1
+     storage              -                1                1                1
+     authcache            -                1                1                 
+     webapi               -                1                1                1
+     loadbalancer         -                1                1                1
+     jobsupervisor        -                1                1                 
+     jobpuller            -                1                                 1
+     medusa               -                                 1                1
+     ops                  -                1                                  
+     madtom               -                                 1                 
+     marlin-dashboard     -                                                  1
+     marlin               -               16               16               16
+     postgres             1                1                1                1
+     moray                1                1                1                1
+--------------------------------------------------
+--------------------------------------------------
+test case: 3-AZ, 1-rack-per-AZ, 2-shard deployment
+input: {
+    "nshards": 2,
+    "servers": [
+        {
+            "type": "metadata",
+            "uuid": "az0_server_r00_metadata00",
+            "memory": 64,
+            "rack": "az0_rack_r00",
+            "az": "az0"
+        },
+        {
+            "type": "storage",
+            "uuid": "az0_server_r00_storage00",
+            "memory": 64,
+            "rack": "az0_rack_r00",
+            "az": "az0"
+        },
+        {
+            "type": "metadata",
+            "uuid": "az1_server_r00_metadata00",
+            "memory": 64,
+            "rack": "az1_rack_r00",
+            "az": "az1"
+        },
+        {
+            "type": "storage",
+            "uuid": "az1_server_r00_storage00",
+            "memory": 64,
+            "rack": "az1_rack_r00",
+            "az": "az1"
+        },
+        {
+            "type": "metadata",
+            "uuid": "az2_server_r00_metadata00",
+            "memory": 64,
+            "rack": "az2_rack_r00",
+            "az": "az2"
+        },
+        {
+            "type": "storage",
+            "uuid": "az2_server_r00_storage00",
+            "memory": 64,
+            "rack": "az2_rack_r00",
+            "az": "az2"
+        }
+    ]
+}
+
+generated config:
+{
+    "az0_server_r00_metadata00": {
+        "authcache": {
+            "AUTHCACHE_IMAGE0": 1
+        },
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "jobpuller": {
+            "JOBPULLER_IMAGE0": 1
+        },
+        "jobsupervisor": {
+            "JOBSUPERVISOR_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "moray": {
+            "1": {
+                "MORAY_IMAGE0": 1
+            },
+            "2": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "nameservice": {
+            "NAMESERVICE_IMAGE0": 1
+        },
+        "ops": {
+            "OPS_IMAGE0": 1
+        },
+        "postgres": {
+            "1": {
+                "POSTGRES_IMAGE0": 1
+            },
+            "2": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "az0_server_r00_storage00": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    }
+}
+{
+    "az1_server_r00_metadata00": {
+        "authcache": {
+            "AUTHCACHE_IMAGE0": 1
+        },
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "jobsupervisor": {
+            "JOBSUPERVISOR_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "madtom": {
+            "MADTOM_IMAGE0": 1
+        },
+        "medusa": {
+            "MEDUSA_IMAGE0": 1
+        },
+        "moray": {
+            "1": {
+                "MORAY_IMAGE0": 1
+            },
+            "2": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "nameservice": {
+            "NAMESERVICE_IMAGE0": 1
+        },
+        "postgres": {
+            "1": {
+                "POSTGRES_IMAGE0": 1
+            },
+            "2": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "az1_server_r00_storage00": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    }
+}
+{
+    "az2_server_r00_metadata00": {
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "jobpuller": {
+            "JOBPULLER_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "marlin-dashboard": {
+            "DASHBOARD_IMAGE0": 1
+        },
+        "medusa": {
+            "MEDUSA_IMAGE0": 1
+        },
+        "moray": {
+            "1": {
+                "MORAY_IMAGE0": 1
+            },
+            "2": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "nameservice": {
+            "NAMESERVICE_IMAGE0": 1
+        },
+        "postgres": {
+            "1": {
+                "POSTGRES_IMAGE0": 1
+            },
+            "2": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "az2_server_r00_storage00": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    }
+}
+warning: requested 2 shards with only 1 metadata server in at least one datacenter.  Multiple primary databases will wind up running on the same servers, and this configuration may not survive server failure.  This is not recommended.
+
+summary:
+     SERVICE          SHARD              az0              az1              az2
+     nameservice          -                1                1                1
+     electric-moray       -                1                1                1
+     storage              -                1                1                1
+     authcache            -                1                1                 
+     webapi               -                1                1                1
+     loadbalancer         -                1                1                1
+     jobsupervisor        -                1                1                 
+     jobpuller            -                1                                 1
+     medusa               -                                 1                1
+     ops                  -                1                                  
+     madtom               -                                 1                 
+     marlin-dashboard     -                                                  1
+     marlin               -               16               16               16
+     postgres             1                1                1                1
+     postgres             2                1                1                1
+     moray                1                1                1                1
+     moray                2                1                1                1
+--------------------------------------------------
+--------------------------------------------------
+test case: 3-AZ, 2-racks-per-AZ, 3-shard deployment
+input: {
+    "nshards": 3,
+    "servers": [
+        {
+            "type": "metadata",
+            "uuid": "az0_server_r00_metadata00",
+            "memory": 64,
+            "rack": "az0_rack_r00",
+            "az": "az0"
+        },
+        {
+            "type": "storage",
+            "uuid": "az0_server_r00_storage00",
+            "memory": 64,
+            "rack": "az0_rack_r00",
+            "az": "az0"
+        },
+        {
+            "type": "metadata",
+            "uuid": "az0_server_r00_metadata01",
+            "memory": 64,
+            "rack": "az0_rack_r00",
+            "az": "az0"
+        },
+        {
+            "type": "storage",
+            "uuid": "az0_server_r00_storage01",
+            "memory": 64,
+            "rack": "az0_rack_r00",
+            "az": "az0"
+        },
+        {
+            "type": "metadata",
+            "uuid": "az0_server_r00_metadata02",
+            "memory": 64,
+            "rack": "az0_rack_r00",
+            "az": "az0"
+        },
+        {
+            "type": "storage",
+            "uuid": "az0_server_r00_storage02",
+            "memory": 64,
+            "rack": "az0_rack_r00",
+            "az": "az0"
+        },
+        {
+            "type": "metadata",
+            "uuid": "az1_server_r00_metadata00",
+            "memory": 64,
+            "rack": "az1_rack_r00",
+            "az": "az1"
+        },
+        {
+            "type": "storage",
+            "uuid": "az1_server_r00_storage00",
+            "memory": 64,
+            "rack": "az1_rack_r00",
+            "az": "az1"
+        },
+        {
+            "type": "metadata",
+            "uuid": "az1_server_r00_metadata01",
+            "memory": 64,
+            "rack": "az1_rack_r00",
+            "az": "az1"
+        },
+        {
+            "type": "storage",
+            "uuid": "az1_server_r00_storage01",
+            "memory": 64,
+            "rack": "az1_rack_r00",
+            "az": "az1"
+        },
+        {
+            "type": "metadata",
+            "uuid": "az1_server_r00_metadata02",
+            "memory": 64,
+            "rack": "az1_rack_r00",
+            "az": "az1"
+        },
+        {
+            "type": "storage",
+            "uuid": "az1_server_r00_storage02",
+            "memory": 64,
+            "rack": "az1_rack_r00",
+            "az": "az1"
+        },
+        {
+            "type": "metadata",
+            "uuid": "az2_server_r00_metadata00",
+            "memory": 64,
+            "rack": "az2_rack_r00",
+            "az": "az2"
+        },
+        {
+            "type": "storage",
+            "uuid": "az2_server_r00_storage00",
+            "memory": 64,
+            "rack": "az2_rack_r00",
+            "az": "az2"
+        },
+        {
+            "type": "metadata",
+            "uuid": "az2_server_r00_metadata01",
+            "memory": 64,
+            "rack": "az2_rack_r00",
+            "az": "az2"
+        },
+        {
+            "type": "storage",
+            "uuid": "az2_server_r00_storage01",
+            "memory": 64,
+            "rack": "az2_rack_r00",
+            "az": "az2"
+        },
+        {
+            "type": "metadata",
+            "uuid": "az2_server_r00_metadata02",
+            "memory": 64,
+            "rack": "az2_rack_r00",
+            "az": "az2"
+        },
+        {
+            "type": "storage",
+            "uuid": "az2_server_r00_storage02",
+            "memory": 64,
+            "rack": "az2_rack_r00",
+            "az": "az2"
+        }
+    ]
+}
+
+generated config:
+{
+    "az0_server_r00_metadata00": {
+        "authcache": {
+            "AUTHCACHE_IMAGE0": 1
+        },
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "moray": {
+            "1": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "nameservice": {
+            "NAMESERVICE_IMAGE0": 1
+        },
+        "ops": {
+            "OPS_IMAGE0": 1
+        },
+        "postgres": {
+            "1": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "az0_server_r00_metadata01": {
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "jobsupervisor": {
+            "JOBSUPERVISOR_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "moray": {
+            "2": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "postgres": {
+            "2": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "az0_server_r00_metadata02": {
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "jobpuller": {
+            "JOBPULLER_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "moray": {
+            "3": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "postgres": {
+            "3": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "az0_server_r00_storage00": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    },
+    "az0_server_r00_storage01": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    },
+    "az0_server_r00_storage02": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    }
+}
+{
+    "az1_server_r00_metadata00": {
+        "authcache": {
+            "AUTHCACHE_IMAGE0": 1
+        },
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "madtom": {
+            "MADTOM_IMAGE0": 1
+        },
+        "moray": {
+            "1": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "nameservice": {
+            "NAMESERVICE_IMAGE0": 1
+        },
+        "postgres": {
+            "1": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "az1_server_r00_metadata01": {
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "jobsupervisor": {
+            "JOBSUPERVISOR_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "moray": {
+            "2": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "postgres": {
+            "2": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "az1_server_r00_metadata02": {
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "medusa": {
+            "MEDUSA_IMAGE0": 1
+        },
+        "moray": {
+            "3": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "postgres": {
+            "3": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "az1_server_r00_storage00": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    },
+    "az1_server_r00_storage01": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    },
+    "az1_server_r00_storage02": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    }
+}
+{
+    "az2_server_r00_metadata00": {
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "marlin-dashboard": {
+            "DASHBOARD_IMAGE0": 1
+        },
+        "moray": {
+            "1": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "nameservice": {
+            "NAMESERVICE_IMAGE0": 1
+        },
+        "postgres": {
+            "1": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "az2_server_r00_metadata01": {
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "jobpuller": {
+            "JOBPULLER_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "moray": {
+            "2": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "postgres": {
+            "2": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "az2_server_r00_metadata02": {
+        "electric-moray": {
+            "ELECTRIC_MORAY_IMAGE0": 1
+        },
+        "loadbalancer": {
+            "LOADBALANCER_IMAGE0": 1
+        },
+        "medusa": {
+            "MEDUSA_IMAGE0": 1
+        },
+        "moray": {
+            "3": {
+                "MORAY_IMAGE0": 1
+            }
+        },
+        "postgres": {
+            "3": {
+                "POSTGRES_IMAGE0": 1
+            }
+        },
+        "webapi": {
+            "WEBAPI_IMAGE0": 1
+        }
+    },
+    "az2_server_r00_storage00": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    },
+    "az2_server_r00_storage01": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    },
+    "az2_server_r00_storage02": {
+        "marlin": {
+            "MARLIN_IMAGE0": 16
+        },
+        "storage": {
+            "STORAGE_IMAGE0": 1
+        }
+    }
+}
+
+summary:
+     SERVICE          SHARD              az0              az1              az2
+     nameservice          -                1                1                1
+     electric-moray       -                3                3                3
+     storage              -                3                3                3
+     authcache            -                1                1                 
+     webapi               -                3                3                3
+     loadbalancer         -                3                3                3
+     jobsupervisor        -                1                1                 
+     jobpuller            -                1                                 1
+     medusa               -                                 1                1
+     ops                  -                1                                  
+     madtom               -                                 1                 
+     marlin-dashboard     -                                                  1
+     marlin               -               48               48               48
+     postgres             1                1                1                1
+     postgres             2                1                1                1
+     postgres             3                1                1                1
+     moray                1                1                1                1
+     moray                2                1                1                1
+     moray                3                1                1                1
 --------------------------------------------------
diff --git a/test/tst.stripe.js b/test/tst.stripe.js
new file mode 100644
index 0000000..923cc04
--- /dev/null
+++ b/test/tst.stripe.js
@@ -0,0 +1,80 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2016, Joyent, Inc.
+ */
+
+/*
+ * tst.stripe.js: tests the stripe() utility function
+ */
+
+var assert = require('assert');
+var common = require('../lib/common');
+var stripe = common.stripe;
+
+/* invalid input */
+assert.throws(function () { stripe(); }, /lists.*array.*required/);
+assert.throws(function () { stripe(true); }, /lists.*array.*required/);
+assert.throws(function () { stripe({}); }, /lists.*array.*required/);
+assert.throws(function () { stripe([ true ]); }, /lists.*array.*required/);
+
+/* degenerate cases */
+assert.deepEqual(stripe([]), []);
+assert.deepEqual(stripe([ [] ]), []);
+assert.deepEqual(stripe([ [ 0, 4, 8, 12 ] ]), [ 0, 4, 8, 12 ]);
+
+/* simple cases */
+assert.deepEqual(stripe([
+    [ 0, 4, 8, 12 ],
+    [ 2, 6, 10, 14 ]
+]), [ 0, 2, 4, 6, 8, 10, 12, 14 ]);
+assert.deepEqual(stripe([
+    [ 2, 6, 10, 14 ],
+    [ 0, 4, 8, 12 ]
+]), [ 2, 0, 6, 4, 10, 8, 14, 12 ]);
+assert.deepEqual(stripe([
+    [ 0, 4,  8, 12 ],
+    [ 1, 5,  9, 13 ],
+    [ 2, 6, 10, 14 ],
+    [ 3, 7, 11, 15 ]
+]), [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 ]);
+
+/* first array smaller than the others */
+assert.deepEqual(stripe([
+    [ 0, 4 ],
+    [ 1, 5,  9, 13 ],
+    [ 2, 6, 10, 14 ],
+    [ 3, 7, 11, 15 ]
+]), [ 0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14, 15 ]);
+/* last array smaller than the others */
+assert.deepEqual(stripe([
+    [ 0, 4,  8, 12 ],
+    [ 1, 5,  9, 13 ],
+    [ 2, 6, 10, 14 ],
+    [ 3, 7 ]
+]), [ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14 ]);
+/* different length arrays, some empty to start with */
+assert.deepEqual(stripe([
+    [ ],
+    [ 0, 1, 2, 3 ],
+    [ 4, 5, 6, 7, 8, 9, 10, 11, 12, 13 ],
+    [ 14, 15, 16 ],
+    [ ]
+]), [ 0, 4, 14, 1, 5, 15, 2, 6, 16, 3, 7, 8, 9, 10, 11, 12, 13 ]);
+
+/*
+ * stripe() doesn't care what's in these arrays, but it's worth verifying that
+ * it works with more complex objects.
+ */
+assert.deepEqual(stripe([
+    [ [ 1, 2, 3 ] ],
+    [ [ 6, 5, 4 ] ],
+    [ null, null ],
+    [ { 'foofaraw': 'arglebargle' } ]
+]), [ [ 1, 2, 3 ], [ 6, 5, 4 ], null, { 'foofaraw': 'arglebargle' }, null ]);
+
+console.log('TEST PASSED');
