From c5c480b332983fefef34c412ab14b67d7bac563e Mon Sep 17 00:00:00 2001
From: Jerry Jelinek <jerry.jelinek@joyent.com>
Date: Mon, 5 Jun 2017 22:03:06 +0000
Subject: [PATCH] OS-5561 rtld needs to learn about AVX512 OS-5560 Need AVX512
 core kernel support

---
 usr/src/cmd/sgs/include/rtld.h        |   5 +-
 usr/src/cmd/sgs/rtld/amd64/boot_elf.s | 289 ++++++++++++++++++--------
 usr/src/common/elfcap/elfcap.c        |   6 +-
 usr/src/common/elfcap/elfcap.h        |   4 +-
 usr/src/uts/common/sys/auxv_386.h     |   7 +-
 usr/src/uts/i86pc/os/cpuid.c          |  12 +-
 6 files changed, 220 insertions(+), 103 deletions(-)

diff --git a/usr/src/cmd/sgs/include/rtld.h b/usr/src/cmd/sgs/include/rtld.h
index 65eb25c4b1..0378b7390e 100644
--- a/usr/src/cmd/sgs/include/rtld.h
+++ b/usr/src/cmd/sgs/include/rtld.h
@@ -21,6 +21,7 @@
 
 /*
  * Copyright (c) 1995, 2010, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2017, Joyent, Inc.
  */
 #ifndef	_RTLD_H
 #define	_RTLD_H
@@ -1112,8 +1113,8 @@ struct sresult {
  * hardware name, thus this structure is a little simpler.
  *
  * Note, the amd64 version of elf_rtbndr assumes that the sc_hw_1 value is at
- * offset zero. If you are changing this structure in a way that invalidates
- * this you need to update that code.
+ * offset zero and sc_hw_2 is at offset 8. If you are changing this structure
+ * in a way that invalidates this, you need to update that code.
  */
 typedef	struct {
 	elfcap_mask_t	sc_hw_1;	/* CA_SUNW_HW_1 capabilities */
diff --git a/usr/src/cmd/sgs/rtld/amd64/boot_elf.s b/usr/src/cmd/sgs/rtld/amd64/boot_elf.s
index 81e46cfd13..4574131b83 100644
--- a/usr/src/cmd/sgs/rtld/amd64/boot_elf.s
+++ b/usr/src/cmd/sgs/rtld/amd64/boot_elf.s
@@ -22,7 +22,7 @@
 /*
  * Copyright 2008 Sun Microsystems, Inc.  All rights reserved.
  * Use is subject to license terms.
- * Copyright (c) 2012 Joyent, Inc. All rights reserved.
+ * Copyright (c) 2017 Joyent, Inc. All rights reserved.
  */
 
 #if	defined(lint)
@@ -107,12 +107,15 @@ elf_plt_trace()
  *	    %r11			 8
  *	    %rax			 8
  *				    =======
- *			    Subtotal:	144 (32byte aligned)
+ *			    Subtotal:	144 (64byte aligned)
+ *
+ * Because the zmm registers are saved off of %rsp, we don't need 48
+ * bytes of padding here to ensure they're on a 64 byte boundary.
  *
  *	Saved Media Regs (used to pass floating point args):
- *	    %xmm0 - %xmm7   32 * 8:	256
+ *	    %zmm0 - %zmm7   64 * 8:	512
  *				    =======
- *			    Total:	400 (32byte aligned)
+ *			    Total:	656 (64byte aligned)
  *  
  *  So - will subtract the following to create enough space
  *
@@ -132,14 +135,14 @@ elf_plt_trace()
  *	-144(%rbp)	entering %r10
  *	-152(%rbp)	entering %r11
  *	-160(%rbp)	entering %rax
- *	-192(%rbp)	entering %xmm0
- *	-224(%rbp)	entering %xmm1
- *	-256(%rbp)	entering %xmm2
+ *	-224(%rbp)	entering %xmm0
+ *	-288(%rbp)	entering %xmm1
+ *	-352(%rbp)	entering %xmm2
  *	-288(%rbp)	entering %xmm3
- *	-320(%rbp)	entering %xmm4
- *	-384(%rbp)	entering %xmm5
- *	-416(%rbp)	entering %xmm6
- *	-448(%rbp)	entering %xmm7
+ *	-416(%rbp)	entering %xmm4
+ *	-480(%rbp)	entering %xmm5
+ *	-544(%rbp)	entering %xmm6
+ *	-608(%rbp)	entering %xmm7
  *
  */
 #define	SPDYNOFF    -8
@@ -149,26 +152,26 @@ elf_plt_trace()
 
 /*
  * The next set of offsets are relative to %rsp.
- * We guarantee %rsp is ABI compliant 32-byte aligned.  This guarantees the
- * ymm registers are saved to 32-byte aligned addresses.
+ * We guarantee %rsp is 64-byte aligned.  This guarantees the zmm registers are
+ * saved to 64-byte aligned addresses.
  * %rbp may only be 8 byte aligned if we came in from non-ABI compliant code.
  */ 
-#define	SPRDIOFF	320
-#define	SPRSIOFF	312
-#define	SPRDXOFF	304
-#define	SPRCXOFF	296
-#define	SPR8OFF		288
-#define	SPR9OFF		280
-#define	SPR10OFF	272
-#define	SPR11OFF	264
-#define	SPRAXOFF	256
-#define	SPXMM0OFF	224
-#define	SPXMM1OFF	192
-#define	SPXMM2OFF	160
-#define	SPXMM3OFF	128
-#define	SPXMM4OFF	96
-#define	SPXMM5OFF	64
-#define	SPXMM6OFF	32
+#define	SPRDIOFF	576
+#define	SPRSIOFF	568
+#define	SPRDXOFF	560
+#define	SPRCXOFF	552
+#define	SPR8OFF		544
+#define	SPR9OFF		536
+#define	SPR10OFF	528
+#define	SPR11OFF	520
+#define	SPRAXOFF	512
+#define	SPXMM0OFF	448
+#define	SPXMM1OFF	384
+#define	SPXMM2OFF	320
+#define	SPXMM3OFF	256
+#define	SPXMM4OFF	192
+#define	SPXMM5OFF	128
+#define	SPXMM6OFF	64
 #define	SPXMM7OFF	0
 
 	/* See elf_rtbndr for explanation behind org_scapset */
@@ -178,12 +181,12 @@ elf_plt_trace()
 	.align 16
 elf_plt_trace:
 	/*
-	 * Enforce ABI 32-byte stack alignment here.
-	 * The next andq instruction does this pseudo code:
-	 * If %rsp is 8 byte aligned then subtract 8 from %rsp.
+	 * Enforce 64-byte stack alignment here.
+	 * The next andq instruction essentially does this:
+	 *     if necessary, subtract N bytes from %rsp to align on 64 bytes.
 	 */
-	andq    $-32, %rsp	/* enforce ABI 32-byte stack alignment */
-	subq	$400,%rsp	/ create some local storage
+	andq    $-64, %rsp	/* enforce 64-byte stack alignment */
+	subq	$656,%rsp	/ create some local storage
 
 	movq	%rdi, SPRDIOFF(%rsp)
 	movq	%rsi, SPRSIOFF(%rsp)
@@ -196,8 +199,11 @@ elf_plt_trace:
 	movq	%rax, SPRAXOFF(%rsp)
 
 	movq	org_scapset@GOTPCREL(%rip),%r9
-	movq	(%r9),%r9
-	movl	(%r9),%edx
+	movq	(%r9),%r9			/* Syscapset_t pointer */
+	movl	8(%r9),%edx			/* sc_hw_2 */
+	testl	$AV_386_2_AVX512,%edx
+	jne	.trace_save_zmm
+	movl	(%r9),%edx			/* sc_hw_1 */
 	testl	$AV_386_AVX,%edx
 	jne	.trace_save_ymm
 
@@ -221,6 +227,17 @@ elf_plt_trace:
 	vmovdqa	%ymm5, SPXMM5OFF(%rsp)
 	vmovdqa	%ymm6, SPXMM6OFF(%rsp)
 	vmovdqa	%ymm7, SPXMM7OFF(%rsp)
+	jmp	.trace_save_finish
+
+.trace_save_zmm:
+	vmovdqa64	%zmm0, SPXMM0OFF(%rsp)
+	vmovdqa64	%zmm1, SPXMM1OFF(%rsp)
+	vmovdqa64	%zmm2, SPXMM2OFF(%rsp)
+	vmovdqa64	%zmm3, SPXMM3OFF(%rsp)
+	vmovdqa64	%zmm4, SPXMM4OFF(%rsp)
+	vmovdqa64	%zmm5, SPXMM5OFF(%rsp)
+	vmovdqa64	%zmm6, SPXMM6OFF(%rsp)
+	vmovdqa64	%zmm7, SPXMM7OFF(%rsp)
 
 .trace_save_finish:
 
@@ -298,8 +315,11 @@ elf_plt_trace:
 	/ Restore registers
 	/
 	movq	org_scapset@GOTPCREL(%rip),%r9
-	movq	(%r9),%r9
-	movl	(%r9),%edx
+	movq	(%r9),%r9			/* Syscapset_t pointer */
+	movl	8(%r9),%edx			/* sc_hw_2 */
+	testl	$AV_386_2_AVX512,%edx
+	jne	.trace_restore_zmm
+	movl	(%r9),%edx			/* sc_hw_1 */
 	testl	$AV_386_AVX,%edx
 	jne	.trace_restore_ymm
 
@@ -323,6 +343,17 @@ elf_plt_trace:
 	vmovdqa	SPXMM5OFF(%rsp), %ymm5
 	vmovdqa	SPXMM6OFF(%rsp), %ymm6
 	vmovdqa	SPXMM7OFF(%rsp), %ymm7
+	jmp	.trace_restore_finish
+
+.trace_restore_zmm:
+	vmovdqa64	SPXMM0OFF(%rsp), %zmm0
+	vmovdqa64	SPXMM1OFF(%rsp), %zmm1
+	vmovdqa64	SPXMM2OFF(%rsp), %zmm2
+	vmovdqa64	SPXMM3OFF(%rsp), %zmm3
+	vmovdqa64	SPXMM4OFF(%rsp), %zmm4
+	vmovdqa64	SPXMM5OFF(%rsp), %zmm5
+	vmovdqa64	SPXMM6OFF(%rsp), %zmm6
+	vmovdqa64	SPXMM7OFF(%rsp), %zmm7
 
 .trace_restore_finish:
 	movq	SPRDIOFF(%rsp), %rdi
@@ -412,8 +443,11 @@ elf_plt_trace:
 
 	/ Yes, we have to do this dance again. Sorry.
 	movq	org_scapset@GOTPCREL(%rip),%r9
-	movq	(%r9),%r9
-	movl	(%r9),%edx
+	movq	(%r9),%r9			/* Syscapset_t pointer */
+	movl	8(%r9),%edx			/* sc_hw_2 */
+	testl	$AV_386_2_AVX512,%edx
+	jne	.trace_r2_zmm
+	movl	(%r9),%edx			/* sc_hw_1 */
 	testl	$AV_386_AVX,%edx
 	jne	.trace_r2_ymm
 
@@ -437,6 +471,17 @@ elf_plt_trace:
 	vmovdqa	SPXMM5OFF(%r11), %ymm5
 	vmovdqa	SPXMM6OFF(%r11), %ymm6
 	vmovdqa	SPXMM7OFF(%r11), %ymm7
+	jmp	.trace_r2_finish
+
+.trace_r2_zmm:
+	vmovdqa64	SPXMM0OFF(%r11), %zmm0
+	vmovdqa64	SPXMM1OFF(%r11), %zmm1
+	vmovdqa64	SPXMM2OFF(%r11), %zmm2
+	vmovdqa64	SPXMM3OFF(%r11), %zmm3
+	vmovdqa64	SPXMM4OFF(%r11), %zmm4
+	vmovdqa64	SPXMM5OFF(%r11), %zmm5
+	vmovdqa64	SPXMM6OFF(%r11), %zmm6
+	vmovdqa64	SPXMM7OFF(%r11), %zmm7
 
 .trace_r2_finish:
 	movq	SPRDIOFF(%r11), %rdi
@@ -476,6 +521,49 @@ elf_plt_trace:
 	/
 	/ Restore registers
 	/
+
+	movq	org_scapset@GOTPCREL(%rip),%r9
+	movq	(%r9),%r9			/* Syscapset_t pointer */
+	movl	8(%r9),%edx			/* sc_hw_2 */
+	testl	$AV_386_2_AVX512,%edx
+	jne	.trace_r3_zmm
+	movl	(%r9),%edx			/* sc_hw_1 */
+	testl	$AV_386_AVX,%edx
+	jne	.trace_r3_ymm
+
+.trace_r3_xmm:
+	movdqa	SPXMM0OFF(%rsp), %xmm0
+	movdqa	SPXMM1OFF(%rsp), %xmm1
+	movdqa	SPXMM2OFF(%rsp), %xmm2
+	movdqa	SPXMM3OFF(%rsp), %xmm3
+	movdqa	SPXMM4OFF(%rsp), %xmm4
+	movdqa	SPXMM5OFF(%rsp), %xmm5
+	movdqa	SPXMM6OFF(%rsp), %xmm6
+	movdqa	SPXMM7OFF(%rsp), %xmm7
+	jmp .trace_r3_finish
+
+.trace_r3_ymm:
+	vmovdqa	SPXMM0OFF(%rsp), %ymm0
+	vmovdqa	SPXMM1OFF(%rsp), %ymm1
+	vmovdqa	SPXMM2OFF(%rsp), %ymm2
+	vmovdqa	SPXMM3OFF(%rsp), %ymm3
+	vmovdqa	SPXMM4OFF(%rsp), %ymm4
+	vmovdqa	SPXMM5OFF(%rsp), %ymm5
+	vmovdqa	SPXMM6OFF(%rsp), %ymm6
+	vmovdqa	SPXMM7OFF(%rsp), %ymm7
+	jmp .trace_r3_finish
+
+.trace_r3_zmm:
+	vmovdqa64	SPXMM0OFF(%rsp), %zmm0
+	vmovdqa64	SPXMM1OFF(%rsp), %zmm1
+	vmovdqa64	SPXMM2OFF(%rsp), %zmm2
+	vmovdqa64	SPXMM3OFF(%rsp), %zmm3
+	vmovdqa64	SPXMM4OFF(%rsp), %zmm4
+	vmovdqa64	SPXMM5OFF(%rsp), %zmm5
+	vmovdqa64	SPXMM6OFF(%rsp), %zmm6
+	vmovdqa64	SPXMM7OFF(%rsp), %zmm7
+
+.trace_r3_finish:
 	movq	SPRDIOFF(%rsp), %rdi
 	movq	SPRSIOFF(%rsp), %rsi
 	movq	SPRDXOFF(%rsp), %rdx
@@ -485,14 +573,6 @@ elf_plt_trace:
 	movq	SPR10OFF(%rsp), %r10
 	movq	SPR11OFF(%rsp), %r11
 	// rax already contains return value
-	movdqa	SPXMM0OFF(%rsp), %xmm0
-	movdqa	SPXMM1OFF(%rsp), %xmm1
-	movdqa	SPXMM2OFF(%rsp), %xmm2
-	movdqa	SPXMM3OFF(%rsp), %xmm3
-	movdqa	SPXMM4OFF(%rsp), %xmm4
-	movdqa	SPXMM5OFF(%rsp), %xmm5
-	movdqa	SPXMM6OFF(%rsp), %xmm6
-	movdqa	SPXMM7OFF(%rsp), %xmm7
 
 	movq	%rbp, %rsp			/
 	popq	%rbp				/
@@ -560,12 +640,14 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
  * arguments before interposing functions to resolve the called function. 
  * Possible arguments must be restored before invoking the resolved function.
  * 
- * Before the AVX instruction set enhancements to AMD64 there were no changes in
- * the set of registers and their sizes across different processors. With AVX,
- * the xmm registers became the lower 128 bits of the ymm registers. Because of
- * this, we need to conditionally save 256 bits instead of 128 bits. Regardless
- * of whether we have ymm registers or not, we're always going to push the stack
- * space assuming that we do to simplify the code.
+ * Before the AVX and AVX512 instruction set enhancements to AMD64, there were
+ * no changes in the set of registers and their sizes across different
+ * processors. With AVX, the xmm registers became the lower 128 bits of the ymm
+ * registers. With AVX512, the ymm registers became the lower 256 bits of the
+ * zmm registers. Because of this, we need to conditionally save either 256 bits
+ * or 512 bits, instead of 128 bits. Regardless of whether we have zmm registers
+ * or not, we're always going to push the stack space assuming that we do, to
+ * simplify the code.
  * 
  * Local stack space storage for elf_rtbndr is allocated as follows:
  *
@@ -579,12 +661,12 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
  *	    %r9				 8
  *	    %r10			 8
  *				    =======
- *			    Subtotal:   64 (32byte aligned)
+ *			    Subtotal:   64 (64byte aligned)
  *
  *	Saved Media Regs (used to pass floating point args):
- *	    %ymm0 - %ymm7   32 * 8     256
+ *	    %zmm0 - %zmm7   64 * 8     512
  *				    =======
- *			    Total:     320 (32byte aligned)
+ *			    Total:     576 (64byte aligned)
  *  
  *  So - will subtract the following to create enough space
  *
@@ -596,25 +678,24 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
  *	40(%rsp)	save %r8
  *	48(%rsp)	save %r9
  *	56(%rsp)	save %r10
- *	64(%rsp)	save %ymm0
- *	96(%rsp)	save %ymm1
- *	128(%rsp)	save %ymm2
- *	160(%rsp)	save %ymm3
- *	192(%rsp)	save %ymm4
- *	224(%rsp)	save %ymm5
- *	256(%rsp)	save %ymm6
- *	288(%rsp)	save %ymm7
+ *	64(%rsp)	save %zmm0
+ *	128(%rsp)	save %zmm1
+ *	192(%rsp)	save %zmm2
+ *	256(%rsp)	save %zmm3
+ *	320(%rsp)	save %zmm4
+ *	384(%rsp)	save %zmm5
+ *	448(%rsp)	save %zmm6
+ *	512(%rsp)	save %zmm7
  *
- * Note: Some callers may use 8-byte stack alignment instead of the
- * ABI required 16-byte alignment.  We use %rsp offsets to save/restore
- * registers because %rbp may not be 16-byte aligned.  We guarantee %rsp
- * is 16-byte aligned in the function preamble.
+ * Note: Some callers may use 8-byte stack alignment. We use %rsp offsets to
+ * save/restore registers because %rbp may not be 64-byte aligned. We guarantee
+ * %rsp is 64-byte aligned in the function preamble.
  */
 /*
- * As the registers may either be xmm or ymm, we've left the name as xmm, but
- * increased the offset between them to always cover the xmm and ymm cases.
+ * As the registers may either be xmm, ymm or zmm, we've left the name as xmm,
+ * but increased the offset between them to always cover the zmm case.
  */
-#define	LS_SIZE	$320	/* local stack space to save all possible arguments */
+#define	LS_SIZE	$576	/* local stack space to save all possible arguments */
 #define	LSRAXOFF	0	/* for SSE register count */
 #define	LSRDIOFF	8	/* arg 0 ... */
 #define	LSRSIOFF	16
@@ -624,13 +705,13 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
 #define	LSR9OFF		48
 #define	LSR10OFF	56	/* ... arg 5 */
 #define	LSXMM0OFF	64	/* SSE arg 0 ... */
-#define	LSXMM1OFF	96
-#define	LSXMM2OFF	128
-#define	LSXMM3OFF	160
-#define	LSXMM4OFF	192
-#define	LSXMM5OFF	224
-#define	LSXMM6OFF	256
-#define	LSXMM7OFF	288	/* ... SSE arg 7 */
+#define	LSXMM1OFF	128
+#define	LSXMM2OFF	192
+#define	LSXMM3OFF	256
+#define	LSXMM4OFF	320
+#define	LSXMM5OFF	384
+#define	LSXMM6OFF	448
+#define	LSXMM7OFF	512	/* ... SSE arg 7 */
 
 	/*
 	 * The org_scapset is a global variable that is a part of rtld. It
@@ -651,11 +732,11 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
 
 	/*
 	 * Some libraries may (incorrectly) use non-ABI compliant 8-byte stack
-	 * alignment.  Enforce ABI 16-byte stack alignment here.
-	 * The next andq instruction does this pseudo code:
-	 * If %rsp is 8 byte aligned then subtract 8 from %rsp.
+	 * alignment.  Enforce 64-byte stack alignment here.
+	 * The next andq instruction essentially does this:
+	 *     if necessary, subtract N bytes from %rsp to align on 64 bytes.
 	 */
-	andq	$-32, %rsp	/* enforce ABI 32-byte stack alignment */
+	andq	$-64, %rsp	/* enforce 64-byte stack alignment */
 
 	subq	LS_SIZE, %rsp	/* save all ABI defined argument registers */
 
@@ -669,11 +750,14 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
 	movq	%r10, LSR10OFF(%rsp)	/* call chain reg */
 
 	/*
-	 * Our xmm registers could secretly by ymm registers in disguise.
+	 * Our xmm registers could secretly be ymm or zmm registers in disguise.
 	 */
 	movq	org_scapset@GOTPCREL(%rip),%r9
-	movq	(%r9),%r9
-	movl	(%r9),%edx
+	movq	(%r9),%r9		/* Syscapset_t pointer */
+	movl	8(%r9),%edx		/* sc_hw_2 */
+	testl	$AV_386_2_AVX512,%edx
+	jne	.save_zmm
+	movl	(%r9),%edx		/* sc_hw_1 */
 	testl	$AV_386_AVX,%edx
 	jne	.save_ymm
 
@@ -686,7 +770,7 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
 	movdqa	%xmm5, LSXMM5OFF(%rsp)
 	movdqa	%xmm6, LSXMM6OFF(%rsp)
 	movdqa	%xmm7, LSXMM7OFF(%rsp)	/* ... SSE arg 7 */
-	jmp	.save_finish	
+	jmp	.save_finish
 
 .save_ymm:
 	vmovdqa	%ymm0, LSXMM0OFF(%rsp)	/* SSE arg 0 ... */
@@ -697,6 +781,17 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
 	vmovdqa	%ymm5, LSXMM5OFF(%rsp)
 	vmovdqa	%ymm6, LSXMM6OFF(%rsp)
 	vmovdqa	%ymm7, LSXMM7OFF(%rsp)	/* ... SSE arg 7 */
+	jmp	.save_finish
+
+.save_zmm:
+	vmovdqa64	%zmm0, LSXMM0OFF(%rsp)	/* SSE arg 0 ... */
+	vmovdqa64	%zmm1, LSXMM1OFF(%rsp)
+	vmovdqa64	%zmm2, LSXMM2OFF(%rsp)
+	vmovdqa64	%zmm3, LSXMM3OFF(%rsp)
+	vmovdqa64	%zmm4, LSXMM4OFF(%rsp)
+	vmovdqa64	%zmm5, LSXMM5OFF(%rsp)
+	vmovdqa64	%zmm6, LSXMM6OFF(%rsp)
+	vmovdqa64	%zmm7, LSXMM7OFF(%rsp)	/* ... SSE arg 7 */
 
 .save_finish:
 	movq	LBPLMPOFF(%rbp), %rdi	/* arg1 - *lmp */
@@ -707,11 +802,14 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
 
 	/*
 	 * Restore possible arguments before invoking resolved function. We
-	 * check the xmm vs. ymm regs first so we can use the others.
+	 * check the xmm, ymm, vs. zmm regs first so we can use the others.
 	 */
 	movq	org_scapset@GOTPCREL(%rip),%r9
-	movq	(%r9),%r9
-	movl	(%r9),%edx
+	movq	(%r9),%r9		/* Syscapset_t pointer */
+	movl	8(%r9),%edx		/* sc_hw_2 */
+	testl	$AV_386_2_AVX512,%edx
+	jne	.restore_zmm
+	movl	(%r9),%edx		/* sc_hw_1 */
 	testl	$AV_386_AVX,%edx
 	jne	.restore_ymm
 
@@ -735,6 +833,17 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
 	vmovdqa	LSXMM5OFF(%rsp), %ymm5
 	vmovdqa	LSXMM6OFF(%rsp), %ymm6
 	vmovdqa	LSXMM7OFF(%rsp), %ymm7
+	jmp .restore_finish
+
+.restore_zmm:
+	vmovdqa64	LSXMM0OFF(%rsp), %zmm0
+	vmovdqa64	LSXMM1OFF(%rsp), %zmm1
+	vmovdqa64	LSXMM2OFF(%rsp), %zmm2
+	vmovdqa64	LSXMM3OFF(%rsp), %zmm3
+	vmovdqa64	LSXMM4OFF(%rsp), %zmm4
+	vmovdqa64	LSXMM5OFF(%rsp), %zmm5
+	vmovdqa64	LSXMM6OFF(%rsp), %zmm6
+	vmovdqa64	LSXMM7OFF(%rsp), %zmm7
 
 .restore_finish:
 	movq	LSRAXOFF(%rsp), %rax
diff --git a/usr/src/common/elfcap/elfcap.c b/usr/src/common/elfcap/elfcap.c
index d63db8e981..6beeea33c3 100644
--- a/usr/src/common/elfcap/elfcap.c
+++ b/usr/src/common/elfcap/elfcap.c
@@ -21,7 +21,7 @@
 
 /*
  * Copyright (c) 2004, 2010, Oracle and/or its affiliates. All rights reserved.
- * Copyright (c) 2015, Joyent, Inc.
+ * Copyright (c) 2017, Joyent, Inc.
  */
 
 /* LINTLIBRARY */
@@ -339,6 +339,10 @@ static const elfcap_desc_t hw2_386[ELFCAP_NUM_HW2_386] = {
 	{						/* 0x00000080 */
 		AV_386_2_RDSEED, STRDESC("AV_386_2_RDSEED"),
 		STRDESC("RDSEED"), STRDESC("rdseed"),
+	},
+	{						/* 0x00000100 */
+		AV_386_2_AVX512, STRDESC("AV_386_2_AVX512"),
+		STRDESC("AVX512"), STRDESC("avx512"),
 	}
 };
 
diff --git a/usr/src/common/elfcap/elfcap.h b/usr/src/common/elfcap/elfcap.h
index f3e29a6a97..a702b6ecbd 100644
--- a/usr/src/common/elfcap/elfcap.h
+++ b/usr/src/common/elfcap/elfcap.h
@@ -21,7 +21,7 @@
 
 /*
  * Copyright (c) 2004, 2010, Oracle and/or its affiliates. All rights reserved.
- * Copyright (c) 2015, Joyent, Inc.
+ * Copyright (c) 2017, Joyent, Inc.
  */
 
 #ifndef _ELFCAP_DOT_H
@@ -115,7 +115,7 @@ typedef enum {
 #define	ELFCAP_NUM_SF1			3
 #define	ELFCAP_NUM_HW1_SPARC		17
 #define	ELFCAP_NUM_HW1_386		32
-#define	ELFCAP_NUM_HW2_386		8
+#define	ELFCAP_NUM_HW2_386		9
 
 
 /*
diff --git a/usr/src/uts/common/sys/auxv_386.h b/usr/src/uts/common/sys/auxv_386.h
index a3256a464f..3974317667 100644
--- a/usr/src/uts/common/sys/auxv_386.h
+++ b/usr/src/uts/common/sys/auxv_386.h
@@ -20,7 +20,7 @@
  */
 /*
  * Copyright (c) 2004, 2010, Oracle and/or its affiliates. All rights reserved.
- * Copyright (c) 2015, Joyent, Inc.
+ * Copyright (c) 2017, Joyent, Inc.
  */
 
 #ifndef	_SYS_AUXV_386_H
@@ -83,6 +83,9 @@ extern "C" {
 	"\017sse3\015sse2\014sse\013fxsr\012amd3dx\011amd3d"		\
 	"\010amdmmx\07mmx\06cmov\05amdsysc\04sep\03cx8\02tsc\01fpu"
 
+/*
+ * Flags used in AT_SUN_HWCAP2 elements
+ */
 #define	AV_386_2_F16C		0x00001	/* F16C half percision extensions */
 #define	AV_386_2_RDRAND		0x00002	/* RDRAND insn */
 #define	AV_386_2_BMI1		0x00004 /* BMI1 insns */
@@ -91,9 +94,11 @@ extern "C" {
 #define	AV_386_2_AVX2		0x00020	/* AVX2 insns */
 #define	AV_386_2_ADX		0x00040	/* ADX insns */
 #define	AV_386_2_RDSEED		0x00080	/* RDSEED insn */
+#define	AV_386_2_AVX512		0x00100	/* AVX512 foundation insns */
 
 #define	FMT_AV_386_2							\
 	"\020"								\
+	"\011avx512"							\
 	"\10rdseed\07adx\06avx2\05fma\04bmi2\03bmi1\02rdrand\01f16c"
 
 #ifdef __cplusplus
diff --git a/usr/src/uts/i86pc/os/cpuid.c b/usr/src/uts/i86pc/os/cpuid.c
index 085b130598..ba4cd41851 100644
--- a/usr/src/uts/i86pc/os/cpuid.c
+++ b/usr/src/uts/i86pc/os/cpuid.c
@@ -976,17 +976,13 @@ setup_xfem(void)
 
 	/*
 	 * TBD:
-	 * Enabling MPX and AVX512 implies that xsave_state is large enough
-	 * to hold the MPX state and the full AVX512 state, or that we're
-	 * supporting xsavec or xsaveopt.
-	 *
 	 * if (is_x86_feature(x86_featureset, X86FSET_MPX))
 	 *	flags |= XFEATURE_MPX;
-	 *
-	 * if (is_x86_feature(x86_featureset, X86FSET_AVX512F))
-	 *	flags |= XFEATURE_AVX512;
 	 */
 
+	if (is_x86_feature(x86_featureset, X86FSET_AVX512F))
+		flags |= XFEATURE_AVX512;
+
 	set_xcr(XFEATURE_ENABLED_MASK, flags);
 
 	xsave_bv_all = flags;
@@ -3046,6 +3042,8 @@ cpuid_pass4(cpu_t *cpu, uint_t *hwcap_out)
 					hwcap_flags_2 |= AV_386_2_BMI2;
 				if (*ebx & CPUID_INTC_EBX_7_0_AVX2)
 					hwcap_flags_2 |= AV_386_2_AVX2;
+				if (*ebx & CPUID_INTC_EBX_7_0_AVX512F)
+					hwcap_flags_2 |= AV_386_2_AVX512;
 			}
 		}
 		if (*ecx & CPUID_INTC_ECX_VMX)
-- 
2.21.0

