From 50930dde714118b0b0d828f264efabd130967184 Mon Sep 17 00:00:00 2001
From: Jerry Jelinek <jerry.jelinek@joyent.com>
Date: Thu, 8 Jun 2017 19:35:40 +0000
Subject: [PATCH] OS-5561 rtld needs to learn about AVX512 OS-5560 Need AVX512
 core kernel support Reviewed by: Robert Mustacchi <rm@joyent.com> Approved
 by: Robert Mustacchi <rm@joyent.com>

---
 usr/src/cmd/sgs/include/rtld.h        |   5 +-
 usr/src/cmd/sgs/rtld/amd64/boot_elf.s | 294 ++++++++++++++++++--------
 usr/src/common/elfcap/elfcap.c        |  52 ++++-
 usr/src/common/elfcap/elfcap.h        |   4 +-
 usr/src/uts/common/sys/auxv_386.h     |  23 +-
 usr/src/uts/i86pc/os/cpuid.c          |  59 ++++--
 usr/src/uts/intel/sys/x86_archext.h   |  34 +--
 7 files changed, 343 insertions(+), 128 deletions(-)

diff --git a/usr/src/cmd/sgs/include/rtld.h b/usr/src/cmd/sgs/include/rtld.h
index 65eb25c4b1..0378b7390e 100644
--- a/usr/src/cmd/sgs/include/rtld.h
+++ b/usr/src/cmd/sgs/include/rtld.h
@@ -21,6 +21,7 @@
 
 /*
  * Copyright (c) 1995, 2010, Oracle and/or its affiliates. All rights reserved.
+ * Copyright (c) 2017, Joyent, Inc.
  */
 #ifndef	_RTLD_H
 #define	_RTLD_H
@@ -1112,8 +1113,8 @@ struct sresult {
  * hardware name, thus this structure is a little simpler.
  *
  * Note, the amd64 version of elf_rtbndr assumes that the sc_hw_1 value is at
- * offset zero. If you are changing this structure in a way that invalidates
- * this you need to update that code.
+ * offset zero and sc_hw_2 is at offset 8. If you are changing this structure
+ * in a way that invalidates this, you need to update that code.
  */
 typedef	struct {
 	elfcap_mask_t	sc_hw_1;	/* CA_SUNW_HW_1 capabilities */
diff --git a/usr/src/cmd/sgs/rtld/amd64/boot_elf.s b/usr/src/cmd/sgs/rtld/amd64/boot_elf.s
index 81e46cfd13..67301a0c96 100644
--- a/usr/src/cmd/sgs/rtld/amd64/boot_elf.s
+++ b/usr/src/cmd/sgs/rtld/amd64/boot_elf.s
@@ -22,7 +22,7 @@
 /*
  * Copyright 2008 Sun Microsystems, Inc.  All rights reserved.
  * Use is subject to license terms.
- * Copyright (c) 2012 Joyent, Inc. All rights reserved.
+ * Copyright (c) 2017 Joyent, Inc. All rights reserved.
  */
 
 #if	defined(lint)
@@ -107,12 +107,20 @@ elf_plt_trace()
  *	    %r11			 8
  *	    %rax			 8
  *				    =======
- *			    Subtotal:	144 (32byte aligned)
+ *			    Subtotal:	144 (64byte aligned)
+ *
+ * The amd64 ABI says that the first 8 floating point registers are used to
+ * pass arguments. Because we may end up loading a DSO which has an _init 
+ * section which uses these registers, we must save them. While various
+ * CPUs have different sized floating point registers, the largest are 
+ * AVX512 512-bit (64-byte) %zmm registers. These need to be 64-byte aligned.
+ * Although the example below is showing offsets for %rbp, we actually save the
+ * registers offset from %rsp, which is forced into 64-bit alignment.
  *
  *	Saved Media Regs (used to pass floating point args):
- *	    %xmm0 - %xmm7   32 * 8:	256
+ *	    %zmm0 - %zmm7   64 * 8:	512
  *				    =======
- *			    Total:	400 (32byte aligned)
+ *			    Total:	656 (64byte aligned)
  *  
  *  So - will subtract the following to create enough space
  *
@@ -132,14 +140,14 @@ elf_plt_trace()
  *	-144(%rbp)	entering %r10
  *	-152(%rbp)	entering %r11
  *	-160(%rbp)	entering %rax
- *	-192(%rbp)	entering %xmm0
- *	-224(%rbp)	entering %xmm1
- *	-256(%rbp)	entering %xmm2
+ *	-224(%rbp)	entering %xmm0
+ *	-288(%rbp)	entering %xmm1
+ *	-352(%rbp)	entering %xmm2
  *	-288(%rbp)	entering %xmm3
- *	-320(%rbp)	entering %xmm4
- *	-384(%rbp)	entering %xmm5
- *	-416(%rbp)	entering %xmm6
- *	-448(%rbp)	entering %xmm7
+ *	-416(%rbp)	entering %xmm4
+ *	-480(%rbp)	entering %xmm5
+ *	-544(%rbp)	entering %xmm6
+ *	-608(%rbp)	entering %xmm7
  *
  */
 #define	SPDYNOFF    -8
@@ -149,26 +157,26 @@ elf_plt_trace()
 
 /*
  * The next set of offsets are relative to %rsp.
- * We guarantee %rsp is ABI compliant 32-byte aligned.  This guarantees the
- * ymm registers are saved to 32-byte aligned addresses.
+ * We guarantee %rsp is 64-byte aligned.  This guarantees the zmm registers are
+ * saved to 64-byte aligned addresses.
  * %rbp may only be 8 byte aligned if we came in from non-ABI compliant code.
  */ 
-#define	SPRDIOFF	320
-#define	SPRSIOFF	312
-#define	SPRDXOFF	304
-#define	SPRCXOFF	296
-#define	SPR8OFF		288
-#define	SPR9OFF		280
-#define	SPR10OFF	272
-#define	SPR11OFF	264
-#define	SPRAXOFF	256
-#define	SPXMM0OFF	224
-#define	SPXMM1OFF	192
-#define	SPXMM2OFF	160
-#define	SPXMM3OFF	128
-#define	SPXMM4OFF	96
-#define	SPXMM5OFF	64
-#define	SPXMM6OFF	32
+#define	SPRDIOFF	576
+#define	SPRSIOFF	568
+#define	SPRDXOFF	560
+#define	SPRCXOFF	552
+#define	SPR8OFF		544
+#define	SPR9OFF		536
+#define	SPR10OFF	528
+#define	SPR11OFF	520
+#define	SPRAXOFF	512
+#define	SPXMM0OFF	448
+#define	SPXMM1OFF	384
+#define	SPXMM2OFF	320
+#define	SPXMM3OFF	256
+#define	SPXMM4OFF	192
+#define	SPXMM5OFF	128
+#define	SPXMM6OFF	64
 #define	SPXMM7OFF	0
 
 	/* See elf_rtbndr for explanation behind org_scapset */
@@ -178,12 +186,12 @@ elf_plt_trace()
 	.align 16
 elf_plt_trace:
 	/*
-	 * Enforce ABI 32-byte stack alignment here.
-	 * The next andq instruction does this pseudo code:
-	 * If %rsp is 8 byte aligned then subtract 8 from %rsp.
+	 * Enforce 64-byte stack alignment here.
+	 * The next andq instruction essentially does this:
+	 *     if necessary, subtract N bytes from %rsp to align on 64 bytes.
 	 */
-	andq    $-32, %rsp	/* enforce ABI 32-byte stack alignment */
-	subq	$400,%rsp	/ create some local storage
+	andq    $-64, %rsp	/* enforce 64-byte stack alignment */
+	subq	$656,%rsp	/ create some local storage
 
 	movq	%rdi, SPRDIOFF(%rsp)
 	movq	%rsi, SPRSIOFF(%rsp)
@@ -196,8 +204,11 @@ elf_plt_trace:
 	movq	%rax, SPRAXOFF(%rsp)
 
 	movq	org_scapset@GOTPCREL(%rip),%r9
-	movq	(%r9),%r9
-	movl	(%r9),%edx
+	movq	(%r9),%r9			/* Syscapset_t pointer */
+	movl	8(%r9),%edx			/* sc_hw_2 */
+	testl	$AV_386_2_AVX512F,%edx
+	jne	.trace_save_zmm
+	movl	(%r9),%edx			/* sc_hw_1 */
 	testl	$AV_386_AVX,%edx
 	jne	.trace_save_ymm
 
@@ -221,6 +232,17 @@ elf_plt_trace:
 	vmovdqa	%ymm5, SPXMM5OFF(%rsp)
 	vmovdqa	%ymm6, SPXMM6OFF(%rsp)
 	vmovdqa	%ymm7, SPXMM7OFF(%rsp)
+	jmp	.trace_save_finish
+
+.trace_save_zmm:
+	vmovdqa64	%zmm0, SPXMM0OFF(%rsp)
+	vmovdqa64	%zmm1, SPXMM1OFF(%rsp)
+	vmovdqa64	%zmm2, SPXMM2OFF(%rsp)
+	vmovdqa64	%zmm3, SPXMM3OFF(%rsp)
+	vmovdqa64	%zmm4, SPXMM4OFF(%rsp)
+	vmovdqa64	%zmm5, SPXMM5OFF(%rsp)
+	vmovdqa64	%zmm6, SPXMM6OFF(%rsp)
+	vmovdqa64	%zmm7, SPXMM7OFF(%rsp)
 
 .trace_save_finish:
 
@@ -298,8 +320,11 @@ elf_plt_trace:
 	/ Restore registers
 	/
 	movq	org_scapset@GOTPCREL(%rip),%r9
-	movq	(%r9),%r9
-	movl	(%r9),%edx
+	movq	(%r9),%r9			/* Syscapset_t pointer */
+	movl	8(%r9),%edx			/* sc_hw_2 */
+	testl	$AV_386_2_AVX512F,%edx
+	jne	.trace_restore_zmm
+	movl	(%r9),%edx			/* sc_hw_1 */
 	testl	$AV_386_AVX,%edx
 	jne	.trace_restore_ymm
 
@@ -323,6 +348,17 @@ elf_plt_trace:
 	vmovdqa	SPXMM5OFF(%rsp), %ymm5
 	vmovdqa	SPXMM6OFF(%rsp), %ymm6
 	vmovdqa	SPXMM7OFF(%rsp), %ymm7
+	jmp	.trace_restore_finish
+
+.trace_restore_zmm:
+	vmovdqa64	SPXMM0OFF(%rsp), %zmm0
+	vmovdqa64	SPXMM1OFF(%rsp), %zmm1
+	vmovdqa64	SPXMM2OFF(%rsp), %zmm2
+	vmovdqa64	SPXMM3OFF(%rsp), %zmm3
+	vmovdqa64	SPXMM4OFF(%rsp), %zmm4
+	vmovdqa64	SPXMM5OFF(%rsp), %zmm5
+	vmovdqa64	SPXMM6OFF(%rsp), %zmm6
+	vmovdqa64	SPXMM7OFF(%rsp), %zmm7
 
 .trace_restore_finish:
 	movq	SPRDIOFF(%rsp), %rdi
@@ -412,8 +448,11 @@ elf_plt_trace:
 
 	/ Yes, we have to do this dance again. Sorry.
 	movq	org_scapset@GOTPCREL(%rip),%r9
-	movq	(%r9),%r9
-	movl	(%r9),%edx
+	movq	(%r9),%r9			/* Syscapset_t pointer */
+	movl	8(%r9),%edx			/* sc_hw_2 */
+	testl	$AV_386_2_AVX512F,%edx
+	jne	.trace_r2_zmm
+	movl	(%r9),%edx			/* sc_hw_1 */
 	testl	$AV_386_AVX,%edx
 	jne	.trace_r2_ymm
 
@@ -437,6 +476,17 @@ elf_plt_trace:
 	vmovdqa	SPXMM5OFF(%r11), %ymm5
 	vmovdqa	SPXMM6OFF(%r11), %ymm6
 	vmovdqa	SPXMM7OFF(%r11), %ymm7
+	jmp	.trace_r2_finish
+
+.trace_r2_zmm:
+	vmovdqa64	SPXMM0OFF(%r11), %zmm0
+	vmovdqa64	SPXMM1OFF(%r11), %zmm1
+	vmovdqa64	SPXMM2OFF(%r11), %zmm2
+	vmovdqa64	SPXMM3OFF(%r11), %zmm3
+	vmovdqa64	SPXMM4OFF(%r11), %zmm4
+	vmovdqa64	SPXMM5OFF(%r11), %zmm5
+	vmovdqa64	SPXMM6OFF(%r11), %zmm6
+	vmovdqa64	SPXMM7OFF(%r11), %zmm7
 
 .trace_r2_finish:
 	movq	SPRDIOFF(%r11), %rdi
@@ -476,6 +526,49 @@ elf_plt_trace:
 	/
 	/ Restore registers
 	/
+
+	movq	org_scapset@GOTPCREL(%rip),%r9
+	movq	(%r9),%r9			/* Syscapset_t pointer */
+	movl	8(%r9),%edx			/* sc_hw_2 */
+	testl	$AV_386_2_AVX512F,%edx
+	jne	.trace_r3_zmm
+	movl	(%r9),%edx			/* sc_hw_1 */
+	testl	$AV_386_AVX,%edx
+	jne	.trace_r3_ymm
+
+.trace_r3_xmm:
+	movdqa	SPXMM0OFF(%rsp), %xmm0
+	movdqa	SPXMM1OFF(%rsp), %xmm1
+	movdqa	SPXMM2OFF(%rsp), %xmm2
+	movdqa	SPXMM3OFF(%rsp), %xmm3
+	movdqa	SPXMM4OFF(%rsp), %xmm4
+	movdqa	SPXMM5OFF(%rsp), %xmm5
+	movdqa	SPXMM6OFF(%rsp), %xmm6
+	movdqa	SPXMM7OFF(%rsp), %xmm7
+	jmp .trace_r3_finish
+
+.trace_r3_ymm:
+	vmovdqa	SPXMM0OFF(%rsp), %ymm0
+	vmovdqa	SPXMM1OFF(%rsp), %ymm1
+	vmovdqa	SPXMM2OFF(%rsp), %ymm2
+	vmovdqa	SPXMM3OFF(%rsp), %ymm3
+	vmovdqa	SPXMM4OFF(%rsp), %ymm4
+	vmovdqa	SPXMM5OFF(%rsp), %ymm5
+	vmovdqa	SPXMM6OFF(%rsp), %ymm6
+	vmovdqa	SPXMM7OFF(%rsp), %ymm7
+	jmp .trace_r3_finish
+
+.trace_r3_zmm:
+	vmovdqa64	SPXMM0OFF(%rsp), %zmm0
+	vmovdqa64	SPXMM1OFF(%rsp), %zmm1
+	vmovdqa64	SPXMM2OFF(%rsp), %zmm2
+	vmovdqa64	SPXMM3OFF(%rsp), %zmm3
+	vmovdqa64	SPXMM4OFF(%rsp), %zmm4
+	vmovdqa64	SPXMM5OFF(%rsp), %zmm5
+	vmovdqa64	SPXMM6OFF(%rsp), %zmm6
+	vmovdqa64	SPXMM7OFF(%rsp), %zmm7
+
+.trace_r3_finish:
 	movq	SPRDIOFF(%rsp), %rdi
 	movq	SPRSIOFF(%rsp), %rsi
 	movq	SPRDXOFF(%rsp), %rdx
@@ -485,14 +578,6 @@ elf_plt_trace:
 	movq	SPR10OFF(%rsp), %r10
 	movq	SPR11OFF(%rsp), %r11
 	// rax already contains return value
-	movdqa	SPXMM0OFF(%rsp), %xmm0
-	movdqa	SPXMM1OFF(%rsp), %xmm1
-	movdqa	SPXMM2OFF(%rsp), %xmm2
-	movdqa	SPXMM3OFF(%rsp), %xmm3
-	movdqa	SPXMM4OFF(%rsp), %xmm4
-	movdqa	SPXMM5OFF(%rsp), %xmm5
-	movdqa	SPXMM6OFF(%rsp), %xmm6
-	movdqa	SPXMM7OFF(%rsp), %xmm7
 
 	movq	%rbp, %rsp			/
 	popq	%rbp				/
@@ -560,12 +645,14 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
  * arguments before interposing functions to resolve the called function. 
  * Possible arguments must be restored before invoking the resolved function.
  * 
- * Before the AVX instruction set enhancements to AMD64 there were no changes in
- * the set of registers and their sizes across different processors. With AVX,
- * the xmm registers became the lower 128 bits of the ymm registers. Because of
- * this, we need to conditionally save 256 bits instead of 128 bits. Regardless
- * of whether we have ymm registers or not, we're always going to push the stack
- * space assuming that we do to simplify the code.
+ * Before the AVX and AVX512 instruction set enhancements to AMD64, there were
+ * no changes in the set of registers and their sizes across different
+ * processors. With AVX, the xmm registers became the lower 128 bits of the ymm
+ * registers. With AVX512, the ymm registers became the lower 256 bits of the
+ * zmm registers. Because of this, we need to conditionally save either 256 bits
+ * or 512 bits, instead of 128 bits. Regardless of whether we have zmm registers
+ * or not, we're always going to push the stack space assuming that we do, to
+ * simplify the code.
  * 
  * Local stack space storage for elf_rtbndr is allocated as follows:
  *
@@ -579,12 +666,12 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
  *	    %r9				 8
  *	    %r10			 8
  *				    =======
- *			    Subtotal:   64 (32byte aligned)
+ *			    Subtotal:   64 (64byte aligned)
  *
  *	Saved Media Regs (used to pass floating point args):
- *	    %ymm0 - %ymm7   32 * 8     256
+ *	    %zmm0 - %zmm7   64 * 8     512
  *				    =======
- *			    Total:     320 (32byte aligned)
+ *			    Total:     576 (64byte aligned)
  *  
  *  So - will subtract the following to create enough space
  *
@@ -596,25 +683,24 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
  *	40(%rsp)	save %r8
  *	48(%rsp)	save %r9
  *	56(%rsp)	save %r10
- *	64(%rsp)	save %ymm0
- *	96(%rsp)	save %ymm1
- *	128(%rsp)	save %ymm2
- *	160(%rsp)	save %ymm3
- *	192(%rsp)	save %ymm4
- *	224(%rsp)	save %ymm5
- *	256(%rsp)	save %ymm6
- *	288(%rsp)	save %ymm7
+ *	64(%rsp)	save %zmm0
+ *	128(%rsp)	save %zmm1
+ *	192(%rsp)	save %zmm2
+ *	256(%rsp)	save %zmm3
+ *	320(%rsp)	save %zmm4
+ *	384(%rsp)	save %zmm5
+ *	448(%rsp)	save %zmm6
+ *	512(%rsp)	save %zmm7
  *
- * Note: Some callers may use 8-byte stack alignment instead of the
- * ABI required 16-byte alignment.  We use %rsp offsets to save/restore
- * registers because %rbp may not be 16-byte aligned.  We guarantee %rsp
- * is 16-byte aligned in the function preamble.
+ * Note: Some callers may use 8-byte stack alignment. We use %rsp offsets to
+ * save/restore registers because %rbp may not be 64-byte aligned. We guarantee
+ * %rsp is 64-byte aligned in the function preamble.
  */
 /*
- * As the registers may either be xmm or ymm, we've left the name as xmm, but
- * increased the offset between them to always cover the xmm and ymm cases.
+ * As the registers may either be xmm, ymm or zmm, we've left the name as xmm,
+ * but increased the offset between them to always cover the zmm case.
  */
-#define	LS_SIZE	$320	/* local stack space to save all possible arguments */
+#define	LS_SIZE	$576	/* local stack space to save all possible arguments */
 #define	LSRAXOFF	0	/* for SSE register count */
 #define	LSRDIOFF	8	/* arg 0 ... */
 #define	LSRSIOFF	16
@@ -624,13 +710,13 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
 #define	LSR9OFF		48
 #define	LSR10OFF	56	/* ... arg 5 */
 #define	LSXMM0OFF	64	/* SSE arg 0 ... */
-#define	LSXMM1OFF	96
-#define	LSXMM2OFF	128
-#define	LSXMM3OFF	160
-#define	LSXMM4OFF	192
-#define	LSXMM5OFF	224
-#define	LSXMM6OFF	256
-#define	LSXMM7OFF	288	/* ... SSE arg 7 */
+#define	LSXMM1OFF	128
+#define	LSXMM2OFF	192
+#define	LSXMM3OFF	256
+#define	LSXMM4OFF	320
+#define	LSXMM5OFF	384
+#define	LSXMM6OFF	448
+#define	LSXMM7OFF	512	/* ... SSE arg 7 */
 
 	/*
 	 * The org_scapset is a global variable that is a part of rtld. It
@@ -651,11 +737,11 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
 
 	/*
 	 * Some libraries may (incorrectly) use non-ABI compliant 8-byte stack
-	 * alignment.  Enforce ABI 16-byte stack alignment here.
-	 * The next andq instruction does this pseudo code:
-	 * If %rsp is 8 byte aligned then subtract 8 from %rsp.
+	 * alignment.  Enforce 64-byte stack alignment here.
+	 * The next andq instruction essentially does this:
+	 *     if necessary, subtract N bytes from %rsp to align on 64 bytes.
 	 */
-	andq	$-32, %rsp	/* enforce ABI 32-byte stack alignment */
+	andq	$-64, %rsp	/* enforce 64-byte stack alignment */
 
 	subq	LS_SIZE, %rsp	/* save all ABI defined argument registers */
 
@@ -669,11 +755,14 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
 	movq	%r10, LSR10OFF(%rsp)	/* call chain reg */
 
 	/*
-	 * Our xmm registers could secretly by ymm registers in disguise.
+	 * Our xmm registers could secretly be ymm or zmm registers in disguise.
 	 */
 	movq	org_scapset@GOTPCREL(%rip),%r9
-	movq	(%r9),%r9
-	movl	(%r9),%edx
+	movq	(%r9),%r9		/* Syscapset_t pointer */
+	movl	8(%r9),%edx		/* sc_hw_2 */
+	testl	$AV_386_2_AVX512F,%edx
+	jne	.save_zmm
+	movl	(%r9),%edx		/* sc_hw_1 */
 	testl	$AV_386_AVX,%edx
 	jne	.save_ymm
 
@@ -686,7 +775,7 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
 	movdqa	%xmm5, LSXMM5OFF(%rsp)
 	movdqa	%xmm6, LSXMM6OFF(%rsp)
 	movdqa	%xmm7, LSXMM7OFF(%rsp)	/* ... SSE arg 7 */
-	jmp	.save_finish	
+	jmp	.save_finish
 
 .save_ymm:
 	vmovdqa	%ymm0, LSXMM0OFF(%rsp)	/* SSE arg 0 ... */
@@ -697,6 +786,17 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
 	vmovdqa	%ymm5, LSXMM5OFF(%rsp)
 	vmovdqa	%ymm6, LSXMM6OFF(%rsp)
 	vmovdqa	%ymm7, LSXMM7OFF(%rsp)	/* ... SSE arg 7 */
+	jmp	.save_finish
+
+.save_zmm:
+	vmovdqa64	%zmm0, LSXMM0OFF(%rsp)	/* SSE arg 0 ... */
+	vmovdqa64	%zmm1, LSXMM1OFF(%rsp)
+	vmovdqa64	%zmm2, LSXMM2OFF(%rsp)
+	vmovdqa64	%zmm3, LSXMM3OFF(%rsp)
+	vmovdqa64	%zmm4, LSXMM4OFF(%rsp)
+	vmovdqa64	%zmm5, LSXMM5OFF(%rsp)
+	vmovdqa64	%zmm6, LSXMM6OFF(%rsp)
+	vmovdqa64	%zmm7, LSXMM7OFF(%rsp)	/* ... SSE arg 7 */
 
 .save_finish:
 	movq	LBPLMPOFF(%rbp), %rdi	/* arg1 - *lmp */
@@ -707,11 +807,14 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
 
 	/*
 	 * Restore possible arguments before invoking resolved function. We
-	 * check the xmm vs. ymm regs first so we can use the others.
+	 * check the xmm, ymm, vs. zmm regs first so we can use the others.
 	 */
 	movq	org_scapset@GOTPCREL(%rip),%r9
-	movq	(%r9),%r9
-	movl	(%r9),%edx
+	movq	(%r9),%r9		/* Syscapset_t pointer */
+	movl	8(%r9),%edx		/* sc_hw_2 */
+	testl	$AV_386_2_AVX512F,%edx
+	jne	.restore_zmm
+	movl	(%r9),%edx		/* sc_hw_1 */
 	testl	$AV_386_AVX,%edx
 	jne	.restore_ymm
 
@@ -735,6 +838,17 @@ elf_rtbndr(Rt_map * lmp, unsigned long reloc, caddr_t pc)
 	vmovdqa	LSXMM5OFF(%rsp), %ymm5
 	vmovdqa	LSXMM6OFF(%rsp), %ymm6
 	vmovdqa	LSXMM7OFF(%rsp), %ymm7
+	jmp .restore_finish
+
+.restore_zmm:
+	vmovdqa64	LSXMM0OFF(%rsp), %zmm0
+	vmovdqa64	LSXMM1OFF(%rsp), %zmm1
+	vmovdqa64	LSXMM2OFF(%rsp), %zmm2
+	vmovdqa64	LSXMM3OFF(%rsp), %zmm3
+	vmovdqa64	LSXMM4OFF(%rsp), %zmm4
+	vmovdqa64	LSXMM5OFF(%rsp), %zmm5
+	vmovdqa64	LSXMM6OFF(%rsp), %zmm6
+	vmovdqa64	LSXMM7OFF(%rsp), %zmm7
 
 .restore_finish:
 	movq	LSRAXOFF(%rsp), %rax
diff --git a/usr/src/common/elfcap/elfcap.c b/usr/src/common/elfcap/elfcap.c
index d63db8e981..7c39f30a2b 100644
--- a/usr/src/common/elfcap/elfcap.c
+++ b/usr/src/common/elfcap/elfcap.c
@@ -21,7 +21,7 @@
 
 /*
  * Copyright (c) 2004, 2010, Oracle and/or its affiliates. All rights reserved.
- * Copyright (c) 2015, Joyent, Inc.
+ * Copyright (c) 2017, Joyent, Inc.
  */
 
 /* LINTLIBRARY */
@@ -339,7 +339,55 @@ static const elfcap_desc_t hw2_386[ELFCAP_NUM_HW2_386] = {
 	{						/* 0x00000080 */
 		AV_386_2_RDSEED, STRDESC("AV_386_2_RDSEED"),
 		STRDESC("RDSEED"), STRDESC("rdseed"),
-	}
+	},
+	{						/* 0x00000100 */
+		AV_386_2_AVX512F, STRDESC("AV_386_2_AVX512F"),
+		STRDESC("AVX512F"), STRDESC("avx512f"),
+	},
+	{						/* 0x00000200 */
+		AV_386_2_AVX512DQ, STRDESC("AV_386_2_AVX512DQ"),
+		STRDESC("AVX512DQ"), STRDESC("avx512dq"),
+	},
+	{						/* 0x00000400 */
+		AV_386_2_AVX512IFMA, STRDESC("AV_386_2_AVX512IFMA"),
+		STRDESC("AVX512IFMA"), STRDESC("avx512ifma"),
+	},
+	{						/* 0x00000800 */
+		AV_386_2_AVX512PF, STRDESC("AV_386_2_AVX512PF"),
+		STRDESC("AVX512PF"), STRDESC("avx512pf"),
+	},
+	{						/* 0x00001000 */
+		AV_386_2_AVX512ER, STRDESC("AV_386_2_AVX512ER"),
+		STRDESC("AVX512ER"), STRDESC("avx512er"),
+	},
+	{						/* 0x00002000 */
+		AV_386_2_AVX512CD, STRDESC("AV_386_2_AVX512CD"),
+		STRDESC("AVX512CD"), STRDESC("avx512cd"),
+	},
+	{						/* 0x00004000 */
+		AV_386_2_AVX512BW, STRDESC("AV_386_2_AVX512BW"),
+		STRDESC("AVX512BW"), STRDESC("avx512bw"),
+	},
+	{						/* 0x00008000 */
+		AV_386_2_AVX512VL, STRDESC("AV_386_2_AVX512VL"),
+		STRDESC("AVX512VL"), STRDESC("avx512vl"),
+	},
+	{						/* 0x00010000 */
+		AV_386_2_AVX512VBMI, STRDESC("AV_386_2_AVX512VBMI"),
+		STRDESC("AVX512VBMI"), STRDESC("avx512vbmi"),
+	},
+	{						/* 0x00020000 */
+		AV_386_2_AVX512VPOPCDQ, STRDESC("AV_386_2_AVX512VPOPCDQ"),
+		STRDESC("AVX512VPOPCDQ"), STRDESC("avx512_vpopcntdq"),
+	},
+	{						/* 0x00040000 */
+		AV_386_2_AVX512_4NNIW, STRDESC("AV_386_2_AVX512_4NNIW"),
+		STRDESC("AVX512_4NNIW"), STRDESC("avx512_4nniw"),
+	},
+	{						/* 0x00080000 */
+		AV_386_2_AVX512_4FMAPS, STRDESC("AV_386_2_AVX512_4FMAPS"),
+		STRDESC("AVX512_4FMAPS"), STRDESC("avx512_4fmaps"),
+	},
 };
 
 /*
diff --git a/usr/src/common/elfcap/elfcap.h b/usr/src/common/elfcap/elfcap.h
index f3e29a6a97..8c883bc208 100644
--- a/usr/src/common/elfcap/elfcap.h
+++ b/usr/src/common/elfcap/elfcap.h
@@ -21,7 +21,7 @@
 
 /*
  * Copyright (c) 2004, 2010, Oracle and/or its affiliates. All rights reserved.
- * Copyright (c) 2015, Joyent, Inc.
+ * Copyright (c) 2017, Joyent, Inc.
  */
 
 #ifndef _ELFCAP_DOT_H
@@ -115,7 +115,7 @@ typedef enum {
 #define	ELFCAP_NUM_SF1			3
 #define	ELFCAP_NUM_HW1_SPARC		17
 #define	ELFCAP_NUM_HW1_386		32
-#define	ELFCAP_NUM_HW2_386		8
+#define	ELFCAP_NUM_HW2_386		20
 
 
 /*
diff --git a/usr/src/uts/common/sys/auxv_386.h b/usr/src/uts/common/sys/auxv_386.h
index a3256a464f..d94eb18351 100644
--- a/usr/src/uts/common/sys/auxv_386.h
+++ b/usr/src/uts/common/sys/auxv_386.h
@@ -20,7 +20,7 @@
  */
 /*
  * Copyright (c) 2004, 2010, Oracle and/or its affiliates. All rights reserved.
- * Copyright (c) 2015, Joyent, Inc.
+ * Copyright (c) 2017, Joyent, Inc.
  */
 
 #ifndef	_SYS_AUXV_386_H
@@ -83,6 +83,9 @@ extern "C" {
 	"\017sse3\015sse2\014sse\013fxsr\012amd3dx\011amd3d"		\
 	"\010amdmmx\07mmx\06cmov\05amdsysc\04sep\03cx8\02tsc\01fpu"
 
+/*
+ * Flags used in AT_SUN_HWCAP2 elements
+ */
 #define	AV_386_2_F16C		0x00001	/* F16C half percision extensions */
 #define	AV_386_2_RDRAND		0x00002	/* RDRAND insn */
 #define	AV_386_2_BMI1		0x00004 /* BMI1 insns */
@@ -91,10 +94,24 @@ extern "C" {
 #define	AV_386_2_AVX2		0x00020	/* AVX2 insns */
 #define	AV_386_2_ADX		0x00040	/* ADX insns */
 #define	AV_386_2_RDSEED		0x00080	/* RDSEED insn */
+#define	AV_386_2_AVX512F	0x00100	/* AVX512 foundation insns */
+#define	AV_386_2_AVX512DQ	0x00200	/* AVX512DQ insns */
+#define	AV_386_2_AVX512IFMA	0x00400	/* AVX512IFMA insns */
+#define	AV_386_2_AVX512PF	0x00800	/* AVX512PF insns */
+#define	AV_386_2_AVX512ER	0x01000	/* AVX512ER insns */
+#define	AV_386_2_AVX512CD	0x02000	/* AVX512CD insns */
+#define	AV_386_2_AVX512BW	0x04000	/* AVX512BW insns */
+#define	AV_386_2_AVX512VL	0x08000	/* AVX512VL insns */
+#define	AV_386_2_AVX512VBMI	0x10000	/* AVX512VBMI insns */
+#define	AV_386_2_AVX512VPOPCDQ	0x20000	/* AVX512VPOPCNTDQ insns */
+#define	AV_386_2_AVX512_4NNIW	0x40000	/* AVX512 4NNIW insns */
+#define	AV_386_2_AVX512_4FMAPS	0x80000	/* AVX512 4FMAPS insns */
 
 #define	FMT_AV_386_2							\
-	"\020"								\
-	"\10rdseed\07adx\06avx2\05fma\04bmi2\03bmi1\02rdrand\01f16c"
+	"\024avx512_4fmaps\023avx512_4nniw\022avx512vpopcntdq"		\
+	"\021avx512vbmi\020avx512vl\017avx512bw\016avx512cd"		\
+	"\015avx512er\014avx512pf\013avx512ifma\012avx512dq\011avx512f"	\
+	"\010rdseed\07adx\06avx2\05fma\04bmi2\03bmi1\02rdrand\01f16c"
 
 #ifdef __cplusplus
 }
diff --git a/usr/src/uts/i86pc/os/cpuid.c b/usr/src/uts/i86pc/os/cpuid.c
index 085b130598..66beefb400 100644
--- a/usr/src/uts/i86pc/os/cpuid.c
+++ b/usr/src/uts/i86pc/os/cpuid.c
@@ -183,11 +183,12 @@ static char *x86_feature_names[NUM_X86_FEATURES] = {
 	"avx512er",
 	"avx512cd",
 	"avx512bw",
+	"avx512vl",
 	"avx512fma",
 	"avx512vbmi",
-	"avx512vpcdq",
-	"avx512nniw",
-	"avx512fmaps",
+	"avx512_vpopcntdq",
+	"avx512_4vnniw",
+	"avx512_4fmaps",
 	"xsaveopt",
 	"xsavec",
 	"xsaves",
@@ -976,17 +977,13 @@ setup_xfem(void)
 
 	/*
 	 * TBD:
-	 * Enabling MPX and AVX512 implies that xsave_state is large enough
-	 * to hold the MPX state and the full AVX512 state, or that we're
-	 * supporting xsavec or xsaveopt.
-	 *
 	 * if (is_x86_feature(x86_featureset, X86FSET_MPX))
 	 *	flags |= XFEATURE_MPX;
-	 *
-	 * if (is_x86_feature(x86_featureset, X86FSET_AVX512F))
-	 *	flags |= XFEATURE_AVX512;
 	 */
 
+	if (is_x86_feature(x86_featureset, X86FSET_AVX512F))
+		flags |= XFEATURE_AVX512;
+
 	set_xcr(XFEATURE_ENABLED_MASK, flags);
 
 	xsave_bv_all = flags;
@@ -1484,15 +1481,19 @@ cpuid_pass1(cpu_t *cpu, uchar_t *featureset)
 				    CPUID_INTC_EBX_7_0_AVX512BW)
 					add_x86_feature(featureset,
 					    X86FSET_AVX512BW);
+				if (cpi->cpi_std[7].cp_ebx &
+				    CPUID_INTC_EBX_7_0_AVX512VL)
+					add_x86_feature(featureset,
+					    X86FSET_AVX512VL);
 
 				if (cpi->cpi_std[7].cp_ecx &
 				    CPUID_INTC_ECX_7_0_AVX512VBMI)
 					add_x86_feature(featureset,
 					    X86FSET_AVX512VBMI);
 				if (cpi->cpi_std[7].cp_ecx &
-				    CPUID_INTC_ECX_7_0_AVX512VPCDQ)
+				    CPUID_INTC_ECX_7_0_AVX512VPOPCDQ)
 					add_x86_feature(featureset,
-					    X86FSET_AVX512VPCDQ);
+					    X86FSET_AVX512VPOPCDQ);
 
 				if (cpi->cpi_std[7].cp_edx &
 				    CPUID_INTC_EDX_7_0_AVX5124NNIW)
@@ -2271,12 +2272,14 @@ cpuid_pass2(cpu_t *cpu)
 					    X86FSET_AVX512CD);
 					remove_x86_feature(x86_featureset,
 					    X86FSET_AVX512BW);
+					remove_x86_feature(x86_featureset,
+					    X86FSET_AVX512VL);
 					remove_x86_feature(x86_featureset,
 					    X86FSET_AVX512FMA);
 					remove_x86_feature(x86_featureset,
 					    X86FSET_AVX512VBMI);
 					remove_x86_feature(x86_featureset,
-					    X86FSET_AVX512VPCDQ);
+					    X86FSET_AVX512VPOPCDQ);
 					remove_x86_feature(x86_featureset,
 					    X86FSET_AVX512NNIW);
 					remove_x86_feature(x86_featureset,
@@ -3035,17 +3038,47 @@ cpuid_pass4(cpu_t *cpu, uint_t *hwcap_out)
 			hwcap_flags |= AV_386_XSAVE;
 
 			if (*ecx & CPUID_INTC_ECX_AVX) {
+				uint32_t *ecx_7 = &CPI_FEATURES_7_0_ECX(cpi);
+				uint32_t *edx_7 = &CPI_FEATURES_7_0_EDX(cpi);
+
 				hwcap_flags |= AV_386_AVX;
 				if (*ecx & CPUID_INTC_ECX_F16C)
 					hwcap_flags_2 |= AV_386_2_F16C;
 				if (*ecx & CPUID_INTC_ECX_FMA)
 					hwcap_flags_2 |= AV_386_2_FMA;
+
 				if (*ebx & CPUID_INTC_EBX_7_0_BMI1)
 					hwcap_flags_2 |= AV_386_2_BMI1;
 				if (*ebx & CPUID_INTC_EBX_7_0_BMI2)
 					hwcap_flags_2 |= AV_386_2_BMI2;
 				if (*ebx & CPUID_INTC_EBX_7_0_AVX2)
 					hwcap_flags_2 |= AV_386_2_AVX2;
+				if (*ebx & CPUID_INTC_EBX_7_0_AVX512F)
+					hwcap_flags_2 |= AV_386_2_AVX512F;
+				if (*ebx & CPUID_INTC_EBX_7_0_AVX512DQ)
+					hwcap_flags_2 |= AV_386_2_AVX512DQ;
+				if (*ebx & CPUID_INTC_EBX_7_0_AVX512IFMA)
+					hwcap_flags_2 |= AV_386_2_AVX512IFMA;
+				if (*ebx & CPUID_INTC_EBX_7_0_AVX512PF)
+					hwcap_flags_2 |= AV_386_2_AVX512PF;
+				if (*ebx & CPUID_INTC_EBX_7_0_AVX512ER)
+					hwcap_flags_2 |= AV_386_2_AVX512ER;
+				if (*ebx & CPUID_INTC_EBX_7_0_AVX512CD)
+					hwcap_flags_2 |= AV_386_2_AVX512CD;
+				if (*ebx & CPUID_INTC_EBX_7_0_AVX512BW)
+					hwcap_flags_2 |= AV_386_2_AVX512BW;
+				if (*ebx & CPUID_INTC_EBX_7_0_AVX512VL)
+					hwcap_flags_2 |= AV_386_2_AVX512VL;
+
+				if (*ecx_7 & CPUID_INTC_ECX_7_0_AVX512VBMI)
+					hwcap_flags_2 |= AV_386_2_AVX512VBMI;
+				if (*ecx_7 & CPUID_INTC_ECX_7_0_AVX512VPOPCDQ)
+					hwcap_flags_2 |= AV_386_2_AVX512VPOPCDQ;
+
+				if (*edx_7 & CPUID_INTC_EDX_7_0_AVX5124NNIW)
+					hwcap_flags_2 |= AV_386_2_AVX512_4NNIW;
+				if (*edx_7 & CPUID_INTC_EDX_7_0_AVX5124FMAPS)
+					hwcap_flags_2 |= AV_386_2_AVX512_4FMAPS;
 			}
 		}
 		if (*ecx & CPUID_INTC_ECX_VMX)
diff --git a/usr/src/uts/intel/sys/x86_archext.h b/usr/src/uts/intel/sys/x86_archext.h
index 8e51d4d77a..646aa5ac81 100644
--- a/usr/src/uts/intel/sys/x86_archext.h
+++ b/usr/src/uts/intel/sys/x86_archext.h
@@ -223,21 +223,22 @@ extern "C" {
 #define	CPUID_INTC_EBX_7_0_AVX512CD	0x10000000	/* AVX512CD */
 #define	CPUID_INTC_EBX_7_0_SHA		0x20000000	/* SHA extensions */
 #define	CPUID_INTC_EBX_7_0_AVX512BW	0x40000000	/* AVX512BW */
+#define	CPUID_INTC_EBX_7_0_AVX512VL	0x80000000	/* AVX512VL */
 
 #define	CPUID_INTC_EBX_7_0_ALL_AVX512 \
 	(CPUID_INTC_EBX_7_0_AVX512F | CPUID_INTC_EBX_7_0_AVX512DQ | \
 	CPUID_INTC_EBX_7_0_AVX512IFMA | CPUID_INTC_EBX_7_0_AVX512PF | \
 	CPUID_INTC_EBX_7_0_AVX512ER | CPUID_INTC_EBX_7_0_AVX512CD | \
-	CPUID_INTC_EBX_7_0_AVX512BW)
+	CPUID_INTC_EBX_7_0_AVX512BW | CPUID_INTC_EBX_7_0_AVX512VL)
 
 #define	CPUID_INTC_ECX_7_0_AVX512VBMI	0x00000002	/* AVX512VBMI */
 #define	CPUID_INTC_ECX_7_0_UMIP		0x00000004	/* UMIP */
 #define	CPUID_INTC_ECX_7_0_PKU		0x00000008	/* umode prot. keys */
 #define	CPUID_INTC_ECX_7_0_OSPKE	0x00000010	/* OSPKE */
-#define	CPUID_INTC_ECX_7_0_AVX512VPCDQ	0x00004000	/* AVX512 VOPPCNTDQ */
+#define	CPUID_INTC_ECX_7_0_AVX512VPOPCDQ 0x00004000	/* AVX512 VPOPCNTDQ */
 
 #define	CPUID_INTC_ECX_7_0_ALL_AVX512 \
-	(CPUID_INTC_ECX_7_0_AVX512VBMI | CPUID_INTC_ECX_7_0_AVX512VPCDQ)
+	(CPUID_INTC_ECX_7_0_AVX512VBMI | CPUID_INTC_ECX_7_0_AVX512VPOPCDQ)
 
 #define	CPUID_INTC_EDX_7_0_AVX5124NNIW	0x00000004	/* AVX512 4NNIW */
 #define	CPUID_INTC_EDX_7_0_AVX5124FMAPS	0x00000008	/* AVX512 4FMAPS */
@@ -438,18 +439,19 @@ extern "C" {
 #define	X86FSET_AVX512ER	53
 #define	X86FSET_AVX512CD	54
 #define	X86FSET_AVX512BW	55
-#define	X86FSET_AVX512FMA	56
-#define	X86FSET_AVX512VBMI	57
-#define	X86FSET_AVX512VPCDQ	58
-#define	X86FSET_AVX512NNIW	59
-#define	X86FSET_AVX512FMAPS	60
-#define	X86FSET_XSAVEOPT	61
-#define	X86FSET_XSAVEC		62
-#define	X86FSET_XSAVES		63
-#define	X86FSET_SHA		64
-#define	X86FSET_UMIP		65
-#define	X86FSET_PKU		66
-#define	X86FSET_OSPKE		67
+#define	X86FSET_AVX512VL	56
+#define	X86FSET_AVX512FMA	57
+#define	X86FSET_AVX512VBMI	58
+#define	X86FSET_AVX512VPOPCDQ	59
+#define	X86FSET_AVX512NNIW	60
+#define	X86FSET_AVX512FMAPS	61
+#define	X86FSET_XSAVEOPT	62
+#define	X86FSET_XSAVEC		63
+#define	X86FSET_XSAVES		64
+#define	X86FSET_SHA		65
+#define	X86FSET_UMIP		66
+#define	X86FSET_PKU		67
+#define	X86FSET_OSPKE		68
 
 /*
  * Intel Deep C-State invariant TSC in leaf 0x80000007.
@@ -708,7 +710,7 @@ extern "C" {
 
 #if defined(_KERNEL) || defined(_KMEMUSER)
 
-#define	NUM_X86_FEATURES	68
+#define	NUM_X86_FEATURES	69
 extern uchar_t x86_featureset[];
 
 extern void free_x86_featureset(void *featureset);
-- 
2.21.0

