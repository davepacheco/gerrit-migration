From 1e80ca17131030b495bff66182b533f1e399914a Mon Sep 17 00:00:00 2001
From: Tim Foster <tim.foster@joyent.com>
Date: Fri, 21 Jun 2019 16:00:24 +0100
Subject: [PATCH] TOOLS-2143 mountain-gorilla should be retired

---
 .gitmodules                                   |    3 -
 Makefile                                      | 2991 -----------------
 README.md                                     |  286 +-
 build.spec.in                                 |   48 -
 configure                                     |  844 -----
 deps/restdown                                 |    1 -
 docs/design.md                                |  115 -
 docs/index.md                                 |  572 ----
 example.build.json                            |    7 -
 gzhosts.json                                  |    6 -
 package.json                                  |   16 -
 platform.imgmanifest.in                       |   27 -
 smartos-live-configure-smartos.mg.in          |   17 -
 smartos-live-configure.mg.in                  |   19 -
 targets.json.in                               | 1506 ---------
 tools/bad-bin/node                            |    2 -
 tools/bad-bin/npm                             |    2 -
 tools/build-zone                              |  255 --
 tools/check-repos-for-release                 |   38 +-
 tools/clean-image.sh                          |   59 -
 tools/jenkins-build                           |  233 --
 tools/jenkins-downstream-projects-for         |   52 -
 tools/json                                    | 1675 ---------
 .../listengprojects.sh => listengprojects}    |    0
 tools/local-bitsdir-copy                      |   36 -
 tools/ls-missing-release-builds               |   41 +-
 tools/manta-upload                            |   57 -
 tools/mantaput-bits                           |  141 -
 tools/prep_dataset_in_jpc.sh                  |  474 ---
 ...urge-mg-builds => purge-builds-from-manta} |  139 +-
 tools/rm-old-builds.py                        |  113 -
 tools/setup-build-zone                        |  148 -
 tools/setup-cloudapi.sh                       |   68 -
 tools/setup-remote-build-zone.sh              |  133 -
 tools/targets-1.6.3.sh                        |   36 -
 tools/targets-13.3.1.sh                       |   27 -
 tools/validate-dc-for-image-creation.sh       |  176 -
 37 files changed, 105 insertions(+), 10258 deletions(-)
 delete mode 100644 .gitmodules
 delete mode 100644 Makefile
 delete mode 100644 build.spec.in
 delete mode 100755 configure
 delete mode 160000 deps/restdown
 delete mode 100644 docs/design.md
 delete mode 100644 docs/index.md
 delete mode 100644 example.build.json
 delete mode 100644 gzhosts.json
 delete mode 100644 package.json
 delete mode 100644 platform.imgmanifest.in
 delete mode 100644 smartos-live-configure-smartos.mg.in
 delete mode 100644 smartos-live-configure.mg.in
 delete mode 100644 targets.json.in
 delete mode 100755 tools/bad-bin/node
 delete mode 100755 tools/bad-bin/npm
 delete mode 100755 tools/build-zone
 delete mode 100644 tools/clean-image.sh
 delete mode 100755 tools/jenkins-build
 delete mode 100755 tools/jenkins-downstream-projects-for
 delete mode 100755 tools/json
 rename tools/{jira/listengprojects.sh => listengprojects} (100%)
 delete mode 100755 tools/local-bitsdir-copy
 delete mode 100755 tools/manta-upload
 delete mode 100755 tools/mantaput-bits
 delete mode 100755 tools/prep_dataset_in_jpc.sh
 rename tools/{purge-mg-builds => purge-builds-from-manta} (75%)
 delete mode 100755 tools/rm-old-builds.py
 delete mode 100644 tools/setup-build-zone
 delete mode 100755 tools/setup-cloudapi.sh
 delete mode 100755 tools/setup-remote-build-zone.sh
 delete mode 100755 tools/targets-1.6.3.sh
 delete mode 100755 tools/targets-13.3.1.sh
 delete mode 100755 tools/validate-dc-for-image-creation.sh

diff --git a/.gitmodules b/.gitmodules
deleted file mode 100644
index 51cf23d..0000000
--- a/.gitmodules
+++ /dev/null
@@ -1,3 +0,0 @@
-[submodule "deps/restdown"]
-	path = deps/restdown
-	url = https://github.com/trentm/restdown.git
diff --git a/Makefile b/Makefile
deleted file mode 100644
index 98e8e97..0000000
--- a/Makefile
+++ /dev/null
@@ -1,2991 +0,0 @@
-#
-# This Source Code Form is subject to the terms of the Mozilla Public
-# License, v. 2.0. If a copy of the MPL was not distributed with this
-# file, You can obtain one at http://mozilla.org/MPL/2.0/.
-#
-
-#
-# Copyright (c) 2019, Joyent, Inc.
-#
-
-#
-# Mountain Gorilla Makefile. See "README.md".
-#
-# Environment variables used:
-#
-# - MG_TARGET (typically set by the Jenkins jobs that run this) is used for
-#   some targets.
-# - UPLOAD_SUBDIRS can be used to specify extra bits subdirs to upload
-#   in the "upload_jenkins" target.
-# ...
-#
-
-#---- Config
-
--include bits/config.mk
-
-# Directories
-TOP := $(shell pwd)
-BUILD_DIR=$(TOP)/build
-BITS_DIR=$(TOP)/bits
-
-#
-# We used to treat $JOB_NAME from Jenkins as the target, so default back to that
-# if $MG_TARGET is not set.
-#
-MG_TARGET ?= $(JOB_NAME)
-
-# Tools
-MAKE = make
-TAR = tar
-RM = rm
-UNAME := $(shell uname)
-PFEXEC =
-ifeq ($(UNAME), SunOS)
-	MAKE = gmake
-	TAR = gtar
-	PFEXEC = pfexec
-	RM = grm
-endif
-JSON=$(MG_NODE) $(TOP)/tools/json
-UPDATES_IMGADM=$(TOP)/node_modules/.bin/updates-imgadm -i $(HOME)/.ssh/automation.id_rsa -u mg --channel="$(UPDATES_CHANNEL)"
-PKGIN = /opt/local/bin/pkgin
-
-# Other
-# Is JOBS=16 reasonable here? The old bamboo plans used this (or higher).
-JOB=16
-
-# A TIMESTAMP to use must be defined (and typically is in 'bits/config.mk').
-#
-# At one point we'd just generate TIMESTAMP at the top of the Makefile, but
-# that seemed to hit a gmake issue when building multiple targets: the 'ca'
-# target would be run three times at (rougly) 4 seconds apart on the time
-# stamp (guessing the 'three times' is because CA_BITS has three elements).
-ifeq ($(TIMESTAMP),)
-	TIMESTAMP=TimestampNotSet
-endif
-
-ifeq ($(UPLOAD_LOCATION),)
-	UPLOAD_LOCATION=bits@bits.joyent.us:builds
-endif
-
-ifeq ($(MG_OUT_PATH),)
-	MG_OUT_PATH=/stor/builds
-endif
-
-#
-# This is set to true by the caller when, and only when, building the a
-# Joyent product.  Doing so causes the inclusion of ancillary repositories that
-# cannot be made publicly available.
-#
-JOYENT_BUILD ?= false
-
-ifeq ($(JOYENT_BUILD),true)
-	FIRMWARE_TOOLS=firmware-tools
-endif
-
-#---- Primary targets
-
-.PHONY: all
-all:
-	@echo "You cannot build all targets on a single build zone at this time."
-	@exit 1
-
-#---- smartlogin
-# TODO:
-# - Re-instate 'gmake lint'?
-
-SMARTLOGIN_BIT=$(BITS_DIR)/smartlogin/smartlogin-$(SDC_SMART_LOGIN_BRANCH)-$(TIMESTAMP)-g$(SDC_SMART_LOGIN_SHA).tgz
-SMARTLOGIN_MANIFEST_BIT=$(BITS_DIR)/smartlogin/smartlogin-$(SDC_SMART_LOGIN_BRANCH)-$(TIMESTAMP)-g$(SDC_SMART_LOGIN_SHA).manifest
-
-.PHONY: smartlogin
-smartlogin: $(SMARTLOGIN_BIT)
-
-# PATH: ensure using GCC from SFW. Not sure this is necessary, but has been
-# the case for release builds pre-MG.
-$(SMARTLOGIN_BIT): build/sdc-smart-login
-	@echo "# Build smartlogin: branch $(SDC_SMART_LOGIN_BRANCH), sha $(SDC_SMART_LOGIN_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-smart-login && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) PATH=/usr/sfw/bin:$(PATH) BITS_DIR=$(BITS_DIR) gmake clean all publish)
-	@echo "# Created smartlogin bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(SMARTLOGIN_BIT)
-	@echo ""
-
-smartlogin_publish_image: $(SMARTLOGIN_BIT)
-	@echo "# Publish smartlogin image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(SMARTLOGIN_MANIFEST_BIT) -f $(SMARTLOGIN_BIT)
-
-clean_smartlogin:
-	$(RM) -rf $(BITS_DIR)/smartlogin
-
-
-
-
-
-
-#---- amon
-
-_amon_stamp=$(SDC_AMON_BRANCH)-$(TIMESTAMP)-g$(SDC_AMON_SHA)
-AMON_BITS=$(BITS_DIR)/amon/amon-pkg-$(_amon_stamp).tar.gz \
-	$(BITS_DIR)/amon/amon-relay-$(_amon_stamp).tgz \
-	$(BITS_DIR)/amon/amon-agent-$(_amon_stamp).tgz
-AMON_BITS_0=$(shell echo $(AMON_BITS) | awk '{print $$1}')
-AMON_IMAGE_BIT=$(BITS_DIR)/amon/amon-zfs-$(_amon_stamp).zfs.gz
-AMON_MANIFEST_BIT=$(BITS_DIR)/amon/amon-zfs-$(_amon_stamp).imgmanifest
-AMON_AGENT_BIT=$(BITS_DIR)/amon/amon-agent-$(_amon_stamp).tgz
-AMON_AGENT_MANIFEST_BIT=$(BITS_DIR)/amon/amon-agent-$(_amon_stamp).manifest
-AMON_RELAY_BIT=$(BITS_DIR)/amon/amon-relay-$(_amon_stamp).tgz
-AMON_RELAY_MANIFEST_BIT=$(BITS_DIR)/amon/amon-relay-$(_amon_stamp).manifest
-
-.PHONY: amon
-amon: $(AMON_BITS_0) amon_image
-
-$(AMON_BITS): build/sdc-amon
-	@echo "# Build amon: branch $(SDC_AMON_BRANCH), sha $(SDC_AMON_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-amon && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake clean all pkg publish)
-	@echo "# Created amon bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(AMON_BITS)
-	@echo ""
-
-.PHONY: amon_image
-amon_image: $(AMON_IMAGE_BIT)
-
-$(AMON_IMAGE_BIT): $(AMON_BITS_0)
-	@echo "# Build amon_image: branch $(SDC_AMON_BRANCH), sha $(SDC_AMON_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(AMON_IMAGE_UUID)" -t $(AMON_BITS_0) \
-		-o "$(AMON_IMAGE_BIT)" -p $(AMON_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(AMON_EXTRA_TARBALLS) -n $(AMON_IMAGE_NAME) \
-		-v $(_amon_stamp) -d $(AMON_IMAGE_DESCRIPTION)
-	@echo "# Created amon image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(AMON_IMAGE_BIT))
-	@echo ""
-
-amon_publish_image: $(AMON_IMAGE_BIT)
-	@echo "# Publish amon images to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(AMON_MANIFEST_BIT) -f $(AMON_IMAGE_BIT)
-	$(UPDATES_IMGADM) import -ddd -m $(AMON_AGENT_MANIFEST_BIT) -f $(AMON_AGENT_BIT)
-	$(UPDATES_IMGADM) import -ddd -m $(AMON_RELAY_MANIFEST_BIT) -f $(AMON_RELAY_BIT)
-
-clean_amon:
-	$(RM) -rf $(BITS_DIR)/amon
-	(cd build/sdc-amon && gmake clean)
-
-#---- UFDS
-
-
-_ufds_stamp=$(SDC_UFDS_BRANCH)-$(TIMESTAMP)-g$(SDC_UFDS_SHA)
-UFDS_BITS=$(BITS_DIR)/ufds/ufds-pkg-$(_ufds_stamp).tar.gz
-UFDS_IMAGE_BIT=$(BITS_DIR)/ufds/ufds-zfs-$(_ufds_stamp).zfs.gz
-UFDS_MANIFEST_BIT=$(BITS_DIR)/ufds/ufds-zfs-$(_ufds_stamp).imgmanifest
-
-.PHONY: ufds
-ufds: $(UFDS_BITS) ufds_image
-
-# PATH for ufds build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(UFDS_BITS): build/sdc-ufds
-	@echo "# Build ufds: branch $(SDC_UFDS_BRANCH), sha $(SDC_UFDS_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-ufds && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake pkg release publish)
-	@echo "# Created ufds bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(UFDS_BITS)
-	@echo ""
-
-.PHONY: ufds_image
-ufds_image: $(UFDS_IMAGE_BIT)
-
-$(UFDS_IMAGE_BIT): $(UFDS_BITS)
-	@echo "# Build ufds_image: branch $(SDC_UFDS_BRANCH), sha $(SDC_UFDS_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(UFDS_IMAGE_UUID)" -t $(UFDS_BITS) \
-		-o "$(UFDS_IMAGE_BIT)" -p $(UFDS_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(UFDS_EXTRA_TARBALLS) -n $(UFDS_IMAGE_NAME) \
-		-v $(_ufds_stamp) -d $(UFDS_IMAGE_DESCRIPTION)
-	@echo "# Created ufds image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(UFDS_IMAGE_BIT))
-	@echo ""
-
-ufds_publish_image: $(UFDS_IMAGE_BIT)
-	@echo "# Publish ufds image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(UFDS_MANIFEST_BIT) -f $(UFDS_IMAGE_BIT)
-
-# Warning: if UFDS's submodule deps change, this 'clean_ufds' is insufficient. It would
-# then need to call 'gmake dist-clean'.
-clean_ufds:
-	$(RM) -rf $(BITS_DIR)/ufds
-	(cd build/sdc-ufds && gmake clean)
-
-
-#---- ASSETS
-
-_assets_stamp=$(SDC_ASSETS_BRANCH)-$(TIMESTAMP)-g$(SDC_ASSETS_SHA)
-ASSETS_BITS=$(BITS_DIR)/assets/assets-pkg-$(_assets_stamp).tar.gz
-ASSETS_IMAGE_BIT=$(BITS_DIR)/assets/assets-zfs-$(_assets_stamp).zfs.gz
-ASSETS_MANIFEST_BIT=$(BITS_DIR)/assets/assets-zfs-$(_assets_stamp).imgmanifest
-
-.PHONY: assets
-assets: $(ASSETS_BITS) assets_image
-
-$(ASSETS_BITS): build/sdc-assets
-	@echo "# Build assets: branch $(ASSETS_BRANCH), sha $(ASSETS_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-assets && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) $(MAKE) release publish)
-	@echo "# Created assets bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(ASSETS_BITS)
-	@echo ""
-
-.PHONY: assets_image
-assets_image: $(ASSETS_IMAGE_BIT)
-
-$(ASSETS_IMAGE_BIT): $(ASSETS_BITS)
-	@echo "# Build assets_image: branch $(ASSETS_BRANCH), sha $(ASSETS_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(ASSETS_IMAGE_UUID)" -t $(ASSETS_BITS) \
-		-o "$(ASSETS_IMAGE_BIT)" -p $(ASSETS_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(ASSETS_EXTRA_TARBALLS) -n $(ASSETS_IMAGE_NAME) \
-		-v $(_assets_stamp) -d $(ASSETS_IMAGE_DESCRIPTION)
-	@echo "# Created assets image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(ASSETS_IMAGE_BIT))
-	@echo ""
-
-assets_publish_image: $(ASSETS_IMAGE_BIT)
-	@echo "# Publish assets image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(ASSETS_MANIFEST_BIT) -f $(ASSETS_IMAGE_BIT)
-
-clean_assets:
-	$(RM) -rf $(BITS_DIR)/assets
-	(cd build/sdc-assets && gmake clean)
-
-#---- ADMINUI
-
-_adminui_stamp=$(ADMINUI_BRANCH)-$(TIMESTAMP)-g$(ADMINUI_SHA)
-ADMINUI_BITS=$(BITS_DIR)/adminui/adminui-pkg-$(_adminui_stamp).tar.gz
-ADMINUI_IMAGE_BIT=$(BITS_DIR)/adminui/adminui-zfs-$(_adminui_stamp).zfs.gz
-ADMINUI_MANIFEST_BIT=$(BITS_DIR)/adminui/adminui-zfs-$(_adminui_stamp).imgmanifest
-
-.PHONY: adminui
-adminui: $(ADMINUI_BITS) adminui_image
-
-$(ADMINUI_BITS): build/adminui
-	@echo "# Build adminui: branch $(ADMINUI_BRANCH), sha $(ADMINUI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/adminui && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) $(MAKE) release publish)
-	@echo "# Created adminui bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(ADMINUI_BITS)
-	@echo ""
-
-.PHONY: adminui_image
-adminui_image: $(ADMINUI_IMAGE_BIT)
-
-$(ADMINUI_IMAGE_BIT): $(ADMINUI_BITS)
-	@echo "# Build adminui_image: branch $(ADMINUI_BRANCH), sha $(ADMINUI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(ADMINUI_IMAGE_UUID)" -t $(ADMINUI_BITS) \
-		-o "$(ADMINUI_IMAGE_BIT)" -p $(ADMINUI_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(ADMINUI_EXTRA_TARBALLS) -n $(ADMINUI_IMAGE_NAME) \
-		-v $(_adminui_stamp) -d $(ADMINUI_IMAGE_DESCRIPTION)
-	@echo "# Created adminui image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(ADMINUI_IMAGE_BIT))
-	@echo ""
-
-adminui_publish_image: $(ADMINUI_IMAGE_BIT)
-	@echo "# Publish adminui image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(ADMINUI_MANIFEST_BIT) -f $(ADMINUI_IMAGE_BIT)
-
-clean_adminui:
-	$(RM) -rf $(BITS_DIR)/adminui
-	(cd build/adminui && gmake clean)
-
-
-#---- REDIS
-
-_redis_stamp=$(SDC_REDIS_BRANCH)-$(TIMESTAMP)-g$(SDC_REDIS_SHA)
-REDIS_BITS=$(BITS_DIR)/redis/redis-pkg-$(_redis_stamp).tar.bz2
-REDIS_IMAGE_BIT=$(BITS_DIR)/redis/redis-zfs-$(_redis_stamp).zfs.gz
-REDIS_MANIFEST_BIT=$(BITS_DIR)/redis/redis-zfs-$(_redis_stamp).imgmanifest
-
-.PHONY: redis
-redis: $(REDIS_BITS) redis_image
-
-$(REDIS_BITS): build/sdc-redis
-	@echo "# Build redis: branch $(REDIS_BRANCH), sha $(REDIS_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-redis && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) $(MAKE) release publish)
-	@echo "# Created redis bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(REDIS_BITS)
-	@echo ""
-
-.PHONY: redis_image
-redis_image: $(REDIS_IMAGE_BIT)
-
-$(REDIS_IMAGE_BIT): $(REDIS_BITS)
-	@echo "# Build redis_image: branch $(REDIS_BRANCH), sha $(REDIS_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(REDIS_IMAGE_UUID)" -t $(REDIS_BITS) \
-		-o "$(REDIS_IMAGE_BIT)" -p $(REDIS_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(REDIS_EXTRA_TARBALLS) -n $(REDIS_IMAGE_NAME) \
-		-v $(_redis_stamp) -d $(REDIS_IMAGE_DESCRIPTION)
-	@echo "# Created redis image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(REDIS_IMAGE_BIT))
-	@echo ""
-
-redis_publish_image: $(REDIS_IMAGE_BIT)
-	@echo "# Publish redis image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(REDIS_MANIFEST_BIT) -f $(REDIS_IMAGE_BIT)
-
-clean_redis:
-	$(RM) -rf $(BITS_DIR)/redis
-	(cd build/sdc-redis && gmake clean)
-
-
-#---- amonredis
-
-_amonredis_stamp=$(SDC_AMONREDIS_BRANCH)-$(TIMESTAMP)-g$(SDC_AMONREDIS_SHA)
-AMONREDIS_BITS=$(BITS_DIR)/amonredis/amonredis-pkg-$(_amonredis_stamp).tar.gz
-AMONREDIS_IMAGE_BIT=$(BITS_DIR)/amonredis/amonredis-zfs-$(_amonredis_stamp).zfs.gz
-AMONREDIS_MANIFEST_BIT=$(BITS_DIR)/amonredis/amonredis-zfs-$(_amonredis_stamp).imgmanifest
-
-.PHONY: amonredis
-amonredis: $(AMONREDIS_BITS) amonredis_image
-
-$(AMONREDIS_BITS): build/sdc-amonredis
-	@echo "# Build amonredis: branch $(AMONREDIS_BRANCH), sha $(AMONREDIS_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-amonredis && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) $(MAKE) release publish)
-	@echo "# Created amonredis bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(AMONREDIS_BITS)
-	@echo ""
-
-.PHONY: amonredis_image
-amonredis_image: $(AMONREDIS_IMAGE_BIT)
-
-$(AMONREDIS_IMAGE_BIT): $(AMONREDIS_BITS)
-	@echo "# Build amonredis_image: branch $(AMONREDIS_BRANCH), sha $(AMONREDIS_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(AMONREDIS_IMAGE_UUID)" -t $(AMONREDIS_BITS) \
-		-o "$(AMONREDIS_IMAGE_BIT)" -p $(AMONREDIS_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(AMONREDIS_EXTRA_TARBALLS) -n $(AMONREDIS_IMAGE_NAME) \
-		-v $(_amonredis_stamp) -d $(AMONREDIS_IMAGE_DESCRIPTION)
-	@echo "# Created amonredis image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(AMONREDIS_IMAGE_BIT))
-	@echo ""
-
-amonredis_publish_image: $(AMONREDIS_IMAGE_BIT)
-	@echo "# Publish amonredis image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(AMONREDIS_MANIFEST_BIT) -f $(AMONREDIS_IMAGE_BIT)
-
-clean_amonredis:
-	$(RM) -rf $(BITS_DIR)/amonredis
-	(cd build/sdc-amonredis && gmake clean)
-
-
-#---- RABBITMQ
-
-_rabbitmq_stamp=$(SDC_RABBITMQ_BRANCH)-$(TIMESTAMP)-g$(SDC_RABBITMQ_SHA)
-RABBITMQ_BITS=$(BITS_DIR)/rabbitmq/rabbitmq-pkg-$(_rabbitmq_stamp).tar.gz
-RABBITMQ_IMAGE_BIT=$(BITS_DIR)/rabbitmq/rabbitmq-zfs-$(_rabbitmq_stamp).zfs.gz
-RABBITMQ_MANIFEST_BIT=$(BITS_DIR)/rabbitmq/rabbitmq-zfs-$(_rabbitmq_stamp).imgmanifest
-
-.PHONY: rabbitmq
-rabbitmq: $(RABBITMQ_BITS) rabbitmq_image
-
-$(RABBITMQ_BITS): build/sdc-rabbitmq
-	@echo "# Build rabbitmq: branch $(RABBITMQ_BRANCH), sha $(RABBITMQ_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-rabbitmq && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) $(MAKE) release publish)
-	@echo "# Created rabbitmq bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(RABBITMQ_BITS)
-	@echo ""
-
-.PHONY: rabbitmq_image
-rabbitmq_image: $(RABBITMQ_IMAGE_BIT)
-
-$(RABBITMQ_IMAGE_BIT): $(RABBITMQ_BITS)
-	@echo "# Build rabbitmq_image: branch $(RABBITMQ_BRANCH), sha $(RABBITMQ_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(RABBITMQ_IMAGE_UUID)" -t $(RABBITMQ_BITS) \
-		-o "$(RABBITMQ_IMAGE_BIT)" -p $(RABBITMQ_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(RABBITMQ_EXTRA_TARBALLS) -n $(RABBITMQ_IMAGE_NAME) \
-		-v $(_rabbitmq_stamp) -d $(RABBITMQ_IMAGE_DESCRIPTION)
-	@echo "# Created rabbitmq image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(RABBITMQ_IMAGE_BIT))
-	@echo ""
-
-rabbitmq_publish_image: $(RABBITMQ_IMAGE_BIT)
-	@echo "# Publish rabbitmq image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(RABBITMQ_MANIFEST_BIT) -f $(RABBITMQ_IMAGE_BIT)
-
-clean_rabbitmq:
-	$(RM) -rf $(BITS_DIR)/rabbitmq
-	(cd build/sdc-rabbitmq && gmake clean)
-
-#---- DHCPD
-
-_dhcpd_stamp=$(DHCPD_BRANCH)-$(TIMESTAMP)-g$(DHCPD_SHA)
-DHCPD_BITS=$(BITS_DIR)/dhcpd/dhcpd-pkg-$(_dhcpd_stamp).tar.gz
-DHCPD_IMAGE_BIT=$(BITS_DIR)/dhcpd/dhcpd-zfs-$(_dhcpd_stamp).zfs.gz
-DHCPD_MANIFEST_BIT=$(BITS_DIR)/dhcpd/dhcpd-zfs-$(_dhcpd_stamp).imgmanifest
-
-.PHONY: dhcpd
-dhcpd: $(DHCPD_BITS) dhcpd_image
-
-$(DHCPD_BITS): build/dhcpd
-	@echo "# Build dhcpd: branch $(DHCPD_BRANCH), sha $(DHCPD_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/dhcpd && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) \
-		$(MAKE) release publish)
-	@echo "# Created dhcpd bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(DHCPD_BITS)
-	@echo ""
-
-.PHONY: dhcpd_image
-dhcpd_image: $(DHCPD_IMAGE_BIT)
-
-$(DHCPD_IMAGE_BIT): $(DHCPD_BITS)
-	@echo "# Build dhcpd_image: branch $(DHCPD_BRANCH), sha $(DHCPD_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(DHCPD_IMAGE_UUID)" -t $(DHCPD_BITS) \
-		-o "$(DHCPD_IMAGE_BIT)" -p $(DHCPD_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(DHCPD_EXTRA_TARBALLS) -n $(DHCPD_IMAGE_NAME) \
-		-v $(_dhcpd_stamp) -d $(DHCPD_IMAGE_DESCRIPTION)
-	@echo "# Created dhcpd image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(DHCPD_IMAGE_BIT))
-	@echo ""
-
-dhcpd_publish_image: $(DHCPD_IMAGE_BIT)
-	@echo "# Publish dhcpd image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(DHCPD_MANIFEST_BIT) -f $(DHCPD_IMAGE_BIT)
-
-clean_dhcpd:
-	$(RM) -rf $(BITS_DIR)/dhcpd
-	(cd build/dhcpd && gmake clean)
-
-#---- MOCKCLOUD
-
-_mockcloud_stamp=$(MOCKCLOUD_BRANCH)-$(TIMESTAMP)-g$(MOCKCLOUD_SHA)
-MOCKCLOUD_BITS=$(BITS_DIR)/mockcloud/mockcloud-pkg-$(_mockcloud_stamp).tar.gz
-MOCKCLOUD_IMAGE_BIT=$(BITS_DIR)/mockcloud/mockcloud-zfs-$(_mockcloud_stamp).zfs.gz
-MOCKCLOUD_MANIFEST_BIT=$(BITS_DIR)/mockcloud/mockcloud-zfs-$(_mockcloud_stamp).imgmanifest
-
-.PHONY: mockcloud
-mockcloud: $(MOCKCLOUD_BITS) mockcloud_image
-
-$(MOCKCLOUD_BITS): build/mockcloud
-	@echo "# Build mockcloud: branch $(MOCKCLOUD_BRANCH), sha $(MOCKCLOUD_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/mockcloud && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) \
-		$(MAKE) release publish)
-	@echo "# Created mockcloud bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(MOCKCLOUD_BITS)
-	@echo ""
-
-.PHONY: mockcloud_image
-mockcloud_image: $(MOCKCLOUD_IMAGE_BIT)
-
-$(MOCKCLOUD_IMAGE_BIT): $(MOCKCLOUD_BITS)
-	@echo "# Build mockcloud_image: branch $(MOCKCLOUD_BRANCH), sha $(MOCKCLOUD_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(MOCKCLOUD_IMAGE_UUID)" -t $(MOCKCLOUD_BITS) \
-		-o "$(MOCKCLOUD_IMAGE_BIT)" -p $(MOCKCLOUD_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(MOCKCLOUD_EXTRA_TARBALLS) -n $(MOCKCLOUD_IMAGE_NAME) \
-		-v $(_mockcloud_stamp) -d $(MOCKCLOUD_IMAGE_DESCRIPTION)
-	@echo "# Created mockcloud image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(MOCKCLOUD_IMAGE_BIT))
-	@echo ""
-
-mockcloud_publish_image: $(MOCKCLOUD_IMAGE_BIT)
-	@echo "# Publish mockcloud image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(MOCKCLOUD_MANIFEST_BIT) -f $(MOCKCLOUD_IMAGE_BIT)
-
-clean_mockcloud:
-	$(RM) -rf $(BITS_DIR)/mockcloud
-	(cd build/mockcloud && gmake clean)
-
-
-#---- CLOUDAPI
-
-_cloudapi_stamp=$(SDC_CLOUDAPI_BRANCH)-$(TIMESTAMP)-g$(SDC_CLOUDAPI_SHA)
-CLOUDAPI_BITS=$(BITS_DIR)/cloudapi/cloudapi-pkg-$(_cloudapi_stamp).tar.gz
-CLOUDAPI_IMAGE_BIT=$(BITS_DIR)/cloudapi/cloudapi-zfs-$(_cloudapi_stamp).zfs.gz
-CLOUDAPI_MANIFEST_BIT=$(BITS_DIR)/cloudapi/cloudapi-zfs-$(_cloudapi_stamp).imgmanifest
-
-.PHONY: cloudapi
-cloudapi: $(CLOUDAPI_BITS) cloudapi_image
-
-# cloudapi still uses platform node, ensure that same version is first
-# node (and npm) on the PATH.
-$(CLOUDAPI_BITS): build/sdc-cloudapi
-	@echo "# Build cloudapi: branch $(SDC_CLOUDAPI_BRANCH), sha $(SDC_CLOUDAPI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-cloudapi && PATH=/opt/node/0.6.12/bin:$(PATH) NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created cloudapi bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(CLOUDAPI_BITS)
-	@echo ""
-
-.PHONY: cloudapi_image
-cloudapi_image: $(CLOUDAPI_IMAGE_BIT)
-
-$(CLOUDAPI_IMAGE_BIT): $(CLOUDAPI_BITS)
-	@echo "# Build cloudapi_image: branch $(SDC_CLOUDAPI_BRANCH), sha $(SDC_CLOUDAPI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(CLOUDAPI_IMAGE_UUID)" -t $(CLOUDAPI_BITS) \
-		-o "$(CLOUDAPI_IMAGE_BIT)" -p $(CLOUDAPI_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(CLOUDAPI_EXTRA_TARBALLS) -n $(CLOUDAPI_IMAGE_NAME) \
-		-v $(_cloudapi_stamp) -d $(CLOUDAPI_IMAGE_DESCRIPTION)
-	@echo "# Created cloudapi image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(CLOUDAPI_IMAGE_BIT))
-	@echo ""
-
-cloudapi_publish_image: $(CLOUDAPI_IMAGE_BIT)
-	@echo "# Publish cloudapi image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(CLOUDAPI_MANIFEST_BIT) -f $(CLOUDAPI_IMAGE_BIT)
-
-
-# Warning: if cloudapi's submodule deps change, this 'clean_ufds' is insufficient. It would
-# then need to call 'gmake dist-clean'.
-clean_cloudapi:
-	$(RM) -rf $(BITS_DIR)/cloudapi
-	(cd build/sdc-cloudapi && gmake clean)
-
-
-#---- NAT
-
-_nat_stamp=$(SDC_NAT_BRANCH)-$(TIMESTAMP)-g$(SDC_NAT_SHA)
-NAT_BITS=$(BITS_DIR)/nat/nat-pkg-$(_nat_stamp).tar.gz
-NAT_IMAGE_BIT=$(BITS_DIR)/nat/nat-zfs-$(_nat_stamp).zfs.gz
-NAT_MANIFEST_BIT=$(BITS_DIR)/nat/nat-zfs-$(_nat_stamp).imgmanifest
-
-.PHONY: nat
-nat: $(NAT_BITS) nat_image
-
-$(NAT_BITS): build/sdc-nat
-	@echo "# Build nat: branch $(SDC_NAT_BRANCH), sha $(SDC_NAT_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-nat && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release test publish)
-	@echo "# Created nat bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(NAT_BITS)
-	@echo ""
-
-.PHONY: nat_image
-nat_image: $(NAT_IMAGE_BIT)
-
-$(NAT_IMAGE_BIT): $(NAT_BITS)
-	@echo "# Build nat_image: branch $(SDC_NAT_BRANCH), sha $(SDC_NAT_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(NAT_IMAGE_UUID)" -t $(NAT_BITS) \
-		-o "$(NAT_IMAGE_BIT)" -p $(NAT_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(NAT_EXTRA_TARBALLS) -n $(NAT_IMAGE_NAME) \
-		-v $(_nat_stamp) -d $(NAT_IMAGE_DESCRIPTION)
-	@echo "# Created nat image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(NAT_IMAGE_BIT))
-	@echo ""
-
-nat_publish_image: $(NAT_IMAGE_BIT)
-	@echo "# Publish nat image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(NAT_MANIFEST_BIT) -f $(NAT_IMAGE_BIT)
-
-clean_nat:
-	$(RM) -rf $(BITS_DIR)/nat
-	(cd build/sdc-nat && gmake clean)
-
-
-#---- DOCKER
-
-_docker_stamp=$(SDC_DOCKER_BRANCH)-$(TIMESTAMP)-g$(SDC_DOCKER_SHA)
-DOCKER_BITS=$(BITS_DIR)/docker/docker-pkg-$(_docker_stamp).tar.gz
-DOCKER_IMAGE_BIT=$(BITS_DIR)/docker/docker-zfs-$(_docker_stamp).zfs.gz
-DOCKER_MANIFEST_BIT=$(BITS_DIR)/docker/docker-zfs-$(_docker_stamp).imgmanifest
-
-.PHONY: docker
-docker: $(DOCKER_BITS) docker_image
-
-$(DOCKER_BITS): build/sdc-docker
-	@echo "# Build docker: branch $(SDC_DOCKER_BRANCH), sha $(SDC_DOCKER_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-docker && PATH=/opt/node/0.6.12/bin:$(PATH) NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release test publish)
-	@echo "# Created docker bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(DOCKER_BITS)
-	@echo ""
-
-.PHONY: docker_image
-docker_image: $(DOCKER_IMAGE_BIT)
-
-$(DOCKER_IMAGE_BIT): $(DOCKER_BITS)
-	@echo "# Build docker_image: branch $(SDC_DOCKER_BRANCH), sha $(SDC_DOCKER_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(DOCKER_IMAGE_UUID)" -t $(DOCKER_BITS) \
-		-o "$(DOCKER_IMAGE_BIT)" -p $(DOCKER_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(DOCKER_EXTRA_TARBALLS) -n $(DOCKER_IMAGE_NAME) \
-		-v $(_docker_stamp) -d $(DOCKER_IMAGE_DESCRIPTION)
-	@echo "# Created docker image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(DOCKER_IMAGE_BIT))
-	@echo ""
-
-docker_publish_image: $(DOCKER_IMAGE_BIT)
-	@echo "# Publish docker image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(DOCKER_MANIFEST_BIT) -f $(DOCKER_IMAGE_BIT)
-
-clean_docker:
-	$(RM) -rf $(BITS_DIR)/docker
-	(cd build/sdc-docker && gmake clean)
-
-
-#---- PORTOLAN
-
-_portolan_stamp=$(SDC_PORTOLAN_BRANCH)-$(TIMESTAMP)-g$(SDC_PORTOLAN_SHA)
-PORTOLAN_BITS=$(BITS_DIR)/portolan/portolan-pkg-$(_portolan_stamp).tar.gz
-PORTOLAN_IMAGE_BIT=$(BITS_DIR)/portolan/portolan-zfs-$(_portolan_stamp).zfs.gz
-PORTOLAN_MANIFEST_BIT=$(BITS_DIR)/portolan/portolan-zfs-$(_portolan_stamp).imgmanifest
-
-.PHONY: portolan
-portolan: $(PORTOLAN_BITS) portolan_image
-
-$(PORTOLAN_BITS): build/sdc-portolan
-	@echo "# Build portolan: branch $(SDC_PORTOLAN_BRANCH), sha $(SDC_PORTOLAN_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-portolan && PATH=/opt/node/0.6.12/bin:$(PATH) NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created portolan bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(PORTOLAN_BITS)
-	@echo ""
-
-.PHONY: portolan_image
-portolan_image: $(PORTOLAN_IMAGE_BIT)
-
-$(PORTOLAN_IMAGE_BIT): $(PORTOLAN_BITS)
-	@echo "# Build portolan_image: branch $(SDC_PORTOLAN_BRANCH), sha $(SDC_PORTOLAN_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(PORTOLAN_IMAGE_UUID)" -t $(PORTOLAN_BITS) \
-		-o "$(PORTOLAN_IMAGE_BIT)" -p $(PORTOLAN_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(PORTOLAN_EXTRA_TARBALLS) -n $(PORTOLAN_IMAGE_NAME) \
-		-v $(_portolan_stamp) -d $(PORTOLAN_IMAGE_DESCRIPTION)
-	@echo "# Created portolan image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(PORTOLAN_IMAGE_BIT))
-	@echo ""
-
-portolan_publish_image: $(PORTOLAN_IMAGE_BIT)
-	@echo "# Publish portolan image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(PORTOLAN_MANIFEST_BIT) -f $(PORTOLAN_IMAGE_BIT)
-
-clean_portolan:
-	$(RM) -rf $(BITS_DIR)/portolan
-	(cd build/sdc-portolan && gmake clean)
-
-#---- MANTA_MANATEE
-
-_manta-manatee_stamp=$(MANTA_MANATEE_BRANCH)-$(TIMESTAMP)-g$(MANTA_MANATEE_SHA)
-MANTA_MANATEE_BITS=$(BITS_DIR)/manta-manatee/manta-manatee-pkg-$(_manta-manatee_stamp).tar.gz
-MANTA_MANATEE_IMAGE_BIT=$(BITS_DIR)/manta-manatee/manta-manatee-zfs-$(_manta-manatee_stamp).zfs.gz
-MANTA_MANATEE_MANIFEST_BIT=$(BITS_DIR)/manta-manatee/manta-manatee-zfs-$(_manta-manatee_stamp).imgmanifest
-
-.PHONY: manta-manatee
-manta-manatee: $(MANTA_MANATEE_BITS) manta-manatee_image
-
-$(MANTA_MANATEE_BITS): build/manta-manatee
-	@echo "# Build manta-manatee: branch $(MANTA_MANATEE_BRANCH), sha $(MANTA_MANATEE_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/manta-manatee && LDFLAGS="-L/opt/local/lib -R/opt/local/lib" NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created manta-manatee bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(MANTA_MANATEE_BITS)
-	@echo ""
-
-.PHONY: manta-manatee_image
-manta-manatee_image: $(MANTA_MANATEE_IMAGE_BIT)
-
-$(MANTA_MANATEE_IMAGE_BIT): $(MANTA_MANATEE_BITS)
-	@echo "# Build manta-manatee_image: branch $(MANTA_MANATEE_BRANCH), sha $(MANTA_MANATEE_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(MANTA_MANATEE_IMAGE_UUID)" -t $(MANTA_MANATEE_BITS) \
-		-b "manta-manatee" -O "$(MG_OUT_PATH)" \
-		-o "$(MANTA_MANATEE_IMAGE_BIT)" -p $(MANTA_MANATEE_PKGSRC) \
-		-t $(MANTA_MANATEE_EXTRA_TARBALLS) -n $(MANTA_MANATEE_IMAGE_NAME) \
-		-v $(_manta-manatee_stamp) -d $(MANTA_MANATEE_IMAGE_DESCRIPTION)
-	@echo "# Created manta-manatee image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(MANTA_MANATEE_IMAGE_BIT))
-	@echo ""
-
-manta-manatee_publish_image: $(MANTA_MANATEE_IMAGE_BIT)
-	@echo "# Publish manta-manatee image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(MANTA_MANATEE_MANIFEST_BIT) -f $(MANTA_MANATEE_IMAGE_BIT)
-
-clean_manta-manatee:
-	$(RM) -rf $(BITS_DIR)/manta-manatee
-	(cd build/manta-manatee && gmake distclean)
-
-
-#---- SDC_MANATEE
-
-_sdc-manatee_stamp=$(SDC_MANATEE_BRANCH)-$(TIMESTAMP)-g$(SDC_MANATEE_SHA)
-SDC_MANATEE_BITS=$(BITS_DIR)/sdc-manatee/sdc-manatee-pkg-$(_sdc-manatee_stamp).tar.gz
-SDC_MANATEE_IMAGE_BIT=$(BITS_DIR)/sdc-manatee/sdc-manatee-zfs-$(_sdc-manatee_stamp).zfs.gz
-SDC_MANATEE_MANIFEST_BIT=$(BITS_DIR)/sdc-manatee/sdc-manatee-zfs-$(_sdc-manatee_stamp).imgmanifest
-
-.PHONY: sdc-manatee
-sdc-manatee: $(SDC_MANATEE_BITS) sdc-manatee_image
-
-$(SDC_MANATEE_BITS): build/sdc-manatee
-	@echo "# Build sdc-manatee: branch $(SDC_MANATEE_BRANCH), sha $(SDC_MANATEE_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-manatee && LDFLAGS="-L/opt/local/lib -R/opt/local/lib" NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created sdc-manatee bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(SDC_MANATEE_BITS)
-	@echo ""
-
-.PHONY: sdc-manatee_image
-sdc-manatee_image: $(SDC_MANATEE_IMAGE_BIT)
-
-$(SDC_MANATEE_IMAGE_BIT): $(SDC_MANATEE_BITS)
-	@echo "# Build sdc-manatee_image: branch $(SDC_MANATEE_BRANCH), sha $(SDC_MANATEE_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(SDC_MANATEE_IMAGE_UUID)" -t $(SDC_MANATEE_BITS) \
-		-b "sdc-manatee" \
-		-o "$(SDC_MANATEE_IMAGE_BIT)" -p $(SDC_MANATEE_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(SDC_MANATEE_EXTRA_TARBALLS) -n $(SDC_MANATEE_IMAGE_NAME) \
-		-v $(_sdc-manatee_stamp) -d $(SDC_MANATEE_IMAGE_DESCRIPTION)
-	@echo "# Created sdc-manatee image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(SDC_MANATEE_IMAGE_BIT))
-	@echo ""
-
-sdc-manatee_publish_image: $(SDC_MANATEE_IMAGE_BIT)
-	@echo "# Publish sdc-manatee image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(SDC_MANATEE_MANIFEST_BIT) -f $(SDC_MANATEE_IMAGE_BIT)
-
-clean_sdc-manatee:
-	$(RM) -rf $(BITS_DIR)/sdc-manatee
-	(cd build/sdc-manatee && gmake distclean)
-
-
-#---- MANATEE
-
-_manatee_stamp=$(MANATEE_BRANCH)-$(TIMESTAMP)-g$(MANATEE_SHA)
-MANATEE_BITS=$(BITS_DIR)/manatee/manatee-pkg-$(_manatee_stamp).tar.bz2
-MANATEE_IMAGE_BIT=$(BITS_DIR)/manatee/manatee-zfs-$(_manatee_stamp).zfs.gz
-MANATEE_MANIFEST_BIT=$(BITS_DIR)/manatee/manatee-zfs-$(_manatee_stamp).imgmanifest
-
-.PHONY: manatee
-manatee: $(MANATEE_BITS) manatee_image
-
-$(MANATEE_BITS): build/manatee
-	@echo "# Build manatee: branch $(MANATEE_BRANCH), sha $(MANATEE_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/manatee && LDFLAGS="-L/opt/local/lib -R/opt/local/lib" NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created manatee bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(MANATEE_BITS)
-	@echo ""
-
-.PHONY: manatee_image
-manatee_image: $(MANATEE_IMAGE_BIT)
-
-$(MANATEE_IMAGE_BIT): $(MANATEE_BITS)
-	@echo "# Build manatee_image: branch $(MANATEE_BRANCH), sha $(MANATEE_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(MANATEE_IMAGE_UUID)" -t $(MANATEE_BITS) \
-		-b "manatee" \
-		-o "$(MANATEE_IMAGE_BIT)" -p $(MANATEE_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(MANATEE_EXTRA_TARBALLS) -n $(MANATEE_IMAGE_NAME) \
-		-v $(_manatee_stamp) -d $(MANATEE_IMAGE_DESCRIPTION)
-	@echo "# Created manatee image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(MANATEE_IMAGE_BIT))
-	@echo ""
-
-manatee_publish_image: $(MANATEE_IMAGE_BIT)
-	@echo "# Publish manatee image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(MANATEE_MANIFEST_BIT) -f $(MANATEE_IMAGE_BIT)
-
-clean_manatee:
-	$(RM) -rf $(BITS_DIR)/manatee
-	(cd build/manatee && gmake distclean)
-
-
-#---- WORKFLOW
-
-_wf_stamp=$(SDC_WORKFLOW_BRANCH)-$(TIMESTAMP)-g$(SDC_WORKFLOW_SHA)
-WORKFLOW_BITS=$(BITS_DIR)/workflow/workflow-pkg-$(_wf_stamp).tar.gz
-WORKFLOW_IMAGE_BIT=$(BITS_DIR)/workflow/workflow-zfs-$(_wf_stamp).zfs.gz
-WORKFLOW_MANIFEST_BIT=$(BITS_DIR)/workflow/workflow-zfs-$(_wf_stamp).imgmanifest
-
-.PHONY: workflow
-workflow: $(WORKFLOW_BITS) workflow_image
-
-# PATH for workflow build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(WORKFLOW_BITS): build/sdc-workflow
-	@echo "# Build workflow: branch $(SDC_WORKFLOW_BRANCH), sha $(SDC_WORKFLOW_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-workflow && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created workflow bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(WORKFLOW_BITS)
-	@echo ""
-
-.PHONY: workflow_image
-workflow_image: $(WORKFLOW_IMAGE_BIT)
-
-$(WORKFLOW_IMAGE_BIT): $(WORKFLOW_BITS)
-	@echo "# Build workflow_image: branch $(SDC_WORKFLOW_BRANCH), sha $(SDC_WORKFLOW_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(WORKFLOW_IMAGE_UUID)" -t $(WORKFLOW_BITS) \
-		-o "$(WORKFLOW_IMAGE_BIT)" -p $(WORKFLOW_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(WORKFLOW_EXTRA_TARBALLS) -n $(WORKFLOW_IMAGE_NAME) \
-		-v $(_wf_stamp) -d $(WORKFLOW_IMAGE_DESCRIPTION)
-	@echo "# Created workflow image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(WORKFLOW_IMAGE_BIT))
-	@echo ""
-
-workflow_publish_image: $(WORKFLOW_IMAGE_BIT)
-	@echo "# Publish workflow image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(WORKFLOW_MANIFEST_BIT) -f $(WORKFLOW_IMAGE_BIT)
-
-# Warning: if workflow's submodule deps change, this 'clean_workflow' is insufficient. It would
-# then need to call 'gmake dist-clean'.
-clean_workflow:
-	$(RM) -rf $(BITS_DIR)/workflow
-	(cd build/sdc-workflow && gmake clean)
-
-
-#---- CMON
-
-_cmon_stamp=$(TRITON_CMON_BRANCH)-$(TIMESTAMP)-g$(TRITON_CMON_SHA)
-CMON_BITS=$(BITS_DIR)/cmon/cmon-pkg-$(_cmon_stamp).tar.gz
-CMON_IMAGE_BIT=$(BITS_DIR)/cmon/cmon-zfs-$(_cmon_stamp).zfs.gz
-CMON_MANIFEST_BIT=$(BITS_DIR)/cmon/cmon-zfs-$(_cmon_stamp).imgmanifest
-
-.PHONY: cmon
-cmon: $(CMON_BITS) cmon_image
-
-$(CMON_BITS): build/triton-cmon
-	@echo "# Build cmon: branch $(TRITON_CMON_BRANCH), sha $(TRITON_CMON_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/triton-cmon && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created cmon bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(CMON_BITS)
-	@echo ""
-
-.PHONY: cmon_image
-cmon_image: $(CMON_IMAGE_BIT)
-
-$(CMON_IMAGE_BIT): $(CMON_BITS)
-	@echo "# Build cmon_image: branch $(TRITON_CMON_BRANCH), sha $(TRITON_CMON_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(CMON_IMAGE_UUID)" -t $(CMON_BITS) \
-		-o "$(CMON_IMAGE_BIT)" -p $(CMON_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(CMON_EXTRA_TARBALLS) -n $(CMON_IMAGE_NAME) \
-		-v $(_cmon_stamp) -d $(CMON_IMAGE_DESCRIPTION)
-	@echo "# Created cmon image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(CMON_IMAGE_BIT))
-	@echo ""
-
-cmon_publish_image: $(CMON_IMAGE_BIT)
-	@echo "# Publish cmon image to Triton Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(CMON_MANIFEST_BIT) -f $(CMON_IMAGE_BIT)
-
-clean_cmon:
-	$(RM) -rf $(BITS_DIR)/cmon
-	(cd build/triton-cmon && gmake clean)
-
-
-#---- cmon agent
-
-_cmon_agent_stamp=$(TRITON_CMON_AGENT_BRANCH)-$(TIMESTAMP)-g$(TRITON_CMON_AGENT_SHA)
-CMON_AGENT_BIT=$(BITS_DIR)/cmon-agent/cmon-agent-$(_cmon_agent_stamp).tgz
-CMON_AGENT_MANIFEST_BIT=$(BITS_DIR)/cmon-agent/cmon-agent-$(_cmon_agent_stamp).manifest
-
-.PHONY: cmon-agent
-cmon-agent: $(CMON_AGENT_BIT)
-
-$(CMON_AGENT_BIT): build/triton-cmon-agent
-	@echo "# Build cmon-agent: branch $(TRITON_CMON_AGENT_BRANCH), sha $(TRITON_CMON_AGENT_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/triton-cmon-agent && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created cmon-agent bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(CMON_AGENT_BIT) $(CMON_AGENT_MANIFEST_BIT)
-	@echo ""
-
-cmon-agent_publish_image: $(CMON_AGENT_BIT)
-	@echo "# Publish cmon-agent image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(CMON_AGENT_MANIFEST_BIT) -f $(CMON_AGENT_BIT)
-
-clean_cmon_agent:
-	$(RM) -rf $(BITS_DIR)/cmon-agent
-	(cd build/sdc-cmon-agent && gmake clean)
-
-
-#---- VMAPI
-
-_vmapi_stamp=$(SDC_VMAPI_BRANCH)-$(TIMESTAMP)-g$(SDC_VMAPI_SHA)
-VMAPI_BITS=$(BITS_DIR)/vmapi/vmapi-pkg-$(_vmapi_stamp).tar.gz
-VMAPI_IMAGE_BIT=$(BITS_DIR)/vmapi/vmapi-zfs-$(_vmapi_stamp).zfs.gz
-VMAPI_MANIFEST_BIT=$(BITS_DIR)/vmapi/vmapi-zfs-$(_vmapi_stamp).imgmanifest
-
-.PHONY: vmapi
-vmapi: $(VMAPI_BITS) vmapi_image
-
-# PATH for vmapi build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(VMAPI_BITS): build/sdc-vmapi
-	@echo "# Build vmapi: branch $(SDC_VMAPI_BRANCH), sha $(SDC_VMAPI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-vmapi && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created vmapi bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(VMAPI_BITS)
-	@echo ""
-
-.PHONY: vmapi_image
-vmapi_image: $(VMAPI_IMAGE_BIT)
-
-$(VMAPI_IMAGE_BIT): $(VMAPI_BITS)
-	@echo "# Build vmapi_image: branch $(SDC_VMAPI_BRANCH), sha $(SDC_VMAPI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(VMAPI_IMAGE_UUID)" -t $(VMAPI_BITS) \
-		-o "$(VMAPI_IMAGE_BIT)" -p $(VMAPI_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(VMAPI_EXTRA_TARBALLS) -n $(VMAPI_IMAGE_NAME) \
-		-v $(_vmapi_stamp) -d $(VMAPI_IMAGE_DESCRIPTION)
-	@echo "# Created vmapi image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(VMAPI_IMAGE_BIT))
-	@echo ""
-
-vmapi_publish_image: $(VMAPI_IMAGE_BIT)
-	@echo "# Publish vmapi image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(VMAPI_MANIFEST_BIT) -f $(VMAPI_IMAGE_BIT)
-
-# Warning: if vmapi's submodule deps change, this 'clean_vmapi' is insufficient. It would
-# then need to call 'gmake dist-clean'.
-clean_vmapi:
-	$(RM) -rf $(BITS_DIR)/vmapi
-	(cd build/sdc-vmapi && gmake clean)
-
-
-
-#---- PAPI
-
-_papi_stamp=$(SDC_PAPI_BRANCH)-$(TIMESTAMP)-g$(SDC_PAPI_SHA)
-PAPI_BITS=$(BITS_DIR)/papi/papi-pkg-$(_papi_stamp).tar.gz
-PAPI_IMAGE_BIT=$(BITS_DIR)/papi/papi-zfs-$(_papi_stamp).zfs.gz
-PAPI_MANIFEST_BIT=$(BITS_DIR)/papi/papi-zfs-$(_papi_stamp).imgmanifest
-
-
-.PHONY: papi
-papi: $(PAPI_BITS) papi_image
-
-# PATH for papi build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(PAPI_BITS): build/sdc-papi
-	@echo "# Build papi: branch $(SDC_PAPI_BRANCH), sha $(SDC_PAPI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-papi && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created papi bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(PAPI_BITS)
-	@echo ""
-
-.PHONY: papi_image
-papi_image: $(PAPI_IMAGE_BIT)
-
-$(PAPI_IMAGE_BIT): $(PAPI_BITS)
-	@echo "# Build papi_image: branch $(SDC_PAPI_BRANCH), sha $(SDC_PAPI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(PAPI_IMAGE_UUID)" -t $(PAPI_BITS) \
-		-o "$(PAPI_IMAGE_BIT)" -p $(PAPI_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(PAPI_EXTRA_TARBALLS) -n $(PAPI_IMAGE_NAME) \
-		-v $(_papi_stamp) -d $(PAPI_IMAGE_DESCRIPTION)
-	@echo "# Created papi image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(PAPI_IMAGE_BIT))
-	@echo ""
-
-papi_publish_image: $(PAPI_IMAGE_BIT)
-	@echo "# Publish papi image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(PAPI_MANIFEST_BIT) -f $(PAPI_IMAGE_BIT)
-
-# Warning: if papi's submodule deps change, this 'clean_papi' is insufficient. It would
-# then need to call 'gmake dist-clean'.
-clean_papi:
-	$(RM) -rf $(BITS_DIR)/papi
-	(cd build/sdc-papi && gmake clean)
-
-
-
-#---- IMGAPI
-
-_imgapi_stamp=$(SDC_IMGAPI_BRANCH)-$(TIMESTAMP)-g$(SDC_IMGAPI_SHA)
-IMGAPI_BITS=$(BITS_DIR)/imgapi/imgapi-pkg-$(_imgapi_stamp).tar.gz
-IMGAPI_IMAGE_BIT=$(BITS_DIR)/imgapi/imgapi-zfs-$(_imgapi_stamp).zfs.gz
-IMGAPI_MANIFEST_BIT=$(BITS_DIR)/imgapi/imgapi-zfs-$(_imgapi_stamp).imgmanifest
-
-.PHONY: imgapi
-imgapi: $(IMGAPI_BITS) imgapi_image
-
-$(IMGAPI_BITS): build/sdc-imgapi
-	@echo "# Build imgapi: branch $(SDC_IMGAPI_BRANCH), sha $(SDC_IMGAPI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-imgapi && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created imgapi bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(IMGAPI_BITS)
-	@echo ""
-
-.PHONY: imgapi_image
-imgapi_image: $(IMGAPI_IMAGE_BIT)
-
-$(IMGAPI_IMAGE_BIT): $(IMGAPI_BITS)
-	@echo "# Build imgapi_image: branch $(SDC_IMGAPI_BRANCH), sha $(SDC_IMGAPI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(IMGAPI_IMAGE_UUID)" -t $(IMGAPI_BITS) \
-		-o "$(IMGAPI_IMAGE_BIT)" -p $(IMGAPI_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(IMGAPI_EXTRA_TARBALLS) -n $(IMGAPI_IMAGE_NAME) \
-		-v $(_imgapi_stamp) -d $(IMGAPI_IMAGE_DESCRIPTION)
-	@echo "# Created imgapi image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(IMGAPI_IMAGE_BIT))
-	@echo ""
-
-imgapi_publish_image: $(IMGAPI_IMAGE_BIT)
-	@echo "# Publish imgapi image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(IMGAPI_MANIFEST_BIT) -f $(IMGAPI_IMAGE_BIT)
-
-clean_imgapi:
-	$(RM) -rf $(BITS_DIR)/imgapi
-	(cd build/sdc-imgapi && gmake clean)
-
-
-#---- sdc
-
-_sdc_stamp=$(SDC_SDC_BRANCH)-$(TIMESTAMP)-g$(SDC_SDC_SHA)
-SDC_BITS=$(BITS_DIR)/sdc/sdc-pkg-$(_sdc_stamp).tar.gz
-SDC_IMAGE_BIT=$(BITS_DIR)/sdc/sdc-zfs-$(_sdc_stamp).zfs.gz
-SDC_MANIFEST_BIT=$(BITS_DIR)/sdc/sdc-zfs-$(_sdc_stamp).imgmanifest
-
-.PHONY: sdc
-sdc: $(SDC_BITS) sdc_image
-
-$(SDC_BITS): build/sdc-sdc
-	@echo "# Build sdc: branch $(SDC_SDC_BRANCH), sha $(SDC_SDC_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-sdc && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created sdc bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(SDC_BITS)
-	@echo ""
-
-.PHONY: sdc_image
-sdc_image: $(SDC_IMAGE_BIT)
-
-$(SDC_IMAGE_BIT): $(SDC_BITS)
-	@echo "# Build sdc_image: branch $(SDC_BRANCH), sha $(SDC_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(SDC_IMAGE_UUID)" -t $(SDC_BITS) \
-		-o "$(SDC_IMAGE_BIT)" -p $(SDC_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(SDC_EXTRA_TARBALLS) -n $(SDC_IMAGE_NAME) \
-		-v $(_sdc_stamp) -d $(SDC_IMAGE_DESCRIPTION)
-	@echo "# Created sdc image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(SDC_IMAGE_BIT))
-	@echo ""
-
-sdc_publish_image: $(SDC_IMAGE_BIT)
-	@echo "# Publish sdc image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(SDC_MANIFEST_BIT) -f $(SDC_IMAGE_BIT)
-
-clean_sdc:
-	$(RM) -rf $(BITS_DIR)/sdc
-	(cd build/sdc && gmake clean)
-
-
-#---- sdc-system-tests (aka systests)
-
-_sdc_system_tests_stamp=$(SDC_SYSTEM_TESTS_BRANCH)-$(TIMESTAMP)-g$(SDC_SYSTEM_TESTS_SHA)
-SDC_SYSTEM_TESTS_BITS=$(BITS_DIR)/sdc-system-tests/sdc-system-tests-$(_sdc_system_tests_stamp).tgz
-
-.PHONY: sdc-system-tests
-sdc-system-tests: $(SDC_SYSTEM_TESTS_BITS)
-
-$(SDC_SYSTEM_TESTS_BITS): build/sdc-system-tests
-	@echo "# Build sdc-system-tests: branch $(SDC_SYSTEM_TESTS_BRANCH), sha $(SDC_SYSTEM_TESTS_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-system-tests && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm \
-		TIMESTAMP=$(TIMESTAMP) \
-		BITS_DIR=$(BITS_DIR) \
-		gmake all release publish)
-	@echo "# Created sdc-system-tests bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(SDC_SYSTEM_TESTS_BITS)
-	@echo ""
-
-
-#---- Agents core
-
-_agents_core_stamp=$(SDC_AGENTS_CORE_BRANCH)-$(TIMESTAMP)-g$(SDC_AGENTS_CORE_SHA)
-AGENTS_CORE_BIT=$(BITS_DIR)/agents_core/agents_core-$(_agents_core_stamp).tgz
-AGENTS_CORE_MANIFEST_BIT=$(BITS_DIR)/agents_core/agents_core-$(_agents_core_stamp).manifest
-
-.PHONY: agents_core
-agents_core: $(AGENTS_CORE_BIT)
-
-# PATH for agents_core build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(AGENTS_CORE_BIT): build/sdc-agents-core
-	@echo "# Build agents_core: branch $(SDC_AGENTS_CORE_BRANCH), sha $(SDC_AGENTS_CORE_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-agents-core && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created agents_core bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(AGENTS_CORE_BIT) $(AGENTS_CORE_MANIFEST_BIT)
-	@echo ""
-
-agents_core_publish_image: $(AGENTS_CORE_BIT)
-	@echo "# Publish agents_core image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(AGENTS_CORE_MANIFEST_BIT) -f $(AGENTS_CORE_BIT)
-
-# Warning: if agents_core's submodule deps change, this 'clean_agents_core' is insufficient. It would
-# then need to call 'gmake dist-clean'.
-clean_agents_core:
-	$(RM) -rf $(BITS_DIR)/agents_core
-	(cd build/sdc-agents-core && gmake clean)
-
-
-#---- VM Agent
-
-# The values for SDC_VM_AGENT_BRANCH and SDC_VM_AGENT_SHAR are generated from
-# the name of the git repo.
-# The repo name is "sdc-vm-agent" and we want the resultant tarball from this
-# process to be "vm-agent-<...>.tgz", not "sdc-vm-agent-<...>.tgz".
-
-_vm_agent_stamp=$(SDC_VM_AGENT_BRANCH)-$(TIMESTAMP)-g$(SDC_VM_AGENT_SHA)
-VM_AGENT_BIT=$(BITS_DIR)/vm-agent/vm-agent-$(_vm_agent_stamp).tgz
-VM_AGENT_MANIFEST_BIT=$(BITS_DIR)/vm-agent/vm-agent-$(_vm_agent_stamp).manifest
-
-.PHONY: vm-agent
-vm-agent: $(VM_AGENT_BIT)
-
-# PATH for vm-agent build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(VM_AGENT_BIT): build/sdc-vm-agent
-	@echo "# Build vm-agent: branch $(SDC_VM_AGENT_BRANCH), sha $(SDC_VM_AGENT_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-vm-agent && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created vm-agent bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(VM_AGENT_BIT) $(VM_AGENT_MANIFEST_BIT)
-	@echo ""
-
-vm-agent_publish_image: $(VM_AGENT_BIT)
-	@echo "# Publish vm-agent image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(VM_AGENT_MANIFEST_BIT) -f $(VM_AGENT_BIT)
-
-# Warning: if vm-agents's submodule deps change, this 'clean_vm_agent' is insufficient. It would
-# then need to call 'gmake dist-clean'.
-clean_vm_agent:
-	$(RM) -rf $(BITS_DIR)/vm-agent
-	(cd build/sdc-vm-agent && gmake clean)
-
-
-#---- Net Agent
-
-# The values for SDC_NET_AGENT_BRANCH and SDC_NET_AGENT_SHAR are generated from
-# the name of the git repo.
-# The repo name is "sdc-net-agent" and we want the resultant tarball from this
-# process to be "net-agent-<...>.tgz", not "sdc-net-agent-<...>.tgz".
-
-_net_agent_stamp=$(SDC_NET_AGENT_BRANCH)-$(TIMESTAMP)-g$(SDC_NET_AGENT_SHA)
-NET_AGENT_BIT=$(BITS_DIR)/net-agent/net-agent-$(_net_agent_stamp).tgz
-NET_AGENT_MANIFEST_BIT=$(BITS_DIR)/net-agent/net-agent-$(_net_agent_stamp).manifest
-
-.PHONY: net-agent
-net-agent: $(NET_AGENT_BIT)
-
-# PATH for net-agent build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(NET_AGENT_BIT): build/sdc-net-agent
-	@echo "# Build net-agent: branch $(SDC_NET_AGENT_BRANCH), sha $(SDC_NET_AGENT_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-net-agent && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created net-agent bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(NET_AGENT_BIT)
-	@echo ""
-
-net-agent_publish_image: $(NET_AGENT_BIT)
-	@echo "# Publish net-agent image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(NET_AGENT_MANIFEST_BIT) -f $(NET_AGENT_BIT)
-
-# Warning: if net-agents's submodule deps change, this 'clean_net_agent' is insufficient. It would
-# then need to call 'gmake dist-clean'.
-clean_net_agent:
-	$(RM) -rf $(BITS_DIR)/net-agent
-	(cd build/sdc-net-agent && gmake clean)
-
-
-#---- CN Agent
-
-# The values for SDC_CN_AGENT_BRANCH and SDC_CN_AGENT_SHAR are generated from
-# the name of the git repo.
-# The repo name is "sdc-cn-agent" and we want the resultant tarball from this
-# process to be "cn-agent-<...>.tgz", not "sdc-cn-agent-<...>.tgz".
-
-_cn_agent_stamp=$(SDC_CN_AGENT_BRANCH)-$(TIMESTAMP)-g$(SDC_CN_AGENT_SHA)
-CN_AGENT_BIT=$(BITS_DIR)/cn-agent/cn-agent-$(_cn_agent_stamp).tgz
-CN_AGENT_MANIFEST_BIT=$(BITS_DIR)/cn-agent/cn-agent-$(_cn_agent_stamp).manifest
-
-.PHONY: cn-agent
-cn-agent: $(CN_AGENT_BIT)
-
-# PATH for cn-agent build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(CN_AGENT_BIT): build/sdc-cn-agent
-	@echo "# Ensuring build dependencies are installed from pkgsrc"
-	$(PKGIN) -y update
-	$(PKGIN) -y install autoconf
-	$(PKGIN) -y install automake
-	$(PKGIN) -y install libtool-base
-	@echo "# Build cn-agent: branch $(SDC_CN_AGENT_BRANCH), sha $(SDC_CN_AGENT_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-cn-agent && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created cn-agent bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(CN_AGENT_BIT) $(CN_AGENT_MANIFEST_BIT)
-	@echo ""
-
-cn-agent_publish_image: $(CN_AGENT_BIT)
-	@echo "# Publish cn-agent image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(CN_AGENT_MANIFEST_BIT) -f $(CN_AGENT_BIT)
-
-# Warning: if cn-agents's submodule deps change, this 'clean_cn_agent' is insufficient. It would
-# then need to call 'gmake dist-clean'.
-clean_cn_agent:
-	$(RM) -rf $(BITS_DIR)/cn-agent
-	(cd build/sdc-cn-agent && gmake clean)
-
-
-#---- Configuration Agent
-
-_config_agent_stamp=$(SDC_CONFIG_AGENT_BRANCH)-$(TIMESTAMP)-g$(SDC_CONFIG_AGENT_SHA)
-CONFIG_AGENT_BIT=$(BITS_DIR)/config-agent/config-agent-pkg-$(_config_agent_stamp).tar.gz
-CONFIG_AGENT_MANIFEST_BIT=$(BITS_DIR)/config-agent/config-agent-pkg-$(_config_agent_stamp).manifest
-
-.PHONY: config-agent
-config-agent: $(CONFIG_AGENT_BIT)
-
-$(CONFIG_AGENT_BIT): build/sdc-config-agent
-	@echo "# Build config-agent: branch $(SDC_CONFIG_AGENT_BRANCH), sha $(SDC_CONFIG_AGENT_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-config-agent && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created config-agent bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(CONFIG_AGENT_BIT) $(CONFIG_AGENT_MANIFEST_BIT)
-	@echo ""
-
-config-agent_publish_image: $(CONFIG_AGENT_BIT)
-	@echo "# Publish config-agent image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(CONFIG_AGENT_MANIFEST_BIT) -f $(CONFIG_AGENT_BIT)
-
-clean_config_agent:
-	$(RM) -rf $(BITS_DIR)/config-agent
-	(cd build/sdc-config-agent && gmake clean)
-
-
-#---- Hagfish Watcher
-
-_hagfish_watcher_stamp=$(SDC_HAGFISH_WATCHER_BRANCH)-$(TIMESTAMP)-g$(SDC_HAGFISH_WATCHER_SHA)
-HAGFISH_WATCHER_BIT=$(BITS_DIR)/hagfish-watcher/hagfish-watcher-$(_hagfish_watcher_stamp).tgz
-HAGFISH_WATCHER_MANIFEST_BIT=$(BITS_DIR)/hagfish-watcher/hagfish-watcher-$(_hagfish_watcher_stamp).manifest
-
-.PHONY: hagfish-watcher
-hagfish-watcher: $(HAGFISH_WATCHER_BIT)
-
-$(HAGFISH_WATCHER_BIT): build/sdc-hagfish-watcher
-	@echo "# Build hagfish-watcher: branch $(SDC_HAGFISH_WATCHER_BRANCH), sha $(HAGFISH_WATCHER_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-hagfish-watcher && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created hagfish-watcher bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(HAGFISH_WATCHER_BIT) $(HAGFISH_WATCHER_MANIFEST_BIT)
-	@echo ""
-
-hagfish-watcher_publish_image: $(HAGFISH_WATCHER_BIT)
-	@echo "# Publish hagfish-watcher image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(HAGFISH_WATCHER_MANIFEST_BIT) -f $(HAGFISH_WATCHER_BIT)
-
-clean_hagfish-watcher:
-	$(RM) -rf $(BITS_DIR)/hagfish-watcher
-	(cd build/sdc-hagfish-watcher && gmake clean)
-
-
-#---- Firewaller
-
-_firewaller_stamp=$(SDC_FIREWALLER_AGENT_BRANCH)-$(TIMESTAMP)-g$(SDC_FIREWALLER_AGENT_SHA)
-FIREWALLER_BIT=$(BITS_DIR)/firewaller/firewaller-$(_firewaller_stamp).tgz
-FIREWALLER_MANIFEST_BIT=$(BITS_DIR)/firewaller/firewaller-$(_firewaller_stamp).manifest
-
-.PHONY: firewaller
-firewaller: $(FIREWALLER_BIT)
-
-$(FIREWALLER_BIT): build/sdc-firewaller-agent
-	@echo "# Build firewaller: branch $(SDC_FIREWALLER_AGENT_BRANCH), sha $(FIREWALLER_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-firewaller-agent && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created firewaller bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(FIREWALLER_BIT) $(FIREWALLER_MANIFEST_BIT)
-	@echo ""
-
-firewaller_publish_image: $(FIREWALLER_BIT)
-	@echo "# Publish firewaller image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(FIREWALLER_MANIFEST_BIT) -f $(FIREWALLER_BIT)
-
-clean_firewaller:
-	$(RM) -rf $(BITS_DIR)/firewaller
-	(cd build/firewaller && gmake clean)
-
-
-
-#---- CNAPI
-
-_cnapi_stamp=$(SDC_CNAPI_BRANCH)-$(TIMESTAMP)-g$(SDC_CNAPI_SHA)
-CNAPI_BITS=$(BITS_DIR)/cnapi/cnapi-pkg-$(_cnapi_stamp).tar.gz
-CNAPI_IMAGE_BIT=$(BITS_DIR)/cnapi/cnapi-zfs-$(_cnapi_stamp).zfs.gz
-CNAPI_MANIFEST_BIT=$(BITS_DIR)/cnapi/cnapi-zfs-$(_cnapi_stamp).imgmanifest
-
-.PHONY: cnapi
-cnapi: $(CNAPI_BITS) cnapi_image
-
-# PATH for cnapi build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(CNAPI_BITS): build/sdc-cnapi
-	@echo "# Build cnapi: branch $(SDC_CNAPI_BRANCH), sha $(SDC_CNAPI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-cnapi && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created cnapi bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(CNAPI_BITS)
-	@echo ""
-
-.PHONY: cnapi_image
-cnapi_image: $(CNAPI_IMAGE_BIT)
-
-$(CNAPI_IMAGE_BIT): $(CNAPI_BITS)
-	@echo "# Build cnapi_image: branch $(SDC_CNAPI_BRANCH), sha $(SDC_CNAPI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(CNAPI_IMAGE_UUID)" -t $(CNAPI_BITS) \
-		-o "$(CNAPI_IMAGE_BIT)" -p $(CNAPI_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(CNAPI_EXTRA_TARBALLS) -n $(CNAPI_IMAGE_NAME) \
-		-v $(_cnapi_stamp) -d $(CNAPI_IMAGE_DESCRIPTION)
-	@echo "# Created cnapi image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(CNAPI_IMAGE_BIT))
-	@echo ""
-
-cnapi_publish_image: $(CNAPI_IMAGE_BIT)
-	@echo "# Publish cnapi image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(CNAPI_MANIFEST_BIT) -f $(CNAPI_IMAGE_BIT)
-
-# Warning: if cnapi's submodule deps change, this 'clean_cnapi' is insufficient. It would
-# then need to call 'gmake dist-clean'.
-clean_cnapi:
-	$(RM) -rf $(BITS_DIR)/cnapi
-	(cd build/sdc-cnapi && gmake clean)
-
-
-#---- FWAPI
-
-_fwapi_stamp=$(SDC_FWAPI_BRANCH)-$(TIMESTAMP)-g$(SDC_FWAPI_SHA)
-FWAPI_BITS=$(BITS_DIR)/fwapi/fwapi-pkg-$(_fwapi_stamp).tar.gz
-FWAPI_IMAGE_BIT=$(BITS_DIR)/fwapi/fwapi-zfs-$(_fwapi_stamp).zfs.gz
-FWAPI_MANIFEST_BIT=$(BITS_DIR)/fwapi/fwapi-zfs-$(_fwapi_stamp).imgmanifest
-
-.PHONY: fwapi
-fwapi: $(FWAPI_BITS) fwapi_image
-
-# PATH for fwapi build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(FWAPI_BITS): build/sdc-fwapi
-	@echo "# Build fwapi: branch $(SDC_FWAPI_BRANCH), sha $(SDC_FWAPI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-fwapi && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake pkg release publish)
-	@echo "# Created fwapi bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(FWAPI_BITS)
-	@echo ""
-
-.PHONY: fwapi_image
-fwapi_image: $(FWAPI_IMAGE_BIT)
-
-$(FWAPI_IMAGE_BIT): $(FWAPI_BITS)
-	@echo "# Build fwapi_image: branch $(FWAPI_BRANCH), sha $(FWAPI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(FWAPI_IMAGE_UUID)" -t $(FWAPI_BITS) \
-		-o "$(FWAPI_IMAGE_BIT)" -p $(FWAPI_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(FWAPI_EXTRA_TARBALLS) -n $(FWAPI_IMAGE_NAME) \
-		-v $(_fwapi_stamp) -d $(FWAPI_IMAGE_DESCRIPTION)
-	@echo "# Created fwapi image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(FWAPI_IMAGE_BIT))
-	@echo ""
-
-fwapi_publish_image: $(FWAPI_IMAGE_BIT)
-	@echo "# Publish fwapi image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(FWAPI_MANIFEST_BIT) -f $(FWAPI_IMAGE_BIT)
-
-# Warning: if FWAPI's submodule deps change, this 'clean_fwapi' is insufficient. It would
-# then need to call 'gmake dist-clean'.
-clean_fwapi:
-	$(RM) -rf $(BITS_DIR)/fwapi
-	(cd build/fwapi && gmake clean)
-
-
-
-#---- NAPI
-
-_napi_stamp=$(SDC_NAPI_BRANCH)-$(TIMESTAMP)-g$(SDC_NAPI_SHA)
-NAPI_BITS=$(BITS_DIR)/napi/napi-pkg-$(_napi_stamp).tar.gz
-NAPI_IMAGE_BIT=$(BITS_DIR)/napi/napi-zfs-$(_napi_stamp).zfs.gz
-NAPI_MANIFEST_BIT=$(BITS_DIR)/napi/napi-zfs-$(_napi_stamp).imgmanifest
-
-.PHONY: napi
-napi: $(NAPI_BITS) napi_image
-
-# PATH for napi build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(NAPI_BITS): build/sdc-napi
-	@echo "# Build napi: branch $(SDC_NAPI_BRANCH), sha $(SDC_NAPI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-napi && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake pkg release publish)
-	@echo "# Created napi bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(NAPI_BITS)
-	@echo ""
-
-.PHONY: napi_image
-napi_image: $(NAPI_IMAGE_BIT)
-
-$(NAPI_IMAGE_BIT): $(NAPI_BITS)
-	@echo "# Build napi_image: branch $(SDC_NAPI_BRANCH), sha $(SDC_NAPI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(NAPI_IMAGE_UUID)" -t $(NAPI_BITS) \
-		-o "$(NAPI_IMAGE_BIT)" -p $(NAPI_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(NAPI_EXTRA_TARBALLS) -n $(NAPI_IMAGE_NAME) \
-		-v $(_napi_stamp) -d $(NAPI_IMAGE_DESCRIPTION)
-	@echo "# Created napi image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(NAPI_IMAGE_BIT))
-	@echo ""
-
-napi_publish_image: $(NAPI_IMAGE_BIT)
-	@echo "# Publish napi image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(NAPI_MANIFEST_BIT) -f $(NAPI_IMAGE_BIT)
-
-# Warning: if NAPI's submodule deps change, this 'clean_napi' is insufficient. It would
-# then need to call 'gmake dist-clean'.
-clean_napi:
-	$(RM) -rf $(BITS_DIR)/napi
-	(cd build/napi && gmake clean)
-
-
-
-#---- SAPI
-
-_sapi_stamp=$(SDC_SAPI_BRANCH)-$(TIMESTAMP)-g$(SDC_SAPI_SHA)
-SAPI_BITS=$(BITS_DIR)/sapi/sapi-pkg-$(_sapi_stamp).tar.gz
-SAPI_IMAGE_BIT=$(BITS_DIR)/sapi/sapi-zfs-$(_sapi_stamp).zfs.gz
-SAPI_MANIFEST_BIT=$(BITS_DIR)/sapi/sapi-zfs-$(_sapi_stamp).imgmanifest
-
-.PHONY: sapi
-sapi: $(SAPI_BITS) sapi_image
-
-
-# PATH for sapi build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(SAPI_BITS): build/sdc-sapi
-	@echo "# Build sapi: branch $(SDC_SAPI_BRANCH), sha $(SDC_SAPI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-sapi && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created sapi bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(SAPI_BITS)
-	@echo ""
-
-.PHONY: sapi_image
-sapi_image: $(SAPI_IMAGE_BIT)
-
-$(SAPI_IMAGE_BIT): $(SAPI_BITS)
-	@echo "# Build sapi_image: branch $(SDC_SAPI_BRANCH), sha $(SDC_SAPI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(SAPI_IMAGE_UUID)" -t $(SAPI_BITS) \
-		-o "$(SAPI_IMAGE_BIT)" -p $(SAPI_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(SAPI_EXTRA_TARBALLS) -n $(SAPI_IMAGE_NAME) \
-		-v $(_sapi_stamp) -d $(SAPI_IMAGE_DESCRIPTION)
-	@echo "# Created sapi image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(SAPI_IMAGE_BIT))
-	@echo ""
-
-sapi_publish_image: $(SAPI_IMAGE_BIT)
-	@echo "# Publish sapi image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(SAPI_MANIFEST_BIT) -f $(SAPI_IMAGE_BIT)
-
-# Warning: if SAPI's submodule deps change, this 'clean_sapi' is insufficient. It would
-# then need to call 'gmake dist-clean'.
-clean_sapi:
-	$(RM) -rf $(BITS_DIR)/sdc-sapi
-	(cd build/sapi && gmake clean)
-
-
-#---- SDCSSO
-
-_sdcsso_stamp=$(SDCSSO_BRANCH)-$(TIMESTAMP)-g$(SDCSSO_SHA)
-SDCSSO_BITS=$(BITS_DIR)/sdcsso/sdcsso-pkg-$(_sdcsso_stamp).tar.bz2
-SDCSSO_IMAGE_BIT=$(BITS_DIR)/sdcsso/sdcsso-zfs-$(_sdcsso_stamp).zfs.gz
-SDCSSO_MANIFEST_BIT=$(BITS_DIR)/sdcsso/sdcsso-zfs-$(_sdcsso_stamp).imgmanifest
-
-.PHONY: sdcsso
-sdcsso: $(SDCSSO_BITS) sdcsso_image
-
-
-# PATH for sdcsso build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(SDCSSO_BITS): build/sdcsso
-	@echo "# Build sdcsso: branch $(SDCSSO_BRANCH), sha $(SDCSSO_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdcsso && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created sdcsso bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(SDCSSO_BITS)
-	@echo ""
-
-.PHONY: sdcsso_image
-sdcsso_image: $(SDCSSO_IMAGE_BIT)
-
-$(SDCSSO_IMAGE_BIT): $(SDCSSO_BITS)
-	@echo "# Build sdcsso_image: branch $(SSDCSSO_BRANCH), sha $(SDCSSO_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(SDCSSO_IMAGE_UUID)" -t $(SDCSSO_BITS) \
-		-o "$(SDCSSO_IMAGE_BIT)" -p $(SDCSSO_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(SDCSSO_EXTRA_TARBALLS) -n $(SDCSSO_IMAGE_NAME) \
-		-v $(_sdcsso_stamp) -d $(SDCSSO_IMAGE_DESCRIPTION)
-	@echo "# Created sdcsso image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(SDCSSO_IMAGE_BIT))
-	@echo ""
-
-# Warning: if SAPI's submodule deps change, this 'clean_sdcsso' is insufficient. It would
-# then need to call 'gmake dist-clean'.
-clean_sdcsso:
-	$(RM) -rf $(BITS_DIR)/sdcsso
-	(cd build/sdcsso && gmake clean)
-
-
-
-#---- Marlin
-
-_marlin_stamp=$(MANTA_MARLIN_BRANCH)-$(TIMESTAMP)-g$(MANTA_MARLIN_SHA)
-MARLIN_BITS=$(BITS_DIR)/marlin/marlin-pkg-$(_marlin_stamp).tar.gz
-MARLIN_IMAGE_BIT=$(BITS_DIR)/marlin/marlin-zfs-$(_marlin_stamp).zfs.gz
-MARLIN_MANIFEST_BIT=$(BITS_DIR)/marlin/marlin-zfs-$(_marlin_stamp).imgmanifest
-MARLIN_AGENT_BIT=$(BITS_DIR)/marlin/marlin-$(_marlin_stamp).tar.gz
-MARLIN_AGENT_MANIFEST_BIT=$(BITS_DIR)/marlin/marlin-$(_marlin_stamp).manifest
-
-.PHONY: marlin
-marlin: $(MARLIN_BITS) marlin_image
-
-# PATH for marlin build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(MARLIN_BITS): build/manta-marlin
-	@echo "# Build marlin: branch $(MANTA_MARLIN_BRANCH), sha $(MANTA_MARLIN_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/manta-marlin && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created marlin bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(MARLIN_BITS) $(MARLIN_AGENT_BIT) $(MARLIN_AGENT_MANIFEST_BIT)
-	@echo ""
-
-.PHONY: marlin_image
-marlin_image: $(MARLIN_IMAGE_BIT)
-
-$(MARLIN_IMAGE_BIT): $(MARLIN_BITS)
-	@echo "# Build marlin_image: branch $(MANTA_MARLIN_BRANCH), sha $(MANTA_MARLIN_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(MARLIN_IMAGE_UUID)" -t $(MARLIN_BITS) \
-		-b "marlin" \
-		-o "$(MARLIN_IMAGE_BIT)" -p $(MARLIN_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(MARLIN_EXTRA_TARBALLS) -n $(MARLIN_IMAGE_NAME) \
-		-v $(_marlin_stamp) -d $(MARLIN_IMAGE_DESCRIPTION)
-	@echo "# Created marlin image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(MARLIN_IMAGE_BIT))
-	@echo ""
-
-marlin_publish_image: $(MARLIN_IMAGE_BIT)
-	@echo "# Publish marlin images to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(MARLIN_MANIFEST_BIT) -f $(MARLIN_IMAGE_BIT)
-	$(UPDATES_IMGADM) import -ddd -m $(MARLIN_AGENT_MANIFEST_BIT) -f $(MARLIN_AGENT_BIT)
-
-clean_marlin:
-	$(RM) -rf $(BITS_DIR)/marlin
-	(cd build/manta-marlin && gmake distclean)
-
-#---- MEDUSA
-
-_medusa_stamp=$(MANTA_MEDUSA_BRANCH)-$(TIMESTAMP)-g$(MANTA_MEDUSA_SHA)
-MEDUSA_BITS=$(BITS_DIR)/medusa/medusa-pkg-$(_medusa_stamp).tar.gz
-MEDUSA_IMAGE_BIT=$(BITS_DIR)/medusa/medusa-zfs-$(_medusa_stamp).zfs.gz
-MEDUSA_MANIFEST_BIT=$(BITS_DIR)/medusa/medusa-zfs-$(_medusa_stamp).imgmanifest
-
-.PHONY: medusa
-medusa: $(MEDUSA_BITS) medusa_image
-
-$(MEDUSA_BITS): build/manta-medusa
-	@echo "# Build medusa: branch $(MANTA_MEDUSA_BRANCH), sha $(MANTA_MEDUSA_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/manta-medusa && LDFLAGS="-L/opt/local/lib -R/opt/local/lib" NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created medusa bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(MEDUSA_BITS)
-	@echo ""
-
-
-.PHONY: medusa_image
-medusa_image: $(MEDUSA_IMAGE_BIT)
-
-$(MEDUSA_IMAGE_BIT): $(MEDUSA_BITS)
-	@echo "# Build medusa_image: branch $(MANTA_MEDUSA_BRANCH), sha $(MANTA_MEDUSA_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(MEDUSA_IMAGE_UUID)" -t $(MEDUSA_BITS) \
-		-b "medusa" \
-		-o "$(MEDUSA_IMAGE_BIT)" -p $(MEDUSA_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(MEDUSA_EXTRA_TARBALLS) -n $(MEDUSA_IMAGE_NAME) \
-		-v $(_medusa_stamp) -d $(MEDUSA_IMAGE_DESCRIPTION)
-	@echo "# Created medusa image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(MEDUSA_IMAGE_BIT))
-	@echo ""
-
-.PHONY: medusa_publish_image
-medusa_publish_image: $(MEDUSA_IMAGE_BIT)
-	@echo "# Publish medusa image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(MEDUSA_MANIFEST_BIT) -f $(MEDUSA_IMAGE_BIT)
-
-clean_medusa:
-	$(RM) -rf $(BITS_DIR)/medusa
-	(cd build/manta-medusa && gmake distclean)
-
-#---- MAHI
-
-_mahi_stamp=$(MAHI_BRANCH)-$(TIMESTAMP)-g$(MAHI_SHA)
-MAHI_BITS=$(BITS_DIR)/mahi/mahi-pkg-$(_mahi_stamp).tar.gz
-MAHI_IMAGE_BIT=$(BITS_DIR)/mahi/mahi-zfs-$(_mahi_stamp).zfs.gz
-MAHI_MANIFEST_BIT=$(BITS_DIR)/mahi/mahi-zfs-$(_mahi_stamp).imgmanifest
-
-.PHONY: mahi
-mahi: $(MAHI_BITS) mahi_image
-
-$(MAHI_BITS): build/mahi
-	@echo "# Build mahi: branch $(MAHI_BRANCH), sha $(MAHI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/mahi && LDFLAGS="-L/opt/local/lib -R/opt/local/lib" NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created mahi bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(MAHI_BITS)
-	@echo ""
-
-.PHONY: mahi_image
-mahi_image: $(MAHI_IMAGE_BIT)
-
-$(MAHI_IMAGE_BIT): $(MAHI_BITS)
-	@echo "# Build mahi_image: branch $(MAHI_BRANCH), sha $(MAHI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(MAHI_IMAGE_UUID)" -t $(MAHI_BITS) \
-		-b "mahi" \
-		-o "$(MAHI_IMAGE_BIT)" -p $(MAHI_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(MAHI_EXTRA_TARBALLS) -n $(MAHI_IMAGE_NAME) \
-		-v $(_mahi_stamp) -d $(MAHI_IMAGE_DESCRIPTION)
-	@echo "# Created mahi image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(MAHI_IMAGE_BIT))
-	@echo ""
-
-mahi_publish_image: $(MAHI_IMAGE_BIT)
-	@echo "# Publish mahi image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(MAHI_MANIFEST_BIT) -f $(MAHI_IMAGE_BIT)
-
-clean_mahi:
-	$(RM) -rf $(BITS_DIR)/mahi
-	(cd build/mahi && gmake distclean)
-
-#---- Triton CNS
-
-_cns_stamp=$(TRITON_CNS_BRANCH)-$(TIMESTAMP)-g$(TRITON_CNS_SHA)
-CNS_BITS=$(BITS_DIR)/cns/cns-pkg-$(_cns_stamp).tar.gz
-CNS_IMAGE_BIT=$(BITS_DIR)/cns/cns-zfs-$(_cns_stamp).zfs.gz
-CNS_MANIFEST_BIT=$(BITS_DIR)/cns/cns-zfs-$(_cns_stamp).imgmanifest
-
-.PHONY: cns
-cns: $(CNS_BITS) cns_image
-
-$(CNS_BITS): build/triton-cns
-	@echo "# Build cns: branch $(TRITON_CNS_BRANCH), sha $(TRITON_CNS_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/triton-cns && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created cns bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(CNS_BITS)
-	@echo ""
-
-.PHONY: cns_image
-cns_image: $(CNS_IMAGE_BIT)
-
-$(CNS_IMAGE_BIT): $(CNS_BITS)
-	@echo "# Build cns_image: branch $(TRITON_CNS_BRANCH), sha $(TRITON_CNS_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(CNS_IMAGE_UUID)" -t $(CNS_BITS) \
-		-b "cns" \
-		-o "$(CNS_IMAGE_BIT)" -p $(CNS_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(CNS_EXTRA_TARBALLS) -n $(CNS_IMAGE_NAME) \
-		-v $(_cns_stamp) -d $(CNS_IMAGE_DESCRIPTION)
-	@echo "# Created cns image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(CNS_IMAGE_BIT))
-	@echo ""
-
-cns_publish_image: $(CNS_IMAGE_BIT)
-	@echo "# Publish CNS image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(CNS_MANIFEST_BIT) -f $(CNS_IMAGE_BIT)
-
-clean_cns:
-	$(RM) -rf $(BITS_DIR)/cns
-	(cd build/triton-cns && gmake distclean)
-
-
-#---- Manta Resharding System
-
-_manta-reshard_stamp=$(MANTA_RESHARD_BRANCH)-$(TIMESTAMP)-g$(MANTA_RESHARD_SHA)
-MANTA_RESHARD_BITS=$(BITS_DIR)/manta-reshard/manta-reshard-pkg-$(_manta-reshard_stamp).tar.gz
-MANTA_RESHARD_IMAGE_BIT=$(BITS_DIR)/manta-reshard/manta-reshard-zfs-$(_manta-reshard_stamp).zfs.gz
-MANTA_RESHARD_MANIFEST_BIT=$(BITS_DIR)/manta-reshard/manta-reshard-zfs-$(_manta-reshard_stamp).imgmanifest
-
-.PHONY: manta-reshard
-manta-reshard: $(MANTA_RESHARD_BITS) manta-reshard_image
-
-$(MANTA_RESHARD_BITS): build/manta-reshard
-	@echo "# Build manta-reshard: branch $(MANTA_RESHARD_BRANCH), sha $(MANTA_RESHARD_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	cd build/manta-reshard && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish
-	@echo "# Created manta-reshard bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(MANTA_RESHARD_BITS)
-	@echo ""
-
-.PHONY: manta-reshard_image
-manta-reshard_image: $(MANTA_RESHARD_IMAGE_BIT)
-
-$(MANTA_RESHARD_IMAGE_BIT): $(MANTA_RESHARD_BITS)
-	@echo "# Build manta-reshard_image: branch $(MANTA_RESHARD_BRANCH), sha $(MANTA_RESHARD_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(MANTA_RESHARD_IMAGE_UUID)" -t $(MANTA_RESHARD_BITS) \
-		-b "manta-reshard" \
-		-o "$(MANTA_RESHARD_IMAGE_BIT)" -p $(MANTA_RESHARD_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(MANTA_RESHARD_EXTRA_TARBALLS) -n $(MANTA_RESHARD_IMAGE_NAME) \
-		-v $(_manta-reshard_stamp) -d $(MANTA_RESHARD_IMAGE_DESCRIPTION)
-	@echo "# Created manta-reshard image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(MANTA_RESHARD_IMAGE_BIT))
-	@echo ""
-
-manta-reshard_publish_image: $(MANTA_RESHARD_IMAGE_BIT)
-	@echo "# Publish manta-reshard image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(MANTA_RESHARD_MANIFEST_BIT) -f $(MANTA_RESHARD_IMAGE_BIT)
-
-
-clean_manta-reshard:
-	$(RM) -rf $(BITS_DIR)/manta-reshard
-	cd build/manta-reshard && gmake distclean
-
-#---- pgstatsmon
-
-_pgstatsmon_stamp=$(PGSTATSMON_BRANCH)-$(TIMESTAMP)-g$(PGSTATSMON_SHA)
-PGSTATSMON_BITS=$(BITS_DIR)/pgstatsmon/pgstatsmon-pkg-$(_pgstatsmon_stamp).tar.gz
-PGSTATSMON_IMAGE_BIT=$(BITS_DIR)/pgstatsmon/pgstatsmon-zfs-$(_pgstatsmon_stamp).zfs.gz
-PGSTATSMON_MANIFEST_BIT=$(BITS_DIR)/pgstatsmon/pgstatsmon-zfs-$(_pgstatsmon_stamp).imgmanifest
-
-.PHONY: pgstatsmon
-pgstatsmon: $(PGSTATSMON_BITS) pgstatsmon_image
-
-$(PGSTATSMON_BITS): build/pgstatsmon
-	@echo "# Build pgstatsmon: branch $(PGSTATSMON_BRANCH), sha $(PGSTATSMON_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	cd build/pgstatsmon && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish
-	@echo "# Created pgstatsmon bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(PGSTATSMON_BITS)
-	@echo ""
-
-.PHONY: pgstatsmon-image
-pgstatsmon_image: $(PGSTATSMON_IMAGE_BIT)
-
-$(PGSTATSMON_IMAGE_BIT): $(PGSTATSMON_BITS)
-	@echo "# Build pgstatsmon_image: branch $(PGSTATSMON_BRANCH), sha $(PGSTATSMON_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(PGSTATSMON_IMAGE_UUID)" -t $(PGSTATSMON_BITS) \
-		-b "pgstatsmon" \
-		-o "$(PGSTATSMON_IMAGE_BIT)" -p $(PGSTATSMON_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(PGSTATSMON_EXTRA_TARBALLS) -n $(PGSTATSMON_IMAGE_NAME) \
-		-v $(_pgstatsmon_stamp) -d $(PGSTATSMON_IMAGE_DESCRIPTION)
-	@echo "# Created pgstatsmon image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(PGSTATSMON_IMAGE_BIT))
-	@echo ""
-
-pgstatsmon_publish_image: $(PGSTATSMON_IMAGE_BIT)
-	@echo "# Publish pgstatsmon image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(PGSTATSMON_MANIFEST_BIT) -f $(PGSTATSMON_IMAGE_BIT)
-
-
-clean_pgstatsmon:
-	$(RM) -rf $(BITS_DIR)/pgstatsmon
-	cd build/pgstatsmon && gmake distclean
-
-#---- Manta Garbage Collector
-
-_manta-garbage-collector_stamp=$(MANTA_GARBAGE_COLLECTOR_BRANCH)-$(TIMESTAMP)-g$(MANTA_GARBAGE_COLLECTOR_SHA)
-MANTA_GARBAGE_COLLECTOR_BITS=$(BITS_DIR)/manta-garbage-collector/manta-garbage-collector-pkg-$(_manta-garbage-collector_stamp).tar.gz
-MANTA_GARBAGE_COLLECTOR_IMAGE_BIT=$(BITS_DIR)/manta-garbage-collector/manta-garbage-collector-zfs-$(_manta-garbage-collector_stamp).zfs.gz
-MANTA_GARBAGE_COLLECTOR_MANIFEST_BIT=$(BITS_DIR)/manta-garbage-collector/manta-garbage-collector-zfs-$(_manta-garbage-collector_stamp).imgmanifest
-
-.PHONY: manta-garbage-collector
-manta-garbage-collector: $(MANTA_GARBAGE_COLLECTOR_BITS) manta-garbage-collector_image
-
-$(MANTA_GARBAGE_COLLECTOR_BITS): build/manta-garbage-collector
-	@echo "# Build manta-garbage-collector: branch $(MANTA_GARBAGE_COLLECTOR_BRANCH), sha $(MANTA_GARBAGE_COLLECTOR_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	cd build/manta-garbage-collector && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish
-	@echo "# Created manta-garbage-collector bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(MANTA_GARBAGE_COLLECTOR_BITS)
-	@echo ""
-
-.PHONY: manta-garbage-collector_image
-manta-garbage-collector_image: $(MANTA_GARBAGE_COLLECTOR_IMAGE_BIT)
-
-$(MANTA_GARBAGE_COLLECTOR_IMAGE_BIT): $(MANTA_GARBAGE_COLLECTOR_BITS)
-	@echo "# Build manta-garbage-collector_image: branch $(MANTA_GARBAGE_COLLECTOR_BRANCH), sha $(MANTA_GARBAGE_COLLECTOR_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(MANTA_GARBAGE_COLLECTOR_IMAGE_UUID)" -t $(MANTA_GARBAGE_COLLECTOR_BITS) \
-		-b "manta-garbage-collector" \
-		-o "$(MANTA_GARBAGE_COLLECTOR_IMAGE_BIT)" -p $(MANTA_GARBAGE_COLLECTOR_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(MANTA_GARBAGE_COLLECTOR_EXTRA_TARBALLS) -n $(MANTA_GARBAGE_COLLECTOR_IMAGE_NAME) \
-		-v $(_manta-garbage-collector_stamp) -d $(MANTA_GARBAGE_COLLECTOR_IMAGE_DESCRIPTION)
-	@echo "# Created manta-garbage-collector image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(MANTA_GARBAGE_COLLECTOR_IMAGE_BIT))
-	@echo ""
-
-manta-garbage-collector_publish_image: $(MANTA_GARBAGE_COLLECTOR_IMAGE_BIT)
-	@echo "# Publish manta-garbage-collector image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(MANTA_GARBAGE_COLLECTOR_MANIFEST_BIT) -f $(MANTA_GARBAGE_COLLECTOR_IMAGE_BIT)
-
-
-clean_manta-garbage-collector:
-	$(RM) -rf $(BITS_DIR)/manta-garbage-collector
-	cd build/manta-garbage-collector && gmake distclean
-
-#---- Mola
-
-_mola_stamp=$(MANTA_MOLA_BRANCH)-$(TIMESTAMP)-g$(MANTA_MOLA_SHA)
-MOLA_BITS=$(BITS_DIR)/mola/mola-pkg-$(_mola_stamp).tar.gz
-MOLA_IMAGE_BIT=$(BITS_DIR)/mola/mola-zfs-$(_mola_stamp).zfs.gz
-MOLA_MANIFEST_BIT=$(BITS_DIR)/mola/mola-zfs-$(_mola_stamp).imgmanifest
-
-.PHONY: mola
-mola: $(MOLA_BITS) mola_image
-
-# PATH for mola build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(MOLA_BITS): build/manta-mola
-	@echo "# Build mola: branch $(MANTA_MOLA_BRANCH), sha $(MANTA_MOLA_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/manta-mola && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created mola bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(MOLA_BITS)
-	@echo ""
-
-.PHONY: mola_image
-mola_image: $(MOLA_IMAGE_BIT)
-
-$(MOLA_IMAGE_BIT): $(MOLA_BITS)
-	@echo "# Build mola_image: branch $(MANTA_MOLA_BRANCH), sha $(MANTA_MOLA_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(MOLA_IMAGE_UUID)" -t $(MOLA_BITS) \
-		-b "mola" \
-		-o "$(MOLA_IMAGE_BIT)" -p $(MOLA_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(MOLA_EXTRA_TARBALLS) -n $(MOLA_IMAGE_NAME) \
-		-v $(_mola_stamp) -d $(MOLA_IMAGE_DESCRIPTION)
-	@echo "# Created mola image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(MOLA_IMAGE_BIT))
-	@echo ""
-
-mola_publish_image: $(MOLA_IMAGE_BIT)
-	@echo "# Publish mola image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(MOLA_MANIFEST_BIT) -f $(MOLA_IMAGE_BIT)
-
-clean_mola:
-	$(RM) -rf $(BITS_DIR)/mola
-	(cd build/manta-mola && gmake distclean)
-
-
-#---- Madtom
-
-_madtom_stamp=$(MANTA_MADTOM_BRANCH)-$(TIMESTAMP)-g$(MANTA_MADTOM_SHA)
-MADTOM_BITS=$(BITS_DIR)/madtom/madtom-pkg-$(_madtom_stamp).tar.gz
-MADTOM_IMAGE_BIT=$(BITS_DIR)/madtom/madtom-zfs-$(_madtom_stamp).zfs.gz
-MADTOM_MANIFEST_BIT=$(BITS_DIR)/madtom/madtom-zfs-$(_madtom_stamp).imgmanifest
-
-.PHONY: madtom
-madtom: $(MADTOM_BITS) madtom_image
-
-# PATH for madtom build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(MADTOM_BITS): build/manta-madtom
-	@echo "# Build madtom: branch $(MANTA_MADTOM_BRANCH), sha $(MANTA_MADTOM_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/manta-madtom && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created madtom bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(MADTOM_BITS)
-	@echo ""
-
-.PHONY: madtom_image
-madtom_image: $(MADTOM_IMAGE_BIT)
-
-$(MADTOM_IMAGE_BIT): $(MADTOM_BITS)
-	@echo "# Build madtom_image: branch $(MANTA_MADTOM_BRANCH), sha $(MANTA_MADTOM_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(MADTOM_IMAGE_UUID)" -t $(MADTOM_BITS) \
-		-b "madtom" \
-		-o "$(MADTOM_IMAGE_BIT)" -p $(MADTOM_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(MADTOM_EXTRA_TARBALLS) -n $(MADTOM_IMAGE_NAME) \
-		-v $(_madtom_stamp) -d $(MADTOM_IMAGE_DESCRIPTION)
-	@echo "# Created madtom image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(MADTOM_IMAGE_BIT))
-	@echo ""
-
-madtom_publish_image: $(MADTOM_IMAGE_BIT)
-	@echo "# Publish madtom image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(MADTOM_MANIFEST_BIT) -f $(MADTOM_IMAGE_BIT)
-
-clean_madtom:
-	$(RM) -rf $(BITS_DIR)/madtom
-	(cd build/manta-madtom && gmake distclean)
-
-
-#---- Marlin Dashboard
-
-_marlin-dashboard_stamp=$(MANTA_MARLIN_DASHBOARD_BRANCH)-$(TIMESTAMP)-g$(MANTA_MARLIN_DASHBOARD_SHA)
-MARLIN_DASHBOARD_BITS=$(BITS_DIR)/marlin-dashboard/marlin-dashboard-pkg-$(_marlin-dashboard_stamp).tar.gz
-MARLIN_DASHBOARD_IMAGE_BIT=$(BITS_DIR)/marlin-dashboard/marlin-dashboard-zfs-$(_marlin-dashboard_stamp).zfs.gz
-MARLIN_DASHBOARD_MANIFEST_BIT=$(BITS_DIR)/marlin-dashboard/marlin-dashboard-zfs-$(_marlin-dashboard_stamp).imgmanifest
-
-.PHONY: marlin-dashboard
-marlin-dashboard: $(MARLIN_DASHBOARD_BITS) marlin-dashboard_image
-
-# PATH for marlin-dashboard build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(MARLIN_DASHBOARD_BITS): build/manta-marlin-dashboard
-	@echo "# Build marlin-dashboard: branch $(MANTA_MARLIN_DASHBOARD_BRANCH), sha $(MANTA_MARLIN_DASHBOARD_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/manta-marlin-dashboard && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created marlin-dashboard bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(MARLIN_DASHBOARD_BITS)
-	@echo ""
-
-.PHONY: marlin-dashboard_image
-marlin-dashboard_image: $(MARLIN_DASHBOARD_IMAGE_BIT)
-
-$(MARLIN_DASHBOARD_IMAGE_BIT): $(MARLIN_DASHBOARD_BITS)
-	@echo "# Build marlin-dashboard_image: branch $(MANTA_MARLIN_DASHBOARD_BRANCH), sha $(MANTA_MARLIN_DASHBOARD_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(MARLIN_DASHBOARD_IMAGE_UUID)" -t $(MARLIN_DASHBOARD_BITS) \
-		-b "marlin-dashboard" \
-		-o "$(MARLIN_DASHBOARD_IMAGE_BIT)" -p $(MARLIN_DASHBOARD_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(MARLIN_DASHBOARD_EXTRA_TARBALLS) -n $(MARLIN_DASHBOARD_IMAGE_NAME) \
-		-v $(_marlin-dashboard_stamp) -d $(MARLIN_DASHBOARD_IMAGE_DESCRIPTION)
-	@echo "# Created marlin-dashboard image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(MARLIN_DASHBOARD_IMAGE_BIT))
-	@echo ""
-
-marlin-dashboard_publish_image: $(MARLIN_DASHBOARD_IMAGE_BIT)
-	@echo "# Publish marlin-dashboard image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(MARLIN_DASHBOARD_MANIFEST_BIT) -f $(MARLIN_DASHBOARD_IMAGE_BIT)
-
-clean_marlin-dashboard:
-	$(RM) -rf $(BITS_DIR)/marlin-dashboard
-	(cd build/manta-marlin-dashboard && gmake distclean)
-
-#---- NFSSERVER
-
-_nfsserver_stamp=$(NFSSERVER_BRANCH)-$(TIMESTAMP)-g$(NFSSERVER_SHA)
-NFSSERVER_BITS=$(BITS_DIR)/nfsserver/nfsserver-pkg-$(_nfsserver_stamp).tar.gz
-NFSSERVER_IMAGE_BIT=$(BITS_DIR)/nfsserver/nfsserver-zfs-$(_nfsserver_stamp).zfs.gz
-NFSSERVER_MANIFEST_BIT=$(BITS_DIR)/nfsserver/nfsserver-zfs-$(_nfsserver_stamp).imgmanifest
-
-.PHONY: nfsserver
-nfsserver: $(NFSSERVER_BITS) nfsserver_image
-
-$(NFSSERVER_BITS): build/nfsserver
-	@echo "# Build nfsserver: branch $(NFSSERVER_BRANCH), sha $(NFSSERVER_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/nfsserver && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) $(MAKE) release publish)
-	@echo "# Created nfsserver bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(NFSSERVER_BITS)
-	@echo ""
-
-.PHONY: nfsserver_image
-nfsserver_image: $(NFSSERVER_IMAGE_BIT)
-
-$(NFSSERVER_IMAGE_BIT): $(NFSSERVER_BITS)
-	@echo "# Build nfsserver_image: branch $(NFSSERVER_BRANCH), sha $(NFSSERVER_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(NFSSERVER_IMAGE_UUID)" -t $(NFSSERVER_BITS) \
-		-o "$(NFSSERVER_IMAGE_BIT)" -p $(NFSSERVER_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(NFSSERVER_EXTRA_TARBALLS) -n $(NFSSERVER_IMAGE_NAME) \
-		-v $(_nfsserver_stamp) -d $(NFSSERVER_IMAGE_DESCRIPTION)
-	@echo "# Created nfsserver image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(NFSSERVER_IMAGE_BIT))
-	@echo ""
-
-nfsserver_publish_image: $(NFSSERVER_IMAGE_BIT)
-	@echo "# Publish nfsserver image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(NFSSERVER_MANIFEST_BIT) -f $(NFSSERVER_IMAGE_BIT)
-
-clean_nfsserver:
-	$(RM) -rf $(BITS_DIR)/nfsserver
-	(cd build/nfsserver && gmake clean)
-
-#---- VOLAPI
-
-_volapi_stamp=$(SDC_VOLAPI_BRANCH)-$(TIMESTAMP)-g$(SDC_VOLAPI_SHA)
-VOLAPI_BITS=$(BITS_DIR)/volapi/volapi-pkg-$(_volapi_stamp).tar.gz
-VOLAPI_IMAGE_BIT=$(BITS_DIR)/volapi/volapi-zfs-$(_volapi_stamp).zfs.gz
-VOLAPI_MANIFEST_BIT=$(BITS_DIR)/volapi/volapi-zfs-$(_volapi_stamp).imgmanifest
-
-.PHONY: volapi
-volapi: $(VOLAPI_BITS) volapi_image
-
-# PATH for volapi build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(VOLAPI_BITS): build/sdc-volapi
-	@echo "# Build volapi: branch $(SDC_VOLAPI_BRANCH), sha $(SDC_VOLAPI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-volapi && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created volapi bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(VOLAPI_BITS)
-	@echo ""
-
-.PHONY: volapi_image
-volapi_image: $(VOLAPI_IMAGE_BIT)
-
-$(VOLAPI_IMAGE_BIT): $(VOLAPI_BITS)
-	@echo "# Build volapi_image: branch $(SDC_VOLAPI_BRANCH), sha $(SDC_VOLAPI_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(VOLAPI_IMAGE_UUID)" -t $(VOLAPI_BITS) \
-		-o "$(VOLAPI_IMAGE_BIT)" -p $(VOLAPI_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(VOLAPI_EXTRA_TARBALLS) -n $(VOLAPI_IMAGE_NAME) \
-		-v $(_volapi_stamp) -d $(VOLAPI_IMAGE_DESCRIPTION)
-	@echo "# Created volapi image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(VOLAPI_IMAGE_BIT))
-	@echo ""
-
-volapi_publish_image: $(VOLAPI_IMAGE_BIT)
-	@echo "# Publish volapi image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(VOLAPI_MANIFEST_BIT) -f $(VOLAPI_IMAGE_BIT)
-
-# Warning: if volapi's submodule deps change, this 'clean_volapi' is insufficient. It would
-# then need to call 'gmake dist-clean'.
-clean_volapi:
-	$(RM) -rf $(BITS_DIR)/volapi
-	(cd build/sdc-volapi && gmake clean)
-
-
-#---- Moray
-
-_moray_stamp=$(MORAY_BRANCH)-$(TIMESTAMP)-g$(MORAY_SHA)
-MORAY_BITS=$(BITS_DIR)/moray/moray-pkg-$(_moray_stamp).tar.gz
-MORAY_IMAGE_BIT=$(BITS_DIR)/moray/moray-zfs-$(_moray_stamp).zfs.gz
-MORAY_MANIFEST_BIT=$(BITS_DIR)/moray/moray-zfs-$(_moray_stamp).imgmanifest
-
-.PHONY: moray
-moray: $(MORAY_BITS) moray_image
-
-# PATH for moray build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(MORAY_BITS): build/moray
-	@echo "# Build moray: branch $(MORAY_BRANCH), sha $(MORAY_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/moray && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created moray bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(MORAY_BITS)
-	@echo ""
-
-.PHONY: moray_image
-moray_image: $(MORAY_IMAGE_BIT)
-
-$(MORAY_IMAGE_BIT): $(MORAY_BITS)
-	@echo "# Build moray_image: branch $(MORAY_BRANCH), sha $(MORAY_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(MORAY_IMAGE_UUID)" -t $(MORAY_BITS) \
-		-b "moray" \
-		-o "$(MORAY_IMAGE_BIT)" -p $(MORAY_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(MORAY_EXTRA_TARBALLS) -n $(MORAY_IMAGE_NAME) \
-		-v $(_moray_stamp) -d $(MORAY_IMAGE_DESCRIPTION)
-	@echo "# Created moray image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(MORAY_IMAGE_BIT))
-	@echo ""
-
-moray_publish_image: $(MORAY_IMAGE_BIT)
-	@echo "# Publish moray image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(MORAY_MANIFEST_BIT) -f $(MORAY_IMAGE_BIT)
-
-clean_moray:
-	$(RM) -rf $(BITS_DIR)/moray
-	(cd build/moray && gmake distclean)
-
-
-#---- Electric-Moray
-
-_electric-moray_stamp=$(ELECTRIC_MORAY_BRANCH)-$(TIMESTAMP)-g$(ELECTRIC_MORAY_SHA)
-ELECTRIC_MORAY_BITS=$(BITS_DIR)/electric-moray/electric-moray-pkg-$(_electric-moray_stamp).tar.gz
-ELECTRIC_MORAY_IMAGE_BIT=$(BITS_DIR)/electric-moray/electric-moray-zfs-$(_electric-moray_stamp).zfs.gz
-ELECTRIC_MORAY_MANIFEST_BIT=$(BITS_DIR)/electric-moray/electric-moray-zfs-$(_electric-moray_stamp).imgmanifest
-
-.PHONY: electric-moray
-electric-moray: $(ELECTRIC_MORAY_BITS) electric-moray_image
-
-# PATH for electric-moray build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(ELECTRIC_MORAY_BITS): build/electric-moray
-	@echo "# Build electric-moray: branch $(ELECTRIC_MORAY_BRANCH), sha $(ELECTRIC_MORAY_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/electric-moray && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created electric-moray bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(ELECTRIC_MORAY_BITS)
-	@echo ""
-
-.PHONY: electric-moray_image
-electric-moray_image: $(ELECTRIC_MORAY_IMAGE_BIT)
-
-$(ELECTRIC_MORAY_IMAGE_BIT): $(ELECTRIC_MORAY_BITS)
-	@echo "# Build electric-moray_image: branch $(ELECTRIC_MORAY_BRANCH), sha $(ELECTRIC_MORAY_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(ELECTRIC_MORAY_IMAGE_UUID)" -t $(ELECTRIC_MORAY_BITS) \
-		-b "electric-moray" -O "$(MG_OUT_PATH)" \
-		-o "$(ELECTRIC_MORAY_IMAGE_BIT)" -p $(ELECTRIC_MORAY_PKGSRC) \
-		-t $(ELECTRIC_MORAY_EXTRA_TARBALLS) -n $(ELECTRIC_MORAY_IMAGE_NAME) \
-		-v $(_electric-moray_stamp) -d $(ELECTRIC_MORAY_IMAGE_DESCRIPTION)
-	@echo "# Created electric-moray image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(ELECTRIC_MORAY_IMAGE_BIT))
-	@echo ""
-
-electric-moray_publish_image: $(ELECTRIC_MORAY_IMAGE_BIT)
-	@echo "# Publish electric-moray image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(ELECTRIC_MORAY_MANIFEST_BIT) -f $(ELECTRIC_MORAY_IMAGE_BIT)
-
-clean_electric-moray:
-	$(RM) -rf $(BITS_DIR)/electric-moray
-	(cd build/electric-moray && gmake distclean)
-
-
-#---- Muskie
-
-_muskie_stamp=$(MANTA_MUSKIE_BRANCH)-$(TIMESTAMP)-g$(MANTA_MUSKIE_SHA)
-MUSKIE_BITS=$(BITS_DIR)/muskie/muskie-pkg-$(_muskie_stamp).tar.gz
-MUSKIE_IMAGE_BIT=$(BITS_DIR)/muskie/muskie-zfs-$(_muskie_stamp).zfs.gz
-MUSKIE_MANIFEST_BIT=$(BITS_DIR)/muskie/muskie-zfs-$(_muskie_stamp).imgmanifest
-
-.PHONY: muskie
-muskie: $(MUSKIE_BITS) muskie_image
-
-# PATH for muskie build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(MUSKIE_BITS): build/manta-muskie
-	@echo "# Build muskie: branch $(MANTA_MUSKIE_BRANCH), sha $(MANTA_MUSKIE_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/manta-muskie && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created muskie bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(MUSKIE_BITS)
-	@echo ""
-
-.PHONY: muskie_image
-muskie_image: $(MUSKIE_IMAGE_BIT)
-
-$(MUSKIE_IMAGE_BIT): $(MUSKIE_BITS)
-	@echo "# Build muskie_image: branch $(MANTA_MUSKIE_BRANCH), sha $(MANTA_MUSKIE_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(MUSKIE_IMAGE_UUID)" -t $(MUSKIE_BITS) \
-		-b "muskie" \
-		-o "$(MUSKIE_IMAGE_BIT)" -p $(MUSKIE_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(MUSKIE_EXTRA_TARBALLS) -n $(MUSKIE_IMAGE_NAME) \
-		-v $(_muskie_stamp) -d $(MUSKIE_IMAGE_DESCRIPTION)
-	@echo "# Created muskie image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(MUSKIE_IMAGE_BIT))
-	@echo ""
-
-muskie_publish_image: $(MUSKIE_IMAGE_BIT)
-	@echo "# Publish muskie image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(MUSKIE_MANIFEST_BIT) -f $(MUSKIE_IMAGE_BIT)
-
-clean_muskie:
-	$(RM) -rf $(BITS_DIR)/muskie
-	(cd build/manta-muskie && gmake distclean)
-
-
-#---- Wrasse
-
-_wrasse_stamp=$(MANTA_WRASSE_BRANCH)-$(TIMESTAMP)-g$(MANTA_WRASSE_SHA)
-WRASSE_BITS=$(BITS_DIR)/wrasse/wrasse-pkg-$(_wrasse_stamp).tar.gz
-WRASSE_IMAGE_BIT=$(BITS_DIR)/wrasse/wrasse-zfs-$(_wrasse_stamp).zfs.gz
-WRASSE_MANIFEST_BIT=$(BITS_DIR)/wrasse/wrasse-zfs-$(_wrasse_stamp).imgmanifest
-
-.PHONY: wrasse
-wrasse: $(WRASSE_BITS) wrasse_image
-
-# PATH for wrasse build: Ensure /opt/local/bin is first to put gcc 4.5 (from
-# pkgsrc) before other GCCs.
-$(WRASSE_BITS): build/manta-wrasse
-	@echo "# Build wrasse: branch $(MANTA_WRASSE_BRANCH), sha $(MANTA_WRASSE_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/manta-wrasse && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created wrasse bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(WRASSE_BITS)
-	@echo ""
-
-.PHONY: wrasse_image
-wrasse_image: $(WRASSE_IMAGE_BIT)
-
-$(WRASSE_IMAGE_BIT): $(WRASSE_BITS)
-	@echo "# Build wrasse_image: branch $(MANTA_WRASSE_BRANCH), sha $(MANTA_WRASSE_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(WRASSE_IMAGE_UUID)" -t $(WRASSE_BITS) \
-		-b "wrasse" \
-		-o "$(WRASSE_IMAGE_BIT)" -p $(WRASSE_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(WRASSE_EXTRA_TARBALLS) -n $(WRASSE_IMAGE_NAME) \
-		-v $(_wrasse_stamp) -d $(WRASSE_IMAGE_DESCRIPTION)
-	@echo "# Created wrasse image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(WRASSE_IMAGE_BIT))
-	@echo ""
-
-wrasse_publish_image: $(WRASSE_IMAGE_BIT)
-	@echo "# Publish wrasse image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(WRASSE_MANIFEST_BIT) -f $(WRASSE_IMAGE_BIT)
-
-clean_wrasse:
-	$(RM) -rf $(BITS_DIR)/wrasse
-	(cd build/manta-wrasse && gmake distclean)
-
-
-#---- Registrar
-
-_registrar_stamp=$(REGISTRAR_BRANCH)-$(TIMESTAMP)-g$(REGISTRAR_SHA)
-REGISTRAR_BITS=$(BITS_DIR)/registrar/registrar-pkg-$(_registrar_stamp).tar.gz
-
-.PHONY: registrar
-registrar: $(REGISTRAR_BITS)
-
-$(REGISTRAR_BITS): build/registrar
-	@echo "# Build registrar: branch $(REGISTRAR_BRANCH), sha $(REGISTRAR_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/registrar && LDFLAGS="-L/opt/local/lib -R/opt/local/lib" NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created registrar bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(REGISTRAR_BITS)
-	@echo ""
-
-clean_registrar:
-	$(RM) -rf $(BITS_DIR)/registrar
-	(cd build/registrar && gmake distclean)
-
-
-#---- waferlock
-
-_waferlock_stamp=$(WAFERLOCK_BRANCH)-$(TIMESTAMP)-g$(WAFERLOCK_SHA)
-WAFERLOCK_BITS=$(BITS_DIR)/waferlock/waferlock-pkg-$(_waferlock_stamp).tar.gz
-
-.PHONY: waferlock
-waferlock: $(WAFERLOCK_BITS)
-
-$(WAFERLOCK_BITS): build/waferlock
-	@echo "# Build waferlock: branch $(WAFERLOCK_BRANCH), sha $(WAFERLOCK_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/waferlock && LDFLAGS="-L/opt/local/lib -R/opt/local/lib" NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created waferlock bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(WAFERLOCK_BITS)
-	@echo ""
-
-clean_waferlock:
-	$(RM) -rf $(BITS_DIR)/waferlock
-	(cd build/waferlock && gmake distclean)
-
-#---- mackerel
-
-_mackerel_stamp=$(MANTA_MACKEREL_BRANCH)-$(TIMESTAMP)-g$(MANTA_MACKEREL_SHA)
-MACKEREL_BITS=$(BITS_DIR)/mackerel/mackerel-pkg-$(_mackerel_stamp).tar.gz
-
-.PHONY: mackerel
-mackerel: $(MACKEREL_BITS)
-
-$(MACKEREL_BITS): build/manta-mackerel
-	@echo "# Build mackerel: branch $(MANTA_MACKEREL_BRANCH), sha $(MANTA_MACKEREL_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/manta-mackerel && LDFLAGS="-L/opt/local/lib -R/opt/local/lib" NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created mackerel bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(MACKEREL_BITS)
-	@echo ""
-
-clean_mackerel:
-	$(RM) -rf $(BITS_DIR)/mackerel
-	(cd build/manta-mackerel && gmake distclean)
-
-
-#---- Binder
-
-_binder_stamp=$(BINDER_BRANCH)-$(TIMESTAMP)-g$(BINDER_SHA)
-BINDER_BITS=$(BITS_DIR)/binder/binder-pkg-$(_binder_stamp).tar.gz
-BINDER_IMAGE_BIT=$(BITS_DIR)/binder/binder-zfs-$(_binder_stamp).zfs.gz
-BINDER_MANIFEST_BIT=$(BITS_DIR)/binder/binder-zfs-$(_binder_stamp).imgmanifest
-
-.PHONY: binder
-binder: $(BINDER_BITS) binder_image
-
-$(BINDER_BITS): build/binder
-	@echo "# Build binder: branch $(BINDER_BRANCH), sha $(BINDER_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/binder && LDFLAGS="-L/opt/local/lib -R/opt/local/lib" NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created binder bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(BINDER_BITS)
-	@echo ""
-
-.PHONY: binder_image
-binder_image: $(BINDER_IMAGE_BIT)
-
-$(BINDER_IMAGE_BIT): $(BINDER_BITS)
-	@echo "# Build binder_image: branch $(BINDER_BRANCH), sha $(BINDER_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(BINDER_IMAGE_UUID)" -t $(BINDER_BITS) \
-		-b "binder" \
-		-o "$(BINDER_IMAGE_BIT)" -p $(BINDER_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(BINDER_EXTRA_TARBALLS) -n $(BINDER_IMAGE_NAME) \
-		-v $(_binder_stamp) -d $(BINDER_IMAGE_DESCRIPTION)
-	@echo "# Created binder image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(BINDER_IMAGE_BIT))
-	@echo ""
-
-binder_publish_image: $(BINDER_IMAGE_BIT)
-	@echo "# Publish binder image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(BINDER_MANIFEST_BIT) -f $(BINDER_IMAGE_BIT)
-
-clean_binder:
-	$(RM) -rf $(BITS_DIR)/binder
-	(cd build/binder && gmake distclean)
-
-
-#---- Muppet
-
-_muppet_stamp=$(MUPPET_BRANCH)-$(TIMESTAMP)-g$(MUPPET_SHA)
-MUPPET_BITS=$(BITS_DIR)/muppet/muppet-pkg-$(_muppet_stamp).tar.gz
-MUPPET_IMAGE_BIT=$(BITS_DIR)/muppet/muppet-zfs-$(_muppet_stamp).zfs.gz
-MUPPET_MANIFEST_BIT=$(BITS_DIR)/muppet/muppet-zfs-$(_muppet_stamp).imgmanifest
-
-.PHONY: muppet
-muppet: $(MUPPET_BITS) muppet_image
-
-$(MUPPET_BITS): build/muppet
-	@echo "# Build muppet: branch $(MUPPET_BRANCH), sha $(MUPPET_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/muppet && LDFLAGS="-L/opt/local/lib -R/opt/local/lib" NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created muppet bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(MUPPET_BITS)
-	@echo ""
-
-.PHONY: muppet_image
-muppet_image: $(MUPPET_IMAGE_BIT)
-
-$(MUPPET_IMAGE_BIT): $(MUPPET_BITS)
-	@echo "# Build muppet_image: branch $(MUPPET_BRANCH), sha $(MUPPET_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(MUPPET_IMAGE_UUID)" -t $(MUPPET_BITS) \
-		-b "muppet" \
-		-o "$(MUPPET_IMAGE_BIT)" -p $(MUPPET_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(MUPPET_EXTRA_TARBALLS) -n $(MUPPET_IMAGE_NAME) \
-		-v $(_muppet_stamp) -d $(MUPPET_IMAGE_DESCRIPTION)
-	@echo "# Created muppet image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(MUPPET_IMAGE_BIT))
-	@echo ""
-
-muppet_publish_image: $(MUPPET_IMAGE_BIT)
-	@echo "# Publish muppet image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(MUPPET_MANIFEST_BIT) -f $(MUPPET_IMAGE_BIT)
-
-clean_muppet:
-	$(RM) -rf $(BITS_DIR)/muppet
-	(cd build/muppet && gmake distclean)
-
-#---- Minnow
-
-_minnow_stamp=$(MANTA_MINNOW_BRANCH)-$(TIMESTAMP)-g$(MANTA_MINNOW_SHA)
-MINNOW_BITS=$(BITS_DIR)/minnow/minnow-pkg-$(_minnow_stamp).tar.gz
-
-.PHONY: minnow
-minnow: $(MINNOW_BITS)
-
-$(MINNOW_BITS): build/manta-minnow
-	@echo "# Build minnow: branch $(MANTA_MINNOW_BRANCH), sha $(MANTA_MINNOW_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/manta-minnow && LDFLAGS="-L/opt/local/lib -R/opt/local/lib" NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created minnow bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(MINNOW_BITS)
-	@echo ""
-
-clean_minnow:
-	$(RM) -rf $(BITS_DIR)/minnow
-	(cd build/manta-minnow && gmake distclean)
-
-
-#---- Mako
-
-_mako_stamp=$(MANTA_MAKO_BRANCH)-$(TIMESTAMP)-g$(MANTA_MAKO_SHA)
-MAKO_BITS=$(BITS_DIR)/mako/mako-pkg-$(_mako_stamp).tar.gz
-MAKO_IMAGE_BIT=$(BITS_DIR)/mako/mako-zfs-$(_mako_stamp).zfs.gz
-MAKO_MANIFEST_BIT=$(BITS_DIR)/mako/mako-zfs-$(_mako_stamp).imgmanifest
-
-.PHONY: mako
-mako: $(MAKO_BITS) mako_image
-
-$(MAKO_BITS): build/manta-mako
-	@echo "# Build mako: branch $(MANTA_MAKO_BRANCH), sha $(MANTA_MAKO_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/manta-mako && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created mako bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(MAKO_BITS)
-	@echo ""
-
-.PHONY: mako_image
-mako_image: $(MAKO_IMAGE_BIT)
-
-$(MAKO_IMAGE_BIT): $(MAKO_BITS)
-	@echo "# Build mako_image: branch $(MANTA_MAKO_BRANCH), sha $(MANTA_MAKO_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh -i "$(MAKO_IMAGE_UUID)" -t $(MAKO_BITS) \
-		-b "mako" \
-		-o "$(MAKO_IMAGE_BIT)" -p $(MAKO_PKGSRC) -O "$(MG_OUT_PATH)" \
-		-t $(MAKO_EXTRA_TARBALLS) -n $(MAKO_IMAGE_NAME) \
-		-v $(_mako_stamp) -d $(MAKO_IMAGE_DESCRIPTION)
-	@echo "# Created mako image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(MAKO_IMAGE_BIT))
-	@echo ""
-
-mako_publish_image: $(MAKO_IMAGE_BIT)
-	@echo "# Publish mako image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(MAKO_MANIFEST_BIT) -f $(MAKO_IMAGE_BIT)
-
-clean_mako:
-	$(RM) -rf $(BITS_DIR)/mako
-	(cd build/manta-mako && gmake distclean)
-
-
-#---- sdcadm
-
-_sdcadm_stamp=$(SDCADM_BRANCH)-$(TIMESTAMP)-g$(SDCADM_SHA)
-SDCADM_PKG_BIT=$(BITS_DIR)/sdcadm/sdcadm-$(_sdcadm_stamp).sh
-SDCADM_MANIFEST_BIT=$(BITS_DIR)/sdcadm/sdcadm-$(_sdcadm_stamp).imgmanifest
-SDCADM_BITS=$(SDCADM_PKG_BIT) $(SDCADM_MANIFEST_BIT)
-
-.PHONY: sdcadm
-sdcadm: $(SDCADM_PKG_BIT)
-
-$(SDCADM_BITS): build/sdcadm/Makefile
-	@echo "# Build sdcadm: branch $(SDCADM_BRANCH), sha $(SDCADM_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)/sdcadm
-	(cd build/sdcadm && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created sdcadm bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(SDCADM_BITS)
-	@echo ""
-
-clean_sdcadm:
-	$(RM) -rf $(BITS_DIR)/sdcadm
-
-sdcadm_publish_image: $(SDCADM_BITS)
-	@echo "# Publish sdcadm image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(SDCADM_MANIFEST_BIT) -f $(SDCADM_PKG_BIT) --compression=none
-
-
-#---- agentsshar
-
-ifeq ($(TRY_BRANCH),)
-_as_stamp=$(SDC_AGENTS_INSTALLER_BRANCH)-$(TIMESTAMP)-g$(SDC_AGENTS_INSTALLER_SHA)
-else
-_as_stamp=$(TRY_BRANCH)-$(TIMESTAMP)-g$(SDC_AGENTS_INSTALLER_SHA)
-endif
-AGENTSSHAR_BITS=$(BITS_DIR)/agentsshar/agents-$(_as_stamp).sh \
-	$(BITS_DIR)/agentsshar/agents-$(_as_stamp).md5sum
-AGENTSSHAR_BITS_0=$(shell echo $(AGENTSSHAR_BITS) | awk '{print $$1}')
-AGENTSSHAR_MANIFEST_BIT=$(BITS_DIR)/agentsshar/agents-$(_as_stamp).manifest
-
-.PHONY: agentsshar
-agentsshar: $(AGENTSSHAR_BITS_0)
-
-$(AGENTSSHAR_BITS): build/sdc-agents-installer/Makefile
-	@echo "# Build agentsshar: branch $(SDC_AGENTS_INSTALLER_BRANCH), sha $(SDC_AGENTS_INSTALLER_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)/agentsshar
-	(cd build/sdc-agents-installer && NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) ./mk-agents-shar -o $(BITS_DIR)/agentsshar/ -d $(BITS_DIR) -b "$(TRY_BRANCH) $(BRANCH)")
-	@echo "# Created agentsshar bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(AGENTSSHAR_BITS)
-	@echo ""
-
-agentsshar_publish_image: $(AGENTSSHAR_BITS)
-	@echo "# Publish agentsshar image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(AGENTSSHAR_MANIFEST_BIT) -f $(AGENTSSHAR_BITS_0)
-
-clean_agentsshar:
-	$(RM) -rf $(BITS_DIR)/agentsshar
-	(if [[ -d build/sdc-agents-installer ]]; then cd build/agents-installer && gmake clean; fi )
-
-
-#---- dockerlogger
-
-ifeq ($(TRY_BRANCH),)
-DOCKERLOGGER_TRY_BRANCH=$(DOCKERLOGGER_BRANCH)
-_dl_stamp=$(DOCKERLOGGER_BRANCH)-$(TIMESTAMP)-g$(DOCKERLOGGER_SHA)
-else
-DOCKERLOGGER_TRY_BRANCH=$(TRY_BRANCH)
-_dl_stamp=$(TRY_BRANCH)-$(TIMESTAMP)-g$(DOCKERLOGGER_SHA)
-endif
-DOCKERLOGGER_BITS=$(BITS_DIR)/dockerlogger/dockerlogger-$(_dl_stamp).sh \
-	$(BITS_DIR)/dockerlogger/dockerlogger-$(_dl_stamp).md5sum
-DOCKERLOGGER_BITS_0=$(shell echo $(DOCKERLOGGER_BITS) | awk '{print $$1}')
-DOCKERLOGGER_MANIFEST_BIT=$(BITS_DIR)/dockerlogger/dockerlogger-$(_dl_stamp).manifest
-
-.PHONY: dockerlogger
-dockerlogger: $(DOCKERLOGGER_BITS_0)
-
-$(DOCKERLOGGER_BITS): build/dockerlogger
-	@echo "# Build dockerlogger: branch $(DOCKERLOGGER_BRANCH), sha $(DOCKERLOGGER_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)/dockerlogger
-	(cd build/dockerlogger && make GOROOT=/root/opt/go TIMESTAMP=$(TIMESTAMP) DESTDIR=$(BITS_DIR)/dockerlogger BRANCH=$(DOCKERLOGGER_TRY_BRANCH) pkg)
-	@echo "# Created dockerlogger bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(DOCKERLOGGER_BITS)
-	@echo ""
-
-dockerlogger_publish_image: $(DOCKERLOGGER_BITS)
-	@echo "# Publish dockerlogger image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(DOCKERLOGGER_MANIFEST_BIT) -f $(DOCKERLOGGER_BITS_0)
-
-clean_dockerlogger:
-	$(RM) -rf $(BITS_DIR)/dockerlogger
-	(if [[ -d build/dockerlogger ]]; then cd build/dockerlogger && gmake clean; fi )
-
-
-#---- Manta deployment (the manta zone)
-
-_manta_deployment_stamp=$(SDC_MANTA_BRANCH)-$(TIMESTAMP)-g$(SDC_MANTA_SHA)
-MANTA_DEPLOYMENT_BITS=$(BITS_DIR)/manta-deployment/manta-deployment-pkg-$(_manta_deployment_stamp).tar.gz
-MANTA_DEPLOYMENT_IMAGE_BIT=$(BITS_DIR)/manta-deployment/manta-deployment-zfs-$(_manta_deployment_stamp).zfs.gz
-MANTA_DEPLOYMENT_MANIFEST_BIT=$(BITS_DIR)/manta-deployment/manta-deployment-zfs-$(_manta_deployment_stamp).imgmanifest
-
-.PHONY: manta-deployment
-manta-deployment: $(MANTA_DEPLOYMENT_BITS) manta-deployment_image
-
-$(MANTA_DEPLOYMENT_BITS): build/sdc-manta
-	@echo "# Build manta-deployment: branch $(SDC_MANTA_BRANCH), sha $(SDC_MANTA_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	mkdir -p $(BITS_DIR)
-	(cd build/sdc-manta && LDFLAGS="-L/opt/local/lib -R/opt/local/lib" NPM_CONFIG_CACHE=$(MG_CACHE_DIR)/npm TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake release publish)
-	@echo "# Created manta-deployment bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(MANTA_DEPLOYMENT_BITS)
-	@echo ""
-
-.PHONY: manta-deployment_image
-manta-deployment_image: $(MANTA_DEPLOYMENT_IMAGE_BIT)
-
-$(MANTA_DEPLOYMENT_IMAGE_BIT): $(MANTA_DEPLOYMENT_BITS)
-	@echo "# Build manta-deployment_image: branch $(SDC_MANTA_BRANCH), sha $(SDC_MANTA_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	./tools/prep_dataset_in_jpc.sh \
-		-O "$(MG_OUT_PATH)" \
-		-i "$(MANTA_DEPLOYMENT_IMAGE_UUID)" -t $(MANTA_DEPLOYMENT_BITS) \
-		-o "$(MANTA_DEPLOYMENT_IMAGE_BIT)" -p $(MANTA_DEPLOYMENT_PKGSRC) \
-		-t $(MANTA_DEPLOYMENT_EXTRA_TARBALLS) -n $(MANTA_DEPLOYMENT_IMAGE_NAME) \
-		-v $(_manta_deployment_stamp) -d $(MANTA_DEPLOYMENT_IMAGE_DESCRIPTION)
-	@echo "# Created manta-deployment image (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $$(dirname $(MANTA_DEPLOYMENT_IMAGE_BIT))
-	@echo ""
-
-manta-deployment_publish_image: $(MANTA_DEPLOYMENT_IMAGE_BIT)
-	@echo "# Publish manta-deployment image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(MANTA_DEPLOYMENT_MANIFEST_BIT) -f $(MANTA_DEPLOYMENT_IMAGE_BIT)
-
-
-clean_manta-deployment:
-	$(RM) -rf $(BITS_DIR)/manta-deployment
-	(cd build/sdc-manta && gmake distclean)
-
-
-
-#---- firmware-tools (Legacy-mode FDUM facilities and firmware for Joyent HW)
-
-ifeq ($(JOYENT_BUILD),true)
-
-_firmware_tools_stamp=$(FIRMWARE_TOOLS_BRANCH)-$(TIMESTAMP)-g$(FIRMWARE_TOOLS_SHA)
-FIRMWARE_TOOLS_BITS=$(BITS_DIR)/firmware-tools/firmware-tools-$(_firmware_tools_stamp).tgz
-
-.PHONY: firmware-tools
-firmware-tools: $(FIRMWARE_TOOLS_BITS)
-
-$(FIRMWARE_TOOLS_BITS): build/firmware-tools
-	mkdir -p $(BITS_DIR)
-	(cd build/firmware-tools && TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) \
-	    gmake pkg release publish)
-	@echo "# Created firmware-tools bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(FIRMWARE_TOOLS_BITS)
-	@echo ""
-
-clean_firmware-tools:
-	$(RM) -rf $(BITS_DIR)/firmware-tools
-	(cd build/firmware-tools && gmake clean)
-
-endif	# $(JOYENT_BUILD) == true
-
-#---- sdc-headnode
-# We are using the '-s STAGE-DIR' option to the sdc-headnode build to
-# avoid rebuilding it. We use the "boot" target to build the stage dir
-# and have the other sdc-headnode targets depend on that.
-#
-# TODO:
-# - solution for datasets
-# - pkgsrc isolation
-
-#
-# These targets will be built for every member of the "headnode*" family
-# of MG targets:
-#
-HEADNODE_TARGETS = \
-	cleanimgcruft \
-	boot \
-	coal \
-	usb \
-	releasejson
-
-#
-# These targets are only built for the base "headnode" target.
-#
-HEADNODE_BASE_ONLY_TARGETS = \
-	gz-tools
-
-.PHONY: headnode
-headnode: $(HEADNODE_TARGETS) $(HEADNODE_BASE_ONLY_TARGETS)
-
-.PHONY: headnode-debug headnode-joyent headnode-joyent-debug
-headnode-debug headnode-joyent headnode-joyent-debug: $(HEADNODE_TARGETS)
-
-headnode: HEADNODE_SUFFIX = ""
-headnode: USE_DEBUG_PLATFORM = false
-headnode-debug: HEADNODE_SUFFIX = "-debug"
-headnode-debug: USE_DEBUG_PLATFORM = true
-headnode-joyent: HEADNODE_SUFFIX = "-joyent"
-headnode-joyent: USE_DEBUG_PLATFORM = false
-headnode-joyent-debug: HEADNODE_SUFFIX = "-joyent-debug"
-headnode-joyent-debug: USE_DEBUG_PLATFORM = true
-
-_headnode_stamp=$(SDC_HEADNODE_BRANCH)-$(TIMESTAMP)-g$(SDC_HEADNODE_SHA)
-
-USB_BUILD_DIR=$(BUILD_DIR)/sdc-headnode
-USB_BITS_DIR=$(BITS_DIR)/headnode$(HEADNODE_SUFFIX)
-
-USB_BITS_SPEC=$(USB_BITS_DIR)/build.spec.local
-
-USB_BUILD_SPEC_ENV = \
-	USE_DEBUG_PLATFORM=$(USE_DEBUG_PLATFORM) \
-	JOYENT_BUILD=$(JOYENT_BUILD)
-
-BOOT_BUILD=$(USB_BUILD_DIR)/boot-$(_headnode_stamp).tgz
-BOOT_OUTPUT=$(USB_BITS_DIR)/boot$(HEADNODE_SUFFIX)-$(_headnode_stamp).tgz
-
-
-# Delete any failed image files that might be sitting around, this is safe
-# because only one headnode build runs at a time. Also cleanup any unused
-# lofi devices (used ones will just fail)
-.PHONY: cleanimgcruft
-cleanimgcruft:
-	$(RM) -vf /tmp/*4gb.img
-	for dev in $(shell lofiadm | cut -d ' ' -f1 | grep -v "^Block"); do  \
-		mount | grep "on $${dev}" | cut -d' ' -f1 | while read mntpath; do \
-			umount $${mntpath}; \
-		done; \
-		pfexec lofiadm -d $${dev}; \
-	done
-
-.PHONY: boot
-boot: $(BOOT_OUTPUT)
-
-$(USB_BITS_DIR):
-	mkdir -p $(USB_BITS_DIR)
-
-$(BOOT_OUTPUT): $(USB_BITS_SPEC) $(USB_BITS_DIR)
-	@echo ""
-	@echo "# Build boot: sdc-headnode branch $(SDC_HEADNODE_BRANCH), sha $(SDC_HEADNODE_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	cd build/sdc-headnode && \
-	    BITS_DIR=$(BITS_DIR) TIMESTAMP=$(TIMESTAMP) \
-	    make tar
-	mv $(BOOT_BUILD) $(BOOT_OUTPUT)
-	@echo "# Created boot bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(BOOT_OUTPUT)
-	@echo ""
-
-
-COAL_BUILD=$(USB_BUILD_DIR)/coal-$(_headnode_stamp)-4gb.tgz
-COAL_OUTPUT=$(USB_BITS_DIR)/coal$(HEADNODE_SUFFIX)-$(_headnode_stamp)-4gb.tgz
-
-$(USB_BITS_SPEC): $(USB_BITS_DIR)
-	$(USB_BUILD_SPEC_ENV) bash <build.spec.in >$(USB_BITS_SPEC)
-	(cd $(USB_BUILD_DIR); $(RM) -f build.spec.local; ln -s $(USB_BITS_SPEC))
-
-.PHONY: coal
-coal: usb $(COAL_OUTPUT)
-
-$(COAL_OUTPUT): $(USB_BITS_SPEC) $(USB_OUTPUT)
-	@echo ""
-	@echo "# Build coal: sdc-headnode branch $(SDC_HEADNODE_BRANCH), sha $(SDC_HEADNODE_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	cd build/sdc-headnode && \
-	    BITS_DIR=$(BITS_DIR) TIMESTAMP=$(TIMESTAMP) \
-	    ./bin/build-coal-image $(USB_OUTPUT)
-	mv $(COAL_BUILD) $(COAL_OUTPUT)
-	@echo "# Created coal bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(COAL_OUTPUT)
-	@echo ""
-
-USB_BUILD=$(USB_BUILD_DIR)/usb-$(_headnode_stamp).tgz
-USB_OUTPUT=$(USB_BITS_DIR)/usb$(HEADNODE_SUFFIX)-$(_headnode_stamp).tgz
-
-.PHONY: usb
-usb: $(USB_OUTPUT)
-
-$(USB_OUTPUT): $(USB_BITS_SPEC) $(BOOT_OUTPUT)
-	@echo ""
-	@echo "# Build usb: sdc-headnode branch $(SDC_HEADNODE_BRANCH), sha $(SDC_HEADNODE_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	cd build/sdc-headnode && \
-	    BITS_DIR=$(BITS_DIR) TIMESTAMP=$(TIMESTAMP) \
-	    ./bin/build-usb-image $(BOOT_OUTPUT)
-	mv $(USB_BUILD) $(USB_OUTPUT)
-	@echo "# Created usb bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(USB_OUTPUT)
-	@echo ""
-
-GZ_TOOLS_BUILD=$(USB_BUILD_DIR)/gz-tools-$(_headnode_stamp).tgz
-GZ_TOOLS_OUTPUT=$(USB_BITS_DIR)/gz-tools$(HEADNODE_SUFFIX)-$(_headnode_stamp).tgz
-GZ_TOOLS_MANIFEST_BUILD=$(USB_BUILD_DIR)/gz-tools-$(_headnode_stamp).manifest
-GZ_TOOLS_MANIFEST_OUTPUT=$(USB_BITS_DIR)/gz-tools$(HEADNODE_SUFFIX)-$(_headnode_stamp).manifest
-
-.PHONY: gz-tools
-gz-tools: $(GZ_TOOLS_OUTPUT)
-
-$(GZ_TOOLS_OUTPUT): $(USB_BITS_SPEC) $(BOOT_OUTPUT)
-	@echo ""
-	@echo "# Build gz-tools: sdc-headnode branch $(SDC_HEADNODE_BRANCH), sha $(SDC_HEADNODE_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	cd "build/sdc-headnode" && \
-	    BITS_DIR=$(BITS_DIR) TIMESTAMP=$(TIMESTAMP) \
-	    gmake gz-tools gz-tools-publish
-	mv $(GZ_TOOLS_BUILD) $(GZ_TOOLS_OUTPUT)
-	mv $(GZ_TOOLS_MANIFEST_BUILD) $(GZ_TOOLS_MANIFEST_OUTPUT)
-	@echo "# Created gz-tools bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(GZ_TOOLS_OUTPUT) $(GZ_TOOLS_MANIFEST_OUTPUT)
-	@echo ""
-
-.PHONY: gz-tools_publish_image
-gz-tools_publish_image:
-	@echo ""
-	@echo "# Publish gz-tools image to SDC Updates repo."
-	$(UPDATES_IMGADM) import -ddd -m $(GZ_TOOLS_MANIFEST_OUTPUT) -f $(GZ_TOOLS_OUTPUT)
-	@echo ""
-
-
-#
-# Of the "headnode*" family of builds, only the base "headnode" job currently
-# publishes any images using a "*_publish_image" target:
-#
-.PHONY: headnode_publish_image
-headnode_publish_image: gz-tools_publish_image
-
-# A headnode image that can be imported to an IMGAPI and used for
-# sdc-on-sdc.
-
-IMAGE_BUILD=$(USB_BUILD)/usb-$(_headnode_stamp).zvol.bz2
-IMAGE_OUTPUT=$(USB_BITS_DIR)/usb$(HEADNODE_SUFFIX)-$(_headnode_stamp).zvol.bz2
-MANIFEST_BUILD=$(USB_BUILD)/usb-$(_headnode_stamp).dsmanifest
-MANIFEST_OUTPUT=$(USB_BITS_DIR)/usb$(HEADNODE_SUFFIX)-$(_headnode_stamp).dsmanifest
-
-.PHONY: image
-image: $(IMAGE_OUTPUT)
-
-$(IMAGE_OUTPUT): $(USB_BITS_SPEC) $(USB_OUTPUT)
-	@echo "# Build sdc-on-sdc image: sdc-headnode branch $(SDC_HEADNODE_BRANCH), sha $(SDC_HEADNODE_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	cd build/sdc-headnode && \
-		BITS_DIR=$(BITS_DIR) TIMESTAMP=$(TIMESTAMP) \
-		./bin/build-dataset $(USB_OUTPUT)
-	mv $(IMAGE_BUILD) $(MANIFEST_BUILD) $(USB_BITS_DIR)/
-	@echo "# Created image bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(MANIFEST_OUTPUT) $(IMAGE_OUTPUT)
-	@echo ""
-
-
-RELEASEJSON_BIT=$(USB_BITS_DIR)/release.json
-
-.PHONY: releasejson
-releasejson: $(USB_BITS_DIR)
-	echo "{ \
-	\"date\": \"$(TIMESTAMP)\", \
-	\"branch\": \"$(BRANCH)\", \
-	\"try-branch\": \"$(TRY-BRANCH)\", \
-	\"coal\": \"$(shell basename $(COAL_OUTPUT))\", \
-	\"boot\": \"$(shell basename $(BOOT_OUTPUT))\", \
-	\"usb\": \"$(shell basename $(USB_OUTPUT))\" \
-}" | $(JSON) >$(RELEASEJSON_BIT)
-
-
-clean_headnode:
-	$(RM) -rf $(BITS_DIR)/headnode*
-
-
-
-#---- platform and debug platform
-
-ifeq ($(TRY_BRANCH),)
-PLATFORM_TRY_BRANCH=$(SMARTOS_LIVE_BRANCH)
-else
-PLATFORM_TRY_BRANCH=$(TRY_BRANCH)
-endif
-
-PLATFORM_BUILD_DIR=build/smartos-live/output
-
-PLATFORM_BUILD_TAR_PLATFORM=$(PLATFORM_BUILD_DIR)/platform-$(TIMESTAMP).tgz
-PLATFORM_BUILD_TAR_BOOT=$(PLATFORM_BUILD_DIR)/boot-$(TIMESTAMP).tgz
-PLATFORM_BUILD_TAR_IMAGES=$(PLATFORM_BUILD_DIR)/images-$(TIMESTAMP).tgz
-
-PLATFORM_BITS_DIR=$(BITS_DIR)/platform$(PLAT_SUFFIX)
-PLATFORM_TAR_SUFFIX=$(PLAT_SUFFIX)-$(PLATFORM_TRY_BRANCH)-$(TIMESTAMP).tgz
-
-PLATFORM_BITS_TAR_PLATFORM=$(PLATFORM_BITS_DIR)/platform$(PLATFORM_TAR_SUFFIX)
-PLATFORM_BITS_TAR_BOOT=$(PLATFORM_BITS_DIR)/boot$(PLATFORM_TAR_SUFFIX)
-PLATFORM_BITS_TAR_IMAGES=$(PLATFORM_BITS_DIR)/images$(PLATFORM_TAR_SUFFIX)
-
-PLATFORM_BITS= \
-	$(PLATFORM_BITS_TAR_PLATFORM) \
-	$(PLATFORM_BITS_TAR_BOOT)
-PLATFORM_MANIFEST_BIT=platform.imgmanifest
-
-ILLUMOS_ENABLE_DEBUG="no"
-PLAT_FLAVOR=""
-
-platform-debug : PLAT_SUFFIX += "-debug"
-platform-debug : ILLUMOS_ENABLE_DEBUG = "exclusive"
-platform-smartos : PLAT_FLAVOR = "-smartos"
-
-
-.PHONY: platform platform-debug platform-smartos
-platform platform-debug platform-smartos: smartos_live_make_check $(PLATFORM_BITS_TAR_PLATFORM)
-
-build/smartos-live/configure.mg:
-	sed -e "s:GITCLONESOURCE:$(shell pwd)/build/:" \
-		<smartos-live-configure$(PLAT_FLAVOR).mg.in >$@
-
-.PHONY: smartos_live_make_check
-smartos_live_make_check:
-	(cd build/smartos-live && make check)
-
-# PATH: Ensure using GCC from SFW as require for platform build.
-$(PLATFORM_BITS): build/smartos-live/configure.mg
-	@echo "# Build platform: branch $(SMARTOS_LIVE_BRANCH), sha $(SMARTOS_LIVE_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	#
-	# We could use -d in PLAT_CONFIGURE_ARGS, but historically, we
-	# enabled a debug build via the environment variable, so we'll keep that
-	# around for building older platforms as needed.
-	#
-	(cd build/smartos-live \
-		&& PATH=/usr/sfw/bin:$(PATH) ILLUMOS_ENABLE_DEBUG=$(ILLUMOS_ENABLE_DEBUG) \
-			./configure $(PLAT_CONFIGURE_ARGS) \
-		&& PATH=/usr/sfw/bin:$(PATH) \
-			BUILDSTAMP=$(TIMESTAMP) \
-			gmake world \
-		&& PATH=/usr/sfw/bin:$(PATH) \
-			BUILDSTAMP=$(TIMESTAMP) \
-			gmake live pkgsrc)
-	#
-	# The "images-tar" target has not always existed, so we need to check
-	# if this version of the platform has it before blindly invoking it.
-	# GNU make has a "-q" flag that exits 0 or 1 if a target exists, or
-	# 2 otherwise.
-	#
-	(cd build/smartos-live || exit 1; \
-		gmake -q images-tar; \
-		if [ $$? -eq 2 ]; then \
-			exit 0; \
-		fi; \
-		BUILDSTAMP=$(TIMESTAMP) gmake images-tar)
-	(mkdir -p $(BITS_DIR)/platform$(PLAT_SUFFIX))
-	(cp $(PLATFORM_BUILD_TAR_PLATFORM) $(PLATFORM_BITS_TAR_PLATFORM))
-	(cp $(PLATFORM_BUILD_TAR_BOOT) $(PLATFORM_BITS_TAR_BOOT))
-	(cd build/smartos-live || exit 1; \
-		gmake -q images-tar; \
-		if [ $$? -eq 2 ]; then \
-			exit 0; \
-		fi; \
-		cd ../.. || exit 1; \
-		cp $(PLATFORM_BUILD_TAR_IMAGES) $(PLATFORM_BITS_TAR_IMAGES))
-	@echo "# Created platform bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-	@ls -l $(PLATFORM_BITS)
-	@echo ""
-
-TMPDIR := /var/tmp/platform
-
-platform_publish_image: $(PLATFORM_BITS)
-	@echo "# Publish platform image to SDC Updates repo."
-	mkdir -p $(TMPDIR)
-	uuid -v4 > $(TMPDIR)/image_uuid
-	cat platform.imgmanifest.in | sed \
-	    -e "s/UUID/$$(cat $(TMPDIR)/image_uuid)/" \
-	    -e "s/VERSION_STAMP/$(PLATFORM_TRY_BRANCH)-$(TIMESTAMP)/" \
-	    -e "s/BUILDSTAMP/$(PLATFORM_TRY_BRANCH)-$(TIMESTAMP)/" \
-	    -e "s/SIZE/$$(stat --printf="%s" $(PLATFORM_BITS_TAR_PLATFORM))/" \
-	    -e "s/SHA/$$(openssl sha1 $(PLATFORM_BITS_TAR_PLATFORM) \
-	        | cut -d ' ' -f2)/" \
-	    > $(PLATFORM_MANIFEST_BIT)
-	$(UPDATES_IMGADM) import -ddd -m $(PLATFORM_MANIFEST_BIT) \
-	    -f $(PLATFORM_BITS_TAR_PLATFORM)
-
-clean_platform:
-	$(RM) -rf $(BITS_DIR)/platform
-	(cd build/smartos-live && gmake clean)
-
-#---- smartos target
-
-SMARTOS_BITS_DIR=$(BITS_DIR)/smartos
-
-SMARTOS_BITS= \
-	$(SMARTOS_BITS_DIR)/changelog.txt \
-	$(SMARTOS_BITS_DIR)/SINGLE_USER_ROOT_PASSWORD.txt \
-	$(SMARTOS_BITS_DIR)/platform-$(TIMESTAMP).tgz \
-	$(SMARTOS_BITS_DIR)/smartos-$(TIMESTAMP).iso \
-	$(SMARTOS_BITS_DIR)/smartos-$(TIMESTAMP)-USB.img.gz \
-	$(SMARTOS_BITS_DIR)/smartos-$(TIMESTAMP).vmwarevm.tar.gz
-
-.PHONY: smartos
-smartos: platform-smartos $(SMARTOS_BITS)
-
-$(SMARTOS_BITS):
-	@echo "# Build smartos release: branch $(SMARTOS_LIVE_BRANCH), sha $(SMARTOS_LIVE_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-	(cd build/smartos-live \
-		&& ./tools/build_changelog \
-		&& ./tools/build_boot_image -I -r . \
-		&& ./tools/build_boot_image -r . \
-		&& ./tools/build_vmware -r .)
-	mkdir -p $(SMARTOS_BITS_DIR)
-	cp build/smartos-live/output/changelog.txt $(SMARTOS_BITS_DIR)
-	cp build/smartos-live/output/platform-$(TIMESTAMP)/root.password $(SMARTOS_BITS_DIR)/SINGLE_USER_ROOT_PASSWORD.txt
-	cp build/smartos-live/output/platform-$(TIMESTAMP).tgz $(SMARTOS_BITS_DIR)
-	cp build/smartos-live/output-iso/platform-$(TIMESTAMP).iso $(SMARTOS_BITS_DIR)/smartos-$(TIMESTAMP).iso
-	cp build/smartos-live/output-usb/platform-$(TIMESTAMP).usb.gz $(SMARTOS_BITS_DIR)/smartos-$(TIMESTAMP)-USB.img.gz
-	cp build/smartos-live/output-vmware/smartos-$(TIMESTAMP).vmwarevm.tar.gz $(SMARTOS_BITS_DIR)
-	(cd $(SMARTOS_BITS_DIR) && $(CURDIR)/tools/smartos-index $(TIMESTAMP) > index.html)
-	(cd $(SMARTOS_BITS_DIR) && /usr/bin/sum -x md5 * > md5sums.txt)
-
-.PHONY: smartos-release
-smartos-release:
-	TRACE=1 ./tools/smartos-release "$(BRANCH)" "$(TIMESTAMP)"
-
-
-#---- docs target (based on eng.git/tools/mk code for this)
-
-deps/%/.git:
-	git submodule update --init deps/$*
-
-RESTDOWN_EXEC	?= deps/restdown/bin/restdown
-RESTDOWN	?= python $(RESTDOWN_EXEC)
-RESTDOWN_FLAGS	?=
-DOC_FILES	= design.md index.md
-DOC_BUILD	= build/docs/public
-
-$(DOC_BUILD):
-	mkdir -p $@
-
-$(DOC_BUILD)/%.json $(DOC_BUILD)/%.html: docs/%.md | $(DOC_BUILD) $(RESTDOWN_EXEC)
-	$(RESTDOWN) $(RESTDOWN_FLAGS) -m $(DOC_BUILD) $<
-	mv $(<:%.md=%.json) $(DOC_BUILD)
-	mv $(<:%.md=%.html) $(DOC_BUILD)
-
-.PHONY: docs
-docs:							\
-	$(DOC_FILES:%.md=$(DOC_BUILD)/%.html)		\
-	$(DOC_FILES:%.md=$(DOC_BUILD)/%.json)
-
-$(RESTDOWN_EXEC): | deps/restdown/.git
-
-clean_docs:
-	$(RM) -rf build/docs
-
-
-
-#---- misc targets
-
-.PHONY: clean
-clean: clean_docs
-
-.PHONY: clean_null
-clean_null:
-
-# Save the last 'build/' and 'bits/' to an 'old/' dir as a safety so an
-# accidental './configure ...' doesn't blow away local changes in 'build/'.
-.PHONY: distclean
-distclean:
-	$(PFEXEC) $(RM) -rf old/build old/bits
-	if [[ -d build ]]; then $(PFEXEC) mkdir -p old && $(PFEXEC) mv build old/; fi
-	if [[ -d bits ]]; then $(PFEXEC) mkdir -p old && $(PFEXEC) mv bits old/; fi
-
-.PHONY: cacheclean
-cacheclean: distclean
-	$(PFEXEC) $(RM) -rf cache
-
-
-
-# DEPRECATED: Live build steps on jenkins.joyent.us still call this target.
-# TODO: Remove those in Jenkins and then remove this target.
-upload_jenkins:
-	@echo "We no longer upload to bits.joyent.us"
-
-# Upload bits we want to keep for a Jenkins build to manta
-manta_upload_jenkins:
-	@[[ -z "$(MG_TARGET)" ]] \
-		&& echo "error: MG_TARGET isn't set (is this being run under Jenkins?)" \
-		&& exit 1 || true
-	TRACE=1 ./tools/mantaput-bits "$(BRANCH)" "$(TRY_BRANCH)" "$(TIMESTAMP)" $(MG_OUT_PATH)/$(MG_TARGET) $(MG_TARGET) $(UPLOAD_SUBDIRS)
-
-%_upload_manta: %
-	./tools/manta-upload "$*"
-
-%_local_bits_dir: %
-	./tools/local-bitsdir-copy "$*"
-
-# Publish the image for this Jenkins job to https://updates.joyent.com, if
-# appropriate. No-op if the current MG_TARGET doesn't have a "*_publish_image"
-# target.
-jenkins_publish_image:
-	@[[ -z "$(MG_TARGET)" ]] \
-		&& echo "error: MG_TARGET isn't set (is this being run under Jenkins?)" \
-		&& exit 1 || true
-	@[[ -z "$(shell grep '^$(MG_TARGET)_publish_image\>' Makefile || true)" ]] \
-		|| make $(MG_TARGET)_publish_image
diff --git a/README.md b/README.md
index 8d1eb87..8b9483b 100644
--- a/README.md
+++ b/README.md
@@ -5,289 +5,19 @@
 -->
 
 <!--
-    Copyright (c) 2016, Joyent, Inc.
+    Copyright 2019 Joyent, Inc.
 -->
 
 # mountain-gorilla
 
-This repository is part of the Joyent Triton project. See the [contribution
-guidelines](https://github.com/joyent/triton/blob/master/CONTRIBUTING.md) --
-*Triton does not use GitHub PRs* -- and general documentation at the main
-[Triton project](https://github.com/joyent/triton) page.
-
-A single repo to build all the parts of Triton. This is just a *build driver*
-repo, all the components are still in their respective repos.
-See <https://mo.joyent.com/docs/mg> for a more complete introduction.
-
-
-# Quick start
-
-While MG theoretically knows how to "build the world", i.e all of Triton,
-the typical usage is to build one piece at a time. There is a make target
-(or targets) for each Triton component. So, for example, here is how you
-build VMAPI:
-
-    git clone --origin=cr https://cr.joyent.us/joyent/mountain-gorilla.git
-    cd mountain-gorilla
-    ./configure -t vmapi -d Joyent_Dev  # generates bits/config.mk and fetches repo and deps
-    make vmapi                          # builds in build/vmapi, bits in bits/...
-
-If that fails for you, you might be missing prerequisites. See
-<https://mo.joyent.com/docs/mg/master/#prerequisites>.
-
-
-If you'll actually be building, see "Prerequisites" section below first.
-
-
-The "bits/config.mk" contains all config information necessary to fully
-reproduce the build. There will be configure options to use some prebuilt
-bits (e.g. a prebuilt platform) -- so to fully reproduce in this scenario
-those pre-built bits will need to be available. The "configure" step might
-take a while because it needs to get/update all of the source repositories to
-get revision information (the git shas to be built are part of the created
-"config.mk" file)
-
-The end result is a "bits" directory with all the built components. Primarily
-this includes the release bits in "bits/usbheadnode": "coal-$VERSION.tgz",
-"usb-$VERSION.tgz", "boot-$VERSION.tgz" and "upgrade-$VERSION.tgz". However,
-also included are all the constituent built bits: agents, platform, ca, etc.
-
-The above configuration is to build the world from scratch. That
-takes around 2 hours. You can also build just the individual pieces, e.g.
-cloudapi:
-
-    ./configure -t cloudapi
-    make cloudapi
-
-Likewise for any target (`cat targets.json | json --keys`)
-
-To include ancillary closed repositories in a usb-headnode build (which the
-'usb-headnode' and 'usb-headnode-debug' targets will build), you need to pass
-the `-j` option to configure.  Note that this will cause your build to fail if
-you do not have access to the private ancillary repositories, so it should be
-used only when building Joyent products.
-
-
-# Prerequisites
-
-You need several components to build our images in Triton. The easiest way to
-get these is to use the (private repo) [jenkins-agent](https://github.com/joyent/jenkins-agent)
-builds but you can also manually install all the things from those images.
-
-You should now be able to build mountain-gorilla (MG): i.e. all of Triton.
-Let's try that:
-
-    git clone git@github.com:joyent/mountain-gorilla.git
-    cd mountain-gorilla
-    time (./configure && gmake) >build.log 2>&1 &; tail -f build.log
-
-
-# Adding a repository quickstart
-
-Add it as a top-lever property in targets.json, as an object with properties
-"repos" and "deps" minimally, both are arrays.
-
-- "repos" is an array of objects, with the property "url", pointing at a git url
-- "deps" is an array of strings, where the string is another top-level target in targets.json
-  (or optionally a "$branch/$target" to lock it to a branch, though the need for this
-  should be rare).
-
-For example:
-
-    {
-      ...
-      mynewrepo: {
-        "repos": [ {"url": "git://github.com/joyent/mynewrepo.git" } ],
-        "deps": [ "platform" ]
-      },
-      ...
-    }
-
-Then you'll add the target to Makefile. MG's configure will automatically
-populate some Makefile values for you, noteably: xxx_BRANCH , xxx_SHA, but
-you will need to fill in the build stamp yourself. Configure will also git
-checkout your repo in build/
-
-    #---- MYNEWREPO
-
-    _mynewrepo_stamp=$(MYNEWREPO_BRANCH)-$(TIMESTAMP)-g$(MYNEWREPO_SHA)
-    MYNEWREPO_BITS=$(BITS_DIR)/mynewrepo/mynewrepo-pkg-$(_mynewrepo_stamp).tar.bz2
-
-    .PHONY: mynewrepo
-    mynewrepo: $(MYNEWREPO_BITS)
-
-    $(mynewrepo_BITS): build/mynewrepo
-      mkdir -p $(BITS_DIR)
-      (cd build/mynewrepo && TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake pkg release publish)
-      @echo "# Created mynewrepo bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-      @ls -1 $(MYNEWREPO_BITS)
-      @echo ""
-
-    clean_mynewrepo:
-      rm -rf $(BITS_DIR)/mynewrepo
-      (cd build/mynewrepo && gmake clean)
-
-if you wish to build an application zone image, the process is roughly
-similar except you will need to add the "appliance":"true" property, the
-"pkgsrc" property and "dataset_uuid"
-
-    {
-      ...
-      "mynewrepo": {
-        "repos" : [ {"url":"git://github.com/joyent/mynewrepo.git"} ],
-        "appliance": "true",
-        "dataset_uuid": "01b2c898-945f-11e1-a523-af1afbe22822",
-        "pkgsrc": [
-          "sun-jre6-6.0.26",
-          "zookeeper-client-3.4.3",
-          "zookeeper-server-3.4.3"
-        ],
-        deps: []
-      },
-      ...
-    }
-
-where dataset\_uuid is the uuid of the source image you wish to build off
-pkgsrc is an array of strings of package names to install.
-
-Your Makefile target will look as above, with the addition of the xxx\_dataset target:
-
-
-    ...
-    MYNEWREPO_DATASET=$(BITS_DIR)/mynewrepo/mynewrepo-zfs-$(_mynewrepo_stamp).zfs.bz2
-
-    .PHONY: mynewrepo_dataset
-
-    mynewrepo_dataset: $(MYNEWREPO_DATASET)
-
-    $(MYNEWREPO_DATASET): $(MYNEWREPO_BITS)
-            @echo "# Build mynewrepo dataset: branch $(MYNEWREPO_BRANCH), sha $(MYNEWREPO_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-            ./tools/prep_dataset.sh -t $(MYNEWREPO_BITS) -o $(MYNEWREPO_DATASET) -p $(MYNEWREPO_PKGSRC)
-            @echo "# Created mynewrepo dataset (time `date -u +%Y%m%dT%H%M%SZ`):"
-            @ls -1 $(MYNEWREPO_DATASET)
-            @echo ""
-    ...
-
-prep\_dataset.sh is a script that generates images out of tarballs and lists
-of packages.
-
-It takes arguments of the form -t <tarball> where <tarball> is a .tar.gz
-file, containing a directory "root", which is unpacked to / -p "list of
-pkgsrc packages" where list of pkgsrc packages is a list of the pkgsrc
-packages to be installed in the zone.
-
-Configure will populate xxx\_DATASET and xxx\_PKGSRC based on targets.json.
-
-Additionally, you can set the dsadm URN for the target by adding the "urn"
-and "version" properties to targets.json, as properties of the target you
-wish to manipulate. These will show up as urn:version ( sdc:sdc:mynewrepo:0.1
-for instance ). To use them, configure will populate xxx\_URN and xxx\_VERSION
-for you in the Makefile.
-
-Note that these images can only be provisioned with the joyent-minimal brand.
-If one is provisioned with the joyent brand, that zone's networking may not be
-working.  Normally, the networking setup is done through zoneinit, but since
-that script has already run and had its effects undone (as part of the MG
-build), there's no mechanism to automatically bring that zone's VNIC up.  You
-can recover by manually enabling network/physical:default, but you should just
-be provisioning with the joyent-minimal brand instead.  See RELENG-337 for
-details.
-
+This repository was part of the Joyent Triton project but has since been
+retired. It acted as a "build driver", containing metadata to clone and build
+the components of Triton and Manta. It was retired as part of
+[an attempt to improve the build.](https://github.com/joyent/rfd/tree/master/rfd/0145)
 
+The repository now contains only this README.md so that links that exist in
+other repositories will continue to resolve.
 
 # Package Versioning
 
-Thou shalt name thy Triton constituent build bits as follows:
-
-    NAME-BRANCH-TIMESTAMP[-GITDESCRIBE].TGZ
-
-where:
-
-- NAME is the package name, e.g. "smartlogin", "ca-pkg".
-- BRANCH is the git branch, e.g. "master", "release-20110714". Use:
-
-        BRANCH=$(shell git symbolic-ref HEAD | awk -F / '{print $$3}')  # Makefile
-        BRANCH=$(git symbolic-ref HEAD | awk -F / '{print $3}')         # Bash script
-
-- TIMESTAMP is an ISO timestamp like "20110729T063329Z". Use:
-
-        TIMESTAMP=$(shell TZ=UTC date "+%Y%m%dT%H%M%SZ")    # Makefile
-        TIMESTAMP=$(TZ=UTC date "+%Y%m%dT%H%M%SZ")          # Bash script
-
-  Good. A timestamp is helpful (and in this position in the package name)
-  because: (a) it often helps to know approx. when a package was built when
-  debugging; and (b) it ensures that simple lexographical sorting of
-  "NAME-BRANCH-*" packages in a directory (as done by agents-installer and
-  usb-headnode) will make choosing "the latest" possible.
-
-  Bad. A timestamp *sucks* because successive builds in a dev tree will get a
-  new timestamp: defeating Makefile dependency attempts to avoid rebuilding.
-  Note that the TIMESTAMP is only necessary for released/published packages,
-  so for projects that care (e.g. ca), the TIMESTAMP can just be added for
-  release.
-
-- GITDESCRIBE gives the git sha for the repo and whether the repo was dirty
-  (had local changes) when it was built, e.g. "gfa1afe1-dirty", "gbadf00d".
-  Use:
-
-        # Need GNU awk for multi-char arg to "-F".
-        AWK=$((which gawk 2>/dev/null | grep -v "^no ") || which awk)
-        # In Bash:
-        GITDESCRIBE=g$(git describe --all --long --dirty | ${AWK} -F'-g' '{print $NF}')
-        # In a Makefile:
-        GITDESCRIBE=g$(shell git describe --all --long --dirty | $(AWK) -F'-g' '{print $$NF}')
-
-  Notes: "--all" allows this to work on a repo with no tags. "--long"
-  ensures we always get the "sha" part even if on a tag. We strip off the
-  head/tag part because we don't reliably use release tags in all our
-  repos, so the results can be misleading in package names. E.g., this
-  was the smartlogin package for the Lime release:
-
-        smartlogin-release-20110714-20110714T170222Z-20110414-2-g07e9e4f.tgz
-
-  The "20110414" there is an old old tag because tags aren't being added
-  to smart-login.git anymore.
-
-  "GITDESCRIBE" is *optional*. However, the only reason I currently see to
-  exclude it is if the downstream user of the package cannot handle it in
-  the package name. The "--dirty" flag is *optional* (though strongly
-  suggested) to allow repos to deal with possibly intractable issues (e.g. a
-  git submodule that has local changes as part of the build that can't be
-  resolved, at least not resolved quickly).
-
-- TGZ is a catch-all for whatever the package format is. E.g.: ".tgz",
-  ".sh" (shar), ".md5sum", ".tar.bz2".
-
-
-## Exceptions
-
-The agents shar is a subtle exception:
-
-    agents-release-20110714-20110726T230725Z.sh
-
-That "release-20110714" really refers to the branch used to build the
-agent packages included in the shar. For typical release builds, however,
-the "agents-installer.git" repo is always also on a branch of the same
-name so there shouldn't be a mismatch.
-
-
-
-## Suggested Versioning Usage
-
-It is suggested that the Triton repos use something like this at the top of
-their Makefile to handle package naming (using the Joyent Engineering
-Guidelines, eng.git):
-
-    include ./Makefile.defs   # provides "STAMP"
-    ...
-    PKG_NAME=$(NAME)-$(STAMP).tgz
-
-
-Notes:
-- This gives the option of the TIMESTAMP being passed in. This is important
-  to allow an external driver -- e.g., moutain-gorilla, bamboo, CI -- to
-  predict the expected output files, and hence be able to raise errors if
-  expected files are not generated.
-- Consistency here will help avoid confusion, and surprises in things like
-  subtle differences in `awk` on Mac vs. SmartOS, various options to
-  `git describe`.
+See https://github.com/joyent/triton/blob/master/docs/developer-guide/release-engineering.md
diff --git a/build.spec.in b/build.spec.in
deleted file mode 100644
index 7f65edd..0000000
--- a/build.spec.in
+++ /dev/null
@@ -1,48 +0,0 @@
-#!/bin/bash
-
-set -o errexit
-set -o pipefail
-
-function fatal {
-	msg="$1"
-	shift
-
-	printf "ERROR: ${msg}\n" "$@"
-
-	exit 1
-}
-
-function valid_bool {
-	if [[ "${2}" = "true" || "${2}" = "false" ]]; then
-		return 0
-	fi
-
-	fatal 'build.spec.in: $%s expected "true" or "false, got "%s"' \
-	    "${1}" "${2}"
-}
-
-valid_bool USE_DEBUG_PLATFORM "${USE_DEBUG_PLATFORM}"
-valid_bool JOYENT_BUILD "${JOYENT_BUILD}"
-
-cat <<JSON
-{
-  "no-internet": "false",
-
-  "build-tgz": "true",
-  "bits-branch": ".*",
-
-  "features": {
-    "debug-platform": {
-      "enabled": $USE_DEBUG_PLATFORM
-    },
-    "joyent-build": {
-      "enabled": $JOYENT_BUILD
-    }
-  },
-
-  "manta-user": "Joyent_Dev",
-  "manta-key-id": "de:e7:73:9a:b0:91:31:3e:72:8d:9f:62:ca:58:a2:ec",
-
-  "override-all-sources": "bits-dir"
-}
-JSON
diff --git a/configure b/configure
deleted file mode 100755
index 51150db..0000000
--- a/configure
+++ /dev/null
@@ -1,844 +0,0 @@
-#!/bin/bash
-# vi: tabstop=4 expandtab shiftwidth=4
-#
-# This Source Code Form is subject to the terms of the Mozilla Public
-# License, v. 2.0. If a copy of the MPL was not distributed with this
-# file, You can obtain one at http://mozilla.org/MPL/2.0/.
-#
-
-#
-# Copyright (c) 2018, Joyent, Inc.
-#
-
-#
-# Configure the mountain gorilla (aka SDC) build.
-#
-# "targets.json" is a mapping of Makefile target to info about that target.
-# It is generated from targets.json.in.
-
-if [ "$TRACE" != "" ]; then
-    export PS4='[\D{%FT%TZ}] ${BASH_SOURCE}:${LINENO}: ${FUNCNAME[0]:+${FUNCNAME[0]}(): }'
-    set -o xtrace
-fi
-set -o errexit
-set -o pipefail
-
-#---- config, globals
-
-BRANCH=master
-TRY_BRANCH=
-MG_DEP_USER=
-MG_DEP_PATH=
-JOYENT_DEP_PATH=
-MG_OUT_PATH=
-
-ROOT=$(pwd)
-MG_CACHE_DIR=${ROOT}/cache
-
-MGET_BIT_ARGS="-q"
-if [[ -n ${MGET_PROGRESS} ]]; then
-    MGET_BIT_ARGS="--progress"
-fi
-MANTA_MAX_RETRIES=3
-
-export PATH="${ROOT}/node_modules/manta/bin:${PATH}"
-
-if [[ $(uname -s) == "SunOS" ]]; then
-    MTIME='stat -c %Z'
-else
-    MTIME='stat -f %m'
-fi
-
-# GNU cp has '--link'.
-if [[ -x `which gcp` ]]; then
-    CP_LINK="gcp --link"
-else
-    CP_LINK=$( (cp --version 2>/dev/null | grep GNU >/dev/null) && echo "cp --link" || echo "cp" )
-fi
-CURL="curl --fail --connect-timeout 10 -s"
-
-#
-# Git has not always supported the "--recursive" option to "git submodule sync".
-#
-GIT_SUBMODULE_SYNC_FLAGS=''
-if git submodule -h | grep 'git submodule .* sync .*--recursive' \
-    >/dev/null; then
-
-    GIT_SUBMODULE_SYNC_FLAGS='--recursive'
-fi
-
-
-#---- internal support functions
-
-function fatal {
-    echo "$(basename $0): error: $1"
-    exit 1
-}
-
-function errexit {
-    [[ $1 -ne 0 ]] || exit 0
-    fatal "error exit status $1 at line $2"
-}
-
-function ensure_manta_credentials {
-    if [[ -z "$MANTA_KEY_ID" ]]; then
-        export MANTA_KEY_ID=`ssh-keygen -l -f ~/.ssh/id_rsa.pub | awk '{print $2}' | tr -d '\n'`
-    fi
-    export MANTA_URL=https://us-east.manta.joyent.com
-    if [[ -z "$MANTA_USER" ]]; then
-        export MANTA_USER="Joyent_Dev";
-    fi
-
-    #
-    # manta_base_path is used for preloading bits. So if we've been
-    # passed in a -d option, we should use that here.
-    #
-    if [[ -z "$MG_DEP_USER" ]]; then
-        MG_DEP_USER="$MANTA_USER"
-    fi
-    export manta_base_path="/$MG_DEP_USER${MG_DEP_PATH}"
-}
-
-# Blow away the bits cache once per-day because don't want it to grow
-# unbounded in size (esp. for the continuous-build system). An alternative
-# would be to put a size limit on it.
-function flush_bits_cache() {
-    if [[ -f $MG_CACHE_DIR/bits/created ]]; then
-        local bits_cache_age=$((`date "+%s"` - `$MTIME $MG_CACHE_DIR/bits/created`))
-        # One day in 86400. We use a bit more to avoid harmonics with
-        # once-per-day builds.
-        if [[ $bits_cache_age -gt 100000 ]]; then
-            echo "# Bit cache '$MG_CACHE_DIR/bits' is more than a day old. Blowing it away to be recreated."
-            rm -rf $MG_CACHE_DIR/bits
-        fi
-    fi
-}
-
-# areload $MG_CACHE_DIR/bits/ with the latest built and uploaded bits for
-# the given target and branch via HTTP(S).
-#
-# Usage:
-#   preload_bits_from_http TARGET BRANCH TRY_BRANCH
-# where:
-#   TARGET is a URL dir that serves Apache/Nginx-index-style directories.
-#
-# Example:
-#   preload_bits_from_http http://download.joyent.com/pub/build/sdcnode master ""
-#
-function preload_bits_from_http() {
-    local target=$1
-    local branch=$2
-    local try_branch=$3
-    local target_base=$(basename $target)
-
-    if [[ "$target_base" == "sdcnode" ]]; then
-        echo "Skip preload of sdcnode (see RELENG-404)."
-        return
-    fi
-
-    echo ""
-    echo "# preload 'bits/$target_base'"
-
-    local target_url
-    [[ $(echo $target | cut -c 1-4) == "http" ]] || fatal "preload_bits_from_http called w/o http URL"
-
-    target_url=$target
-    if [[ "${target_url:(-1)}" != "/" ]]; then
-        target_url=$target_url/
-    fi
-    local_cache_reldir=${target_url#*://}
-    local_cache_reldir=${local_cache_reldir#*@}
-
-    # Branch dir: try $try_branch, then $branch.
-    local best_branch=$try_branch
-    local latest_url=$target_url$try_branch-latest/
-    if [[ -z "$best_branch" ||
-          $($CURL -kI $latest_url | head -1 | awk '{print $2}') != "200" ]]; then
-        best_branch=$branch
-        latest_url=$target_url$branch-latest/
-        if [[ $($CURL -kIS $latest_url | head -1 | awk '{print $2}') != "200" ]]; then
-            fatal "'$target_url/$branch-latest/' does not exist"
-        fi
-    fi
-    echo "# use branch '$best_branch'"
-
-    # Find the latest build time for this branch.
-    local latest_build=$($CURL -Sk $target_url \
-        | grep "href=\"" \
-        | cut -d'"' -f2 \
-        | grep "^$best_branch-" \
-        | grep -v -- '-latest/$' \
-        | sort \
-        | tail -1)
-    local local_cache_dir=$MG_CACHE_DIR/bits/$local_cache_reldir$latest_build$target_base
-
-    if [[ -d $local_cache_dir ]]; then
-        echo "# local cache at '$local_cache_dir' already exists"
-    else
-        # Mark creation for bits cache flush every day.
-        mkdir -p $MG_CACHE_DIR/bits
-        if [[ ! -f $MG_CACHE_DIR/bits/created ]]; then
-            touch $MG_CACHE_DIR/bits/created
-        fi
-
-        latest_url=$target_url$latest_build
-
-        # Update cache in $MG_CACHE_DIR/bits
-        # `-l 2`: Two levels deep needed for 2-level depth of 'agents' bits area.
-        echo "# download from $latest_url"
-        (cd $MG_CACHE_DIR/bits \
-            && wget -q -np -l 2 --no-check-certificate -r -L -R 'index.html,*.log' \
-                $latest_url$target_base/)
-    fi
-    local md5sums_path=$local_cache_dir/../md5sums.txt
-    if [[ ! -f $md5sums_path ]]; then
-        $CURL -Sk ${latest_url}md5sums.txt -o $md5sums_path
-    fi
-
-    # MD5 check of downloaded bits.
-    for bit in $(cd $local_cache_dir/../ && find . -type f | grep -v md5sums.txt); do
-        local correct_md5=$(grep $bit $md5sums_path | cut -d ' ' -f1)
-        local actual_md5=$(openssl dgst -md5 < $local_cache_dir/../$bit | awk '{print $NF}')
-        if [[ $correct_md5 != $actual_md5 ]]; then
-            rm $local_cache_dir/../$bit
-            fatal "md5 check failure on $local_cache_dir/../$bit (actual '$actual_md5', from md5sums '$correct_md5')"
-        fi
-    done
-
-    # Copy over to bits dir.
-    mkdir -p bits
-    [[ -d "bits/$target_base" ]] && fatal "'bits/$target_base' already exists"
-    $CP_LINK -PR $local_cache_dir bits/$target_base
-}
-
-#
-# Preload $MG_CACHE_DIR/bits/ with the latest built and uploaded bits for
-# the given target and branch.
-#
-# Usage:
-#   preload_bits_from_manta TARGET BRANCH TRY_BRANCH
-#
-# Example:
-#   preload_bits_from_manta smartlogin master ""
-#   preload_bits_from_manta release-20110901-upgrade/agents-upgrade master ""
-#
-function preload_bits_from_manta() {
-    local target=$1
-    local branch=$2
-    local try_branch=$3
-    local target_base=$(basename $target)
-    local start_time=$(date +%s)
-
-    # Don't bork if this was already preloaded by another target.
-    if [[ -d bits/$target_base ]]; then
-        return;
-    fi
-
-    # If bit is in the LOCAL_BITS_CACHE, use that
-    if [[ -n ${LOCAL_BITS_DIR} && -d ${LOCAL_BITS_DIR}/${target_base} ]]; then
-        mkdir -p bits
-        cp -PR ${LOCAL_BITS_DIR}/${target_base} bits/${target_base}
-        return;
-    fi
-
-    # This is a hack here for the case where we've got something like:
-    #
-    # https://download.joyent.com/pub/build/sdcnode
-    #
-    # in the 'deps'. We'll then use HTTP to download that bit.
-    if [[ $(echo $target | cut -c 1-4) == "http" ]]; then
-        preload_bits_from_http $@
-        return 0
-    fi
-
-    echo ""
-    echo "# preload 'bits/$target_base from Manta'"
-
-    ensure_manta_credentials
-
-    # You can specify "target" as branch/target, in which case we will use
-    # the branch from there. If target does not have '/', dirname returns '.'
-    local target_branch=$(dirname $target)
-    if [[ "$target_branch" != "." ]]; then
-        branch=$target_branch
-    fi
-
-    # Remember: target_base is something like 'agents-upgrade'
-    local target_mpath=${manta_base_path}/${target_base}
-
-    # Allow to set JOYENT_DEP_PATH=/stor/builds for joyent bits
-    if [[ ${target_base} == "firmware-tools" && -n ${JOYENT_DEP_PATH} ]]; then
-        target_mpath="/${MG_DEP_USER}${JOYENT_DEP_PATH}/${target_base}"
-    fi
-
-    # Branch dir: try $try_branch, then $branch.
-    local best_branch=$try_branch
-    if [[ -z $best_branch ]]; then
-        best_branch=$branch
-    fi
-
-    local builds_subdir="public"
-    local consider_prev_releases=0
-    local delay=0
-    local dirnames=
-    #
-    # Each build dir has a "${branch}-latest" file, e.g.
-    # "/Joyent_Dev/stor/builds/amon/master-latest" that contains the name
-    # of the actual latest path. Something similar but with a "-<buildstamp>"
-    # instead of "-latest" and we'll grab that into latest_dir.
-    #
-    local latest_dir=
-    local previous_releases=
-    local success=
-
-    if [[ $(cat ${ROOT}/targets.json | $JSON ${target_base}.public) == 'false' \
-        ]]; then
-        builds_subdir="stor"
-    fi
-
-    dirnames[${#dirnames[@]}]="${best_branch}-latest"
-    if [[ "$branch" != "$best_branch" ]]; then
-        dirnames[${#dirnames[@]}]="${branch}-latest"
-    fi
-
-    consider_prev_releases=$(echo ,"${CONSIDER_PREV_RELEASES_FOR}", | \
-        grep ",$target_base,") || true
-    if [[ "$consider_prev_releases" != "" ]]; then
-        dirnames[${#dirnames[@]}]=$(mls /Joyent_Dev/${builds_subdir}/builds/${target_base} | \
-            grep -E 'release-[[:digit:]]{8}-latest' | sort | tail -1)
-    fi;
-
-    set +o errexit  # want to do our own error-handling here
-    for dir_name in ${dirnames[@]}; do
-        local count=0
-        while [[ ${count} -lt ${MANTA_MAX_RETRIES} && -z ${success} ]]; do
-
-            if [[ ${count} -gt 0 ]]; then
-                cat /tmp/mget-output.$$
-                delay=$((${count} * 20))
-                if [[ $delay -gt 60 ]]; then
-                    delay=60
-                fi
-                echo "Could not get latest dir for target ${target}, retrying in ${delay}s"
-                sleep ${delay}
-            fi
-
-            latest_dir=$(mget -v -q ${target_mpath}/${dir_name} 2> /tmp/mget-output.$$)
-
-            if [[ $? == 0 ]]; then
-                success=1
-                break;
-            fi
-
-            count=$((${count} + 1))
-        done
-
-        if [[ $success -eq 1 ]]; then
-            break;
-        fi
-    done
-    set -o errexit  # back to errexit
-
-    if [[ -z ${latest_dir} ]]; then
-        cat /tmp/mget-output.$$
-        fatal "Could not get latest dir for target $target"
-    fi
-
-    # local_cache_dir will be something like:
-    # /root/MG/cache/bits/agents-upgrade/master-20131213T003614Z
-    local local_cache_dir=${MG_CACHE_DIR}/bits/${latest_dir#$manta_base_path/}
-    mkdir -p ${local_cache_dir}
-
-    # Mark creation for bits cache flush every day.
-    if [[ ! -f $MG_CACHE_DIR/bits/created ]]; then
-        touch $MG_CACHE_DIR/bits/created
-    fi
-
-    local cache_dirname=
-    local cache_filename=
-    local dir=
-    local files=$(mfind -v -t o ${latest_dir})
-    local found=
-    local local_file_md5=
-    local manta_file_md5=
-    local retries=
-
-    if [[ -z ${files} ]]; then
-        fatal "failed to get files from ${latest_dir}"
-    fi
-
-    # loop through all the objects in the latest_dir
-    for file in $files; do
-        dir=$(dirname $file)
-        cache_dirname=$(echo "${local_cache_dir}/${dir#${latest_dir}}" | sed -e "s/\/$//g" | sed -e "s|//|/|g")
-        cache_filename=$(basename ${file})
-        manta_file_md5=$(mmd5 -v ${file} | cut -d' ' -f1)
-
-        # Ensure directory exists
-        mkdir -p ${cache_dirname}
-
-        retries=0
-        while [[ ${retries} -lt ${MANTA_MAX_RETRIES} ]]; do
-            if [[ ! -f ${cache_dirname}/${cache_filename} ]]; then
-                mget -v ${MGET_BIT_ARGS} -o ${cache_dirname}/${cache_filename} ${file}
-            fi
-
-            local_file_md5=$(openssl dgst -md5 ${cache_dirname}/${cache_filename} | awk '{print $NF}')
-            if [[ ${local_file_md5} == ${manta_file_md5} ]]; then
-                echo "# ${cache_dirname}/${cache_filename} has correct MD5"
-                break;
-            else
-                echo "# ${cache_dirname}/${cache_filename} has incorrect MD5, deleting"
-                rm -f ${cache_dirname}/${cache_filename}
-            fi
-            retries=$((${retries} + 1))
-        done
-
-        [[ -f ${cache_dirname}/${cache_filename} ]] \
-            || fatal "failed to preload ${cache_dirname}/${cache_filename}"
-    done
-
-    # Copy over to bits dir.
-    mkdir -p bits
-    [[ -d "bits/${target_base}" ]] && fatal "'bits/${target_base}' already exists"
-    $CP_LINK -PR ${local_cache_dir}/${target_base} bits/${target_base}
-
-    local end_time=$(date +%s)
-    echo "$((${end_time} - ${start_time})) seconds to preload ${target_base}"
-}
-
-# Clone/update the given git repo in the repo cache.
-# Usage:
-#   get_repo_cache REPO-URL REPO-DIR [GERRIT-CR]
-# Example:
-#   get_repo_cache git@github.com:joyent/smart-login.git \
-#     $MG_CACHE_DIR/repos/smart-login 4095/2
-function get_repo_cache() {
-    local repo_url=$1
-    local cache_dir=$2
-    local gerrit_cr=$3
-
-    if [[ ! -d "$cache_dir" ]]; then
-        local parent_dir=$(dirname $cache_dir)
-        local tmp_dir=$parent_dir/.tmp.$repo_dir
-        mkdir -p $parent_dir
-        rm -rf $tmp_dir
-        git clone $repo_url $tmp_dir
-        mv $tmp_dir $cache_dir
-    else
-        (cd $cache_dir; git pull)
-    fi
-
-    if [[ -n "$gerrit_cr" ]]; then
-        (cd $cache_dir && git fetch origin \
-        +refs/heads/*:refs/remotes/origin/* +refs/changes/*:refs/remotes/origin/changes/*)
-    fi
-
-    # Error out if the cached repo is dirty. These things should always
-    # be pristine.
-    if [[ "$(cd $cache_dir && git describe --all --dirty | grep dirty)" != "" ]]; then
-        fatal "Repo cache $cache_dir is dirty!"
-    fi
-}
-
-
-# Get the requested repo.
-# Usage:
-#   get_repo2 REPO-URL BRANCH SUBMODULE-UPDATE [NAME] [TRY-BRANCH] [GERRIT-CR]
-#
-# If "SUBMODULE-UPDATE" is "true", then "git submodule update ..." is
-# used on the repo.
-#
-# If "NAME" is given, that subdir under "build/" will be used as the
-# clone dir. Else it is inferred from the REPO-URL.
-#
-# Only one of TRY-BRANCH or GERRIT_CR should be set.
-#
-function get_repo2() {
-    local repo_url=$1
-    local branch=$2
-    local submodule_update=$3
-    local repo_dir=$4
-    local try_branch=$5
-    local gerrit_cr=$6
-
-    if [[ -z "$repo_dir" ]]; then
-        repo_dir=${repo_url##*/}    # strip to last '/'
-        repo_dir=${repo_dir##*:}    # strip to last ':'
-        repo_dir=${repo_dir%*.git}    # strip '.git' at tail
-    fi
-
-    local cache_dir=$MG_CACHE_DIR/repos/$repo_dir
-    if [[ -n "$gerrit_cr" ]]; then
-        cache_dir=$MG_CACHE_DIR/repos.cr/$repo_dir
-    fi
-
-    echo "# get '$repo_url' to repo cache dir '$cache_dir'"
-    get_repo_cache ${repo_url} ${cache_dir} ${gerrit_cr}
-
-    echo "# copy '$cache_dir' to 'build/$repo_dir'"
-    mkdir -p build
-    cp -PR $cache_dir build/$repo_dir
-    (cd build/$repo_dir ; git checkout $branch)
-
-    if [[ -n "$try_branch" ]]; then
-        (cd build/$repo_dir ; git checkout $try_branch && git pull || true)
-    fi
-
-    if [[ -n "$gerrit_cr" ]]; then
-        # 4920/2 -> remotes/origin/changes/20/4920/2
-        ref=$(echo $gerrit_cr | \
-            sed 's+\(.*\)\(..\)/+remotes/origin/changes/\2/\1\2/+')
-        (cd build/$repo_dir ; git checkout $ref || true)
-    fi
-
-    if [[ "$submodule_update" == "true" ]]; then
-        (cd "build/$repo_dir" &&
-            git submodule sync $GIT_SUBMODULE_SYNC_FLAGS &&
-            git submodule update --init --recursive)
-    fi
-}
-
-function gen_config() {
-    local output
-    mkdir -p bits
-    local bra_name
-    # Determine UPDATES_CHANNEL based into branch name:
-    if [[ ! -z "$TRY_BRANCH" ]]; then
-        bra_name=${TRY_BRANCH}
-    else
-        bra_name=${BRANCH}
-    fi
-    if [[ -z "$TRY_BRANCH" && "$(echo ${BRANCH} | grep '^release-[0-9]\{8\}$' || true)" ]]; then
-        UPDATES_CHANNEL=staging
-    else
-        if [[ "${bra_name}" == "master" ]]; then
-            UPDATES_CHANNEL=dev
-        else
-            UPDATES_CHANNEL=experimental
-        fi
-    fi
-    cat <<EOF >bits/config.mk
-TIMESTAMP=$(TZ=UTC date "+%Y%m%dT%H%M%SZ")
-BRANCH=$BRANCH
-TRY_BRANCH=$TRY_BRANCH
-MG_GERRIT_CR=$MG_GERRIT_CR
-MG_NODE=$MG_NODE
-MG_CACHE_DIR=$(cd $MG_CACHE_DIR >/dev/null; pwd)
-JOYENT_BUILD=$JOYENT_BUILD
-MG_OUT_PATH=${MG_OUT_PATH}
-UPDATES_CHANNEL=${UPDATES_CHANNEL}
-EOF
-  mkdir -p build
-  for repo in $(ls -1 build/); do
-      repo_mk_name=$(echo $repo | tr [:lower:] [:upper:] | tr - _)
-      branch_name=$((cd build/${repo} && git symbolic-ref HEAD 2> /dev/null ) || echo "")
-      echo ${repo_mk_name}_BRANCH=$(echo ${branch_name##refs/heads/} || echo "") >> ${ROOT}/bits/config.mk
-      echo ${repo_mk_name}_SHA=$((cd build/${repo} && git log --pretty=format:'%h' -1 ) || echo "") >> ${ROOT}/bits/config.mk
-  done
-
-  # Makefile vars for appliance builds (i.e. the "foo_image" targets).
-  for targ in $TARGETS; do
-      targ_mk_name=$(echo $targ | tr [:lower:] [:upper:] | tr - _)
-      if [[ $(cat ${ROOT}/targets.json | $JSON ${targ} | $JSON appliance) == 'true' ]]; then
-        echo ${targ_mk_name}_IMAGE_UUID=$(cat ${ROOT}/targets.json | $JSON ${targ}.image_uuid) >> ${ROOT}/bits/config.mk
-        echo ${targ_mk_name}_IMAGE_NAME=\"$(cat ${ROOT}/targets.json | $JSON ${targ}.image_name)\" >> ${ROOT}/bits/config.mk
-        echo ${targ_mk_name}_IMAGE_DESCRIPTION=\"$(cat ${ROOT}/targets.json | $JSON ${targ}.image_description)\" >> ${ROOT}/bits/config.mk
-        echo ${targ_mk_name}_PKGSRC=\"$(cat ${ROOT}/targets.json | $JSON ${targ}.pkgsrc | $JSON -a |  xargs)\" >> ${ROOT}/bits/config.mk
-        local num_tarballs=$(cat ${ROOT}/targets.json | $JSON ${targ}.tarballs.length)
-        local tarballs=""
-        local index=0
-        while [[ ${index} -lt ${num_tarballs} ]]; do
-            local tb_name=$(cat ${ROOT}/targets.json | $JSON ${targ}.tarballs.${index}.name)
-            local tb_tarball=$(cat ${ROOT}/targets.json | $JSON ${targ}.tarballs.${index}.tarball)
-            [[ -z "$tb_tarball" ]] && fatal "no '$targ.tarballs.$index.tarball' in targets.json"
-            tb_tarball=$ROOT/bits/$tb_tarball
-            local tb_sysroot=$(cat ${ROOT}/targets.json | $JSON ${targ}.tarballs.${index}.sysroot)
-            [[ -z "$tb_sysroot" ]] && fatal "no '$targ.tarballs.$index.sysroot' in targets.json"
-            tarballs="$tarballs $tb_tarball:$tb_sysroot"
-            index=$((${index} + 1))
-        done
-        echo ${targ_mk_name}_EXTRA_TARBALLS=\"${tarballs}\" >> ${ROOT}/bits/config.mk
-      fi
-  done
-}
-
-
-function print_help() {
-    echo "Configure this SDC build. This involves cloning/pulling the "
-    echo "component source repositories."
-    echo ""
-    echo "Usage:"
-    echo "  ./configure [OPTIONS]"
-    echo ""
-    echo "Options:"
-    echo "  -B TRY-BRANCH"
-    echo "               Branch to try to checkout (if it exists). '-b' value"
-    echo "               is used as the default. This is useful for building"
-    echo "               a feature branch on one of the many repos used for"
-    echo "               a target."
-    echo "  -D PATH      Alternate Manta path for dependencies. The default is"
-    echo "               '/stor/builds'."
-    echo "  -J PATH      Alternate manta path for joyent-only dependencies. The"
-    echo "               default is to use the -D path."
-    echo "  -O PATH      Alternate Manta path for 'make <target>_upload_manta'."
-    echo "               The default is '\${MANTA_USER}/stor/builds'."
-    echo "  -P           Ignore the 'build_platform' check that is meant to ensure"
-    echo "               that a component is built on the exact platform version"
-    echo "               specified for it in 'targets.json' for release."
-    echo "  -R [COMPONENT_NAME1,COMPONENT_NAME2,...]"
-    echo "               Consider using previous releases if latest release is not"
-    echo "               available."
-    echo "  -b BRANCH    Branch to checkout. Defaults to 'master'."
-    echo "               Note that this is for *all* core repositories."
-    echo "  -c CACHE-DIR Specify a cache directory. Default: './cache'."
-    echo "               The cache is used for caching git clones, preloaded"
-    echo "               bits and npm downloads. The 'bits cache' is"
-    echo "               automatically removed if it is more than a day old."
-    echo "  -d USER      Alternate Manta user to pull dependencies from. eg. if"
-    echo "               building as yourself, -d Joyent_Dev will pull deps "
-    echo "               from normal builds."
-    echo "  -g [gerrit]  build using the given Gerrit CR changeset"
-    echo "  -h, --help   Print this help and exit."
-    echo "  -j           Configure this workspace to build Joyent products."
-    echo "  -r           Regenerate config.mk. This doesn't touch repos in"
-    echo "               'build/' or preload in 'bits/'."
-    echo "  -t TARGET    Prepare to build only this target. Valid targets are"
-    echo "               the keys of 'targets.json'."
-    exit 0
-}
-
-
-function get_target_repos() {
-    local target=$1
-    local info
-    for info in `cat targets.json | $JSON $target.repos | $JSON -a -d, url dir submodule-update`; do
-        local repo_url=$(echo "$info" | cut -d, -f 1)
-        local repo_dir=$(echo "$info" | cut -d, -f 2)
-        local submodule_update=$(echo "$info" | cut -d, -f 3)
-        [[ -z "$submodule_update" ]] && submodule_update=true
-        get_repo2 $repo_url $BRANCH "$submodule_update" \
-            "$repo_dir" "$TRY_BRANCH" "$MG_GERRIT_CR"
-    done
-}
-
-# Ensure that we are building on the specified build_platform in targets.json.
-function ensure_build_platform() {
-    local target=$1
-    local build_platform
-    local errmsg
-    build_platform=$(cat targets.json | $JSON $target.build_platform)
-    if [[ -n "$build_platform" ]]; then
-        curr_platform=$(uname -v | cut -d_ -f2)
-        if [[ "$curr_platform" != "$build_platform" ]]; then
-            errmsg="current platform, $curr_platform, does not match the expected '$target.build_platform', $build_platform"
-            if [[ $IGNORE_BUILD_PLATFORM_CHECK == "yes" ]]; then
-                echo "warning: $errmsg (ignoring, per '-P' option)"
-            else
-                fatal "$errmsg"
-            fi
-        fi
-    fi
-}
-
-function get_pkgsrc() {
-  if [[ -d ${ROOT}/build/usb-headnode ]]; then
-    echo "# get pkgsrc packages for build/usb-headnode zones (to build/pkgsrc)"
-
-    mkdir -p build/pkgsrc/
-
-    shopt -s extglob
-
-    for dataset in $(find ${ROOT}/build/usb-headnode/zones -name "dataset"); do
-      local ds=$(cat ${dataset})
-      local pkgsrc_url=$((${JSON} datasets | ${JSON} -a name pkgsrc_url \
-          | grep "$(basename ${ds%\.dsmanifest})" \
-          | cut -d ' ' -f2) < ${ROOT}/build/usb-headnode/build.spec)
-      if [[ ${pkgsrc_url:(-1)} != "/" ]]; then
-        pkgsrc_url=$pkgsrc_url/
-      fi
-      local pkgsrc_ver=$(echo "${pkgsrc_url}" | cut -d '/' -f5)
-      local gccver=$(echo "${pkgsrc_url}" | cut -d '/' -f6)
-
-      local cache_dir=$MG_CACHE_DIR/pkgsrc/$pkgsrc_ver/$gccver/All
-      local dest_dir=build/pkgsrc/${pkgsrc_ver}/${gccver}/All
-      mkdir -p $cache_dir
-      mkdir -p $dest_dir
-
-      if [[ ! -f $dest_dir/../SHA512.bz2 ]]; then
-        echo "# get ${pkgsrc_url}/../SHA512.bz2"
-        if ${CURL} -S -o $cache_dir/../SHA512.bz2 ${pkgsrc_url}/../SHA512.bz2; then
-          $CP_LINK $cache_dir/../SHA512.bz2 $dest_dir/../SHA512.bz2
-        elif [[ ! -f $dest_dir/md5sums.txt ]]; then
-          echo "# get ${pkgsrc_url}md5sums.txt"
-          ${CURL} -S -o $cache_dir/md5sums.txt ${pkgsrc_url}md5sums.txt
-          $CP_LINK $cache_dir/md5sums.txt $dest_dir/md5sums.txt
-        fi
-      fi
-
-      for file in $(cat $(dirname ${dataset})/pkgsrc); do
-        if [[ -f $dest_dir/$file.tgz ]]; then
-          true # pass through
-        else
-          if [[ ! -f $cache_dir/${file}.tgz ]]; then
-            echo "# get ${pkgsrc_url}${file}.tgz"
-            ${CURL} -S -o $cache_dir/$file.tgz ${pkgsrc_url}/${file}.tgz
-
-            if [[ -f $cache_dir/../SHA512.bz2 ]]; then
-              local correct_sha512=$(bzcat ${cache_dir}/../SHA512.bz2 | grep "/${file}.tgz)" | cut -d' ' -f4)
-              local actual_sha512=$(openssl dgst -sha512 < $cache_dir/$file.tgz | awk '{print $NF}')
-              if [[ $correct_sha512 != $actual_sha512 ]]; then
-                rm $cache_dir/$file.tgz
-                fatal "SHA512 check failure on $cache_dir/$file.tgz (actual $actual_sha512, from SHA512.bz2 $correct_sha512)"
-              fi
-            elif [[ -f $cache_dir/md5sums.txt ]]; then
-              local correct_md5=$(grep $file $cache_dir/md5sums.txt | cut -d ' ' -f1)
-              local actual_md5=$(openssl dgst -md5 < $cache_dir/$file.tgz | awk '{print $NF}')
-              if [[ $correct_md5 != $actual_md5 ]]; then
-                rm $cache_dir/$file.tgz
-                fatal "md5 check failure on $cache_dir/$file.tgz (actual $actual_md5, from md5sums $correct_md5)"
-              fi
-            fi
-          else
-            echo "# have $cache_dir/$file.tgz"
-          fi
-          $CP_LINK $cache_dir/$file.tgz $dest_dir/
-        fi
-      done
-    done
-
-  fi
-}
-
-
-
-
-#---- mainline
-
-# Can be a target name to tell 'configure' to (a) limit prep to just that
-# target and (b) pre-load "bits/" with pre-built dependent target bits.
-# If empty it means that we are configuring for a full build.
-TARGET=
-REGENERATE='false'
-MG_NODE=$(which node) || fatal "node binary not found on PATH"
-IGNORE_BUILD_PLATFORM_CHECK=no
-
-trap 'errexit $? $LINENO' EXIT
-
-if [[ "$1" == "--help" ]]; then
-  print_help
-fi
-while getopts "B:D:J:O:PR:b:c:d:g:hjrt:" opt; do
-    case "$opt" in
-        B) TRY_BRANCH=$OPTARG ;;
-        D) MG_DEP_PATH=${OPTARG} ;;
-        J) JOYENT_DEP_PATH=${OPTARG} ;;
-        O) MG_OUT_PATH=${OPTARG} ;;
-        P) IGNORE_BUILD_PLATFORM_CHECK=yes ;;
-        R) CONSIDER_PREV_RELEASES_FOR=${OPTARG} ;;
-        b) BRANCH=$OPTARG ;;
-        c) MG_CACHE_DIR=${OPTARG} ;;
-        d) MG_DEP_USER=${OPTARG} ;;
-        g) MG_GERRIT_CR=${OPTARG} ;;
-        h) print_help ;;
-        j) JOYENT_BUILD=true ;;
-        r) REGENERATE='true' ;;
-        t) TARGET=${OPTARG} ;;
-        ?) fatal "unknown option: $opt" ;;
-    esac
-done
-shift $((OPTIND-1))
-
-[[ -n "$MG_GERRIT_CR" ]] && {
-    [[ -n "$TRY_BRANCH" ]] && {
-        echo "Can't use -g with -B" >&2
-        exit 1
-    }
-
-    export MG_GERRIT_CR
-    export MG_GIT_PREFIX=http://cr.joyent.us/joyent
-}
-
-[[ -z ${MG_DEP_PATH} ]] && MG_DEP_PATH=/public/builds
-[[ -z $JOYENT_BUILD ]] && JOYENT_BUILD=false
-export JOYENT_BUILD
-rm -f targets.json
-bash < targets.json.in | json > targets.json
-
-# Pre-condition: must have node >=0.10 first on path (see RELENG-266).
-echo "Ensure have a 'node' >= v0.10."
-NODE_MAJOR_VERSION=$($MG_NODE --version | cut -c2- | awk 'BEGIN{ FS="." } { print $1 }')
-NODE_MINOR_VERSION=$($MG_NODE --version | awk 'BEGIN{ FS="." } { print $2 }')
-if [[ $NODE_MAJOR_VERSION -lt 1 && $NODE_MINOR_VERSION -lt 10 ]]; then
-    fatal "Incorrect node version, '${NODE_MAJOR_VERSION}.${NODE_MINOR_VERSION}'. MG needs a node v0.10 or later on your PATH."
-fi
-JSON="$MG_NODE $ROOT/tools/json"
-
-# make sure we clean out old versions before installing the new one
-for nodething in manta smartdc; do
-    if [[ -f node_modules/${nodething}/package.json
-         && $(json version < node_modules/${nodething}/package.json) \
-         != $(json dependencies.${nodething} < package.json) ]]; then
-
-        rm -rf node_modules/${nodething}
-    fi
-done
-
-# Make sure npm stuff is installed
-npm install
-
-# '-r' regenerate early out.
-if [[ "$REGENERATE" == 'true' ]]; then
-    gen_config
-    exit 0
-fi
-
-
-# Else we are doing a full configure for a fresh build. Start fresh:
-mkdir -p bits
-touch bits/config.mk
-make distclean
-
-if [[ ! -z "$TARGET" ]]; then
-    flush_bits_cache
-    # Validate target.
-    TARGETS=$TARGET
-    if [[ -z "$(cat targets.json | $JSON $TARGET)" ]]; then
-        fatal "Unknown target: $TARGET"
-    fi
-    ensure_build_platform $TARGET
-    get_target_repos $TARGET
-    for dep in `cat targets.json | $JSON $TARGET.deps | $JSON -a`; do
-        if [[ -n "$(echo $dep | egrep '^[^/]+/[^/]+$' || true)" ]]; then
-            # Using the 'BRANCH/NAME' form to lock to a branch.
-            dep_branch=$(echo $dep | cut -d/ -f1)
-            dep=$(echo $dep | cut -d/ -f2)
-            preload_bits_from_manta ${dep} "${dep_branch}"
-        else
-            preload_bits_from_manta ${dep} "$BRANCH" "$TRY_BRANCH"
-        fi
-    done
-else
-    TARGETS=$(${MG_NODE} -e 'fs=require("fs"); c=fs.readFileSync("targets.json"); console.log(Object.keys(JSON.parse(c)).join("\n"))')
-    for targ in $TARGETS; do
-        ensure_build_platform $targ
-        get_target_repos $targ
-    done
-fi
-
-# "all" is a special target with deps to always preload.
-# I.e. for which "make all" doesn't build it.
-#for dep in `cat targets.json | $JSON all.deps | $JSON -a`; do
-    #if [[ -n "$(echo $dep | egrep '^[^/]+/[^/]+$' || true)" ]]; then
-        ## Using the 'BRANCH/NAME' form to lock to a branch.
-        #dep_branch=$(echo $dep | cut -d/ -f1)
-        #dep=$(echo $dep | cut -d/ -f2)
-        #preload_bits_from_manta ${dep} "${dep_branch}"
-    #else
-        #preload_bits_from_manta ${dep} "$BRANCH" "$TRY_BRANCH"
-    #fi
-#done
-
-#get_pkgsrc
-
-gen_config
diff --git a/deps/restdown b/deps/restdown
deleted file mode 160000
index 4aa9e59..0000000
--- a/deps/restdown
+++ /dev/null
@@ -1 +0,0 @@
-Subproject commit 4aa9e59b9c4bc66a13d6c79e8e79ca40b46dbdf1
diff --git a/docs/design.md b/docs/design.md
deleted file mode 100644
index 658324c..0000000
--- a/docs/design.md
+++ /dev/null
@@ -1,115 +0,0 @@
----
-title: MG Design Discussions
-markdown2extras: tables, cuddled-lists
-apisections:
----
-<!--
-    This Source Code Form is subject to the terms of the Mozilla Public
-    License, v. 2.0. If a copy of the MPL was not distributed with this
-    file, You can obtain one at http://mozilla.org/MPL/2.0/.
--->
-
-<!--
-    Copyright (c) 2014, Joyent, Inc.
--->
-
-# MG Design Discussions
-
-**This is a Joyent internal document.** Here-in some design discussions for
-MG, collected here because they might be helpful to understand why MG
-is the way it is. Each section is dated to give context if reading this
-in the future when Amon design might have moved on. *Add new sections to
-the top.*
-
-
-# build and deploy with same dataset (19-Apr-2012)
-
-Here is the problem: The MG build machine, e.g. the "master" Jenkins build
-slave is a zone based on dataset smartos-1.3.18 (that's a guess, I'm not
-sure which dataset it is). The "moray" component built on this build slave
-is *deployed* to a zone based on dataset smartos-1.6.0. These have different
-gcc's and different libstdc++. This has shown to result in, at least,
-node 0.7.x builds blowing up.
-
-Basically, given g++ compat for different versions being questionable and
-v8 being heavy C++, a rule: **Thou shalt build on the same dataset as to
-which you deploy.**
-
-Note that while the rule talks about "dataset", likely the differentiator is
-which *pkgsrc* version is being used. For the time being we'll be a little
-more paranoid/convenient and talk in terms of scoping to the same dataset.
-Also note that we aren't hitting crash problems currently with other software,
-or with node **0.6.x** builds, so the hard commandment above could be
-too strict.
-
-This is a hard change from the current state of building all of the SDC world
-on one machine (current `./configure && make`). Two suggested ways to
-attempt to add support for this (other suggestions welcome):
-
-1.  Jenkins controlled. Having the *Jenkins Job configuration* for each
-    component, e.g. moray here, specify the build slave on which it must
-    build. Add a smartos-1.6.0-based build slave.
-
-2.  MG controlled. Make MG all fancy-ass 'n shit: I.e. it manages the whole
-    build and knows to task out building sub-components to separate zones
-    (e.g. moray on a smartos-1.6.0 zone). Possibly MG would handle
-    provisioning that zone itself if necessary. MG would then gather the
-    results and put everything together.
-
-
-Jenkins controlled pros and cons:
-
-- Pro: Easier to get going. Create a Jenkins slave for each deployment
-  dataset and tie that job's build to that build slave.
-- Con (maybe Pro): Requires stopping using the full MG build (the current
-  "sdc" Jenkins job) which builds the world on one machine. The "usbheadnode"
-  Jenkins Job which just pulls all prebuilt pieces together, because the
-  primary Jenkins producer of SDC release bits. Is this actually a Con? It is
-  one less Jenkins Job that builds the output bits to worry about.
-- Con: The knowledge of build requirements (on which dataset to build) is
-  spread into Jenkins. Frankly this isn't a huge loss because the relevant
-  dataset info is currently *already* spread between the particular repo,
-  e.g. moray.git, and "usb-headnode.git/zones/moray/setup.json".
-
-MG controlled pros and cons:
-
-- Con: Having MG control tasking out build steps to separate zones, and
-  possibly having it dynamically provision those zones, is a lot of work.
-  It would require a working SDC headnode -- presuming we'd use an SDC for
-  provisioning rather than GZ access and manual "vmadm create" usage.
-- Pro: That headnode setup for our build system would be great dogfood. :)
-  Possibly even include compute nodes to spread out the build load.
-
-
-My (Trent's) opinion right now is to just go with "Jenkins controlled".
-Implementation details:
-
--   Add build slaves for each required dataset:
-
-        $ cat usb-headnode/zones/*/dataset | sort | uniq
-        smartos-1.3.18
-        smartos-1.6.0
-        smartos64-1.4.7
-        smartos64-1.6.0
-
--   Add safeguards in each project to check that they are building on the
-    dataset they expect. Thinking of adding the dataset name to the build
-    output filename. E.g. we'd have:
-
-        moray-pkg-master-20120419T155730Z-g22f7c32-smartos-1.6.0.tar.bz2
-
-    An alternative would be for each component build to publish a separate
-    build config data file, so you'd have:
-
-        https://stuff.joyent.us/stuff/builds/moray/master-latest/moray/build-config.json
-
-    with relevant binary compat build info.
-
-    The safeguard is that usbheadnode build will bork if the dataset in the
-    package filename doesn't match the "zones/ZONENAME/dataset" value.
-
-    Not sure the best way to get that dataset name from within the build
-    zone. Anybody? Could be passed in from Jenkins in the build environment.
-
-IOW, pretty straightforward. Thoughts?
-
diff --git a/docs/index.md b/docs/index.md
deleted file mode 100644
index 0f34596..0000000
--- a/docs/index.md
+++ /dev/null
@@ -1,572 +0,0 @@
----
-title: Mountain Gorilla
-markdown2extras: tables, cuddled-lists
-apisections:
----
-<!--
-    This Source Code Form is subject to the terms of the Mozilla Public
-    License, v. 2.0. If a copy of the MPL was not distributed with this
-    file, You can obtain one at http://mozilla.org/MPL/2.0/.
--->
-
-<!--
-    Copyright (c) 2018, Joyent, Inc.
--->
-
-# Mountain Gorilla
-
-A single repo to build all the parts of SDC and Manta. This is just a *build
-driver* repo, all the components are still in their respective repos.
-Ideally this repo isn't necessary. Instead we should move to eng.git holding
-shared tooling and each repo knowing fully how to build its own release bits.
-
-
-# tl;dr
-
-A new build of a SDC or Manta component typically works like this:
-
-    push to vmapi.git
-        -> triggers a <https://jenkins.joyent.us/job/vmapi> build in Jenkins
-        -> which uses **MG's `vmapi` Makefile target** to build and upload
-           new vmapi bits to `/Joyent_Dev/stor/builds/vmapi`
-        -> which triggers a <https://jenkins.joyent.us/job/usbheadnode> build
-        -> uses MG's `usbheadnode` target to build and upload to
-           `/Joyent_Dev/stor/builds/usbheadnode`
-
-Then you can reflash your headnode using usb-headnode.git/bin/reflash, which
-will grab the latest tarball from that builds area directory. Or you can
-used incremental upgrade tooling (soon to be `sdcadm update vmapi`) to get
-the new vmapi build.
-
-Roughly the same process happens in appropriate dep order for all other
-SDC repos. See <https://jenkins.joyent.us/>.
-
-
-# Overview
-
-SDC (SmartDataCenter) has lots of components: the platform, agents, the core
-zones like vmapi & napi, components that build both like amon and ca, and the
-usb-headnode.git build that puts together the final shipping products. MG is the
-meta-repo that knows how to fetch and build each of them.
-
-An MG build generally works as follows (see ./tools/jenkins-build for more
-details).
-
-1. Clone mountain-gorilla.git
-2. Configure to build one component. This pre-fetches the relevant repo(s)
-   and dependencies (pre-built bits, dependent SDC component bits, pkgsrc,
-   images).
-3. Build it.
-4. Upload its built "bits" to a structured layout of bits at
-   `/Joyent_Dev/stor/builds` in Manta.
-5. Publish a new image to updates.joyent.com.
-
-Using vmapi as an example (see "Prerequisites" section below):
-
-    git clone git@github.com:joyent/mountain-gorilla.git    # 1.
-    cd mountain-gorilla
-    ./configure -t vmapi -b master -d Joyent_Dev         # 2.
-    gmake vmapi                                          # 3.
-    MG_TARGET=vmapi gmake manta_upload_jenkins            # 4.
-        # New bits at /Joyent_Dev/stor/builds/vmapi/$branch-$timestamp/
-    MG_TARGET=vmapi gmake jenkins_publish_image           # 5.
-        # `updates-imgadm list name=vmapi` to see added VMAPI image.
-
-The full set of targets that MG supports is both in
-[targets.json](https://mo.joyent.com/mountain-gorilla/blob/master/targets.json)
-(nice and clean) and
-[Makefile](https://mo.joyent.com/mountain-gorilla/blob/master/Makefile)
-(ugly boilerplate that should be templatized away).
-
-
-
-# Prerequisites
-
-There are a lot of prerequisites to build the SDC components. For all
-but the platform you will need:
-
-- a SmartOS zone of the appropriate image. At the time of writing most
-  services are using 'sdc-smartos@1.6.3', and a few are using 'sdc-multiarch'.
-- python, gcc, gmake, et al from pkgsrc
-
-The easiest way to get the required components is to use the jenkins-agent-*
-builds from updates.joyent.com (experimental channel).
-
-
-# Branches
-
-99% of the time MG builds just use (and default to) the "master" branch
-and that'll be all you need to know. However, MG supports building branches
-other than just "master". This is used for our bi-weekly sprint release
-builds:
-
-    ./configure -t amon -b release-YYYYMMDD
-
-However, to get a build of a feature branch, typically you only have that
-branch in *your* repo and not in ancillary repos. For that reason, MG supports
-the idea of a "TRY_BRANCH" to mean: "try pulling from TRY_BRANCH first, else
-fallback to BRANCH". This allows you to get a feature branch build like so:
-
-    ./configure -t cnapi -B CNAPI-1234 -b master
-
-or using the UI in jenkins:
-
-    # https://jenkins.joyent.us/job/cnapi/build
-    BRANCH:     master
-    TRY_BRANCH: CNAPI-123
-
-# Gerrit integration
-
-Mountain Gorilla also supports building a particular patchset from the Joyent
-Gerrit instance http://cr.joyent.us/ with the `-g` option:
-
-    ./configure -t cnapi -g 5013/11
-
-or `MG_GERRIT_CR` in Jenkins.  This will translate from the given patchset
-version into the necessary refspec, and use it in the same manner as
-`TRY_BRANCH` above.  (As a consequence, this doesn't support building from
-multiple different Gerrit CRs.)
-
-# Bits directory structure
-
-MG uploads build bits to a controlled directory structure at
-`/Joyent_Dev/stor/builds` in Manta. The MG `./configure ...` step handles
-downloading pre-built dependencies from this structure. The usb-headnode and
-agents-installer builds also rely on this structure.
-
-    /Joyent_Dev/stor/builds/
-        $job/                   # Typically $job === MG $target name
-            $branch-latest      # File with path to latest ".../$branch-$timestamp"
-            ...
-            $branch-$timestamp/
-                $target/
-                    ...the target's built bits...
-                ...all dependent bits and MG configuration...
-
-For example:
-
-    /Joyent_Dev/stor/builds/
-        amon/
-            master-latest
-            master-20130208T215745Z/
-            ...
-            master-20130226T191921Z/
-                config.mk
-                md5sums.txt
-                amon/
-                    amon-agent-master-20130226T191921Z-g7cd3e28.tgz
-                    amon-pkg-master-20130226T191921Z-g7cd3e28.tar.bz2
-                    amon-relay-master-20130226T191921Z-g7cd3e28.tgz
-                    build.log
-        usbheadnode
-            master-latest
-            ...
-            master-20130301T004335Z/
-                config.mk
-                md5sums.txt
-                usbheadnode/
-                    boot-master-20130301T004335Z-gad6dfc4.tgz
-                    coal-master-20130301T004335Z-gad6dfc4.tgz
-                    usb-master-20130301T004335Z-gad6dfc4.tgz
-                    build.log
-                    build.spec.local
-
-All those "extra" pieces (build log, md5sums.txt, config.mk)
-are there to be able to debug and theoretically reproduce builds.
-The "md5sums.txt" is used by the usb-headnode build to ensure uncorrupted
-downloads.
-
-
-# Jenkins
-
-We use [Jenkins](https://jenkins.joyent.us)
-([docs](https://hub.joyent.com/wiki/display/dev/Jenkins)] for continuous
-builds of all of SDC. Almost every relevant git repo is setup to trigger
-a build of the relevant jobs in Jenkins. See the "tl;dr" above for
-what happens after a push to a repo.
-
-
-## How Jenkins builds an SDC zone image
-
-In the "Overview" above we showed the list of files generated by an amon build.
-This section will explain how we go from clicking the build button in Jenkins to
-having those files in Manta.
-
-Details on the Jenkins setup itself are available only within Joyent at:
-
-https://mo.joyent.com/docs/engdoc/master/jenkins/index.html
-
-but the rest of this section will explain what is done once a Jenkins agent is
-chosen.
-
-The determination of which agent we should send a job to happens based on
-"labels". Which you can see in the job configuration and the build agent
-configuration. The job will only build where the labels between the two
-match.
-
-Having selected a agent, Jenkins will send the job to the agent over the
-connection it holds open to the jenkins process running in the agent zone. What
-gets run is what's listed in the 'Execute Shell / Command' section of the job's
-configuration.
-
-Simplified, what most of our jobs do here is:
-
-    git clone git@github.com:joyent/mountain-gorilla.git   # aka "MG"
-    cd mountain-gorilla
-    ./configure -t <target>
-    gmake <target>
-    gmake manta_upload_jenkins  # which runs mountain-gorilla.git/tools/mantaput-bits
-    gmake jenkins_publish_image # which uploads to updates.joyent.com
-
-The `configure -t <target>` here usually clones the repo(s) required and pulls
-down dependencies from npm and Manta. This is mountain-gorilla.git/configure in
-case you need to look at it.
-
-With all the components downloaded `gmake <job>` builds all the bits that will
-end up in /opt/smartdc/$app in the zone. This is the part that's critical to
-be run on an ancient platform so that we know it will work on any HN/CN we want
-to deploy to in JPC. This generates a tarball.
-
-After building the tarball but still within `gmake <job>`, we call:
-`./tools/prep_dataset_in_jpc.sh` which is the newest component here and the one
-that includes many of the components that talk to Manta and JPC and are the
-ones that fail most often. What this does (again simplified) is:
-
- - provision a new zone in JPC using the g3-standard-2-smartos
- - wait for the zone to be ssh-loginable (this sometimes times out at 20m)
- - use ssh to send over the tarball and unpack
- - install packages listed in mountain-gorilla.git/targets.json
- - if smartos-1.6.3: drop tools/clean-image.sh into /opt/local/bin/sm-prepare-image
-   (necessary for image creation to work on the old smartos-1.6.3 image)
- - use sdc-createimagefrommachine to create image from the VM
- - wait for the state of the image to be 'active' (or fail if it goes to failed)
- - deletes the VM
- - uses sdc-exportimage to send the image to Manta
- - deletes the image
- - downloads the manifest + image from Manta to push to updates.joyent.com
- - modifies the manifest and pushes it back to Manta
-
-This is most of the new stuff over the previous build setup in BH1 and the
-primary place where problems have been occurring.
-
-
-# Automatic builds (post-receive hooks)
-
-We use Github webhooks (for Github-hosted repos) to trigger the appropriate
-project build for pushes to any repository.
-
-An example of the former is:
-
-    [git@083a9e4b-8e3a-44f1-9e79-2056b3569e9d ~]$ cat repositories/imgapi.git/hooks/post-receive
-    #!/bin/bash
-    read oldrev newrev refname
-    bash $HOME/bin/common-post-receive-v2 -m -d imgapi -J imgapi $oldrev $newrev $refname
-
-It is the "-J imgapi" switch that results in jenkins being called:
-
-    ...
-    payload="{\"before\":\"$oldrev\",\"after\":\"$newrev\",\"ref\":\"$refname\"}"
-    curl -g --max-time 5 -sSf -k -X POST \
-        https://automation:PASS@jenkins.joyent.us/job/$JENKINS_JOB/buildWithParameters?payload="$payload"
-
-"common-post-receive-v2" lives in "gitosis-admin.git". It currently includes
-passwords so cannot be included in MG (arguably a better place).
-
-
-An example of the latter is (where "PASS" is the password of the special
-"github" user in our Jenkins, actually a ldap.joyent.com account):
-
-    https://github:PASS@jenkins.joyent.us/job/platform/buildWithParameters
-    SSL verification disabled (jenkins.jo uses a self-signed cert)
-    Content-type: application/x-www-form-encoded (to allow jenkins to take 'payload' as a build param)
-
-For example: <https://github.com/joyent/smartos-live/settings/hooks/286660>
-
-
-The design of our post-receive hooks is that a 'payload' param with JSON
-content minimally with:
-
-    {"ref": "<pushed git ref, e.g. refs/heads/master>"}
-
-is passed. This is a subset of the Github webhook payload. Each [jenkins job
-build step](./tools/jenkins-build) is prefixed with some code that will set
-`BRANCH` and `TRY_BRANCH` for that ref.  It strictly uses "BRANCH=$branch" for
-release branches to ensure that a release branch is *just bits using that
-release branch*. For feature branches, it uses "TRY_BRANCH=$branch
-BRANCH=master" as a convenience to allow builds where only one or a subset of
-involved repos have that branch.
-
-
-
-# Versioning
-
-No excuses. The [JEG](https://mo.joyent.com/docs/eng/master/) makes this
-easy for you.
-
-Thou shalt name thy SDC constituent build bits as follows:
-
-    NAME-BRANCH-TIMESTAMP[-GITDESCRIBE].TGZ
-
-Where:
-
-- NAME is the package name, e.g. "smartlogin", "ca-pkg".
-- BRANCH is the git branch, e.g. "master", "release-20110714". Use:
-
-        BRANCH=$(shell git symbolic-ref HEAD | awk -F / '{print $$3}')  # Makefile
-        BRANCH=$(git symbolic-ref HEAD | awk -F / '{print $3}')         # Bash script
-
-- TIMESTAMP is an ISO timestamp like "20110729T063329Z". Use:
-
-        TIMESTAMP=$(shell TZ=UTC date "+%Y%m%dT%H%M%SZ")    # Makefile
-        TIMESTAMP=$(TZ=UTC date "+%Y%m%dT%H%M%SZ")          # Bash script
-
-  Good. A timestamp is helpful (and in this position in the package name)
-  because: (a) it often helps to know approx. when a package was built when
-  debugging; and (b) it ensures that simple lexographical sorting of
-  "NAME-BRANCH-*" packages in a directory (as done by agents-installer and
-  usb-headnode) will make choosing "the latest" possible.
-
-  Bad. A timestamp *sucks* because successive builds in a dev tree will get a
-  new timestamp: defeating Makefile dependency attempts to avoid rebuilding.
-  Note that the TIMESTAMP is only necessary for released/published packages,
-  so for projects that care (e.g. ca), the TIMESTAMP can just be added for
-  release.
-
-- GITDESCRIBE gives the git sha for the repo and whether the repo was dirty
-  (had local changes) when it was built, e.g. "gfa1afe1-dirty", "gbadf00d".
-  Use:
-
-        # Need GNU awk for multi-char arg to "-F".
-        AWK=$((which gawk 2>/dev/null | grep -v "^no ") || which awk)
-        # In Bash:
-        GITDESCRIBE=g$(git describe --all --long --dirty | ${AWK} -F'-g' '{print $NF}')
-        # In a Makefile:
-        GITDESCRIBE=g$(shell git describe --all --long --dirty | $(AWK) -F'-g' '{print $$NF}')
-
-  Notes: "--all" allows this to work on a repo with no tags. "--long"
-  ensures we always get the "sha" part even if on a tag. We strip off the
-  head/tag part because we don't reliably use release tags in all our
-  repos, so the results can be misleading in package names. E.g., this
-  was the smartlogin package for the Lime release:
-
-        smartlogin-release-20110714-20110714T170222Z-20110414-2-g07e9e4f.tgz
-
-  The "20110414" there is an old old tag because tags aren't being added
-  to smart-login.git anymore.
-
-  "GITDESCRIBE" is *optional*. However, the only reason I currently see to
-  exclude it is if the downstream user of the package cannot handle it in
-  the package name. The "--dirty" flag is *optional* (though strongly
-  suggested) to allow repos to deal with possibly intractable issues (e.g. a
-  git submodule that has local changes as part of the build that can't be
-  resolved, at least not resolved quickly).
-
-- TGZ is a catch-all for whatever the package format is. E.g.: ".tgz",
-  ".sh" (shar), ".md5sum", ".tar.bz2".
-
-
-# HOWTO: Cut an SDC bi-weekly sprint release
-
-The SmartDataCenter product currently does bi-weekly sprint release builds.
-This involves branching/tagging all relevant repositories, building them
-all, and "releasing/archiving" versions in our Jira issue tracker. This
-section documents how to perform a release.
-
-1. Branch/tag all the repos.
-   Run the following on a machine with good connectivity (you'll be cloning
-   all SDC's repos).
-
-        git clone git@github.com:joyent/mountain-gorilla.git
-        cd mountain-gorilla
-        RELDATE=$(date +%Y%m%d)
-        ./tools/check-repos-for-release -h    # grok what this does
-        ./tools/check-repos-for-release -a $RELDATE
-
-   If running this fails, it is safe to re-run to pick up where it left off.
-
-   This will trigger builds with "BRANCH=release-$RELDATE" for all components.
-   You can watch the build queue pile up at <https://jenkins.joyent.us/>.
-
-   *Release branch* builds are strict (see the section above on automatic
-   builds) in that they will fail if dependencies do not yet have a build for
-   that branch. Therefore we expect some builds (e.g. usbheadnode) to fail
-   until all components, in particular the slower "platform" build, have had
-   time to complete.
-
-2. Babysit builds in Jenkins. Currently our builds are not as reliable as
-   we'd like, so failures do occur. The best strategy is probably:
-
-    - Allow an initial pass of many of the builds to complete.
-    - Watch the agentsshar build (https://jenkins.joyent.us/job/agentsshar/)
-      (with BRANCH=release-$RELDATE) for failures. Check its console log
-      to see if failures are due to dependent agent builds not having
-      completed. If so, manually restart those builds.
-    - Watch the usbheadnode build (https://jenkins.joyent.us/job/usbheadnode/)
-      (with BRANCH=release-$RELDATE) for failures. Check its console log
-      to see if failures are due to dependent agent builds not having
-      completed. If so, manually restart those builds.
-    - Use `./tools/ls-missing-release-builds release-YYYYMMDD` to list
-      builds that are missing for this release. It provides URLs to the
-      Jenkins page to start the appropriate build.
-
-    That this babysitting is required is lame. We should attempt to fix
-    this (TODO).
-
-3. "Release" this sprint version in our Jira projects:
-
-        cd .../mountain-gorilla/tools/jira
-        ./releaseversion.sh 'YYYY-MM-DD NAME'
-        # E.g.: ./releaseversion.sh '2014-07-10 Skynet'
-
-4. "Archive" the *previous* sprint version in our Jira projects:
-
-        cd .../mountain-gorilla/tools/jira
-        ./archiveversion.sh 'YYYY-MM-DD NAME'
-        # E.g.: ./archiveversion.sh '2014-06-26 Robocop'
-
-5. When branches are cut, tell Keith that he can do the SmartOS build.
-   SmartOS does bi-weekly releases with the same timeline as SDC.
-
-6. (Eventually, the plan is to) Publish these release bits to the "staging"
-   channel of updates.joyent.com. Then, upgrade staging-1/2/3 to the latest
-   in the "staging" channel.
-
-
-There are some common failure pathologies:
-
-- "error: unexpected ref 'refs/tags/20140703': is not 'refs/heads'"
-  The webhook used on github-hosted repos will POST for both the *tag* push
-  and the *branch* push. We only build for branch pushes (i.e. where "ref"
-  startswith "refs/heads/"). IOW, this can be ignored.
-
-- "Read from remote host github.com: Connection timed out"
-  We hit git repos hard when everything starts building. Github often doesn't
-  like this. Short of us moving to not using github repos directly in our
-  "package.json" files, I don't know of a fix here.
-
-
-
-# HOWTO: Troubleshooting Jenkins build failures
-
-The first step in debugging build failures is to ensure that you understand how
-builds work. See the previous section for details. With that understanding in
-hand you should start looking at the "Console Output" in Jenkins for the failed
-build. Usually the end of the log will tell you why the build failed if you
-follow along in the code.
-
-The main sources of problems have been in the tools/prep_dataset_in_jpc.sh
-step from MG. This primarily has failed when provisioning to JPC or when talking
-to manta.
-
-
-
-# HOWTO: Add a new project/repo/job to MG
-
-*(Warning: This section is a little out of date.)*
-
-Add it as a top-lever property in targets.json, as an object with properties
-"repos" and "deps" minimally, both are arrays.
-
-- "repos" is an array of objects, with the property "url", pointing at a git url
-- "deps" is an array of strings, where the string is another top-level target in targets.json
-
-For example:
-
-    {
-      ...
-      mynewrepo: {
-        "repos": [ {"url": "git://github.com/joyent/mynewrepo.git" } ],
-        "deps": [ "platform" ]
-      },
-      ...
-    }
-
-Then you'll add the target to Makefile. MG's configure will automatically
-populate some Makefile values for you, noteably: xxx_BRANCH , xxx_SHA, but
-you will need to fill in the build stamp yourself. Configure will also git
-checkout your repo in build/
-
-    #---- MYNEWREPO
-
-    _mynewrepo_stamp=$(MYNEWREPO_BRANCH)-$(TIMESTAMP)-g$(MYNEWREPO_SHA)
-    MYNEWREPO_BITS=$(BITS_DIR)/mynewrepo/mynewrepo-pkg-$(_mynewrepo_stamp).tar.bz2
-
-    .PHONY: mynewrepo
-    mynewrepo: $(MYNEWREPO_BITS)
-
-    $(mynewrepo_BITS): build/mynewrepo
-      mkdir -p $(BITS_DIR)
-      (cd build/mynewrepo && TIMESTAMP=$(TIMESTAMP) BITS_DIR=$(BITS_DIR) gmake pkg release publish)
-      @echo "# Created mynewrepo bits (time `date -u +%Y%m%dT%H%M%SZ`):"
-      @ls -1 $(MYNEWREPO_BITS)
-      @echo ""
-
-    clean_mynewrepo:
-      rm -rf $(BITS_DIR)/mynewrepo
-      (cd build/mynewrepo && gmake clean)
-
-if you wish to build an application zone image, the process is roughly
-similar except you will need to add the "appliance":"true" property, the
-"pkgsrc" property and "dataset_uuid"
-
-    {
-      ...
-      "mynewrepo": {
-        "repos" : [ {"url":"git://github.com/joyent/mynewrepo.git"} ],
-        "appliance": "true",
-        "dataset_uuid": "01b2c898-945f-11e1-a523-af1afbe22822",
-        "pkgsrc": [
-          "sun-jre6-6.0.26",
-          "zookeeper-client-3.4.3",
-          "zookeeper-server-3.4.3"
-        ],
-        deps: []
-      },
-      ...
-    }
-
-where dataset\_uuid is the uuid of the source image you wish to build off
-pkgsrc is an array of strings of package names to install.
-
-Your Makefile target will look as above, with the addition of the xxx\_dataset target:
-
-
-    ...
-    MYNEWREPO_DATASET=$(BITS_DIR)/mynewrepo/mynewrepo-zfs-$(_mynewrepo_stamp).zfs.bz2
-
-    .PHONY: mynewrepo_dataset
-
-    mynewrepo_dataset: $(MYNEWREPO_DATASET)
-
-    $(MYNEWREPO_DATASET): $(MYNEWREPO_BITS)
-            @echo "# Build mynewrepo dataset: branch $(MYNEWREPO_BRANCH), sha $(MYNEWREPO_SHA), time `date -u +%Y%m%dT%H%M%SZ`"
-            ./tools/prep_dataset.sh -t $(MYNEWREPO_BITS) -o $(MYNEWREPO_DATASET) -p $(MYNEWREPO_PKGSRC)
-            @echo "# Created mynewrepo dataset (time `date -u +%Y%m%dT%H%M%SZ`):"
-            @ls -1 $(MYNEWREPO_DATASET)
-            @echo ""
-    ...
-
-prep\_dataset.sh is a script that generates images out of tarballs and lists
-of packages.
-
-It takes arguments of the form -t <tarball> where <tarball> is a .tar.gz
-file, containing a directory "root", which is unpacked to / -p "list of
-pkgsrc packages" where list of pkgsrc packages is a list of the pkgsrc
-packages to be installed in the zone.
-
-Configure will populate xxx\_DATASET and xxx\_PKGSRC based on targets.json.
-
-Additionally, you can set the dsadm URN for the target by adding the "urn"
-and "version" properties to targets.json, as properties of the target you
-wish to manipulate. These will show up as urn:version ( sdc:sdc:mynewrepo:0.1
-for instance ). To use them, configure will populate xxx\_URN and xxx\_VERSION
-for you in the Makefile.
-
-Note that these images can only be provisioned with the joyent-minimal brand.
-If one is provisioned with the joyent brand, that zone's networking may not be
-working.  Normally, the networking setup is done through zoneinit, but since
-that script has already run and had its effects undone (as part of the MG
-build), there's no mechanism to automatically bring that zone's VNIC up.  You
-can recover by manually enabling network/physical:default, but you should just
-be provisioning with the joyent-minimal brand instead.  See RELENG-337 for
-details.
diff --git a/example.build.json b/example.build.json
deleted file mode 100644
index b8087dc..0000000
--- a/example.build.json
+++ /dev/null
@@ -1,7 +0,0 @@
-{
-	"clapi_url": "https://10.2.122.6",
-	"clapi_user": "rmustacc",
-	"clapi_passwd": "aprettypassword",
-	"ssh_private": "/root/.ssh/id_rsa",
-	"output_dir": "/root/mountain-gorilla/bits/"
-}
diff --git a/gzhosts.json b/gzhosts.json
deleted file mode 100644
index 00ea01b..0000000
--- a/gzhosts.json
+++ /dev/null
@@ -1,6 +0,0 @@
-[
-  { 
-    "// name" : "bh1-build0",
-    "hostname" : "10.2.0.191"
-  }
-]
diff --git a/package.json b/package.json
deleted file mode 100644
index f72a187..0000000
--- a/package.json
+++ /dev/null
@@ -1,16 +0,0 @@
-{
-  "name": "mountain-gorilla",
-  "description": "Build SDC components. Eat bamboo.",
-  "version": "2.0.1",
-  "author": "Joyent (joyent.com)",
-  "private": true,
-  "dependencies": {
-    "manta": "3.1.3",
-    "smartdc": "7.3.0",
-    "imgapi-cli": "git+https://github.com/joyent/sdc-imgapi-cli.git#38332b1"
-  },
-  "engines": {
-    "node": ">=0.8"
-  },
-  "license": "MPL-2.0"
-}
diff --git a/platform.imgmanifest.in b/platform.imgmanifest.in
deleted file mode 100644
index 7f7a0f5..0000000
--- a/platform.imgmanifest.in
+++ /dev/null
@@ -1,27 +0,0 @@
-{
-  "v": 2,
-  "uuid": "UUID",
-  "owner": "00000000-0000-0000-0000-000000000000",
-  "name": "platform",
-  "type": "other",
-  "version": "VERSION_STAMP",
-  "state": "active",
-  "disabled": false,
-  "public": false,
-  "os": "other",
-  "files": [
-    {
-      "sha1": "SHA",
-      "size": SIZE,
-      "compression": "gzip"
-    }
-  ],
-  "description": "SmartDataCenter platform image",
-  "tags": {
-    "smartdc_service": "true",
-    "buildstamp": "BUILDSTAMP"
-  },
-  "channels": [
-    "dev"
-  ]
-}
diff --git a/smartos-live-configure-smartos.mg.in b/smartos-live-configure-smartos.mg.in
deleted file mode 100644
index e3a01c4..0000000
--- a/smartos-live-configure-smartos.mg.in
+++ /dev/null
@@ -1,17 +0,0 @@
-mkdir -p projects/local
-(cd projects/local && [[ -d kvm ]] \
-    || git clone GITCLONESOURCEillumos-kvm kvm)
-(cd projects/local && [[ -d kvm-cmd ]] \
-    || git clone GITCLONESOURCEillumos-kvm-cmd kvm-cmd)
-(cd projects/local && [[ -d mdata-client ]] \
-    || git clone GITCLONESOURCEmdata-client mdata-client)
-
-PUBLISHER="joyent"
-RELEASE_VER="joyent_147"
-SUNW_SPRO12_URL="https://download.joyent.com/pub/build/SunStudio.tar.bz2"
-ON_CLOSED_BINS_URL="https://download.joyent.com/pub/build/illumos/on-closed-bins.i386.tar.bz2"
-ON_CLOSED_BINS_ND_URL="https://download.joyent.com/pub/build/illumos/on-closed-bins-nd.i386.tar.bz2"
-GET_ILLUMOS="(git clone GITCLONESOURCEillumos-joyent illumos)"
-GET_ILLUMOS_EXTRA="git clone GITCLONESOURCEillumos-extra illumos-extra"
-ILLUMOS_ADJUNCT_TARBALL_URL="https://download.joyent.com/pub/build/adjuncts/"
-OVERLAYS="generic"
diff --git a/smartos-live-configure.mg.in b/smartos-live-configure.mg.in
deleted file mode 100644
index e5a0429..0000000
--- a/smartos-live-configure.mg.in
+++ /dev/null
@@ -1,19 +0,0 @@
-mkdir -p projects/local
-(cd projects/local && [[ -d ur-agent ]] \
-    || git clone GITCLONESOURCEsdc-ur-agent ur-agent)
-(cd projects/local && [[ -d kvm ]] \
-    || git clone GITCLONESOURCEillumos-kvm kvm)
-(cd projects/local && [[ -d kvm-cmd ]] \
-    || git clone GITCLONESOURCEillumos-kvm-cmd kvm-cmd)
-(cd projects/local && [[ -d mdata-client ]] \
-    || git clone GITCLONESOURCEmdata-client mdata-client)
-
-PUBLISHER="joyent"
-RELEASE_VER="joyent_147"
-SUNW_SPRO12_URL="https://download.joyent.com/pub/build/SunStudio.tar.bz2"
-ON_CLOSED_BINS_URL="https://download.joyent.com/pub/build/illumos/on-closed-bins.i386.tar.bz2"
-ON_CLOSED_BINS_ND_URL="https://download.joyent.com/pub/build/illumos/on-closed-bins-nd.i386.tar.bz2"
-GET_ILLUMOS="(git clone GITCLONESOURCEillumos-joyent illumos)"
-GET_ILLUMOS_EXTRA="git clone GITCLONESOURCEillumos-extra illumos-extra"
-ILLUMOS_ADJUNCT_TARBALL_URL="https://download.joyent.com/pub/build/adjuncts/"
-OVERLAYS="generic"
diff --git a/targets.json.in b/targets.json.in
deleted file mode 100644
index e5c4c0f..0000000
--- a/targets.json.in
+++ /dev/null
@@ -1,1506 +0,0 @@
-set -o errexit
-set -o pipefail
-
-[[ -z $JOYENT_BUILD ]] && JOYENT_BUILD=false
-[[ -z $MG_GIT_PREFIX ]] && MG_GIT_PREFIX=git@github.com:joyent
-
-function build_headnode_target()
-{
-    local name=$1
-    local platform=$2
-    local firmware_tools=$3
-
-    local firmware=""
-
-    if [[ -n ${firmware_tools} ]]; then
-        firmware="\"${firmware_tools}\","
-    fi
-
-    if [[ ${name} == "headnode-joyent"
-        || ${name} == "headnode-joyent-debug" ]]; then
-
-        public='"public": false,'
-    else
-        public='"public": true,'
-    fi
-
-    cat <<EOF
-"${name}": {
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/sdc-headnode.git"}
-    ],
-    ${public}
-    "deps": [
-      "adminui",
-      "agentsshar",
-      "amon",
-      "amonredis",
-      "assets",
-      "registrar",
-      "binder",
-      "cloudapi",
-      "cnapi",
-      "dhcpd",
-      ${firmware}
-      "fwapi",
-      "imgapi",
-      "ipxe",
-      "sdc-manatee",
-      "mahi",
-      "manta-deployment",
-      "moray",
-      "napi",
-      "papi",
-      "${platform}",
-      "rabbitmq",
-      "sapi",
-      "sdc",
-      "sdcadm",
-      "ufds",
-      "vmapi",
-      "workflow"
-    ]
-  },
-
-EOF
-
-}
-
-HEADNODE_TARGETS="";
-HEADNODE_TARGETS="${HEADNODE_TARGETS}$(build_headnode_target headnode platform '')"
-HEADNODE_TARGETS="${HEADNODE_TARGETS}$(build_headnode_target headnode-debug platform-debug '')"
-
-if [[ "${JOYENT_BUILD}" == "true" ]]; then
-  FIRMWARE_TOOLS_DEP='"firmware-tools",'
-  FIRMWARE_TOOLS_ENTRY="
-  \"firmware-tools\": {
-    \"build_platform\": \"20151126T062538Z\",
-    \"repos\": [
-        {\"url\": \"$MG_GIT_PREFIX/firmware-tools.git\"}
-    ],
-    \"public\": false,
-    \"deps\": []
-  },"
-  HEADNODE_TARGETS="${HEADNODE_TARGETS}$(build_headnode_target headnode-joyent platform 'firmware-tools')"
-  HEADNODE_TARGETS="${HEADNODE_TARGETS}$(build_headnode_target headnode-joyent-debug platform-debug 'firmware-tools')"
-elif [[ "${JOYENT_BUILD}" == "false" ]]; then
-  echo "targets.json: Non-Joyent build; disabling ancillary repositories." >&2
-else
-  echo "targets.json: ERROR: JOYENT_BUILD must be set to true|false." >&2
-fi
-
-cat <<EOF
-{
-  "smartlogin": {
-    "build_platform": "20151126T062538Z",
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdc-smart-login.git"}
-    ],
-    "public": true,
-    "deps": []
-  },
-  "amon": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "amon",
-    "image_description": "SDC AMON",
-    "image_version": "1.0.0",
-    "image_uuid": "fd2cc906-8938-11e3-beab-4359c665ac99",
-    "pkgsrc": [],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/sdc-amon.git"}
-    ],
-    "public": true,
-    "deps": [
-      "config-agent",
-      "registrar",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-  "assets": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "assets",
-    "image_description": "SDC Assets",
-    "image_version": "1.0.0",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [
-      "nginx-1.11.1nb1"
-    ],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/sdc-assets.git"}
-    ],
-    "public": true,
-    "deps": [
-      "amon"
-    ],
-    "tarballs": [
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"}
-    ]
-  },
-  "cns": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "cns",
-    "image_description": "Triton Container Naming Service",
-    "image_version": "1.0.0",
-    "image_uuid": "18b094b0-eb01-11e5-80c1-175dac7ddf02",
-    "pkgsrc": [
-      "redis-3.0.5"
-    ],
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/triton-cns.git", "dir": "triton-cns"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-  "adminui": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "adminui",
-    "image_description": "SDC AdminUI",
-    "image_version": "1.0.0",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [],
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdc-adminui.git", "dir": "adminui"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-  "mockcloud": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "mockcloud",
-    "image_description": "Triton Mockcloud",
-    "image_version": "2.0.0",
-    "// image_uuid": "triton-origin-multiarch-18.1.0@1.0.1",
-    "image_uuid": "b6ea7cb4-6b90-48c0-99e7-1d34c2895248",
-    "pkgsrc": [],
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/triton-mockcloud.git", "dir": "mockcloud"}
-    ],
-    "public": false,
-    "deps": [],
-    "tarballs": []
-  },
-  "nfsserver": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "nfsserver",
-    "image_description": "SDC NFS Server",
-    "image_version": "1.0.0",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [],
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdc-nfsserver.git", "dir": "nfsserver"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "tarballs": []
-  },
-  "dhcpd": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "dhcpd",
-    "image_description": "SDC DHCPD",
-    "image_version": "1.0.0",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [
-      "nginx-1.10.1",
-      "tftp-hpa-5.2"
-    ],
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdc-booter.git", "dir": "dhcpd"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-  "amonredis": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "amonredis",
-    "image_description": "SDC Amon Redis",
-    "image_version": "1.0.0",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/sdc-amonredis.git"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-  "rabbitmq": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "rabbitmq",
-    "image_description": "SDC RabbitMQ",
-    "image_version": "1.0.0",
-    "image_uuid": "fd2cc906-8938-11e3-beab-4359c665ac99",
-    "pkgsrc": [
-      "perl-5.14.2nb3",
-      "iodbc-3.52.7",
-      "libffi-3.0.9nb1",
-      "python27-2.7.2nb2",
-      "py27-setuptools-0.6c11nb1",
-      "py27-simplejson-2.1.1",
-      "erlang-14.1.4nb1",
-      "rabbitmq-2.7.1"
-    ],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/sdc-rabbitmq.git"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-  "cloudapi": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "cloudapi",
-    "image_description": "SDC CloudAPI",
-    "image_version": "1.0.0",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/sdc-cloudapi.git"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "pkgsrc": [
-      "openssl-1.0.2o",
-      "stud-0.3p53nb5",
-      "haproxy-1.6.2"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-  "nat": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "nat",
-    "image_description": "SmartDataCenter per-user NAT zone",
-    "image_version": "1.0.0",
-    "image_uuid": "de411e86-548d-11e4-a4b7-3bb60478632a",
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/sdc-nat.git"}
-    ],
-    "public": true,
-    "deps": [],
-    "pkgsrc": [],
-    "tarballs": []
-  },
-  "docker": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "docker",
-    "image_description": "SDC Docker Engine",
-    "image_version": "1.0.0",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/sdc-docker.git"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "pkgsrc": [],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-  "volapi": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "volapi",
-    "image_description": "SDC Volumes API",
-    "image_version": "1.0.0",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/sdc-volapi.git"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "pkgsrc": [],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-  "portolan": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "portolan",
-    "image_description": "SDC Portolan Service",
-    "image_version": "1.0.0",
-    "image_uuid": "de411e86-548d-11e4-a4b7-3bb60478632a",
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/sdc-portolan.git"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "pkgsrc": [],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "ufds": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "ufds",
-    "image_description": "SDC UFDS",
-    "image_version": "1.0.0",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [
-      "postgresql91-client-9.1.2"
-    ],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/sdc-ufds.git"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "workflow": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "workflow",
-    "image_description": "SDC Workflow",
-    "image_version": "1.0.0",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [
-      "apg-2.3.0bnb3"
-    ],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/sdc-workflow.git"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "cmon": {
-    "appliance": "true",
-    "image_name": "cmon",
-    "image_description": "Triton Container Monitor",
-    "image_version": "1.0.0",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [],
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/triton-cmon.git"}
-    ],
-    "public": true,
-    "deps": [
-      "config-agent",
-      "registrar",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "cmon-agent": {
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/triton-cmon-agent.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode"
-    ]
-  },
-
-  "vmapi": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "vmapi",
-    "image_description": "SDC VMAPI",
-    "image_version": "1.0.0",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [],
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdc-vmapi.git"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "papi": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "papi",
-    "image_description": "SDC PAPI",
-    "image_version": "1.0.0",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [],
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdc-papi.git"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "imgapi": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "imgapi",
-    "image_description": "SDC IMGAPI",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [
-        "dateutils-0.3.1nb1",
-        "haproxy-1.6.2",
-        "openssl-1.0.2o",
-        "smtools-20160926",
-        "stud-0.3p53nb5",
-        "the_silver_searcher-0.31.0",
-        "xz-5.2.2"
-    ],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/sdc-imgapi.git"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "ipxe": {
-    "build_platform": "20151126T062538Z",
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/ipxe.git"}
-    ],
-    "public": true,
-    "deps": []
-  },
-
-  "sdc": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "sdc",
-    "image_description": "SDC tools/ops zone",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [
-        "mtr-0.86nb3"
-    ],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/sdc-sdc.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode",
-      "amon",
-      "config-agent"
-    ],
-    "tarballs": [
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "agents_core": {
-    "build_platform": "20151126T062538Z",
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdc-agents-core.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode"
-    ]
-  },
-
-  "vm-agent": {
-    "build_platform": "20151126T062538Z",
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdc-vm-agent.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode"
-    ]
-  },
-
-  "net-agent": {
-    "build_platform": "20151126T062538Z",
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdc-net-agent.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode"
-    ]
-  },
-
-  "cn-agent": {
-    "build_platform": "20151126T062538Z",
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdc-cn-agent.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode"
-    ]
-  },
-
-  "config-agent": {
-    "build_platform": "20151126T062538Z",
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdc-config-agent.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode"
-    ]
-  },
-
-  "hagfish-watcher": {
-    "build_platform": "20151126T062538Z",
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdc-hagfish-watcher.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode"
-    ]
-  },
-
-  "firewaller": {
-    "build_platform": "20151126T062538Z",
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdc-firewaller-agent.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode"
-    ]
-  },
-
-  "cnapi": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "cnapi",
-    "image_description": "SDC CNAPI",
-    "image_version": "1.0.0",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [],
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdc-cnapi.git"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "fwapi": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "fwapi",
-    "image_description": "SDC FWAPI",
-    "image_version": "1.0.0",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [],
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdc-fwapi.git"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "napi": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "napi",
-    "image_description": "SDC NAPI",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/sdc-napi.git"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "sapi": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "sapi",
-    "image_description": "SDC SAPI",
-    "image_version": "1.0.0",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [],
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdc-sapi.git"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "registrar",
-      "config-agent",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "registrar": {
-    "build_platform": "20151126T062538Z",
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/registrar.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode"
-    ]
-  },
-
-  "waferlock": {
-    "build_platform": "20151126T062538Z",
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/waferlock.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode"
-    ]
-  },
-
-  "minnow": {
-    "build_platform": "20151126T062538Z",
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/manta-minnow.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode"
-    ]
-  },
-
-  "mackerel": {
-    "build_platform": "20151126T062538Z",
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/manta-mackerel.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode"
-    ]
-  },
-
-  "binder": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "manta-nameservice",
-    "image_description": "Manta nameservice",
-    "image_uuid": "fd2cc906-8938-11e3-beab-4359c665ac99",
-    "pkgsrc": [
-      "sun-jre6-6.0.26",
-      "zookeeper-client-3.4.3",
-      "zookeeper-server-3.4.3"
-    ],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/binder.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode",
-      "registrar",
-      "amon",
-      "config-agent"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "manta-manatee": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "manta-postgres",
-    "image_description": "Manta manatee",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [
-        "lz4-131nb1"
-    ],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/manta-manatee.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode",
-      "registrar",
-      "waferlock",
-      "amon",
-      "config-agent"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "waferlock/waferlock-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "sdc-manatee": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "sdc-postgres",
-    "image_description": "SDC manatee",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [
-        "lz4-131nb1"
-    ],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/sdc-manatee.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode",
-      "registrar",
-      "waferlock",
-      "amon",
-      "config-agent"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "waferlock/waferlock-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "medusa": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "manta-medusa",
-    "image_description": "Manta medusa",
-    "image_uuid": "fd2cc906-8938-11e3-beab-4359c665ac99",
-    "pkgsrc": [
-      "zookeeper-client-3.4.3"
-    ],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/manta-medusa.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode",
-      "registrar",
-      "amon",
-      "config-agent"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "mahi": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "manta-authcache",
-    "image_description": "Manta authcache",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [
-      "redis-3.0.5"
-    ],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/mahi.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode",
-      "registrar",
-      "amon",
-      "config-agent"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "moray": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "manta-moray",
-    "image_description": "Manta moray",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [
-      "haproxy-1.6.2",
-      "postgresql92-client-9.2.19"
-    ],
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/moray.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode",
-      "registrar",
-      "amon",
-      "config-agent"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "electric-moray": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "manta-electric-moray",
-    "image_description": "Manta moray proxy",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [
-      "haproxy-1.6.2"
-    ],
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/electric-moray.git"}
-    ],
-    "public": true,
-    "deps": [
-      "registrar",
-      "moray",
-      "amon",
-      "config-agent"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "muppet": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "manta-loadbalancer",
-    "image_description": "Manta loadbalancer",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [
-      "py27-curses",
-      "openssl-1.0.2o",
-      "stud-0.3p53nb5",
-      "libzookeeper-3.4.6"
-    ],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/muppet.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode",
-      "registrar",
-      "amon",
-      "config-agent"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "muskie": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "manta-webapi",
-    "image_description": "Manta webapi",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [
-      "haproxy-1.6.2"
-    ],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/manta-muskie.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode",
-      "registrar",
-      "amon",
-      "config-agent"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "mola": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "manta-ops",
-    "image_description": "Manta ops",
-    "image_uuid": "fd2cc906-8938-11e3-beab-4359c665ac99",
-    "pkgsrc": [
-      "redis-2.4.13",
-      "zookeeper-client-3.4.3"
-    ],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/manta-mola.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode",
-      "registrar",
-      "mackerel",
-      "amon",
-      "config-agent"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "mackerel/mackerel-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "madtom": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "manta-madtom",
-    "image_description": "Manta madtom",
-    "image_uuid": "fd2cc906-8938-11e3-beab-4359c665ac99",
-    "pkgsrc": [
-      "postgresql91-client-9.1.2",
-      "redis-2.4.13",
-      "zookeeper-client-3.4.3"
-    ],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/manta-madtom.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode",
-      "registrar",
-      "amon",
-      "config-agent"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "marlin-dashboard": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "manta-marlin-dashboard",
-    "image_description": "Manta marlin dashboard",
-    "image_uuid": "fd2cc906-8938-11e3-beab-4359c665ac99",
-    "pkgsrc": [
-    ],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/manta-marlin-dashboard.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode",
-      "registrar",
-      "amon",
-      "config-agent"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "mako": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "manta-storage",
-    "image_description": "Manta storage",
-    "// image_uuid": "triton-origin-multiarch-18.1.0@1.0.1",
-    "image_uuid": "b6ea7cb4-6b90-48c0-99e7-1d34c2895248",
-    "pkgsrc": [
-      "pcre-8.42",
-      "findutils-4.6.0nb1",
-      "gawk-4.1.4nb1"
-    ],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/manta-mako.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode",
-      "minnow",
-      "registrar",
-      "amon",
-      "config-agent"
-    ],
-    "tarballs": [
-      {"tarball": "minnow/minnow-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "marlin": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "manta-jobsupervisor",
-    "image_description": "Manta jobsupervisor",
-    "image_uuid": "fd2cc906-8938-11e3-beab-4359c665ac99",
-    "pkgsrc": [
-      "zookeeper-client-3.4.3"
-    ],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/manta-marlin.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode",
-      "registrar",
-      "amon",
-      "config-agent"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "wrasse": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "manta-jobpuller",
-    "image_description": "Manta Job Puller",
-    "image_uuid": "fd2cc906-8938-11e3-beab-4359c665ac99",
-    "pkgsrc": [
-      "zookeeper-client-3.4.3"
-    ],
-    "repos": [
-      {"url": "$MG_GIT_PREFIX/manta-wrasse.git"}
-    ],
-    "public": true,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode",
-      "registrar",
-      "amon",
-      "config-agent"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "sdcadm": {
-    "build_platform": "20151126T062538Z",
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdcadm.git"}
-    ],
-    "public": true,
-    "deps": []
-  },
-
-  "sdcsso": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "sdcsso",
-    "image_description": "SDC SSO",
-    "image_version": "1.0.0",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [
-      "openssl-1.0.2o",
-      "postfix-3.0.2nb2"
-    ],
-    "repos": [
-        {"url":  "$MG_GIT_PREFIX/sdcsso.git" }
-    ],
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "agentsshar": {
-    "build_platform": "20151126T062538Z",
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdc-agents-installer.git"}
-    ],
-    "public": true,
-    "deps": [
-      "agents_core",
-      "hagfish-watcher",
-      "cmon-agent",
-      "cn-agent",
-      "net-agent",
-      "vm-agent",
-      "amon",
-      "smartlogin",
-      "marlin",
-      "firewaller",
-      "config-agent"
-    ]
-  },
-
-  "dockerlogger": {
-    "build_platform": "20151126T062538Z",
-    "repos": [
-        {
-            "url": "$MG_GIT_PREFIX/sdc-dockerlogger.git",
-            "dir": "dockerlogger"
-        }
-    ],
-    "public": true,
-    "deps": [
-    ]
-  },
-
-  "pgstatsmon": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "manta-pgstatsmon",
-    "image_description": "Postgres Monitoring Service",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/pgstatsmon.git"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "manta-reshard": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "manta-reshard",
-    "image_description": "Manta Resharding System",
-    "image_version": "1.0.0",
-    "// image_uuid": "sdc-minimal-multiarch-lts@15.4.1",
-    "image_uuid": "18b094b0-eb01-11e5-80c1-175dac7ddf02",
-    "pkgsrc": [
-      "coreutils-8.23nb2"
-    ],
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/manta-reshard.git"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "registrar"
-    ],
-    "tarballs": [
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"},
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"}
-    ]
-  },
-
-  "manta-garbage-collector": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "manta-garbage-collector",
-    "image_description": "Manta Garbage Collector",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/manta-garbage-collector.git"}
-    ],
-    "public": true,
-    "deps": [
-        "amon",
-        "config-agent",
-        "registrar"
-    ],
-    "tarballs": [
-      {"tarball": "registrar/registrar-pkg-*.tar.gz", "sysroot": "/"},
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "manta-deployment": {
-    "appliance": "true",
-    "build_platform": "20151126T062538Z",
-    "image_name": "manta-deployment",
-    "image_description": "Manta deployment tools",
-    "image_version": "1.0.0",
-    "// image_uuid": "triton-origin-multiarch-15.4.1@1.0.1",
-    "image_uuid": "04a48d7d-6bb5-4e83-8c3b-e60a99e0f48f",
-    "pkgsrc": [
-      "openldap-client-2.4.44nb2"
-    ],
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdc-manta.git"}
-    ],
-    "public": true,
-    "deps": [
-      "amon",
-      "config-agent",
-      "https://download.joyent.com/pub/build/sdcnode"
-    ],
-    "tarballs": [
-      {"tarball": "amon/amon-agent-*.tgz", "sysroot": "/opt"},
-      {"tarball": "config-agent/config-agent-*.tar.gz", "sysroot": "/opt/smartdc"}
-    ]
-  },
-
-  "sdc-system-tests": {
-    "build_platform": "20151126T062538Z",
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/sdc-system-tests.git"}
-    ],
-    "public": false,
-    "deps": [
-      "https://download.joyent.com/pub/build/sdcnode"
-    ]
-  },
-
-  ${FIRMWARE_TOOLS_ENTRY}
-  ${HEADNODE_TARGETS}
-
-  "platform": {
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/smartos-live.git"},
-        {"url": "$MG_GIT_PREFIX/illumos-joyent.git"},
-        {"url": "$MG_GIT_PREFIX/illumos-extra.git"},
-        {"url": "$MG_GIT_PREFIX/sdc-ur-agent.git"},
-        {"url": "$MG_GIT_PREFIX/illumos-kvm.git"},
-        {"url": "$MG_GIT_PREFIX/illumos-kvm-cmd.git"},
-        {"url": "$MG_GIT_PREFIX/mdata-client.git"}
-    ],
-    "public": true,
-    "deps": []
-  },
- "platform-debug": {
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/smartos-live.git"},
-        {"url": "$MG_GIT_PREFIX/illumos-joyent.git"},
-        {"url": "$MG_GIT_PREFIX/illumos-extra.git"},
-        {"url": "$MG_GIT_PREFIX/sdc-ur-agent.git"},
-        {"url": "$MG_GIT_PREFIX/illumos-kvm.git"},
-        {"url": "$MG_GIT_PREFIX/illumos-kvm-cmd.git"},
-        {"url": "$MG_GIT_PREFIX/mdata-client.git"}
-    ],
-    "public": true,
-    "deps": []
-  },
-  "smartos": {
-    "repos": [
-        {"url": "$MG_GIT_PREFIX/smartos-live.git"},
-        {"url": "$MG_GIT_PREFIX/illumos-joyent.git"},
-        {"url": "$MG_GIT_PREFIX/illumos-extra.git"},
-        {"url": "$MG_GIT_PREFIX/illumos-kvm.git"},
-        {"url": "$MG_GIT_PREFIX/illumos-kvm-cmd.git"},
-        {"url": "$MG_GIT_PREFIX/mdata-client.git"}
-    ],
-    "public": true,
-    "deps": []
-  }
-}
-
-EOF
diff --git a/tools/bad-bin/node b/tools/bad-bin/node
deleted file mode 100755
index f17537d..0000000
--- a/tools/bad-bin/node
+++ /dev/null
@@ -1,2 +0,0 @@
-echo "$0: Bad node!" 1>&2
-exit 42
diff --git a/tools/bad-bin/npm b/tools/bad-bin/npm
deleted file mode 100755
index b7c6d42..0000000
--- a/tools/bad-bin/npm
+++ /dev/null
@@ -1,2 +0,0 @@
-echo "$0: Bad npm!" 1>&2
-exit 42
diff --git a/tools/build-zone b/tools/build-zone
deleted file mode 100755
index 131a33d..0000000
--- a/tools/build-zone
+++ /dev/null
@@ -1,255 +0,0 @@
-#!/bin/bash
-#
-# This Source Code Form is subject to the terms of the Mozilla Public
-# License, v. 2.0. If a copy of the MPL was not distributed with this
-# file, You can obtain one at http://mozilla.org/MPL/2.0/.
-#
-
-#
-# Copyright (c) 2014, Joyent, Inc.
-#
-
-#
-# Run this script on the headnode to set up a local zone for developing Cloud
-# Analytics.  The script takes a single argument: the desired username.
-#
-
-shopt -s xpg_echo
-set -o xtrace
-
-#
-# Glogal vars/tuning
-#
-sh_arg0=$(basename $0)
-sh_bconfig=$1
-sh_zconfig=$2
-sh_zone=$3
-sh_sha=$4
-sh_branch=$BRANCH # Passed as env var
-sh_sshopts="-o StrictHostKeyChecking=no -o UserKnownhostsFile=/dev/null"
-sh_bits="mg-build-output.$$"
-sh_zoneip=
-sh_dsurn=
-sh_zid=
-
-#
-# Should all be loaded from a config file.
-#
-sh_dataset=
-sh_dataset_vers=
-sh_package=
-sh_pkgsrc=
-sh_repo=
-
-sh_clapibase=
-sh_clapiuser=
-sh_clapipasswd=
-sh_sshkey=
-sh_output=
-
-#
-# Determined by config
-#
-sh_clapicurl=
-sh_clapiurl=
-
-
-#
-# Based on config file contents
-#
-
-
-function fail
-{
-	local msg="$*"
-	[[ -z "$msg" ]] && msg="failed"
-	echo "$sh_arg0: $msg" >&2
-	exit 1
-}
-
-function sh_load_config
-{
-	local curzone branch use
-	[[ -f $sh_bconfig ]] || fail "failed to read base conifg"
-
-	sh_clapibase=$(json clapi_url < $sh_bconfig)
-	[[ $? -eq 0 ]] || fail "failed to read cloud api base url"
-	[[ -n $sh_clapibase ]] || fail "missing cloud api base url"
-	sh_clapiuser=$(json clapi_user < $sh_bconfig)
-	[[ $? -eq 0 ]] || fail "failed to read cloud api user"
-	[[ -n $sh_clapiuser ]] || fail "missing cloud api user"
-	sh_clapipasswd=$(json clapi_passwd < $sh_bconfig)
-	[[ $? -eq 0 ]] || fail "failed to read cloud api user"
-	[[ -n $sh_clapipasswd ]] || fail "missing cloud api password"
-	sh_sshkey=$(json ssh_private < $sh_bconfig)
-	[[ $? -eq 0 ]] || fail "failed to read ssh private key for gitosis"
-	[[ -f $sh_sshkey ]] || fail "can't access ssh private key"
-	sh_output=$(json output_dir < $sh_bconfig)
-	[[ $? -eq 0 ]] || fail "failed to read output directory"
-	[[ -n $sh_output ]] || fail "missing output directory"
-
-	[[ -f $sh_zconfig ]] || fail "failed to read zone config"
-	use=$(json $sh_zone.usezone < $sh_zconfig)
-	[[ $? -eq 0 ]] || fail "usezone not set, this shouldn't happen"
-	[[ "$use" == "true" ]] || fail "usezone not true, this shouldn't happen"
-
-	branch=$(json $sh_zone.branch < $sh_zconfig)
-	[[ $? -eq 0 ]] || fail "failed to zone's config: $sh_zone"
-	curzone=$(echo $branch | json $sh_branch)
-	if [[ -z "$curzone" ]]; then
-		echo "Warning: $sh_branch undefined in targets.json. " \
-		    "Defaulting to master"
-		curzone=$(echo $branch | json master)
-		[[ -n "$curzone" ]] || fail "Couldn't find master branch"
-	fi
-
-	sh_dataset=$(echo $curzone | json dataset)
-	[[ $? -eq 0 ]] || fail "failed to load dataset"
-	[[ -n $sh_dataset ]] || fail "missing dataset info"
-	sh_dataset_vers=$(echo $curzone | json version)
-	[[ $? -eq 0 ]] || fail "failed to load dataset version"
-	[[ -n $sh_dataset_vers ]] || fail "missing dataset version info"
-	sh_package=$(echo $curzone | json package)
-	[[ $? -eq 0 ]] || fail "failed to load package"
-	[[ -n $sh_package ]] || fail "missing pacakge info"
-	sh_pkgsrc=$(echo $curzone | json pkgsrc)
-	[[ $? -eq 0 ]] || fail "failed to load pkgsrc packages"
-	[[ -n $sh_pkgsrc ]] || fail "missing pkgsrc info"
-	sh_repo=$(echo $curzone | json rname)
-	[[ $? -eq 0 ]] || fail "failed to load git repository"
-	[[ -n $sh_repo ]] || fail "missing repo info"
-	sh_clapicurl="curl -k -H x-api-version:\ ~6.5 -u ${sh_clapiuser}:${sh_clapipasswd}"
-	sh_clapiurl="$sh_clapibase/$sh_clapiuser"
-}
-
-function sh_clapi_curl
-{
-	curl -k -H 'X-api-version: ~6.5' -H 'Accept: application/json' -u \
-	    ${sh_clapiuser}:${sh_clapipasswd} -s --url $*
-}
-
-function sh_verif_package
-{
-	local out npkgs pname found ii
-	out=$(sh_clapi_curl $sh_clapiurl/packages)
-
-	[[ $? -eq 0 ]] || fail "failed to list packages"	
-	found=
-	npkgs=$(echo "$out" | json length)
-	for ((ii=0; ii < $npkgs; ii++)); do
-		pname=$(echo "$out" | json [$ii].name)
-		[[ $? -eq 0 ]] || fail "Failed to get package name"
-		[[ "$sh_package" == $pname ]] && found="true"
-	done
-
-	[[ ! -z "$found" ]]
-
-}
-
-function sh_verif_dataset
-{
-	local out nds vers found ii
-	out=$(sh_clapi_curl $sh_clapiurl/datasets)
-
-	[[ $? -eq 0 ]] || fail "failed to list datasets"	
-	found=
-	nds=$(echo "$out" | json length)
-	for ((ii=0; ii < $nds; ii++)); do
-		vers=$(echo "$out" | json [$ii].urn | \
-		    nawk 'BEGIN { FS=":" } ; { print $3"-"$4 }')
-		[[ $? -eq 0 ]] || fail "Failed to get data"
-		if [[ "$sh_dataset-$sh_dataset_vers" == $vers ]]; then
-			found="true"
-			sh_dsurn=$(echo "$out" | json [$ii].urn)
-		fi
-	done
-
-	[[ ! -z "$found" ]]
-}
-
-function sh_provision
-{
-	local out id cont
-
-        out=$(sh_clapi_curl $sh_clapiurl/machines -X POST \
-	    -d dataset=$sh_dsurn -d package="$sh_package" name="$sh_repo")
-
-	id=$(echo "$out" | json id )
-	[[ -n $id ]] || fail "no uuid: $out"
-
-	cont=
-	while [[ -z $cont ]]; do
-		sleep 1
-		out=$(sh_clapi_curl $sh_clapiurl/machines/$id)
-        	[[ $? -eq 0 ]] || fail "failed to get machine state: $out"
-		state=$(echo "$out" | json state)
-		[[ -n $state ]] || fail "no state: $out"
-		if [[ "running" == $state ]]; then
-			cont="false"
-		fi
-		echo ".\c "
-	done
-
-	sh_zid=$id
-	sh_zoneip=$(echo "$out" | json ips[0])
-	[[ -n "$sh_zoneip" ]] || fail "Couldn't get zone ip"
-}
-
-function sh_delete
-{
-	local out cont
-	sh_clapi_curl $sh_clapiurl/machines/$sh_zid -X POST -d action="stop"
-
-	cont=
-	while [[ -z $cont ]]; do
-		sleep 1
-		out=$(sh_clapi_curl $sh_clapiurl/machines/$sh_zid)
-        	[[ $? -eq 0 ]] || fail "failed to get machine state: $out"
-		state=$(echo "$out" | json state)
-		[[ -n $state ]] || fail "no state: $out"
-		if [[ "stopped" == $state ]]; then
-			cont="false"
-		fi
-	done
-	sh_clapi_curl $sh_clapiurl/machines/$sh_zid -X DELETE
-}
-
-[[ -n $sh_zone ]] || fail "missing requsted zone"
-[[ -n $sh_sha ]] || fail "missing requested branch"
-sh_load_config || fail "failed to load configuration"
-sh_verif_dataset || fail "Missing dataset $sh_dataset"
-sh_verif_package || fail "Missing package $sh_package"
-sh_provision 
-
-sleep 5
-ssh $sh_sshopts root@$sh_zoneip pkgin -y install $sh_pkgsrc
-[[ $? -eq 0 ]] || fail "failed to install packages"
-cat > sshconfig.$$ <<EOF
-host git.joyent.com
-	StrictHostKeyChecking no
-	UserKnownHostsFile=/dev/null
-EOF
-scp $sh_sshopts sshconfig.$$ root@$sh_zoneip:~/.ssh/config
-[[ $? -eq 0 ]] || fail "failed to set .ssh/config"
-rm -f sshconfig.$$
-scp $sh_sshopts $sh_sshkey root@$sh_zoneip:~/.ssh/id_rsa
-[[ $? -eq 0 ]] || fail "failed to copy private key"
-
-ssh $sh_sshopts root@$sh_zoneip git clone git@git.joyent.com:$sh_repo.git
-[[ $? -eq 0 ]] || fail "failed to clone repo"
-
-ssh $sh_sshopts root@$sh_zoneip "cd $sh_repo && git checkout $sh_sha"
-[[ $? -eq 0 ]] || fail "failed to clone repo"
-
-ssh $sh_sshopts root@$sh_zoneip BRANCH=$BRANCH TIMESTAMP=$TIMESTAMP gmake -C $sh_repo release
-[[ $? -eq 0 ]] || fail "failed to build release target"
-
-ssh $sh_sshopts root@$sh_zoneip BRANCH=$BRANCH TIMESTAMP=$TIMESTAMP BITS_DIR=$sh_bits gmake -C $sh_repo publish
-[[ $? -eq 0 ]] || fail "failed to build release target"
-
-mkdir -p $sh_output 2>&1 >/dev/null
-scp $sh_sshopts -r root@$sh_zoneip:~/$sh_repo/$sh_bits/* $sh_output/
-[[ $? -eq 0 ]] || fail "failed to retrieve release object"
-
-sh_delete
-exit 0
diff --git a/tools/check-repos-for-release b/tools/check-repos-for-release
index 0a45ea3..cf22318 100755
--- a/tools/check-repos-for-release
+++ b/tools/check-repos-for-release
@@ -6,12 +6,12 @@
 #
 
 #
-# Copyright (c) 2015, Joyent, Inc.
+# Copyright 2019 Joyent, Inc.
 #
 
 #
-# Check if the SDC repos (those listed in targets.json) are prepared (branched
-# as appropriate) for an SDC release build. Will also apply
+# Check if the Triton repos are prepared (branched
+# as appropriate) for an Triton/SmartOS release build. Will also apply
 # missing branch with the '-a' flag.
 #
 #
@@ -20,6 +20,7 @@ if [[ -n "$TRACE" ]]; then
     export PS4='[\D{%FT%TZ}] ${BASH_SOURCE}:${LINENO}: ${FUNCNAME[0]:+${FUNCNAME[0]}(): }'
     set -o xtrace
 fi
+
 set -o errexit
 set -o pipefail
 
@@ -43,7 +44,7 @@ function usage() {
     echo "  -c CREDS        Provide creds for mo.joyent.com. If not given"
     echo "                  will use 'MOLYBDENUM_CREDENTIALS' envvar."
     echo "  -a              Create branch (*A*pply) if it is missing"
-    echo "  -t TARGET       Just run for the given MG target. If not given"
+    echo "  -t TARGET       Just run for the given Jenkins target. If not given"
     echo "                  then all targets in 'targets.json' are used. Can be"
     echo "                  specified multiple times."
     echo ""
@@ -54,17 +55,15 @@ function usage() {
     exit 1
 }
 
-function fatal {
-    echo "$(basename $0): error: $1"
-    exit 1
-}
-
 function get_repo_url_from_targ {
   local targ=$1
-  local urls=$(cat ${TOP}/targets.json | ${TOP}/tools/json $targ | grep -v undefined | ${TOP}/tools/json repos | ${TOP}/tools/json -a url )
+  if [[ -z "$targ" ]]; then
+    targ='*'
+  fi
+  local urls=$(jr list -H -l mg=$targ -o httpsCloneUrl)
   local ret=""
   for url in $urls; do
-   ret="$ret $(echo $url)" # | cut -d ':' -f2 | xargs -I {} basename {} .git)"
+   ret="$ret $(echo $url)"
   done
   echo $ret
 }
@@ -107,7 +106,7 @@ function check_repo_url {
     fi
     if [[ -z "$hit" ]]; then
         hit=$(curl -sS https://mo.joyent.com/api/repos/$repo_name/refs -u $CREDS \
-            | $TOP/tools/json branches \
+            | json branches \
             | grep "\"$BRANCH\"" \
             || true)
     fi
@@ -178,21 +177,12 @@ if [[ -z "$CREDS" ]]; then
     usage "no mo.joyent.com crendentials: use '-c CREDS' or set MOLYBDENUM_CREDENTIALS"
 fi
 
-(cd $TOP && JOYENT_BUILD=true bash <targets.json.in >targets.json)
 if [[ -z "$TARGETS" ]]; then
-    TARGETS=$(node -e "var fs = require('fs');
-        var targets = JSON.parse(fs.readFileSync('$TOP/targets.json'));
-        Object.keys(targets).forEach( function(targ) { console.log(targ) });" \
-        | grep -v sdcsso \
-        | xargs)
+    # gets a list of the 'mg' label, which is the jenkins job name.
+    TARGETS=$(jr list -H -l mg='*' -o labels | json -ag mg | sort -u \
+        grep -v sdcsso)
 fi
 
-# Get some "low on the food chain" repos first (RELENG-552).
-check_repo_url git@github.com:joyent/mountain-gorilla.git '(none)'
-check_repo_url git@github.com:joyent/registrar.git '(none)'
-check_repo_url git@github.com:joyent/sdc-config-agent.git '(none)'
-check_repo_url git@github.com:joyent/sdc-amon.git '(none)'
-
 for targ in $TARGETS; do
   if [[ $targ == 'all' ]]; then
     continue
diff --git a/tools/clean-image.sh b/tools/clean-image.sh
deleted file mode 100644
index 6b5c616..0000000
--- a/tools/clean-image.sh
+++ /dev/null
@@ -1,59 +0,0 @@
-#!/usr/bin/bash
-#
-# This Source Code Form is subject to the terms of the Mozilla Public
-# License, v. 2.0. If a copy of the MPL was not distributed with this
-# file, You can obtain one at http://mozilla.org/MPL/2.0/.
-#
-
-#
-# Copyright (c) 2014, Joyent, Inc.
-#
-
-set -o xtrace
-
-# removes old files and attempts to restore the machine to a
-# consistent (clean) state. It removes known sensitive files.
-
-cleanup() {
-
-  # clean up old log files
-  echo "==> removing tmp and log files"
-  rm -rf /var/svc/log/*
-  find /var/log -type f | xargs -n1 cp /dev/null
-  find /var/adm -type f | xargs rm -f
-  find /var/db/pkgin -type f | grep -v pkgin.db | xargs rm -f
-  find /var/cron -type f | xargs rm -f
-  rm -f /var/spool/postfix/deferred/*
-
-  echo "==> removing /var/svc/provision* from image"
-  rm -f /var/svc/provision*
-
-  # touch necessary log files
-  touch /var/adm/wtmpx
-
-  # unset passwords that may have been mistakenly left
-  echo "==> unsetting 'root' and 'admin' passwords"
-  out=$(passwd -N root)
-  out=$(passwd -N admin)
-
-  # remove old ssh keys
-  echo "==> removing old ssh host keys"
-  rm -f /etc/ssh/ssh_*key*
-
-  echo "==> cleaning up old network configuration"
-  # remove old network configuration files
-  echo "::1        localhost"          > /etc/hosts
-  echo "127.0.0.1  localhost loghost" >> /etc/hosts
-
-  # interface configuration files
-  find /etc/hostname.net* | xargs rm -f
-
-  # remove zoneconfig
-  if [[ -e /root/zoneconfig ]]; then
-     rm -f /root/zoneconfig
-  fi
-}
-
-cleanup
-
-exit 0
diff --git a/tools/jenkins-build b/tools/jenkins-build
deleted file mode 100755
index d85381a..0000000
--- a/tools/jenkins-build
+++ /dev/null
@@ -1,233 +0,0 @@
-#!/bin/bash
-#
-# This Source Code Form is subject to the terms of the Mozilla Public
-# License, v. 2.0. If a copy of the MPL was not distributed with this
-# file, You can obtain one at http://mozilla.org/MPL/2.0/.
-#
-
-#
-# Copyright (c) 2018, Joyent, Inc.
-#
-
-#
-# This script lives at <mountain-gorilla.git/tools/jenkins-build>. It is
-# meant to be used for building SDC components in jenkins jobs. Like this:
-#
-#        set -o errexit
-#        set -o pipefail
-#
-#        # Poorman's backup of last build run.
-#        rm -rf MG.last
-#        mkdir -p MG && mv MG MG.last
-#        rm -rf MG
-#
-#        git clone git@github.com:joyent/mountain-gorilla.git MG
-#        cp MG/tools/jenkins-build ./
-#        ./jenkins-build
-#
-# E.g. https://jenkins.joyent.us/job/imgapi/configure
-#
-
-set -o errexit
-unset LD_LIBRARY_PATH   # ensure don't get Java's libs (see OS-703)
-
-
-#---- globals, config
-
-# The Manta directory that build artifacts are uploaded to.  The same
-# name is used for both public (under ~~/public) and private (under
-# ~~/stor) artifacts.
-MG_JENKINS_BUILDS_DIR=${MG_JENKINS_BUILDS_DIR:-builds}
-
-#
-# Refuse to build if /tmp is tmpfs until OS-5608 is fixed.
-# Try to umount if mounted, if that fails: blow up.
-#
-if mount | grep ^/tmp; then
-    if ! umount /tmp; then
-        echo "FATAL: refusing to use mounted /tmp (See: RELENG-711)" >&2
-        exit 1
-    fi
-fi
-
-# ---- First pass:
-# 1. parse params
-# 2. change MG branch (if necessary) and restart
-if [[ -z "$MG_JENKINS_BUILD_SECOND_PASS" ]]; then
-    echo ""
-    echo "#---------------------- start first pass"
-
-    # Note: As of TOOLS-659, use of 'payload' in hooks is deprecated.
-    #
-    # If "payload" is defined, we presume this is from a post-receive hook.
-    # `payload.ref` will defined the git branch. For "release-YYYYMMDD" branches
-    # (the Joyent engineering convention for release branches) we'll be strict
-    # and have:
-    #   TRY_BRANCH=  BRANCH=$branch
-    # but for other branches we'll be "nice" and use
-    #   TRY_BRANCH=$branch  BRANCH=master
-    # which allows, for example, a commit to a feature branch (say "foo") to
-    # work when ancillary repos (like mountain-gorilla.git and usb-headnode.git)
-    # don't have that branch.
-    #
-    if [[ -n "$payload" ]]; then
-        ref=$(echo "$payload" | json ref)
-        if [[ $(echo "$ref" | cut -d/ -f2) != "heads" ]]; then
-            echo "error: unexpected ref '$ref': is not 'refs/heads'"
-            exit 1
-        fi
-        BRANCH=$(echo "$ref" | cut -d/ -f3)
-        if [[ -z "$(echo $BRANCH | egrep '^release-[0-9]+' || true)" ]]; then
-            TRY_BRANCH=$BRANCH
-            BRANCH=master
-        fi
-    fi
-    if [[ -z "$BRANCH" ]]; then
-        BRANCH=master
-    fi
-
-    #
-    # We impose a restriction on BRANCH being a "short form" branch, so
-    # require "master" rather than "refs/remotes/origin/master" by
-    # modifying the given value with /usr/bin/basename. This matches the way
-    # components compute $BRANCH from Makefile.defs. Otherwise, MG
-    # would attempt to lookup non-existent paths in Manta, e.g.
-    # /Joyent_Dev/public/builds/registrar/refs/remotes/origin/master-latest
-    # rather than /Joyent_Dev/public/builds/registrar/master-latest
-    #
-    if [[ -n "$BRANCH" ]]; then
-        BRANCH=$(/usr/bin/basename $BRANCH)
-    fi
-    if [[ -n "$TRY_BRANCH" ]]; then
-        TRY_BRANCH=$(/usr/bin/basename $TRY_BRANCH)
-    fi
-    export BRANCH TRY_BRANCH
-    echo "BRANCH: $BRANCH"
-    echo "TRY_BRANCH: $TRY_BRANCH"
-
-    cd MG
-    if [[ -n "$TRY_BRANCH" ]]; then
-        mg_has_try_branch=$(git branch -a | cut -c3- \
-            | awk '{print $1}' | xargs -n1 basename \
-            | (grep "^$TRY_BRANCH\$" || true))
-        if [[ -n "$mg_has_try_branch" ]]; then
-            git checkout $TRY_BRANCH
-        else
-            git checkout $BRANCH
-        fi
-    else
-        git checkout $BRANCH
-    fi
-
-    # Restart jenkins-build now on the correct MG branch.
-    export MG_JENKINS_BUILD_SECOND_PASS=1
-    exec ./tools/jenkins-build
-fi
-
-
-# ---- Second pass: Do the build.
-
-echo ""
-echo "#---------------------- start second pass"
-
-start_time=$(date +%s)
-last_time=${start_time}
-
-LOG=build.log
-touch $LOG
-exec > >(tee ${LOG}) 2>&1
-
-
-echo ""
-echo "#---------------------- env"
-
-echo "MG repo commit sha: $(git describe --all --long --dirty | awk -F'-g' '{print $NF}')"
-date
-pwd
-whoami
-env
-
-
-
-echo ""
-echo "#---------------------- configure"
-
-#
-# The target defaults to the job name (e.g. platform-debug), but a job can also
-# directly specify MG_TARGET as a parameter to over-ride this.
-#
-[[ -z "$MG_TARGET" ]] && MG_TARGET=$JOB_NAME
-
-[[ -z "$BRANCH" ]] && BRANCH=master
-# Note the "-c" to use a cache dir one up, i.e. shared between builds of this job.
-CACHE_DIR=$(cd ../; pwd)/cache
-if [[ "$CLEAN_CACHE" == "true" ]]; then
-    rm -rf $CACHE_DIR
-fi
-
-# Builds that are joyent-specific get uploaded to a directory under
-# /stor whereas all other builds now go under /public.  The specific
-# directory is controlled by MG_JENKINS_BUILDS_DIR which defaults to
-# "builds". In either case, dependencies come from the public directory.
-if [[ ! -f targets.json ]]; then
-    bash < targets.json.in | json > targets.json
-fi
-IS_PUBLIC=$(json < targets.json ${MG_TARGET}.public)
-if [[ ${IS_PUBLIC} == "true" ]]; then
-    JENKINS_OUTPUT_BASE="/public/${MG_JENKINS_BUILDS_DIR}"
-else
-    JENKINS_OUTPUT_BASE="/stor/${MG_JENKINS_BUILDS_DIR}"
-fi
-
-EXTRA_ARGS=""
-
-if [[ ! -z ${USE_PREV_RELEASES_FOR} ]]; then
-    EXTRA_ARGS="${EXTRA_ARGS}-R ${USE_PREV_RELEASES_FOR}"
-fi
-
-if [[ -z ${JOYENT_BUILD} || ${JOYENT_BUILD} == "true" ]]; then
-    TRACE=1 ./configure -j -t $MG_TARGET -c "$CACHE_DIR" -b "$BRANCH" \
-        -J /stor/builds -D /public/builds -O ${JENKINS_OUTPUT_BASE} \
-        -g "$MG_GERRIT_CR" -B "$TRY_BRANCH" ${EXTRA_ARGS}
-else
-    TRACE=1 ./configure -t $MG_TARGET -c "$CACHE_DIR" -b "$BRANCH" \
-        -D /public/builds -O ${JENKINS_OUTPUT_BASE} \
-        -g "$MG_GERRIT_CR" -B "$TRY_BRANCH" \
-        ${EXTRA_ARGS}
-fi
-
-now_time=$(date +%s)
-elapsed=$((${now_time} - ${last_time}))
-last_time=${now_time}
-echo "TIME: MG configure took ${elapsed} seconds"
-
-
-
-echo ""
-echo "#---------------------- make"
-
-V=1 gmake $MG_TARGET
-
-now_time=$(date +%s)
-elapsed=$((${now_time} - ${last_time}))
-last_time=${now_time}
-echo "TIME: build took ${elapsed} seconds"
-
-
-
-echo ""
-echo "#---------------------- upload"
-
-cp $LOG bits/$MG_TARGET/
-gmake manta_upload_jenkins
-
-if [[ ${MG_TARGET} == "smartos" ]] && [[ ${BRANCH} == release* ]]; then
-    gmake smartos-release
-fi
-
-gmake jenkins_publish_image
-
-now_time=$(date +%s)
-elapsed=$((${now_time} - ${last_time}))
-last_time=${now_time}
-echo "TIME: upload took ${elapsed} seconds"
diff --git a/tools/jenkins-downstream-projects-for b/tools/jenkins-downstream-projects-for
deleted file mode 100755
index a0c9ed1..0000000
--- a/tools/jenkins-downstream-projects-for
+++ /dev/null
@@ -1,52 +0,0 @@
-#!/usr/bin/env /bin/bash
-#
-# List downstream projects for the give targets. This is intended to be used
-# to update the Jenkins job config:
-#
-#   Post-build Actions
-#       > Trigger parameteried build on other projects
-#           > Projects to build
-#
-# Assumption: MG target names match Jenkins job names. I think that is currently
-# always the case.
-#
-# Usage:
-#   ./tools/jenkins-downstream-projects-for TARGET
-#
-# Example:
-#   ./tools/jenkins-downstream-projects-for registrar
-#
-
-if [[ -n "$TRACE" ]]; then
-    export PS4='[\D{%FT%TZ}] ${BASH_SOURCE}:${LINENO}: ${FUNCNAME[0]:+${FUNCNAME[0]}(): }'
-    set -o xtrace
-fi
-set -o errexit
-set -o pipefail
-
-
-#---- support stuff
-
-function fatal
-{
-    echo "$0: fatal error: $*"
-    exit 1
-}
-
-#---- mainline
-
-
-TARGET=$1
-[[ -n "$TARGET" ]] || fatal "missing TARGET argument"
-
-
-echo "# Update 'Projects to build' at:" >&2
-echo "#     https://jenkins.joyent.us/job/$TARGET/configure" >&2
-echo "# with the following:" >&2
-
-JOYENT_BUILD=true bash targets.json.in >targets.json
-# Skip 'headnode*' builds because we just build them periodically. They are
-# huge and less often used.
-json -f targets.json -M -a key \
-    -c "~this.value.deps.indexOf('$TARGET') && this.key.indexOf('headnode') !== 0" \
-    | sort | xargs | sed -e 's/ /,/g'
diff --git a/tools/json b/tools/json
deleted file mode 100755
index 1ea071c..0000000
--- a/tools/json
+++ /dev/null
@@ -1,1675 +0,0 @@
-#!/usr/bin/env node
-/**
- * Copyright (c) 2014 Trent Mick. All rights reserved.
- * Copyright (c) 2014 Joyent Inc. All rights reserved.
- *
- * json -- JSON love for your command line.
- *
- * See <https://github.com/trentm/json> and <https://trentm.com/json/>
- */
-
-var VERSION = '9.0.1';
-
-var p = console.warn;
-var util = require('util');
-var assert = require('assert');
-var path = require('path');
-var vm = require('vm');
-var fs = require('fs');
-var warn = console.warn;
-var EventEmitter = require('events').EventEmitter;
-
-
-
-//--- exports for module usage
-
-exports.main = main;
-exports.getVersion = getVersion;
-exports.parseLookup = parseLookup;
-
-// As an exported API, these are still experimental:
-exports.lookupDatum = lookupDatum;
-exports.printDatum = printDatum; // DEPRECATED
-
-
-
-//---- globals and constants
-
-// Output modes.
-var OM_JSONY = 1;
-var OM_JSON = 2;
-var OM_INSPECT = 3;
-var OM_COMPACT = 4;
-var OM_FROM_NAME = {
-    'jsony': OM_JSONY,
-    'json': OM_JSON,
-    'inspect': OM_INSPECT,
-    'compact': OM_COMPACT
-}
-
-
-
-//---- support functions
-
-function getVersion() {
-    return VERSION;
-}
-
-/**
- * Return a *shallow* copy of the given object.
- *
- * Only support objects that you get out of JSON, i.e. no functions.
- */
-function objCopy(obj) {
-    var copy;
-    if (Array.isArray(obj)) {
-        copy = obj.slice();
-    } else if (typeof (obj) === 'object') {
-        copy = {};
-        Object.keys(obj).forEach(function (k) {
-            copy[k] = obj[k];
-        });
-    } else {
-        copy = obj; // immutable type
-    }
-    return copy;
-}
-
-if (util.format) {
-    format = util.format;
-} else {
-    // From <https://github.com/joyent/node/blob/master/lib/util.js#L22>:
-    var formatRegExp = /%[sdj%]/g;
-
-    function format(f) {
-        if (typeof (f) !== 'string') {
-            var objects = [];
-            for (var i = 0; i < arguments.length; i++) {
-                objects.push(util.inspect(arguments[i]));
-            }
-            return objects.join(' ');
-        }
-        var i = 1;
-        var args = arguments;
-        var len = args.length;
-        var str = String(f).replace(formatRegExp, function (x) {
-            if (i >= len)
-              return x;
-            switch (x) {
-            case '%s':
-                return String(args[i++]);
-            case '%d':
-                return Number(args[i++]);
-            case '%j':
-                return JSON.stringify(args[i++]);
-            case '%%':
-                return '%';
-            default:
-                return x;
-            }
-        });
-        for (var x = args[i]; i < len; x = args[++i]) {
-            if (x === null || typeof (x) !== 'object') {
-                str += ' ' + x;
-            } else {
-                str += ' ' + util.inspect(x);
-            }
-        }
-        return str;
-    };
-}
-
-/**
- * Parse the given string into a JS string. Basically: handle escapes.
- */
-function _parseString(s) {
-    /* JSSTYLED */
-    var quoted = '"' + s.replace(/\\"/, '"').replace('"', '\\"') + '"';
-    return eval(quoted);
-}
-
-// json_parse.js (<https://github.com/douglascrockford/JSON-js>)
-/* BEGIN JSSTYLED */
-// START json_parse
-var json_parse=function(){"use strict";var a,b,c={'"':'"',"\\":"\\","/":"/",b:"\b",f:"\f",n:"\n",r:"\r",t:"\t"},d,e=function(b){throw{name:"SyntaxError",message:b,at:a,text:d}},f=function(c){return c&&c!==b&&e("Expected '"+c+"' instead of '"+b+"'"),b=d.charAt(a),a+=1,b},g=function(){var a,c="";b==="-"&&(c="-",f("-"));while(b>="0"&&b<="9")c+=b,f();if(b==="."){c+=".";while(f()&&b>="0"&&b<="9")c+=b}if(b==="e"||b==="E"){c+=b,f();if(b==="-"||b==="+")c+=b,f();while(b>="0"&&b<="9")c+=b,f()}a=+c;if(!isFinite(a))e("Bad number");else return a},h=function(){var a,d,g="",h;if(b==='"')while(f()){if(b==='"')return f(),g;if(b==="\\"){f();if(b==="u"){h=0;for(d=0;d<4;d+=1){a=parseInt(f(),16);if(!isFinite(a))break;h=h*16+a}g+=String.fromCharCode(h)}else if(typeof c[b]=="string")g+=c[b];else break}else g+=b}e("Bad string")},i=function(){while(b&&b<=" ")f()},j=function(){switch(b){case"t":return f("t"),f("r"),f("u"),f("e"),!0;case"f":return f("f"),f("a"),f("l"),f("s"),f("e"),!1;case"n":return f("n"),f("u"),f("l"),f("l"),null}e("Unexpected '"+b+"'")},k,l=function(){var a=[];if(b==="["){f("["),i();if(b==="]")return f("]"),a;while(b){a.push(k()),i();if(b==="]")return f("]"),a;f(","),i()}}e("Bad array")},m=function(){var a,c={};if(b==="{"){f("{"),i();if(b==="}")return f("}"),c;while(b){a=h(),i(),f(":"),Object.hasOwnProperty.call(c,a)&&e('Duplicate key "'+a+'"'),c[a]=k(),i();if(b==="}")return f("}"),c;f(","),i()}}e("Bad object")};return k=function(){i();switch(b){case"{":return m();case"[":return l();case'"':return h();case"-":return g();default:return b>="0"&&b<="9"?g():j()}},function(c,f){var g;return d=c,a=0,b=" ",g=k(),i(),b&&e("Syntax error"),typeof f=="function"?function h(a,b){var c,d,e=a[b];if(e&&typeof e=="object")for(c in e)Object.prototype.hasOwnProperty.call(e,c)&&(d=h(e,c),d!==undefined?e[c]=d:delete e[c]);return f.call(a,b,e)}({"":g},""):g}}();
-// END json_parse
-/* END JSSTYLED */
-
-function printHelp() {
-    /* BEGIN JSSTYLED */
-    var w = console.log;
-    w('Usage:');
-    w('  <something generating JSON on stdout> | json [OPTIONS] [LOOKUPS...]');
-    w('  json -f FILE [OPTIONS] [LOOKUPS...]');
-    w('');
-    w('Pipe in your JSON for pretty-printing, JSON validation, filtering, ');
-    w('and modification. Supply one or more `LOOKUPS` to extract a ');
-    w('subset of the JSON. HTTP header blocks are skipped by default.');
-    w('Roughly in order of processing, features are:');
-    w('');
-    w('Grouping:');
-    w('  Use "-g" or "--group" to group adjacent objects, separated by');
-    w('  by no space or a by a newline, or adjacent arrays, separate by');
-    w('  by a newline. This can be helpful for, e.g.: ');
-    w('     $ cat *.json | json -g ... ');
-    w('  and similar.');
-    w('');
-    w('Execution:');
-    w('  Use the "-e CODE" option to execute JavaScript code on the input JSON.');
-    w('     $ echo \'{"name":"trent","age":38}\' | json -e \'this.age++\'');
-    w('     {');
-    w('       "name": "trent",');
-    w('       "age": 39');
-    w('     }');
-    w('  If input is an array, this will automatically process each');
-    w('  item separately.');
-    w('');
-    w('Conditional filtering:');
-    w('  Use the "-c CODE" option to filter the input JSON.');
-    w('     $ echo \'[{"age":38},{"age":4}]\' | json -c \'this.age>21\'');
-    w('     [{\'age\':38}]');
-    w('  If input is an array, this will automatically process each');
-    w('  item separately. Note: "CODE" is JavaScript code.');
-    w('');
-    w('Lookups:');
-    w('  Use lookup arguments to extract particular values:');
-    w('     $ echo \'{"name":"trent","age":38}\' | json name');
-    w('     trent');
-    w('');
-    w('  Use "-a" for *array processing* of lookups and *tabular output*:');
-    w('     $ echo \'{"name":"trent","age":38}\' | json name age');
-    w('     trent');
-    w('     38');
-    w('     $ echo \'[{"name":"trent","age":38},');
-    w('               {"name":"ewan","age":4}]\' | json -a name age');
-    w('     trent 38');
-    w('     ewan 4');
-    w('');
-    w('In-place editing:');
-    w('  Use "-I, --in-place" to edit a file in place:');
-    w('     $ json -I -f config.json  # reformat');
-    w('     $ json -I -f config.json -c \'this.logLevel="debug"\' # add field');
-    w('');
-    w('Pretty-printing:');
-    w('  Output is "jsony" by default: 2-space indented JSON, except a');
-    w('  single string value is printed without quotes.');
-    w('     $ echo \'{"name": "trent", "age": 38}\' | json');
-    w('     {');
-    w('       "name": "trent",');
-    w('       "age": 38');
-    w('     }');
-    w('     $ echo \'{"name": "trent", "age": 38}\' | json name');
-    w('     trent');
-    w('');
-    w("  Use '-j' or '-o json' for explicit JSON, '-o json-N' for N-space indent:");
-    w('     $ echo \'{"name": "trent", "age": 38}\' | json -o json-0');
-    w('     {"name":"trent","age":38}');
-    w('');
-    w('Options:');
-    w('  -h, --help    Print this help info and exit.');
-    w('  --version     Print version of this command and exit.');
-    w('  -q, --quiet   Don"t warn if input isn"t valid JSON.');
-    w('');
-    w('  -f FILE       Path to a file to process. If not given, then');
-    w('                stdin is used.');
-    w('  -I, --in-place  In-place edit of the file given with "-f".');
-    w('                Lookups are not allow with in-place editing');
-    w('                because it makes it too easy to lose content.');
-    w('');
-    w('  -H            Drop any HTTP header block (as from `curl -i ...`).');
-    w('  -g, --group   Group adjacent objects or arrays into an array.');
-    w('  --merge       Merge adjacent objects into one. Keys in last ');
-    w('                object win.');
-    w('  --deep-merge  Same as "--merge", but will recurse into objects ');
-    w('                under the same key in both.')
-    w('  -a, --array   Process input as an array of separate inputs');
-    w('                and output in tabular form.');
-    w('  -A            Process input as a single object, i.e. stop');
-    w('                "-e" and "-c" automatically processing each');
-    w('                item of an input array.');
-    w('  -d DELIM      Delimiter char for tabular output (default is " ").');
-    w('  -D DELIM      Delimiter char between lookups (default is "."). E.g.:');
-    w('                    $ echo \'{"a.b": {"b": 1}}\' | json -D / a.b/b');
-    w('');
-    w('  -M, --items   Itemize an object into an array of ');
-    w('                    {"key": <key>, "value": <value>}');
-    w('                objects for easier processing.');
-    w('');
-    w('  -e CODE       Execute the given JavaScript code on the input. If input');
-    w('                is an array, then each item of the array is processed');
-    w('                separately (use "-A" to override).');
-    w('  -c CODE       Filter the input with JavaScript `CODE`. If `CODE`');
-    w('                returns false-y, then the item is filtered out. If');
-    w('                input is an array, then each item of the array is ');
-    w('                processed separately (use "-A" to override).');
-    w('');
-    w('  -k, --keys    Output the input object\'s keys.');
-    w('  -n, --validate  Just validate the input (no processing or output).');
-    w('                Use with "-q" for silent validation (exit status).');
-    w('');
-    w('  -o, --output MODE');
-    w('                Specify an output mode. One of:');
-    w('                    jsony (default): JSON with string quotes elided');
-    w('                    json: JSON output, 2-space indent');
-    w('                    json-N: JSON output, N-space indent, e.g. "json-4"');
-    w('                    inspect: node.js `util.inspect` output');
-    w('  -i            Shortcut for `-o inspect`');
-    w('  -j            Shortcut for `-o json`');
-    w('  -0, -2, -4    Set indentation to the given value w/o setting MODE.');
-    w('                    -0   =>  -o jsony-0');
-    w('                    -4   =>  -o jsony-4');
-    w('                    -j0  =>  -o json-0');
-    w('');
-    w('See <http://trentm.com/json> for more docs and ');
-    w('<https://github.com/trentm/json> for project details.');
-    /* END JSSTYLED */
-}
-
-
-/**
- * Parse the command-line options and arguments into an object.
- *
- *    {
- *      'args': [...]       // arguments
- *      'help': true,       // true if '-h' option given
- *       // etc.
- *    }
- *
- * @return {Object} The parsed options. `.args` is the argument list.
- * @throws {Error} If there is an error parsing argv.
- */
-function parseArgv(argv) {
-    var parsed = {
-        args: [],
-        help: false,
-        quiet: false,
-        dropHeaders: false,
-        exeSnippets: [],
-        condSnippets: [],
-        outputMode: OM_JSONY,
-        jsonIndent: 2,
-        array: null,
-        delim: ' ',
-        lookupDelim: '.',
-        items: false,
-        outputKeys: false,
-        group: false,
-        merge: null, // --merge -> 'shallow', --deep-merge -> 'deep'
-        inputFiles: [],
-        validate: false,
-        inPlace: false
-    };
-
-    // Turn '-iH' into '-i -H', except for argument-accepting options.
-    var args = argv.slice(2); // drop ['node', 'scriptname']
-    var newArgs = [];
-    var optTakesArg = {
-        'd': true,
-        'o': true,
-        'D': true
-    };
-    for (var i = 0; i < args.length; i++) {
-        if (args[i] === '--') {
-            newArgs = newArgs.concat(args.slice(i));
-            break;
-        }
-        if (args[i].charAt(0) === '-' && args[i].charAt(1) !== '-' &&
-            args[i].length > 2)
-        {
-            var splitOpts = args[i].slice(1).split('');
-            for (var j = 0; j < splitOpts.length; j++) {
-                newArgs.push('-' + splitOpts[j])
-                if (optTakesArg[splitOpts[j]]) {
-                    var optArg = splitOpts.slice(j + 1).join('');
-                    if (optArg.length) {
-                        newArgs.push(optArg);
-                    }
-                    break;
-                }
-            }
-        } else {
-            newArgs.push(args[i]);
-        }
-    }
-    args = newArgs;
-
-    endOfOptions = false;
-    while (args.length > 0) {
-        var arg = args.shift();
-        if (endOfOptions) {
-            parsed.args.push(arg);
-            break;
-        }
-        switch (arg) {
-        case '--':
-            endOfOptions = true;
-            break;
-        case '-h': // display help and exit
-        case '--help':
-            parsed.help = true;
-            break;
-        case '--version':
-            parsed.version = true;
-            break;
-        case '-q':
-        case '--quiet':
-            parsed.quiet = true;
-            break;
-        case '-H': // drop any headers
-            parsed.dropHeaders = true;
-            break;
-        case '-o':
-        case '--output':
-            var name = args.shift();
-            if (!name) {
-                throw new Error('no argument given for "-o|--output" option');
-            }
-            var idx = name.lastIndexOf('-');
-            if (idx !== -1) {
-                var indent = name.slice(idx + 1);
-                if (/^\d+$/.test(indent)) {
-                    parsed.jsonIndent = Number(indent);
-                    name = name.slice(0, idx);
-                } else if (indent === 'tab') {
-                    parsed.jsonIndent = '\t';
-                    name = name.slice(0, idx);
-                }
-            }
-            parsed.outputMode = OM_FROM_NAME[name];
-            if (parsed.outputMode === undefined) {
-                throw new Error('unknown output mode: "' + name + '"');
-            }
-            break;
-        case '-0':
-            parsed.jsonIndent = 0;
-            break;
-        case '-2':
-            parsed.jsonIndent = 2;
-            break;
-        case '-4':
-            parsed.jsonIndent = 4;
-            break;
-        case '-I':
-        case '--in-place':
-            parsed.inPlace = true;
-            break;
-        case '-i': // output with util.inspect
-            parsed.outputMode = OM_INSPECT;
-            break;
-        case '-j': // output with JSON.stringify
-            parsed.outputMode = OM_JSON;
-            break;
-        case '-a':
-        case '--array':
-            parsed.array = true;
-            break;
-        case '-A':
-            parsed.array = false;
-            break;
-        case '-d':
-            parsed.delim = _parseString(args.shift());
-            break;
-        case '-D':
-            parsed.lookupDelim = args.shift();
-            if (parsed.lookupDelim.length !== 1) {
-                throw new Error(format(
-                    'invalid lookup delim "%s" (must be a single char)',
-                    parsed.lookupDelim));
-            }
-            break;
-        case '-e':
-        case '-E':  // DEPRECATED in v9
-            parsed.exeSnippets.push(args.shift());
-            break;
-        case '-c':
-        case '-C':  // DEPRECATED in v9
-            parsed.condSnippets.push(args.shift());
-            break;
-        case '-M':
-        case '--items':
-            parsed.items = true;
-            break;
-        case '-k':
-        case '--keys':
-            parsed.outputKeys = true;
-            break;
-        case '-g':
-        case '--group':
-            parsed.group = true;
-            break;
-        case '--merge':
-            parsed.merge = 'shallow';
-            break;
-        case '--deep-merge':
-            parsed.merge = 'deep';
-            break;
-        case '-f':
-            parsed.inputFiles.push(args.shift());
-            break;
-        case '-n':
-        case '--validate':
-            parsed.validate = true;
-            break;
-        default: // arguments
-            if (!endOfOptions && arg.length > 0 && arg[0] === '-') {
-                throw new Error('unknown option "' + arg + '"');
-            }
-            parsed.args.push(arg);
-            break;
-        }
-    }
-
-    if (parsed.group && parsed.merge) {
-        throw new Error('cannot use -g|--group and --merge options together');
-    }
-    if (parsed.outputKeys && parsed.args.length > 0) {
-        throw new Error(
-            'cannot use -k|--keys option and lookup arguments together');
-    }
-    if (parsed.inPlace && parsed.inputFiles.length !== 1) {
-        throw new Error('must specify exactly one file with "-f FILE" to ' +
-            'use -I/--in-place');
-    }
-    if (parsed.inPlace && parsed.args.length > 0) {
-        throw new Error('lookups cannot be specified with in-place editing ' +
-            '(-I/--in-place), too easy to lose content');
-    }
-
-    return parsed;
-}
-
-
-
-/**
- * Streams chunks from given file paths or stdin.
- *
- * @param opts {Object} Parsed options.
- * @returns {Object} An emitter that emits 'chunk', 'error', and 'end'.
- *    - `emit('chunk', chunk, [obj])` where chunk is a complete block of JSON
- *       ready to parse. If `obj` is provided, it is the already parsed
- *       JSON.
- *    - `emit('error', error)` when an underlying stream emits an error
- *    - `emit('end')` when all streams are done
- */
-function chunkEmitter(opts) {
-    var emitter = new EventEmitter();
-    var streaming = true;
-    var chunks = [];
-    var leftover = '';
-    var finishedHeaders = false;
-
-    function stripHeaders(s) {
-        // Take off a leading HTTP header if any and pass it through.
-        while (true) {
-            if (s.slice(0, 5) === 'HTTP/') {
-                var index = s.indexOf('\r\n\r\n');
-                var sepLen = 4;
-                if (index == -1) {
-                    index = s.indexOf('\n\n');
-                    sepLen = 2;
-                }
-                if (index != -1) {
-                    if (!opts.dropHeaders) {
-                        emit(s.slice(0, index + sepLen));
-                    }
-                    var is100Continue = (
-                        s.slice(0, 21) === 'HTTP/1.1 100 Continue');
-                    s = s.slice(index + sepLen);
-                    if (is100Continue) {
-                        continue;
-                    }
-                    finishedHeaders = true;
-                }
-            } else {
-                finishedHeaders = true;
-            }
-            break;
-        }
-        //console.warn('stripHeaders done, finishedHeaders=%s', finishedHeaders)
-        return s;
-    }
-
-    function emitChunks(block, emitter) {
-        //console.warn('emitChunks start: block="%s"', block)
-        /* JSSTYLED */
-        var splitter = /(})(\s*\n\s*)?({\s*")/;
-        var leftTrimmedBlock = block.trimLeft();
-        if (leftTrimmedBlock && leftTrimmedBlock[0] !== '{') {
-            // Currently only support streaming consecutive *objects*.
-            streaming = false;
-            chunks.push(block);
-            return '';
-        }
-        /**
-         * Example:
-         * > '{"a":"b"}\n{"a":"b"}\n{"a":"b"}'.split(/(})(\s*\n\s*)?({\s*")/)
-         * [ '{"a":"b"',
-         *   '}',
-         *   '\n',
-         *   '{"',
-         *   'a":"b"',
-         *   '}',
-         *   '\n',
-         *   '{"',
-         *   'a":"b"}' ]
-         */
-        var bits = block.split(splitter);
-        //console.warn('emitChunks: bits (length %d): %j', bits.length, bits);
-        if (bits.length === 1) {
-            /*
-             * An unwanted side-effect of using a regex to find
-             * newline-separated objects *with a regex*, is that we are looking
-             * for the end of one object leading into the start of a another.
-             * That means that we can end up buffering a complete object until
-             * a subsequent one comes in. If the input stream has large delays
-             * between objects, then this is unwanted buffering.
-             *
-             * One solution would be full stream parsing of objects a la
-             * <https://github.com/creationix/jsonparse>. This would nicely
-             * also remove the artibrary requirement that the input stream be
-             * newline separated. jsonparse apparently has some issues tho, so
-             * I don't want to use it right now. It also isn't *small* so not
-             * sure I want to inline it (`json` doesn't have external deps).
-             *
-             * An alternative: The block we have so far one of:
-             * 1. some JSON that we don't support grouping (e.g. a stream of
-             *    non-objects),
-             * 2. a JSON object fragment, or
-             * 3. a complete JSON object (with a possible trailing '{')
-             *
-             * If #3, then we can just emit this as a chunk right now.
-             *
-             * TODO(PERF): Try out avoiding the first more complete regex split
-             * for a presumed common case of single-line newline-separated JSON
-             * objects (e.g. a bunyan log).
-             */
-            // An object must end with '}'. This is an early out to avoid
-            // `JSON.parse` which I'm *presuming* is slower.
-            var trimmed = block.split(/\s*\r?\n/)[0];
-            if (trimmed[trimmed.length - 1] === '}') {
-                var obj;
-                try {
-                    obj = JSON.parse(block);
-                } catch (e) {
-                    /* pass through */
-                }
-                if (obj !== undefined) {
-                    // Emit the parsed `obj` to avoid re-parsing it later.
-                    emitter.emit('chunk', block, obj);
-                    block = '';
-                }
-            }
-            return block;
-        } else {
-            var n = bits.length - 2;
-            var s;
-            s = bits[0] + bits[1];
-            emitter.emit('chunk', s, JSON.parse(s));
-            for (var i = 3; i < n; i += 4) {
-                s = bits[i] + bits[i + 1] + bits[i + 2];
-                emitter.emit('chunk', s, JSON.parse(s));
-            }
-            return bits[n] + bits[n + 1];
-        }
-    }
-
-    function addDataListener(stream) {
-        stream.on('data', function (chunk) {
-            var s = leftover + chunk;
-            if (!finishedHeaders) {
-                s = stripHeaders(s);
-            }
-            if (!finishedHeaders) {
-                leftover = s;
-            } else {
-                if (!streaming) {
-                    chunks.push(chunk);
-                    return;
-                }
-                leftover = emitChunks(s, emitter);
-            }
-        });
-    }
-
-    if (opts.inputFiles.length > 0) {
-        // Stream each file in order.
-        var i = 0;
-
-        function addErrorListener(file) {
-            file.on('error', function (err) {
-                emitter.emit(
-                    'error',
-                    format('could not read "%s": %s', opts.inputFiles[i], e)
-                );
-            });
-        }
-
-        function addEndListener(file) {
-            file.on('end', function () {
-                if (i < opts.inputFiles.length) {
-                    var next = opts.inputFiles[i++];
-                    var nextFile = fs.createReadStream(next,
-                        {encoding: 'utf8'});
-                    addErrorListener(nextFile);
-                    addEndListener(nextFile);
-                    addDataListener(nextFile);
-                } else {
-                    if (!streaming) {
-                        emitter.emit('chunk', chunks.join(''));
-                    } else if (leftover) {
-                        leftover = emitChunks(leftover, emitter);
-                        emitter.emit('chunk', leftover);
-                    }
-                    emitter.emit('end');
-                }
-            });
-        }
-        var first = fs.createReadStream(opts.inputFiles[i++],
-            {encoding: 'utf8'});
-        addErrorListener(first);
-        addEndListener(first);
-        addDataListener(first);
-    } else {
-        // Streaming from stdin.
-        var stdin = process.openStdin();
-        stdin.setEncoding('utf8');
-        addDataListener(stdin);
-        stdin.on('end', function () {
-            if (!streaming) {
-                emitter.emit('chunk', chunks.join(''));
-            } else if (leftover) {
-                leftover = emitChunks(leftover, emitter);
-                emitter.emit('chunk', leftover);
-            }
-            emitter.emit('end');
-        });
-    }
-    return emitter;
-}
-
-/**
- * Get input from either given file paths or stdin. If `opts.inPlace` then
- * this calls the callback once for each `opts.inputFiles`.
- *
- * @param opts {Object} Parsed options.
- * @param callback {Function} `function (err, content, filename)` where err
- *    is an error string if there was a problem, `content` is the read
- *    content and `filename` is the associated file name from which content
- *    was loaded if applicable.
- */
-function getInput(opts, callback) {
-    if (opts.inputFiles.length === 0) {
-        // Read from stdin.
-        var chunks = [];
-
-        var stdin = process.openStdin();
-        stdin.setEncoding('utf8');
-        stdin.on('data', function (chunk) {
-            chunks.push(chunk);
-        });
-
-        stdin.on('end', function () {
-            callback(null, chunks.join(''));
-        });
-    } else if (opts.inPlace) {
-        for (var i = 0; i < opts.inputFiles.length; i++) {
-            var file = opts.inputFiles[i];
-            var content;
-            try {
-                content = fs.readFileSync(file, 'utf8');
-            } catch (e) {
-                callback(e, null, file);
-            }
-            if (content) {
-                callback(null, content, file);
-            }
-        }
-    } else {
-        // Read input files.
-        var i = 0;
-        var chunks = [];
-        try {
-            for (; i < opts.inputFiles.length; i++) {
-                chunks.push(fs.readFileSync(opts.inputFiles[i], 'utf8'));
-            }
-        } catch (e) {
-            return callback(
-                format('could not read "%s": %s', opts.inputFiles[i], e));
-        }
-        callback(null, chunks.join(''),
-            (opts.inputFiles.length === 1 ? opts.inputFiles[0] : undefined));
-    }
-}
-
-
-function isInteger(s) {
-    return (s.search(/^-?[0-9]+$/) == 0);
-}
-
-
-/**
- * Parse a lookup string into a list of lookup bits. E.g.:
- *
- *    'a.b.c' -> ["a","b","c"]
- *    'b["a"]' -> ["b","a"]
- *    'b["a" + "c"]' -> ["b","ac"]
- *
- * Optionally receives an alternative lookup delimiter (other than '.')
- */
-function parseLookup(lookup, lookupDelim) {
-    var debug = function () {};
-    //var debug = console.warn;
-
-    var bits = [];
-    debug('\n*** ' + lookup + ' ***');
-
-    bits = [];
-    lookupDelim = lookupDelim || '.';
-    var bit = '';
-    var states = [null];
-    var escaped = false;
-    var ch = null;
-    for (var i = 0; i < lookup.length; ++i) {
-        var escaped = (!escaped && ch === '\\');
-        var ch = lookup[i];
-        debug('-- i=' + i + ', ch=' + JSON.stringify(ch) + ' escaped=' +
-            JSON.stringify(escaped));
-        debug('states: ' + JSON.stringify(states));
-
-        if (escaped) {
-            bit += ch;
-            continue;
-        }
-
-        switch (states[states.length - 1]) {
-        case null:
-            switch (ch) {
-            case '"':
-            case '\'':
-                states.push(ch);
-                bit += ch;
-                break;
-            case '[':
-                states.push(ch);
-                if (bit !== '') {
-                    bits.push(bit);
-                    bit = ''
-                }
-                bit += ch;
-                break;
-            case lookupDelim:
-                if (bit !== '') {
-                    bits.push(bit);
-                    bit = ''
-                }
-                break;
-            default:
-                bit += ch;
-                break;
-            }
-            break;
-
-        case '[':
-            bit += ch;
-            switch (ch) {
-            case '"':
-            case '\'':
-            case '[':
-                states.push(ch);
-                break;
-            case ']':
-                states.pop();
-                if (states[states.length - 1] === null) {
-                    var evaled = vm.runInNewContext(
-                        '(' + bit.slice(1, -1) + ')', {}, '<lookup>');
-                    bits.push(evaled);
-                    bit = ''
-                }
-                break;
-            }
-            break;
-
-        case '"':
-            bit += ch;
-            switch (ch) {
-            case '"':
-                states.pop();
-                if (states[states.length - 1] === null) {
-                    bits.push(bit);
-                    bit = ''
-                }
-                break;
-            }
-            break;
-
-        case '\'':
-            bit += ch;
-            switch (ch) {
-            case '\'':
-                states.pop();
-                if (states[states.length - 1] === null) {
-                    bits.push(bit);
-                    bit = ''
-                }
-                break;
-            }
-            break;
-        }
-        debug('bit: ' + JSON.stringify(bit));
-        debug('bits: ' + JSON.stringify(bits));
-    }
-
-    if (bit !== '') {
-        bits.push(bit);
-        bit = ''
-    }
-
-    // Negative-intify: strings that are negative ints we change to a Number for
-    // special handling in `lookupDatum`: Python-style negative array indexing.
-    var negIntPat = /^-\d+$/;
-    for (var i = 0; i < bits.length; i++) {
-        if (negIntPat.test(bits[i])) {
-            bits[i] = Number(bits[i]);
-        }
-    }
-
-    debug(JSON.stringify(lookup) + ' -> ' + JSON.stringify(bits));
-    return bits
-}
-
-
-/**
- * Parse the given stdin input into:
- *  {
- *    'error': ... error object if there was an error ...,
- *    'datum': ... parsed object if content was JSON ...
- *   }
- *
- * @param buffer {String} The text to parse as JSON.
- * @param obj {Object} Optional. Set when in streaming mode to avoid
- *    re-interpretation of `group`. Also avoids reparsing.
- * @param group {Boolean} Default false. If true, then non-JSON input
- *    will be attempted to be 'arrayified' (see inline comment).
- * @param merge {Boolean} Default null. Can be 'shallow' or 'deep'. An
- *    attempt will be made to interpret the input as adjacent objects to
- *    be merged, last key wins. See inline comment for limitations.
- */
-function parseInput(buffer, obj, group, merge) {
-    if (obj) {
-        return {
-            datum: obj
-        };
-    } else if (group) {
-        /**
-         * Special case: Grouping (previously called auto-arrayification)
-         * of unjoined list of objects:
-         *    {"one": 1}{"two": 2}
-         * and auto-concatenation of unjoined list of arrays:
-         *    ["a", "b"]["c", "d"]
-         *
-         * This can be nice to process a stream of JSON objects generated from
-         * multiple calls to another tool or `cat *.json | json`.
-         *
-         * Rules:
-         * - Only JS objects and arrays. Don't see strong need for basic
-         *   JS types right now and this limitation simplifies.
-         * - The break between JS objects has to include a newline:
-         *      {"one": 1}
-         *      {"two": 2}
-         *   or no spaces at all:
-         *      {"one": 1}{"two": 2}
-         *   I.e., not this:
-         *      {"one": 1}  {"two": 2}
-         *   This condition should be fine for typical use cases and ensures
-         *   no false matches inside JS strings.
-         * - The break between JS *arrays* has to include a newline:
-         *      ["one", "two"]
-         *      ["three"]
-         *   The 'no spaces' case is NOT supported for JS arrays as of v6.0.0
-         *   because <https://github.com/trentm/json/issues/55> shows that that
-         *   is not safe.
-         */
-        var newBuffer = buffer;
-        /* JSSTYLED */
-        [/(})\s*\n\s*({)/g, /(})({")/g].forEach(function (pat) {
-            newBuffer = newBuffer.replace(pat, '$1,\n$2');
-        });
-        [/(\])\s*\n\s*(\[)/g].forEach(function (pat) {
-            newBuffer = newBuffer.replace(pat, ',\n');
-        });
-        newBuffer = newBuffer.trim();
-        if (newBuffer[0] !== '[') {
-            newBuffer = '[\n' + newBuffer;
-        }
-        if (newBuffer.slice(-1) !== ']') {
-            newBuffer = newBuffer + '\n]\n';
-        }
-        try {
-            return {
-                datum: JSON.parse(newBuffer)
-            };
-        } catch (e2) {
-            return {
-                error: e2
-            };
-        }
-    } else if (merge) {
-        // See the 'Rules' above for limitations on boundaries for 'adjacent'
-        // objects: KISS.
-        var newBuffer = buffer;
-        /* JSSTYLED */
-        [/(})\s*\n\s*({)/g, /(})({")/g].forEach(function (pat) {
-            newBuffer = newBuffer.replace(pat, '$1,\n$2');
-        });
-        newBuffer = '[\n' + newBuffer + '\n]\n';
-        var objs;
-        try {
-            objs = JSON.parse(newBuffer);
-        } catch (e) {
-            return {
-                error: e
-            };
-        }
-        var merged = objs[0];
-        if (merge === 'shallow') {
-            for (var i = 1; i < objs.length; i++) {
-                var obj = objs[i];
-                Object.keys(obj).forEach(function (k) {
-                    merged[k] = obj[k];
-                });
-            }
-        } else if (merge === 'deep') {
-            function deepExtend(a, b) {
-                Object.keys(b).forEach(function (k) {
-                    if (a[k] && b[k] &&
-                        toString.call(a[k]) === '[object Object]' &&
-                        toString.call(b[k]) === '[object Object]')
-                    {
-                        deepExtend(a[k], b[k])
-                    } else {
-                        a[k] = b[k];
-                    }
-                });
-            }
-            for (var i = 1; i < objs.length; i++) {
-                deepExtend(merged, objs[i]);
-            }
-        } else {
-            throw new Error(format('unknown value for "merge": "%s"', merge));
-        }
-        return {
-            datum: merged
-        };
-    } else {
-        try {
-            return {
-                datum: JSON.parse(buffer)
-            };
-        } catch (e) {
-            return {
-                error: e
-            };
-        }
-    }
-}
-
-
-/**
- * Apply a lookup to the given datum.
- *
- * @argument datum {Object}
- * @argument lookup {Array} The parsed lookup (from
- *    `parseLookup(<string>, <string>)`). Might be empty.
- * @returns {Object} The result of the lookup.
- */
-function lookupDatum(datum, lookup) {
-    var d = datum;
-    for (var i = 0; i < lookup.length; i++) {
-        var bit = lookup[i];
-        if (typeof (bit) === 'number' && bit < 0) {
-            d = d[d.length + bit];
-        } else {
-            d = d[bit];
-        }
-        if (d === undefined) {
-            return undefined;
-        }
-    }
-    return d;
-}
-
-
-/**
- * Output the given datasets.
- *
- * @param datasets {Array} Array of data sets to print, in the form:
- *    `[ [<datum>, <sep>, <alwaysPrintSep>], ... ]`
- * @param filename {String} The filename to which to write the output. If
- *    not set, then emit to stdout.
- * @param headers {String} The HTTP header block string, if any, to emit
- *    first.
- * @param opts {Object} Parsed tool options.
- */
-function printDatasets(datasets, filename, headers, opts) {
-    var isTTY = (filename ? false : process.stdout.isTTY)
-    var write = emit;
-    if (filename) {
-        var tmpPath = path.resolve(path.dirname(filename),
-            format('.%s-json-%s-%s.tmp', path.basename(filename), process.pid,
-                Date.now()));
-        var stats = fs.statSync(filename);
-        var f = fs.createWriteStream(tmpPath,
-            {encoding: 'utf8', mode: stats.mode});
-        write = f.write.bind(f);
-    }
-    if (headers && headers.length > 0) {
-        write(headers)
-    }
-    for (var i = 0; i < datasets.length; i++) {
-        var dataset = datasets[i];
-        var output = stringifyDatum(dataset[0], opts, isTTY);
-        var sep = dataset[1];
-        if (output && output.length) {
-            write(output);
-            write(sep);
-        } else if (dataset[2]) {
-            write(sep);
-        }
-    }
-    if (filename) {
-        f.end();
-        fs.renameSync(tmpPath, filename);
-        if (!opts.quiet) {
-            warn('json: updated "%s" in-place', filename);
-        }
-    }
-}
-
-
-/**
- * Stringify the given datum according to the given output options.
- */
-function stringifyDatum(datum, opts, isTTY) {
-    var output = null;
-    switch (opts.outputMode) {
-    case OM_INSPECT:
-        output = util.inspect(datum, false, Infinity, isTTY);
-        break;
-    case OM_JSON:
-        if (typeof (datum) !== 'undefined') {
-            output = JSON.stringify(datum, null, opts.jsonIndent);
-        }
-        break;
-    case OM_COMPACT:
-        // Dev Note: A still relatively experimental attempt at a more
-        // compact ouput somewhat a la Python's repr of a dict. I.e. try to
-        // fit elements on one line as much as reasonable.
-        if (datum === undefined) {
-            // pass
-        } else if (Array.isArray(datum)) {
-            var bits = ['[\n'];
-            datum.forEach(function (d) {
-                bits.push('  ')
-                bits.push(JSON.stringify(d, null, 0).replace(
-                    /* JSSTYLED */
-                    /,"(?![,:])/g, ', "'));
-                bits.push(',\n');
-            });
-            bits.push(bits.pop().slice(0, -2) + '\n') // drop last comma
-            bits.push(']');
-            output = bits.join('');
-        } else {
-            output = JSON.stringify(datum, null, 0);
-        }
-        break;
-    case OM_JSONY:
-        if (typeof (datum) === 'string') {
-            output = datum;
-        } else if (typeof (datum) !== 'undefined') {
-            output = JSON.stringify(datum, null, opts.jsonIndent);
-        }
-        break;
-    default:
-        throw new Error('unknown output mode: ' + opts.outputMode);
-    }
-    return output;
-}
-
-
-/**
- * Print out a single result, considering input options.
- *
- * @deprecated
- */
-function printDatum(datum, opts, sep, alwaysPrintSep) {
-    var output = stringifyDatum(datum, opts);
-    if (output && output.length) {
-        emit(output);
-        emit(sep);
-    } else if (alwaysPrintSep) {
-        emit(sep);
-    }
-}
-
-
-var stdoutFlushed = true;
-function emit(s) {
-    // TODO:PERF If this is try/catch is too slow (too granular): move up to
-    //    mainline and be sure to only catch this particular error.
-    if (drainingStdout) {
-        return;
-    }
-    try {
-        stdoutFlushed = process.stdout.write(s);
-    } catch (e) {
-        // Handle any exceptions in stdout writing in the 'error' event above.
-    }
-}
-
-process.stdout.on('error', function (err) {
-    if (err.code === 'EPIPE') {
-        // See <https://github.com/trentm/json/issues/9>.
-        drainStdoutAndExit(0);
-    } else {
-        warn(err)
-        drainStdoutAndExit(1);
-    }
-});
-
-
-/**
- * A hacked up version of 'process.exit' that will first drain stdout
- * before exiting. *WARNING: This doesn't stop event processing.* IOW,
- * callers have to be careful that code following this call isn't
- * accidentally executed.
- *
- * In node v0.6 "process.stdout and process.stderr are blocking when they
- * refer to regular files or TTY file descriptors." However, this hack might
- * still be necessary in a shell pipeline.
- */
-var drainingStdout = false;
-function drainStdoutAndExit(code) {
-    if (drainingStdout) {
-        return;
-    }
-    drainingStdout = true;
-    process.stdout.on('drain', function () {
-        process.exit(code);
-    });
-    process.stdout.on('close', function () {
-        process.exit(code);
-    });
-    if (stdoutFlushed) {
-        process.exit(code);
-    }
-}
-
-
-/**
- * Return a function for the given JS code that returns.
- *
- * If no 'return' in the given javascript snippet, then assume we are a single
- * statement and wrap in 'return (...)'. This is for convenience for short
- * '-c ...' snippets.
- */
-function funcWithReturnFromSnippet(js) {
-    // auto-"return"
-    if (js.indexOf('return') === -1) {
-        if (js.substring(js.length - 1) === ';') {
-            js = js.substring(0, js.length - 1);
-        }
-        js = 'return (' + js + ')';
-    }
-    return (new Function(js));
-}
-
-
-
-//---- mainline
-
-function main(argv) {
-    var opts;
-    try {
-        opts = parseArgv(argv);
-    } catch (e) {
-        warn('json: error: %s', e.message)
-        return drainStdoutAndExit(1);
-    }
-    //warn(opts);
-    if (opts.help) {
-        printHelp();
-        return;
-    }
-    if (opts.version) {
-        if (opts.outputMode === OM_JSON) {
-            var v = {
-                version: getVersion(),
-                author: 'Trent Mick',
-                project: 'https://github.com/trentm/json'
-            };
-            console.log(JSON.stringify(v, null, opts.jsonIndent));
-        } else {
-            console.log('json ' + getVersion());
-            console.log('written by Trent Mick');
-            console.log('https://github.com/trentm/json');
-        }
-        return;
-    }
-    var lookupStrs = opts.args;
-
-    // Prepare condition and execution funcs (and vm scripts) for -c/-e.
-    var execVm = Boolean(process.env.JSON_EXEC &&
-        process.env.JSON_EXEC === 'vm');
-    var i;
-    var condFuncs = [];
-    if (!execVm) {
-        for (i = 0; i < opts.condSnippets.length; i++) {
-            condFuncs[i] = funcWithReturnFromSnippet(opts.condSnippets[i]);
-        }
-    }
-    var condScripts = [];
-    if (execVm) {
-        for (i = 0; i < opts.condSnippets.length; i++) {
-            condScripts[i] = vm.createScript(opts.condSnippets[i]);
-        }
-    }
-    var cond = Boolean(condFuncs.length + condScripts.length);
-    var exeFuncs = [];
-    if (!execVm) {
-        for (i = 0; i < opts.exeSnippets.length; i++) {
-            exeFuncs[i] = new Function(opts.exeSnippets[i]);
-        }
-    }
-    var exeScripts = [];
-    if (execVm) {
-        for (i = 0; i < opts.exeSnippets.length; i++) {
-            exeScripts[i] = vm.createScript(opts.exeSnippets[i]);
-        }
-    }
-    var exe = Boolean(exeFuncs.length + exeScripts.length);
-
-    var lookups = lookupStrs.map(function (lookup) {
-        return parseLookup(lookup, opts.lookupDelim);
-    });
-
-    if (opts.group && opts.array && opts.outputMode !== OM_JSON) {
-        // streaming
-        var chunker = chunkEmitter(opts);
-        chunker.on('error', function (error) {
-            warn('json: error: %s', err);
-            return drainStdoutAndExit(1);
-        });
-        chunker.on('chunk', parseChunk);
-    } else if (opts.inPlace) {
-        assert.equal(opts.inputFiles.length, 1,
-            'cannot handle more than one file with -I');
-        getInput(opts, function (err, content, filename) {
-            if (err) {
-                warn('json: error: %s', err)
-                return drainStdoutAndExit(1);
-            }
-
-            // Take off a leading HTTP header if any and pass it through.
-            var headers = [];
-            while (true) {
-                if (content.slice(0, 5) === 'HTTP/') {
-                    var index = content.indexOf('\r\n\r\n');
-                    var sepLen = 4;
-                    if (index == -1) {
-                        index = content.indexOf('\n\n');
-                        sepLen = 2;
-                    }
-                    if (index != -1) {
-                        if (!opts.dropHeaders) {
-                            headers.push(content.slice(0, index + sepLen));
-                        }
-                        var is100Continue = (
-                            content.slice(0, 21) === 'HTTP/1.1 100 Continue');
-                        content = content.slice(index + sepLen);
-                        if (is100Continue) {
-                            continue;
-                        }
-                    }
-                }
-                break;
-            }
-            parseChunk(content, undefined, filename, true, headers.join(''));
-        });
-    } else {
-        // not streaming
-        getInput(opts, function (err, buffer, filename) {
-            if (err) {
-                warn('json: error: %s', err)
-                return drainStdoutAndExit(1);
-            }
-            // Take off a leading HTTP header if any and pass it through.
-            while (true) {
-                if (buffer.slice(0, 5) === 'HTTP/') {
-                    var index = buffer.indexOf('\r\n\r\n');
-                    var sepLen = 4;
-                    if (index == -1) {
-                        index = buffer.indexOf('\n\n');
-                        sepLen = 2;
-                    }
-                    if (index != -1) {
-                        if (!opts.dropHeaders) {
-                            emit(buffer.slice(0, index + sepLen));
-                        }
-                        var is100Continue = (
-                            buffer.slice(0, 21) === 'HTTP/1.1 100 Continue');
-                        buffer = buffer.slice(index + sepLen);
-                        if (is100Continue) {
-                            continue;
-                        }
-                    }
-                }
-                break;
-            }
-            parseChunk(buffer, null, filename, false);
-        });
-    }
-
-    /**
-     * Parse a single chunk of JSON. This may be called more than once
-     * (when streaming or when operating on multiple files).
-     *
-     * @param chunk {String} The JSON-encoded string.
-     * @param obj {Object} Optional. For some code paths while streaming `obj`
-     *    will be provided. This is an already parsed JSON object.
-     * @param filename {String} Optional. The filename from which this content
-     *    came, if relevant.
-     * @param inPlace {Boolean} Optional. If true, then output will be written
-     *    to `filename`.
-     * @param headers {String} Optional. Leading HTTP headers, if any to emit.
-     */
-    function parseChunk(chunk, obj, filename, inPlace, headers) {
-        // Expect the chunk to be JSON.
-        if (!chunk.length) {
-            return;
-        }
-        // parseInput() -> {datum: <input object>, error: <error object>}
-        var input = parseInput(chunk, obj, opts.group, opts.merge);
-        if (input.error) {
-            // Doesn't look like JSON. Just print it out and move on.
-            if (!opts.quiet) {
-                // Use JSON-js' "json_parse" parser to get more detail on the
-                // syntax error.
-                var details = '';
-                var normBuffer = chunk.replace(/\r\n|\n|\r/, '\n');
-                try {
-                    json_parse(normBuffer);
-                    details = input.error;
-                } catch (err) {
-                    // err.at has the position. Get line/column from that.
-                    var at = err.at - 1; // `err.at` looks to be 1-based.
-                    var lines = chunk.split('\n');
-                    var line, col, pos = 0;
-                    for (line = 0; line < lines.length; line++) {
-                        pos += lines[line].length + 1;
-                        if (pos > at) {
-                            col = at - (pos - lines[line].length - 1);
-                            break;
-                        }
-                    }
-                    var spaces = '';
-                    for (var i = 0; i < col; i++) {
-                        spaces += '.';
-                    }
-                    details = err.message + ' at line ' + (line + 1) +
-                        ', column ' + (col + 1) + ':\n        ' +
-                        lines[line] + '\n        ' + spaces + '^';
-                }
-                warn('json: error: %s is not JSON: %s',
-                    filename ? '"' + filename + '"' : 'input', details);
-            }
-            if (!opts.validate) {
-                emit(chunk);
-                if (chunk.length && chunk[chunk.length - 1] !== '\n') {
-                    emit('\n');
-                }
-            }
-            return drainStdoutAndExit(1);
-        }
-        if (opts.validate) {
-            return drainStdoutAndExit(0);
-        }
-        var data = input.datum;
-
-        // Process: items (-M, --items)
-        if (opts.items) {
-            if (!Array.isArray(data)) {
-                var key;
-                var array = [];
-                for (key in data) {
-                    if (data.hasOwnProperty(key)) {
-                        array.push({
-                          key: key,
-                          value: data[key]
-                        });
-                    }
-                }
-                data = array;
-            }
-        }
-
-        // Process: executions (-e, -E)
-        var i, j;
-        if (!exe) {
-            /* pass */
-        } else if (opts.array || (opts.array === null && Array.isArray(data))) {
-            var arrayified = false;
-            if (!Array.isArray(data)) {
-                arrayified = true;
-                data = [data];
-            }
-            for (i = 0; i < data.length; i++) {
-                var datum = data[i];
-                for (j = 0; j < exeFuncs.length; j++) {
-                    exeFuncs[j].call(datum);
-                }
-                for (j = 0; j < exeScripts.length; j++) {
-                    exeScripts[j].runInNewContext(datum);
-                }
-            }
-            if (arrayified) {
-                data = data[0];
-            }
-        } else {
-            for (j = 0; j < exeFuncs.length; j++) {
-                exeFuncs[j].call(data);
-            }
-            for (j = 0; j < exeScripts.length; j++) {
-                exeScripts[j].runInNewContext(data);
-            }
-        }
-
-        // Process: conditionals (-c)
-        if (!cond) {
-            /* pass */
-        } else if (opts.array || (opts.array === null && Array.isArray(data))) {
-            var arrayified = false;
-            if (!Array.isArray(data)) {
-                arrayified = true;
-                data = [data];
-            }
-            var filtered = [];
-            for (i = 0; i < data.length; i++) {
-                var datum = data[i];
-                var datumCopy = objCopy(datum);
-                var keep = true;
-                // TODO(perf): Perhaps drop the 'datumCopy'? "this is a gun"
-                for (j = 0; j < condFuncs.length; j++) {
-                    if (!condFuncs[j].call(datumCopy)) {
-                        keep = false;
-                        break;
-                    }
-                }
-                if (keep) {
-                    for (j = 0; j < condScripts.length; j++) {
-                        if (!condScripts[j].runInNewContext(datumCopy)) {
-                            keep = false;
-                            break;
-                        }
-                    }
-                    if (keep) {
-                        filtered.push(datum);
-                    }
-                }
-            }
-            if (arrayified) {
-                data = (filtered.length ? filtered[0] : []);
-            } else {
-                data = filtered;
-            }
-        } else {
-            var keep = true;
-            var dataCopy = objCopy(data);
-            for (j = 0; j < condFuncs.length; j++) {
-                // TODO(perf): Perhaps drop the 'dataCopy'? "this is a gun"
-                if (!condFuncs[j].call(dataCopy)) {
-                    keep = false;
-                    break;
-                }
-            }
-            if (keep) {
-                for (j = 0; j < condScripts.length; j++) {
-                    if (!condScripts[j].runInNewContext(dataCopy)) {
-                        keep = false;
-                        break;
-                    }
-                }
-            }
-            if (!keep) {
-                data = undefined;
-            }
-        }
-
-        // Process: lookups
-        var lookupsAreIndeces = false;
-        if (lookups.length) {
-            if (opts.array) {
-                if (!Array.isArray(data)) data = [data];
-                var table = [];
-                for (j = 0; j < data.length; j++) {
-                    var datum = data[j];
-                    var row = {};
-                    for (i = 0; i < lookups.length; i++) {
-                        var lookup = lookups[i];
-                        var value = lookupDatum(datum, lookup);
-                        if (value !== undefined) {
-                            row[lookup.join('.')] = value;
-                        }
-                    }
-                    table.push(row);
-                }
-                data = table;
-            } else {
-                // Special case handling: Note if the 'lookups' are indeces into
-                // an array. This may be used below to change the output
-                // representation.
-                if (Array.isArray(data)) {
-                    lookupsAreIndeces = true;
-                    for (i = 0; i < lookups.length; i++) {
-                        if (lookups[i].length !== 1 ||
-                            isNaN(Number(lookups[i])))
-                        {
-                            lookupsAreIndeces = false;
-                            break;
-                        }
-                    }
-                }
-                var row = {};
-                for (i = 0; i < lookups.length; i++) {
-                    var lookup = lookups[i];
-                    var value = lookupDatum(data, lookup);
-                    if (value !== undefined) {
-                        row[lookup.join('.')] = value;
-                    }
-                }
-                data = row;
-            }
-        }
-
-        // --keys
-        if (opts.outputKeys) {
-            var data = Object.keys(data);
-        }
-
-        // Output
-        var datasets = [];
-        if (opts.outputMode === OM_JSON) {
-            if (lookups.length === 1 && !opts.array) {
-                /**
-                 * Special case: For JSON output of a *single* lookup, *don't*
-                 * use the full table structure, else there is no way to get
-                 * string quoting for a single value:
-                 *      $ echo '{"a": [], "b": "[]"}' | json -j a
-                 *      []
-                 *      $ echo '{"a": [], "b": "[]"}' | json -j b
-                 *      '[]'
-                 * See <https://github.com/trentm/json/issues/35> for why.
-                 */
-                data = data[lookups[0].join('.')];
-            } else if (lookupsAreIndeces) {
-                /**
-                 * Special case: Lookups that are all indeces into an input
-                 * array are more likely to be wanted as an array of selected
-                 * items rather than a 'JSON table' thing that we use otherwise.
-                 */
-                var flattened = [];
-                for (i = 0; i < lookups.length; i++) {
-                    var lookupStr = lookups[i].join('.');
-                    if (data.hasOwnProperty(lookupStr)) {
-                        flattened.push(data[lookupStr])
-                    }
-                }
-                data = flattened;
-            }
-            // If JSON output mode, then always just output full set of data to
-            // ensure valid JSON output.
-            datasets.push([data, '\n', false]);
-        } else if (lookups.length) {
-            if (opts.array) {
-                // Output `data` as a 'table' of lookup results.
-                for (j = 0; j < data.length; j++) {
-                    var row = data[j];
-                    for (i = 0; i < lookups.length - 1; i++) {
-                        datasets.push([row[lookups[i].join('.')],
-                            opts.delim, true]);
-                    }
-                    datasets.push([row[lookups[i].join('.')], '\n', true]);
-                }
-            } else {
-                for (i = 0; i < lookups.length; i++) {
-                    datasets.push([data[lookups[i].join('.')], '\n', false]);
-                }
-            }
-        } else if (opts.array) {
-            if (!Array.isArray(data)) data = [data];
-            for (j = 0; j < data.length; j++) {
-                datasets.push([data[j], '\n', false]);
-            }
-        } else {
-            // Output `data` as is.
-            datasets.push([data, '\n', false]);
-        }
-        printDatasets(datasets, inPlace ? filename : undefined, headers, opts);
-    }
-}
-
-if (require.main === module) {
-    // HACK guard for <https://github.com/trentm/json/issues/24>.
-    // We override the `process.stdout.end` guard that core node.js puts in
-    // place. The real fix is that `.end()` shouldn't be called on stdout
-    // in node core. Hopefully node v0.6.9 will fix that. Only guard
-    // for v0.6.0..v0.6.8.
-    var nodeVer = process.versions.node.split('.').map(Number);
-    if ([0, 6, 0] <= nodeVer && nodeVer <= [0, 6, 8]) {
-        var stdout = process.stdout;
-        stdout.end = stdout.destroy = stdout.destroySoon = function () {
-            /* pass */
-        };
-    }
-
-    main(process.argv);
-}
diff --git a/tools/jira/listengprojects.sh b/tools/listengprojects
similarity index 100%
rename from tools/jira/listengprojects.sh
rename to tools/listengprojects
diff --git a/tools/local-bitsdir-copy b/tools/local-bitsdir-copy
deleted file mode 100755
index 4ea85a0..0000000
--- a/tools/local-bitsdir-copy
+++ /dev/null
@@ -1,36 +0,0 @@
-#!/bin/bash
-#
-# This Source Code Form is subject to the terms of the Mozilla Public
-# License, v. 2.0. If a copy of the MPL was not distributed with this
-# file, You can obtain one at http://mozilla.org/MPL/2.0/.
-#
-
-#
-# Copyright (c) 2018, Joyent, Inc.
-#
-
-#
-# This script copies a single build's bits to the directory pointed to by
-# LOCAL_BITS_DIR
-#
-
-set -o errexit
-set -o xtrace
-
-TOP=$(cd $(dirname $0)/../ >/dev/null; pwd)
-
-function fatal ()
-{
-    echo "$@" >&2
-    exit 1
-}
-
-MG_TARGET=$1
-if [[ -z ${MG_TARGET} || -n $2 ]]; then
-    fatal "Usage: $0 <job_name>"
-fi
-
-mkdir -p ${LOCAL_BITS_DIR}/${MG_TARGET}
-rsync -va ${TOP}/bits/${MG_TARGET}/* ${LOCAL_BITS_DIR}/${MG_TARGET}/
-
-exit 0
diff --git a/tools/ls-missing-release-builds b/tools/ls-missing-release-builds
index 5628783..75f8937 100755
--- a/tools/ls-missing-release-builds
+++ b/tools/ls-missing-release-builds
@@ -6,7 +6,7 @@
 #
 
 #
-# Copyright 2019, Joyent, Inc.
+# Copyright 2019 Joyent, Inc.
 #
 
 #
@@ -20,23 +20,37 @@ if [[ -n "$TRACE" ]]; then
     export PS4='[\D{%FT%TZ}] ${BASH_SOURCE}:${LINENO}: ${FUNCNAME[0]:+${FUNCNAME[0]}(): }'
     set -o xtrace
 fi
+
+function fatal {
+    echo "$(basename $0): error: $1"
+    exit 1
+}
+
+# sanity-check that we can run jr and json. Do this before setting up pipefail.
+# Look for the environment variable before invoking jr, which otherwise prints
+# different advice.
+if [[ -z "$JR_MANIFESTS" ]]; then
+    fatal "$(basename $0): error: \$JR_MANIFESTS should be set in the environment. \
+    Example value: \
+    JR_MANIFESTS=/Volumes/projects/triton.git/repos.json,/Volumes/projects/triton-dev.git/repos-manta.json,/Volumes/projects/triton-dev.git/repos-smartos.json"
+fi
+
+for command in jr json; do
+    $($command -h 2>&1 > /dev/null)
+    if [[ $? -ne 0 ]]; then
+        fatal "Unable to run $command, please check your \$PATH"
+    fi
+done
+
 set -o errexit
 set -o pipefail
 
-
 #---- globals
 
 TOP=$(cd $(dirname $0)/../; pwd)
 
-
 #---- support stuff
 
-function fatal
-{
-    echo "$0: fatal error: $*"
-    exit 1
-}
-
 function usage() {
     if [[ -n "$1" ]]; then
         echo "error: $1"
@@ -78,11 +92,10 @@ shift
 
 targets="$*"
 if [[ -z "$targets" ]]; then
-    (cd $TOP && JOYENT_BUILD=true bash <targets.json.in >targets.json)
-    targets=$($TOP/tools/json -f $TOP/targets.json -Ma key value.public \
-        | grep -v '^sdcsso ' \
-        | grep -v '^headnode-debug ' \
-        | grep -v '^headnode-joyent-debug ')
+    targets=$(jr list -H -l mg='*' -o labels | json -ag mg public | sort -u \
+        | grep -v '^sdcsso' \
+        | grep -v '^headnode-debug' \
+        | grep -v '^headnode-joyent-debug')
 fi
 
 # Ensure mls is setup properly at all.
diff --git a/tools/manta-upload b/tools/manta-upload
deleted file mode 100755
index 097d865..0000000
--- a/tools/manta-upload
+++ /dev/null
@@ -1,57 +0,0 @@
-#!/bin/bash
-#
-# This Source Code Form is subject to the terms of the Mozilla Public
-# License, v. 2.0. If a copy of the MPL was not distributed with this
-# file, You can obtain one at http://mozilla.org/MPL/2.0/.
-#
-
-#
-# Copyright (c) 2018, Joyent, Inc.
-#
-
-#
-# This script uploads a single build's bits to a chosen manta path.
-#
-
-set -o errexit
-set -o xtrace
-
-TOP=$(cd $(dirname $0)/../ >/dev/null; pwd)
-ENV_MG_OUT_PATH=${MG_OUT_PATH}
-
-function fatal ()
-{
-    echo "$@" >&2
-    exit 1
-}
-
-MG_TARGET=$1
-if [[ -z ${MG_TARGET} || -n $2 ]]; then
-    fatal "Usage: $0 <target>"
-fi
-
-if [[ ! -f ${TOP}/bits/config.mk ]]; then
-    fatal "No ${TOP}/bits/config.mk found! Run ./configure and 'make <target>' first."
-fi
-
-. ${TOP}/bits/config.mk
-
-[[ -n ${BRANCH} ]] || fatal "Missing BRANCH. (check bits/config.mk)"
-[[ -n ${TIMESTAMP} ]] || fatal "Missing TIMESTAMP. (check bits/config.mk)"
-
-if [[ -n ${ENV_MG_OUT_PATH} ]]; then
-    echo "WARNING: MG_OUT_PATH='${ENV_MG_OUT_PATH}' set in environment, overriding config.mk value." >&2
-    MG_OUT_PATH=${ENV_MG_OUT_PATH}
-elif [[ -z ${MG_OUT_PATH} ]]; then
-    MG_OUT_PATH=/stor/builds
-fi
-
-${TOP}/tools/mantaput-bits \
-    "${BRANCH}" \
-    "${TRY_BRANCH}" \
-    "${TIMESTAMP}" \
-    ${MG_OUT_PATH}/${MG_TARGET} \
-    ${MG_TARGET} \
-    ${UPLOAD_SUBDIRS}
-
-exit 0
diff --git a/tools/mantaput-bits b/tools/mantaput-bits
deleted file mode 100755
index da13908..0000000
--- a/tools/mantaput-bits
+++ /dev/null
@@ -1,141 +0,0 @@
-#!/bin/bash
-#
-# This Source Code Form is subject to the terms of the Mozilla Public
-# License, v. 2.0. If a copy of the MPL was not distributed with this
-# file, You can obtain one at http://mozilla.org/MPL/2.0/.
-#
-
-#
-# Copyright (c) 2018, Joyent, Inc.
-#
-
-# Upload the bits dir to Manta
-if [ "$TRACE" != "" ]; then
-    export PS4='${BASH_SOURCE}:${LINENO}: '
-    set -o xtrace
-fi
-set -o errexit
-
-TOP=$(cd $(dirname $0)/../; pwd)
-BITS_DIR=bits/
-PATH=$PATH:${TOP}/node_modules/manta/bin
-UPLOAD_LOG=${TOP}/upload.log
-
-# --- Manta config
-if [[ -z "$MANTA_KEY_ID" ]]; then
-    export MANTA_KEY_ID=`ssh-keygen -l -f ~/.ssh/id_rsa.pub | awk '{print $2}' | tr -d '\n'`
-fi
-if [[ -z "$MANTA_URL" ]]; then
-    export MANTA_URL=https://us-east.manta.joyent.com
-fi
-if [[ -z "$MANTA_USER" ]]; then
-    export MANTA_USER="Joyent_Dev";
-fi
-
-
-function fatal {
-    echo "$(basename $0): error: $1"
-    exit 1
-}
-
-function errexit {
-    [[ $1 -ne 0 ]] || exit 0
-    fatal "error exit status $1 at line $2"
-}
-
-function print_help() {
-    echo "Usage:"
-    echo "  ./tools/mantaput-bits BRANCH TRY-BRANCH TIMESTAMP UPLOAD-BASE-DIR [SUBDIRS...]"
-    echo ""
-    echo "Upload bits to Manta. The UPLOAD-BASE-DIR is presumed to be either a subdir of"
-    echo "\${MANTA_USER}/stor or (if starts with '/') a path under \${MANTA_USER}"
-}
-
-function msg() {
-    echo $* | tee -a $UPLOAD_LOG
-}
-
-trap 'errexit $? $LINENO' EXIT
-
-BRANCH=$1
-shift
-TRY_BRANCH=$1
-shift
-TIMESTAMP=$1
-shift
-UPLOAD_BASE_DIR=$1
-shift
-SUBDIRS=$*
-
-if [[ ${UPLOAD_BASE_DIR:0:1} != '/' ]]; then
-    # if it starts with a / we assume it's /stor/<something> or
-    # /public/<something> if not, we prepend /stor
-    UPLOAD_BASE_DIR="/stor/${UPLOAD_BASE_DIR}"
-fi
-
-UPLOAD_BRANCH=$TRY_BRANCH
-if [[ -z "$UPLOAD_BRANCH" ]]; then
-    UPLOAD_BRANCH=$BRANCH
-fi
-
-upload_subdir=$UPLOAD_BRANCH-$TIMESTAMP
-
-start_time=$(date +%s)
-msg "Uploading bits to /${MANTA_USER}${UPLOAD_BASE_DIR}/${upload_subdir}"
-
-# need to create the directory structure first
-if [[ -z "$SUBDIRS" ]]; then
-    subs=$(find $BITS_DIR -type d)
-    files=$(find $BITS_DIR -type f)
-else
-    for subdir in $SUBDIRS; do
-        if [[ -d $BITS_DIR/$subdir ]]; then
-          subs="$subs $(find $BITS_DIR/$subdir -type d)"
-          files="$files $(find $BITS_DIR/$subdir -type f)"
-        fi
-    done
-fi
-
-for sub in $subs; do
-    mmkdir -v -p /${MANTA_USER}${UPLOAD_BASE_DIR}/${upload_subdir}/${sub#${BITS_DIR}}
-done
-
-md5sums=""
-# now we can upload the files
-for file in $files; do
-    manta_object=/${MANTA_USER}${UPLOAD_BASE_DIR}/${upload_subdir}/${file#$BITS_DIR}
-
-    local_md5_line=$(md5sum ${file})
-    local_md5=$(echo "${local_md5_line}" | cut -d ' ' -f1)
-    manta_md5=$(mmd5 ${manta_object} | cut -d ' ' -f1)
-
-    if [[ -n ${manta_md5} && ${manta_md5} != ${local_md5} ]]; then
-        fatal "${manta_object} exists but MD5 does not match ${file}"
-    fi
-
-    if [[ -z ${manta_md5} ]]; then
-        # file doesn't exist, upload it
-
-        echo ${MANTA_URL}/${manta_object} >>$UPLOAD_LOG
-        mput -v -f ${file} ${manta_object}
-        [[ $? == 0 ]] || fatal "Failed to upload ${file} to ${manta_object}"
-    else
-        msg "${manta_object} already exists and matches local file, skipping upload."
-    fi
-    md5sums="${md5sums}${local_md5_line}\n"
-done
-
-# upload the md5sums and config.mk
-echo -e $md5sums | mput -v -H 'content-type: text/plain' /${MANTA_USER}${UPLOAD_BASE_DIR}/${upload_subdir}/md5sums.txt
-mput -v -f $BITS_DIR/config.mk /${MANTA_USER}${UPLOAD_BASE_DIR}/${upload_subdir}/config.mk
-
-# now update the branch latest link
-echo "/${MANTA_USER}${UPLOAD_BASE_DIR}/${upload_subdir}" | mput -v -H 'content-type: text/plain' /${MANTA_USER}${UPLOAD_BASE_DIR}/${UPLOAD_BRANCH}-latest
-# If this is a bi-weekly release branch, also update latest-release link
-if [[ $UPLOAD_BRANCH =~ ^release- ]]; then
-    echo "/${MANTA_USER}${UPLOAD_BASE_DIR}/${upload_subdir}" | mput -v -H 'content-type: text/plain' /${MANTA_USER}${UPLOAD_BASE_DIR}/latest-release
-fi
-
-end_time=$(date +%s)
-elapsed=$((${end_time} - ${start_time}))
-msg "Uploaded to /${MANTA_USER}${UPLOAD_BASE_DIR}/${upload_subdir} in ${elapsed} seconds"
diff --git a/tools/prep_dataset_in_jpc.sh b/tools/prep_dataset_in_jpc.sh
deleted file mode 100755
index a301bd8..0000000
--- a/tools/prep_dataset_in_jpc.sh
+++ /dev/null
@@ -1,474 +0,0 @@
-#!/bin/bash
-# vi: expandtab sw=2 ts=2
-#
-# This Source Code Form is subject to the terms of the Mozilla Public
-# License, v. 2.0. If a copy of the MPL was not distributed with this
-# file, You can obtain one at http://mozilla.org/MPL/2.0/.
-#
-
-#
-# Copyright (c) 2018, Joyent, Inc.
-#
-
-#
-# "Prepare a dataset, by deploying to JPC"
-#
-# This is called for "appliance" image/dataset builds to: (a) provision
-# a new zone of a given image, (b) drop in an fs tarball and
-# optionally some other tarballs, and (c) make an image out of this.
-#
-
-export PS4='[\D{%FT%TZ}] ${BASH_SOURCE}:${LINENO}: ${FUNCNAME[0]:+${FUNCNAME[0]}(): }'
-if [[ -z "$(echo "$*" | grep -- ' -h ' || /bin/true)" ]]; then
-  # Try to avoid xtrace goop when print help/usage output.
-  set -o xtrace
-fi
-set -o errexit
-
-
-
-#---- globals, config
-
-CREATED_MACHINE_UUID=
-CREATED_MACHINE_IMAGE_UUID=
-MG_OUT_PATH="/stor/builds"
-TOP=$(cd $(dirname $0)/../ >/dev/null; pwd)
-JSON=${TOP}/tools/json
-export PATH="${TOP}/node_modules/manta/bin:${TOP}/node_modules/smartdc/bin:${PATH}"
-image_package="g4-highcpu-4G"
-
-if [[ -n "${MG_SDC_ACCOUNT}" ]]; then
-  export SDC_ACCOUNT=$MG_SDC_ACCOUNT
-elif [[ -z ${SDC_ACCOUNT} ]]; then
-  export SDC_ACCOUNT="Joyent_Dev"
-fi
-if [[ -n "${MG_SDC_URL}" ]]; then
-  export SDC_URL=$MG_SDC_URL
-elif [[ -z ${SDC_URL} ]]; then
-  export SDC_URL="https://us-east-3.api.joyent.com"
-fi
-if [[ -n "${MG_SDC_KEY_ID}" ]]; then
-  export SDC_KEY_ID=$MG_SDC_KEY_ID
-elif [[ -z ${SDC_KEY_ID} ]]; then
-  export SDC_KEY_ID="$(ssh-keygen -l -f ~/.ssh/id_rsa.pub | awk '{print $2}' | tr -d '\n')"
-fi
-
-if [[ -z ${MANTA_USER} ]]; then
-  export MANTA_USER=${SDC_ACCOUNT}
-fi
-if [[ -z ${MANTA_URL} ]]; then
-  export MANTA_URL=https://us-east.manta.joyent.com
-fi
-
-if [[ -z ${MANTA_KEY_ID} ]]; then
-  export MANTA_KEY_ID="${SDC_KEY_ID}"
-fi
-
-# UUID of the created image/dataset.
-uuid=""
-
-image_uuid=""
-tarballs=""
-packages=""
-output=""
-
-
-#---- functions
-
-function fatal {
-  echo "$(basename $0): error: $1"
-
-  cleanup 1
-}
-
-function cleanup() {
-  local exit_status=${1:-$?}
-  if [[ "$KEEP_INFRA_ON_FAILURE" == "true" || "$KEEP_INFRA_ON_FAILURE" == 1 ]]; then
-    echo "$0: NOT cleaning up (KEEP_INFRA_ON_FAILURE=$KEEP_INFRA_ON_FAILURE)"
-  else
-    if [[ -n ${CREATED_MACHINE_UUID} ]]; then
-      sdc-deletemachine ${CREATED_MACHINE_UUID}
-    fi
-    if [[ -n ${CREATED_MACHINE_IMAGE_UUID} ]]; then
-      sdc-deleteimage ${CREATED_MACHINE_IMAGE_UUID}
-    fi
-  fi
-  exit ${exit_status}
-}
-
-function usage() {
-  if [[ -n "$1" ]]; then
-    echo "error: $1"
-    echo ""
-  fi
-  echo "Usage:"
-  echo "  prep_dataset.sh [OPTIONS]"
-  echo ""
-  echo "Options:"
-  echo "  -h              Print this help and exit."
-  echo "  -b BUILD_NAME   The name of the build (if different from the image name)"
-  echo "  -i IMAGE_UUID   The base image UUID."
-  echo "  -t TARBALL      Space-separated list of tarballs to unarchive into"
-  echo "                  the new image. A tarball is of the form:"
-  echo "                    TARBALL-ABSOLUTE-PATH-PATTERN[:SYSROOT]"
-  echo "                  The default 'SYSROOT' is '/'. A '/' sysroot is the"
-  echo "                  typical fs tarball layout with '/root' and '/site'"
-  echo "                  base dirs. This can be called multiple times for"
-  echo "                  more tarballs."
-  echo "  -p PACKAGES     Space-separated list of pkgsrc package to install."
-  echo "                  This can be called multiple times."
-
-  echo "  -P PACKAGE      Package (instance / limit) name to use (eg sdc_256)"
-  echo "  -o OUTPUT       Image output path. Should be of the form:"
-  echo "                  '/path/to/name.manta'."
-  echo "  -O MANTA DIR    Output dir in Manta (under \${MANTA_USER}) default:"
-  echo "                  '/stor/builds'"
-  echo "  -v VERSION      Version for produced image manifest."
-  echo "  -n NAME         NAME for the produced image manifest."
-  echo "  -d DESCRIPTION  DESCRIPTION for the produced image manifest."
-  echo ""
-  echo "Environment variables:"
-  echo "  MG_SDC_URL      If defined, this cloudapi endpoint will be used for"
-  echo "                  image creation. Otherwise it falls back to SDC_URL"
-  echo "                  or to a hardcoded default."
-  echo "  MG_SDC_ACCOUNT  If defined, this account will be used for image "
-  echo "                  creation and for Manta uploads. Otherwise it "
-  echo "                  falls back to SDC_ACCOUNT, or to 'Joyent_Dev'."
-  echo "  MG_SDC_KEY_ID   If defined, this is used for cloudapi and manta"
-  echo "                  auth. Otherwise it falls back to SDC_KEY_ID, or "
-  echo "                  to using '~/.ssh/id_rsa.pub'."
-  echo ""
-  exit 1
-}
-
-
-#---- mainline
-
-trap cleanup ERR
-
-if [[ -n ${SDC_LOCAL_BUILD} ]]; then
-  if [[ -z ${SDC_IMGAPI_URL} ]]; then
-    fatal "When building without Manta, you need to set \${SDC_IMGAPI_URL}"
-  fi
-  if [[ -z ${SDC_IMAGE_PACKAGE} ]]; then
-    fatal "When building without JPC, you need to set \${SDC_IMAGE_PACKAGE}"
-  fi
-
-  image_package=${SDC_IMAGE_PACKAGE}
-fi
-
-while getopts ht:p:P:i:o:O:n:v:d:b: opt; do
-  case ${opt} in
-  h)
-    usage
-    ;;
-  t)
-    if [[ -n ${OPTARG} ]]; then
-      tarballs="${tarballs} ${OPTARG}"
-    fi
-    ;;
-  p)
-    if [[ -n ${OPTARG} ]]; then
-      packages="${packages} ${OPTARG}"
-    fi
-    ;;
-  P)
-    if [[ -n ${OPTARG} ]]; then
-      image_package="${OPTARG}"
-    fi
-    ;;
-  i)
-    if [[ -n ${OPTARG} ]]; then
-        image_uuid=${OPTARG}
-    fi
-    ;;
-  o)
-    if [[ -n ${OPTARG} ]]; then
-        output="${OPTARG}"
-    fi
-    ;;
-  O)
-    if [[ -n ${OPTARG} ]]; then
-        MG_OUT_PATH="${OPTARG}"
-    fi
-    ;;
-  b)
-    if [[ -n ${OPTARG} ]]; then
-        build_name=${OPTARG}
-    fi
-    ;;
-  n)
-    if [[ -n ${OPTARG} ]]; then
-        image_name=${OPTARG}
-    fi
-    ;;
-  v)
-    if [[ -n ${OPTARG} ]]; then
-        image_version=${OPTARG}
-    fi
-    ;;
-  d)
-    if [[ -n ${OPTARG} ]]; then
-        image_description="${OPTARG}"
-    fi
-    ;;
-  \?)
-    echo "Invalid flag"
-    exit 1;
-  esac
-done
-
-if [[ -z ${output} ]]; then
-  fatal "No output file specified. Use '-o' option."
-fi
-
-[[ -n ${image_name} ]] || fatal "No image name, use '-n NAME'."
-[[ -n ${image_version} ]] || fatal "No image version, use '-v VERSION'."
-[[ -n ${image_description} ]] || image_description="${image_name}"
-[[ -n ${build_name} ]] || build_name=${image_name}
-
-if [[ -z ${image_uuid} ]]; then
-  fatal "No image_uuid provided. Use the '-i' option."
-fi
-
-# Create the machine in the specified DC
-package=$(sdc-listpackages | ${JSON} -c "this.name == '${image_package}'" 0.id)
-[[ -n ${package} ]] || fatal "cannot find package \"${image_package}\""
-
-machine=$(sdc-createmachine --dataset ${image_uuid} --package ${package} --tag MG_IMAGE_BUILD=true --name "TEMP-${build_name}-$(date +%s)"  | json id)
-[[ -n ${machine} ]] || fatal "cannot get uuid for new VM."
-
-# Set this here so from here out fatal() can try to destroy too.
-CREATED_MACHINE_UUID=${machine}
-
-echo "Wait up to 30 minutes for machine $machine to provision"
-for i in {1..360}; do
-  sleep 5
-  state=$(sdc-getmachine ${machine} | json state)
-  echo "Checking if machine $machine is provisioned (check $i of 360): $state"
-  if [[ $state != 'provisioning' ]]; then
-    break
-  fi
-done
-
-machine_json=$(sdc-getmachine ${machine})
-
-if [[ ${state} != 'running' ]]; then
-  echo "Problem with machine ${machine}"
-  exit 1
-fi
-SSH="ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null root@$(echo "${machine_json}" | json ips.0)"
-
-# Wait for the broken networking in east to settle (EASTONE-111)
-# we'll wait up to 20 minutes then attempt to delete the VM, this
-# used to be 10m but that wasn't even enough. :(
-waited=0
-while [[ ${waited} -lt 1200 && -z $(${SSH} zonename) ]]; do
-  sleep 5
-  waited=$((${waited} + 5))
-done
-if [[ ${waited} -ge 600 ]]; then
-  fatal "VM ${machine} still unavailable after ${waited} seconds."
-fi
-
-# The current smartos-prepare-image script built-in to IMGAPI that are
-# used for as part of image creation relies on having
-# /opt/local/bin/sm-prepare-image. The old smartos/1.6.3 image that
-# we currently use for some SDC images doesn't have that script. Add it.
-#     01b2c898-945f-11e1-a523-af1afbe22822  smartos/1.6.3
-#     fd2cc906-8938-11e3-beab-4359c665ac99  sdc-smartos/1.6.3
-if [[ ${image_uuid} == "01b2c898-945f-11e1-a523-af1afbe22822"
-      || ${image_uuid} == "fd2cc906-8938-11e3-beab-4359c665ac99" ]]; then
-    cat tools/clean-image.sh | ${SSH} "cat > /opt/local/bin/sm-prepare-image && chmod 755 /opt/local/bin/sm-prepare-image && cat /opt/local/bin/sm-prepare-image"
-fi
-
-# "tarballs" is a list of:
-#   TARBALL-ABSOLUTE-PATH-PATTERN[:SYSROOT]
-# e.g.:
-#   /root/joy/mountain-gorilla/bits/amon/amon-agent-*.tgz:/opt
-for tb_info in ${tarballs}; do
-  tb_tarball=$(echo "${tb_info}" | awk -F':' '{print $1}')
-  tb_sysroot=$(echo "${tb_info}" | awk -F':' '{print $2}')
-  [[ -z "${tb_sysroot}" ]] && tb_sysroot=/
-
-  bzip=$(echo ${tb_tarball} | grep "bz2$" || true)
-  if [[ -n ${bzip} ]]; then
-    uncompress=bzcat
-  else
-    uncompress=gzcat
-  fi
-
-  echo "Copying tarball '${tb_tarball}' to zone '${CREATED_MACHINE_UUID}'."
-  if [[ ${tb_sysroot} == "/" ]]; then
-    # Special case: for tb_sysroot == '/' we presume these are fs-tarball
-    # style tarballs with "/root/..." and "/site/...". We strip
-    # appropriately.
-    cat ${tb_tarball} | ${SSH} "cd / ; ${uncompress} | gtar --strip-components 1 -xf - root"
-  else
-    cat ${tb_tarball} | ${SSH} "cd ${tb_sysroot} ; ${uncompress} | gtar -xf -"
-  fi
-done
-
-##
-# install packages
-if [[ -n "${packages}" ]]; then
-  echo "Installing these pkgsrc package: '${packages}'"
-
-  ${SSH} "/opt/local/bin/pkgin -f -y update"
-  ${SSH} "touch /opt/local/.dlj_license_accepted"
-
-  #
-  # When pkgin fails, it ridiculously tells you that there are errors, but not
-  # what the errors are (those are hidden in a log file). Since we destroy the
-  # temporary VM, we are not able to access those log files. So whe pkgin fails,
-  # we want to grab some data about the pkgsrc setup to help figure out what
-  # went wrong.
-  #
-  ${SSH} "/opt/local/bin/pkgin -y in ${packages} || \
-      (echo ' === BEGIN /var/db/pkgin/pkg_install-err.log === '; \
-      cat /var/db/pkgin/pkg_install-err.log; \
-      echo '  === END /var/db/pkgin/pkg_install-err.log === '; \
-      echo '  === BEGIN pkg_info === '; \
-      pkg_info; \
-      echo '  === END pkg_info === '; \
-      echo '  === BEGIN /etc/pkgsrc_version === '; \
-      cat /etc/pkgsrc_version; \
-      echo '  === END /etc/pkgsrc_version === '; \
-      exit 1)"
-
-  echo "Validating pkgsrc installation"
-  for p in ${packages}
-  do
-    echo "Checking for ${p}"
-    PKG_OK=$(${SSH} "/opt/local/bin/pkgin -y list | grep ${p} || true")
-    if [[ -z "${PKG_OK}" ]]; then
-      echo "error: pkgin install failed (${p})"
-      exit 1
-    fi
-  done
-
-fi
-
-# And then turn it in to an image
-image=$(sdc-createimagefrommachine --machine ${machine} --name ${image_name}-zfs --imageVersion ${image_version} --description ${image_description} --tags '{"smartdc_service": true}')
-image_id=$(echo "${image}" | json -H 'id')
-
-# Set this here so from here out fatal() can try to destroy too.
-CREATED_MACHINE_IMAGE_UUID=${image_id}
-
-# wait up to 10 minutes for image creation
-waited=0
-state=$(sdc-getimage ${image_id} | json 'state')
-while [[ ${waited} -lt 600 ]] && [[ ${state} == "creating" || ${state} == "unactivated" ]]; do
-  sleep 5
-  waited=$((${waited} + 5))
-  state=$(sdc-getimage ${image_id} | json 'state')
-done
-
-if [[ "$KEEP_INFRA_ON_FAILURE" == "true" || "$KEEP_INFRA_ON_FAILURE" == 1 ]]; then
-    echo "$0: NOT deleting machine (KEEP_INFRA_ON_FAILURE=$KEEP_INFRA_ON_FAILURE)"
-else
-    sdc-deletemachine ${machine}
-fi
-
-if [[ "$(sdc-getimage ${image_id} | json 'state')" != "active" ]]; then
-  echo "Error creating image"
-  exit 1
-fi
-
-output_dir=$(dirname ${output})
-
-if [[ -n ${SDC_LOCAL_BUILD} ]]; then
-    echo "Downloading image ${image_id} from imgapi"
-
-    image_manifest_filename=${build_name}-zfs-${image_version}.imgmanifest
-    image_filename=${build_name}-zfs-${image_version}.zfs.gz
-    curl -sS -f -o ${output_dir}/${image_manifest_filename} ${SDC_IMGAPI_URL}/images/${image_id}
-    curl -sS -f -o ${output_dir}/${image_filename} ${SDC_IMGAPI_URL}/images/${image_id}/file
-
-else
-    mantapath=/${SDC_ACCOUNT}${MG_OUT_PATH}/${build_name}/$(echo ${image_version} | sed -E 's/-g[a-f0-9]{7,40}$//')/${build_name}
-    mmkdir -p ${mantapath}
-
-    manta_bits=/tmp/manta-exported-image.$$
-    sdc-exportimage --mantaPath ${mantapath} ${image_id} > ${manta_bits}
-
-    image_path=$(json image_path < ${manta_bits})
-    manifest_path=$(json manifest_path < ${manta_bits})
-
-    image_filename=$(basename ${image_path})
-    image_manifest_filename=$(basename ${manifest_path})
-
-    # XXX See TOOLS-359, basically binder has image_name = manta-nameservice which breaks
-    # backward compat when we switch to using JPC images. So I need to rename to the old
-    # name here.
-    if [[ ${image_name} != ${build_name} ]]; then
-      new_image_filename=$(echo ${image_filename} | sed -e "s/^${image_name}/${build_name}/")
-      new_image_manifest_filename=$(echo ${image_manifest_filename} | sed -e "s/^${image_name}/${build_name}/")
-      mln ${mantapath}/${image_filename} ${mantapath}/${new_image_filename}
-      mln ${mantapath}/${image_manifest_filename} ${mantapath}/${new_image_manifest_filename}
-      mrm ${mantapath}/${image_filename}
-      mrm ${mantapath}/${image_manifest_filename}
-      image_filename=${new_image_filename}
-      image_manifest_filename=${new_image_manifest_filename}
-      image_path=${mantapath}/${image_filename}
-      manifest_path=${mantapath}/${image_manifest_filename}
-
-      json \
-        -e "this.image_path='${image_path}'" \
-        -e "this.manifest_path='${manifest_path}'" \
-        < ${manta_bits} \
-        > ${manta_bits}.new \
-        && mv ${manta_bits}.new ${manta_bits}
-    fi
-
-    # XXX we download back from manta now just so other scripts work and we can publish
-    # to updates. Obviously it makes more sense not to do this, but there is not time to
-    # fix everything at once.
-    mget -o ${output_dir}/${image_filename} ${image_path}
-    [[ -f ${output_dir}/${image_filename} ]] || fatal "Failed to download ${image_filename}"
-    mget -o ${output_dir}/${image_manifest_filename} ${manifest_path}
-    [[ -f ${output_dir}/${image_manifest_filename} ]] || fatal "Failed to download ${image_manifest_filename}"
-fi
-
-# Image Notes:
-# - requirements.networks: Feels right to have the images saying they need
-#   to be on the admin network. (However, it isn't required for either of
-#   headnode setup or 'sdcadm up'.)
-# - We also set the min_platform to the platform we just built the bits on,
-#   not the platform we created the image on, since that's where the binary
-#   dependency should come from.
-# - And we remove the '-zfs' from the end of the name which we added
-#   for the filename.
-#   TODO: Use a basename in the 'sdc-exportimage' arg above, then change this
-#   to prefix the name with "BUILD-" or similar as a sign to not use these
-#   images for provisioning.
-# - We set owner to the "not set" UUID (see IMGAPI-408), as is done by
-#   updates.joyent.com itself on import.
-# - We change the UUID because with the above changes this really is a different
-#   beast. See RELENG-518.
-#
-cat ${output_dir}/${image_manifest_filename} \
-  | json -e 'this.requirements.networks = [{name: "net0", description: "admin"}]' \
-    -e "this.requirements.min_platform['7.0'] = '$(uname -v | cut -d '_' -f 2)'" \
-    -e "this.owner = '00000000-0000-0000-0000-000000000000'" \
-    -e "this.name = '${image_name}'" \
-    -e "this.uuid = '$(uuid)'" \
-  > ${output_dir}/${image_manifest_filename}.new \
-  && mv ${output_dir}/${image_manifest_filename}.new ${output_dir}/${image_manifest_filename}
-
-if [[ -z ${SDC_LOCAL_BUILD} ]]; then
-  mput -f ${output_dir}/${image_manifest_filename} ${manifest_path}
-  cat ${manta_bits}
-else
-  echo "Image is in ${output_dir} (not pushed to Manta)"
-fi
-
-if [[ "$KEEP_INFRA_ON_FAILURE" == "true" || "$KEEP_INFRA_ON_FAILURE" == 1 ]]; then
-    echo "$0: NOT deleting image (KEEP_INFRA_ON_FAILURE=$KEEP_INFRA_ON_FAILURE)"
-else
-    sdc-deleteimage ${image_id}
-fi
-
-
-exit 0
diff --git a/tools/purge-mg-builds b/tools/purge-builds-from-manta
similarity index 75%
rename from tools/purge-mg-builds
rename to tools/purge-builds-from-manta
index 45fe99d..8842966 100755
--- a/tools/purge-mg-builds
+++ b/tools/purge-builds-from-manta
@@ -6,25 +6,25 @@
 #
 
 #
-# Copyright 2019, Joyent, Inc.
+# Copyright 2019 Joyent, Inc.
 #
 
 #
-# Purge mountain-gorilla (MG) builds from Manta.
+# Purge Joyent eng builds from Manta.
 #
 # Usage:
-#       purge-mg-builds -h                              # help output
-#       purge-mg-builds /Joyent_Dev/public/builds       # dry-run by default
-#       purge-mg-builds -f /Joyent_Dev/public/builds    # '-f' to actually del
+#       purge-builds-from-manta -h                              # help output
+#       purge-builds-from-manta /Joyent_Dev/public/builds       # dry-run by default
+#       purge-builds-from-manta -f /Joyent_Dev/public/builds    # '-f' to actually del
 #
 # Builds build up in Manta. They need to eventually be purged so they don't
-# take up ever increasing space. This script knows how to remove old MG-style
+# take up ever increasing space. This script knows how to remove Joyent eng
 # builds from a given Manta dir. This script encodes retention policy for
 # Joyent builds per
 # [RFD 47](https://github.com/joyent/rfd/blob/master/rfd/0047/README.md#builds).
 #
-# Here "MG-style" means the typical build dir layout used by tooling in
-# mountain-gorilla.git (and some others) for Joyent engineering builds:
+# Here "eng builds" means the typical build dir layout used by tooling in
+# eng.git (and some others) for Joyent engineering builds:
 #
 #                                   # Example:
 #   $basedir/                       #   /Joyent_Dev/public/builds/
@@ -81,79 +81,25 @@ export PATH=$TOP/node_modules/.bin:$PATH
 #    platform-debug                 want to discuss with OS guys
 #    sdcsso                         build story isn't clear, eng hands off here
 #
+# If the build name is in neither the exclude list, nor the list of specific
+# TTL values, ttl_from_name() uses `jr` to verify it's a Manta/Triton component
+# and if so, applies a 365 day retention policy, and otherwise excludes it
+# from the purge.
+#
+DEFAULT_TTL_DAYS=365
+
+EXCLUDE_FROM_PURGE='[
+    "agentsshar-upgrade",
+    "platform",
+    "platform-debug",
+    "sdcsso"
+    ]'
+
 TTL_DAYS_FROM_NAME='{
     "headnode": 30,
     "headnode-debug": 30,
     "headnode-joyent": 30,
-    "headnode-joyent-debug": 30,
-
-    "adminui": 365,
-    "agents_core": 365,
-    "agentsshar": 365,
-    "amon": 365,
-    "amonredis": 365,
-    "assets": 365,
-    "binder": 365,
-    "cloudapi": 365,
-    "cmon": 365,
-    "cmon-agent": 365,
-    "cn-agent": 365,
-    "cnapi": 365,
-    "cns": 365,
-    "config-agent": 365,
-    "dhcpd": 365,
-    "docker": 365,
-    "dockerlogger": 365,
-    "electric-moray": 365,
-    "firewaller": 365,
-    "firmware-tools": 365,
-    "fwapi": 365,
-    "gz-tools": 365,
-    "hagfish-watcher": 365,
-    "heartbeater": 365,
-    "imgapi": 365,
-    "ipxe": 365,
-    "mackerel": 365,
-    "madtom": 365,
-    "mahi": 365,
-    "mako": 365,
-    "manta-deployment": 365,
-    "manta-manatee": 365,
-    "marlin": 365,
-    "marlin-dashboard": 365,
-    "medusa": 365,
-    "minnow": 365,
-    "mockcloud": 365,
-    "mockcn": 365,
-    "mola": 365,
-    "moray": 365,
-    "muppet": 365,
-    "muskie": 365,
-    "napi": 365,
-    "nat": 365,
-    "net-agent": 365,
-    "nfs": 365,
-    "nfsserver": 365,
-    "papi": 365,
-    "portolan": 365,
-    "provisioner": 365,
-    "rabbitmq": 365,
-    "redis": 365,
-    "registrar": 365,
-    "sapi": 365,
-    "sdc": 365,
-    "sdc-manatee": 365,
-    "sdc-system-tests": 365,
-    "sdc-zookeeper": 365,
-    "sdcadm": 365,
-    "smartlogin": 365,
-    "smartos": 365,
-    "ufds": 365,
-    "vm-agent": 365,
-    "vmapi": 365,
-    "volapi": 365,
-    "workflow": 365,
-    "wrasse": 365
+    "headnode-joyent-debug": 30
 }'
 
 
@@ -169,7 +115,7 @@ function usage() {
         echo ""
     fi
     echo 'Usage:'
-    echo '  purge-mg-builds [<options>] MANTA-BUILDS-DIR [NAMES...]'
+    echo '  purge-builds-from-manta [<options>] MANTA-BUILDS-DIR [NAMES...]'
     echo ''
     echo 'Options:'
     echo '  -h          Print this help and exit.'
@@ -178,9 +124,9 @@ function usage() {
     echo '  -f          Force actually doing deletions.'
     echo ''
     echo 'Examples:'
-    echo '  purge-mg-builds /Joyent_Dev/public/builds     # dry-run by default'
-    echo '  purge-mg-builds -f /Joyent_Dev/public/builds  # -f to actually rm'
-    echo '  purge-mg-builds -f /Joyent_Dev/public/builds vmapi   # limit subdir'
+    echo '  purge-builds-from-manta /Joyent_Dev/public/builds     # dry-run by default'
+    echo '  purge-builds-from-manta -f /Joyent_Dev/public/builds  # -f to actually rm'
+    echo '  purge-builds-from-manta -f /Joyent_Dev/public/builds vmapi   # limit subdir'
     if [[ -n "$1" ]]; then
         exit 1
     else
@@ -209,9 +155,36 @@ function log
 function ttl_days_from_name
 {
     local name
+    local ttl_days
+    local exclude
+    local known_component
+
     name=$1
 
-    echo "$TTL_DAYS_FROM_NAME" | json -- $name
+    # See whether we have a non-default TTL for this component.
+    ttl_days=$(echo "$TTL_DAYS_FROM_NAME" | json -- $name)
+    if [[ -n "$ttl_days" ]]; then
+        echo $ttl_days
+    fi
+
+    # See if it's in the exclude list. If json has not filtered any results,
+    # we get non-empty output, indicating $name was in the exclude list.
+    exclude=$(echo "$EXCLUDE_FROM_PURGE" |
+        json -ac "this.indexOf('$name') != -1")
+    if [[ -n "$exclude" ]]; then
+        echo ""
+    fi
+
+    # finally, check to see if this is a known manta/triton component, which
+    # gets our default retention policy. If json returns a non-empty string,
+    # that indicates that we know about this component.
+    known_component=$(jr list -j -l mg='*' |
+        json -ag -c "this.labels.mg === '$name'")
+    if [[ -n "$known_component" ]]; then
+        echo $DEFAULT_TTL_DAYS
+    else
+        echo ""
+    fi
 }
 
 function has_dir_expired
@@ -289,7 +262,7 @@ function purge_mg_builds
     if [[ $opt_dryrun != "no" ]]; then
         dryrun_msg=", dry-run"
     fi
-    log "# purge-mg-builds in $builds_dir/$name older than $cutoff" \
+    log "# purge-builds-from-manta in $builds_dir/$name older than $cutoff" \
         "(ttl $ttl_days days$dryrun_msg)"
     dir=$builds_dir/$name
     dirs=$(mls --type d $dir | sed -E 's#/$##')
diff --git a/tools/rm-old-builds.py b/tools/rm-old-builds.py
deleted file mode 100755
index 2d4eb45..0000000
--- a/tools/rm-old-builds.py
+++ /dev/null
@@ -1,113 +0,0 @@
-#!/usr/bin/env python
-#
-# This Source Code Form is subject to the terms of the Mozilla Public
-# License, v. 2.0. If a copy of the MPL was not distributed with this
-# file, You can obtain one at http://mozilla.org/MPL/2.0/.
-#
-
-#
-# Copyright (c) 2014, Joyent, Inc.
-#
-
-#
-# Remove old builds under here. This presumes the MG-uploaded
-# structure (see:
-# <https://mo.joyent.com/mountain-gorilla/blob/master/tools/upload-bits>)
-#
-# This script lives here: <https://mo.joyent.com/mountain-gorilla/tree/master/tools>
-#
-# Usage:
-#       python rm-old-builds.py [DIRS...]
-#
-# For each given DIR, remove all old MG-style build dirs. If DIRS is not given, then
-# all subdirs in the cwd are checked.
-#
-# "Old" is anything older than 20 days.
-#
-
-import sys
-import os
-from os.path import isdir, abspath, join, basename, islink, lexists
-import datetime
-import time
-from glob import glob
-import re
-from collections import defaultdict
-from pprint import pprint
-
-
-DRYRUN = False
-VERBOSE = True
-OLD = datetime.timedelta(days=20)
-BUILD_DIR_PATH = re.compile(r"^(?P<branch>.*?)-(?P<time>\d{8}T\d{6}Z)$")
-
-
-def rm_old_builds(dir):
-    print "# %s" % dir
-    # Get list of all build dirs.
-    subdirs = [d for d in glob(dir + "/*") if isdir(d)]
-    dirs_from_branch = defaultdict(set)
-    for d in subdirs:
-        match = BUILD_DIR_PATH.match(basename(d))
-        if not match:
-            continue
-        dirs_from_branch[match.group("branch")].add(d)
-    # Skip "release-*" branches.
-    for branch in list(dirs_from_branch.keys()):
-        if branch.startswith("release-"):
-            if VERBOSE:
-                print "# skip '%s' branch '%s' (release branch)" % (dir, branch)
-            del dirs_from_branch[branch]
-    # Skip most recent two and anything younger than "OLD".
-    for branch, dirs in dirs_from_branch.items():
-        mtime_and_dirs = [(os.stat(d).st_mtime, d) for d in dirs]
-        mtime_and_dirs.sort()
-        for mtime, d in mtime_and_dirs[-2:]: # skip most recent two
-            if VERBOSE:
-                print "# skip '%s' (most recent two)" % d
-            dirs.discard(d)
-    # Skip "$BRANCH-latest" linked dirs.
-    for branch, dirs in dirs_from_branch.items():
-        latest_dir = join(dir, branch + '-latest')
-        if islink(latest_dir) and lexists(latest_dir):
-            latest_target = os.readlink(latest_dir)
-            if latest_target in dirs:
-                if VERBOSE:
-                    print "# skip '%s' (*-latest target)" % join(dir, latest_target)
-                dirs.discard(latest_target)
-    # Skip young ones.
-    cutoff = time.mktime((datetime.datetime.now() - OLD).timetuple())
-    for branch, dirs in dirs_from_branch.items():
-        marked_for_death = set()
-        if len(dirs) > 20:
-            # Only keep up to 20 recent ones in the same branch, this is
-            # to avoid a dir with frequent builds swamping things.
-            marked_for_death = set(list(sorted(dirs))[:-20])
-        for d in list(dirs):
-            if d in marked_for_death:
-                continue
-            mtime = os.stat(d).st_mtime
-            if mtime > cutoff:
-                if VERBOSE:
-                    print "# skip '%s' (young)" % d
-                dirs.discard(d)
-    # Do it
-    for branch, dirs in dirs_from_branch.items():
-        for d in dirs:
-            cmd = "rm -rf %s" % d
-            print cmd
-            if not DRYRUN:
-                os.system(cmd)
-
-
-def main(argv):
-    if len(argv) == 1:
-        dirs = [d for d in os.listdir('.') if isdir(d)]
-    else:
-        dirs = argv[1:]
-        for d in dirs:
-            assert isdir(d), "'%s' is not a directory" % d
-    for d in dirs:
-        rm_old_builds(d)
-
-main(sys.argv)
diff --git a/tools/setup-build-zone b/tools/setup-build-zone
deleted file mode 100644
index 5b04edb..0000000
--- a/tools/setup-build-zone
+++ /dev/null
@@ -1,148 +0,0 @@
-#!/bin/bash
-#
-# This Source Code Form is subject to the terms of the Mozilla Public
-# License, v. 2.0. If a copy of the MPL was not distributed with this
-# file, You can obtain one at http://mozilla.org/MPL/2.0/.
-#
-
-#
-# Copyright (c) 2014, Joyent, Inc.
-#
-
-#
-# This script lives at 
-# <https://mo.joyent.com/mountain-gorilla/blob/master/tools/setup-build-zone>.
-#
-# This script automates part of the process of setting up a zone for SDC builds
-# and getting all the dependent pieces.
-#
-shopt -s xpg_echo
-
-#
-# We are purposefully putting some of the GNU tools first... While this is
-# usually not a good thing, it does make our lives a fair bit easier for what we
-# have to do.
-#
-PATH=/opt/local/bin:/opt/local/sbin:/usr/bin:/usr/sbin:/bin
-export "PATH=$PATH"
-
-sbz_arg0=$(basename $0)
-sbz_fake_tgz=fake-subset.tbz2
-sbz_fake_loc="/root/$sbz_fake_tgz"
-sbz_fake_root="/opt"
-sbz_fake_target="$sbz_fake_root/$sbz_fake_tgz"
-sbz_fake_dir="$sbz_fake_root/fake-subset"
-sbz_pkgsrc_dir="/opt/local"
-
-sbz_pkgin_list="scmgit"
-
-sbz_env_flag=
-
-
-function fail
-{
-	local msg="$*"
-	[[ -z "$msg" ]] && msg="failed"
-	echo "$sbz_arg0: $msg" >&2
-	exit 1
-}
-
-function usage
-{
-	local msg="$*"
-	[[ -z "$msg" ]] && echo "$msg" >&2
-	cat >&2 <<EOF
-	Usage: $sbz_arg0 [ -e ]
-
-Set up a provisioned zone for building illumos-live, illumos-joyent, and
-illumos-extra. This sets up fake files, insures certain pkgsrc prequisites
-are met, and handles crle and mounts.
-
-	-e		Only set up mounts and crle. This should be used any
-			time a zone has been rebooted after it has been
-			prepared.
-EOF
-	exit 2
-}
-
-function fail_if_already_setup
-{
-	if [[ -n $(mount | grep fake-subset) ]]; then
-		fail "Already setup."
-	fi
-}
-
-#
-# XXX We assume that fake-subset.tbz2 is in /root
-#
-function setup_fakes
-{
-	local usrdirs
-	[[ -f $sbz_fake_loc ]] || fail "missing $sbz_fake_loc"
-	cp $sbz_fake_loc $sbz_fake_target
-	gtar xjf $sbz_fake_target -C $sbz_fake_root
-	[[ -d $sbz_fake_dir/usr ]] || fail "fake files missing /usr"
-	[[ $? -eq 0 ]] || fail "failed to extract fake files"
-	gcp -a /usr $sbz_fake_dir
-	[[ $? -eq 0 ]] || fail "failed to copy /usr"
-	gfind /usr -type f \( -perm -04000 \) -exec chmod u+s $sbz_fake_dir/'{}' \;
-	[[ $? -eq 0 ]] || fail "failed to fix suid on /usr"
-	gfind /usr -type f \( -perm -02000 \) -exec chmod g+s $sbz_fake_dir/'{}' \;
-	[[ $? -eq 0 ]] || fail "failed to fix sgid on /usr"
-	return 0
-}
-
-#
-# Take care of all the crle and mounting necessary. This is done as a separate
-# function to facilitate itself as an eventual separate target
-#
-function setup_environment
-{
-	mount -O -F lofs $sbz_fake_dir/usr /usr
-	[[ $? -eq 0 ]] || fail "failed to mount /usr"
-	crle -u -l /usr/sfw/lib
-	[[ $? -eq 0 ]] || fail "failed to add /usr/sfw/lib to crle"
-	crle -64 -u -l /usr/sfw/lib/64
-	[[ $? -eq 0 ]] || fail "failed to add /usr/sfw/lib/64 to crle"
-	return 0
-}
-
-function install_packages
-{
-	pkgin -f update > /dev/null
-	[[ $? -eq 0 ]] || fail "failed to update packages"
-	pkgin -y in $sbz_pkgin_list > /dev/null
-	[[ $? -eq 0 ]]  || fail "failed to install packages"
-	return 0
-}
-
-while getopts "e" c $@; do
-	case "$c" in
-	e)
-		sbz_env_flag=1
-		;;
-	*)
-		usage "invalid option: $OPTARG"
-		;;
-	esac
-done
-
-fail_if_already_setup
-
-if [[ $sbz_env_flag -ne 1 ]]; then
-	echo "Setting up fake files ... \c "
-	setup_fakes || fail "failed"
-	echo "done."
-
-	echo "Installing necessary packages ... \c "
-	install_packages || fail "failed"
-	echo "done."
-
-  echo "Installing node-manta ... \c "
-  npm install -g manta
-  echo "done."
-fi
-
-echo "Setting up environment ... \c "
-setup_environment || fail "failed"
-echo "done."
diff --git a/tools/setup-cloudapi.sh b/tools/setup-cloudapi.sh
deleted file mode 100755
index 65a29a2..0000000
--- a/tools/setup-cloudapi.sh
+++ /dev/null
@@ -1,68 +0,0 @@
-#!/bin/bash
-#
-# This Source Code Form is subject to the terms of the Mozilla Public
-# License, v. 2.0. If a copy of the MPL was not distributed with this
-# file, You can obtain one at http://mozilla.org/MPL/2.0/.
-#
-
-#
-# Copyright (c) 2014, Joyent, Inc.
-#
-
-#
-# This script gets called as:
-#
-# tools/setup-cloudapi.sh <remote>
-#
-# where <remote> is a user@host combination to which you can ssh and connect
-# to the global zone of a SDC headnode. This script will then setup cloudapi on
-# this headnode.
-#
-
-set -o errexit
-set -o xtrace
-
-REMOTE=$1
-
-function fatal() {
-    echo "$@" >&2
-    exit 2
-}
-
-[[ -n ${REMOTE} && -z $2 ]] || fatal "Usage: $0 <remote>"
-
-ssh -T ${REMOTE} <<EOF
-export PATH="/usr/bin:/usr/sbin:/smartdc/bin:/opt/smartdc/bin:/opt/local/bin:/opt/local/sbin:/opt/smartdc/agents/bin"
-
-# Create cloudapi
-ADMIN_UUID=\$(bash /lib/sdc/config.sh -json | json ufds_admin_uuid)
-
-NUM_ZONES=\$(sdc-vmapi /vms?owner_uuid=\$ADMIN_UUID\&tag.smartdc_role=cloudapi </dev/null | json -H length)
-if [[ \$NUM_ZONES < 1 ]]; then
-    echo "Provision cloudapi zone."
-    cat <<EOM | sapiadm provision
-{
-    "service_uuid": "\$(sdc-sapi /services?name=cloudapi </dev/null | json -H 0.uuid)",
-    "params": {
-        "alias": "cloudapi0",
-        "networks": [
-            {
-                "uuid": "\$(sdc-napi /networks </dev/null | json -H -c 'this.name=="admin"' 0.uuid)"
-            },
-            {
-                "uuid": "\$(sdc-napi /networks </dev/null | json -H -c 'this.name=="external"' 0.uuid)",
-                "primary": true
-            }
-        ]
-    }
-}
-EOM
-else
-    echo "Already have a cloudapi zone."
-fi
-
-echo "Should set SDC_URL to one of:"
-for ip in \$(sdc-vmapi /vms?owner_uuid=\$ADMIN_UUID\&tag.smartdc_role=cloudapi | json -Ha nics.0.ip nics.1.ip); do
-    echo "SDC_URL=\"https://\${ip}\""
-done
-EOF
diff --git a/tools/setup-remote-build-zone.sh b/tools/setup-remote-build-zone.sh
deleted file mode 100755
index 5680f98..0000000
--- a/tools/setup-remote-build-zone.sh
+++ /dev/null
@@ -1,133 +0,0 @@
-#!/bin/bash
-#
-# This Source Code Form is subject to the terms of the Mozilla Public
-# License, v. 2.0. If a copy of the MPL was not distributed with this
-# file, You can obtain one at http://mozilla.org/MPL/2.0/.
-#
-
-#
-# Copyright (c) 2014, Joyent, Inc.
-#
-
-#
-# This script gets called as:
-#
-# setup-remote-build-zone.sh <remote>
-#
-# where <remote> is an SSH target like root@<IP/hostname>. This will then
-# login to the remote zone and set it up to be able to build MG targets
-# designated for the image the zone is running.
-#
-# If the remote is not a zone or is not running a known image, this script
-# will abort.
-#
-# In order to work successfully, the remote zone should be running one of the
-# following images:
-#
-#   sdc-smartos 1.6.3
-#   sdc-multiarch 13.3.1
-#
-# IMPORTANT:
-#
-#  You must have the variables: MANTA_USER, MANTA_KEY_ID, MANTA_URL, SDC_ACCOUNT
-#  SDC_KEY_ID and SDC_URL set in your environment when you run this script.
-#
-#  Ideally you will point this script at a *brand new* zone without existing
-#  modifications. If there are existing modifications, there may be unexpected
-#  build problems.
-#
-
-set -o errexit
-if [[ -n ${TRACE} ]]; then
-    set -o xtrace
-fi
-
-function fatal() {
-    echo "$@" >&2
-    exit 2
-}
-
-[[ -n ${MANTA_USER} ]] || fatal "\${MANTA_USER} is not set"
-[[ -n ${MANTA_KEY_ID} ]] || fatal "\${MANTA_KEY_ID} is not set"
-[[ -n ${MANTA_URL} ]] || fatal "\${MANTA_URL} is not set"
-[[ -n ${SDC_ACCOUNT} ]] || fatal "\${SDC_ACCOUNT} is not set"
-[[ -n ${SDC_KEY_ID} ]] || fatal "\${SDC_KEY_ID} is not set"
-[[ -n ${SDC_URL} ]] || fatal "\${SDC_URL} is not set"
-
-REMOTE=$1
-
-[[ -z ${REMOTE} || -n $2 ]] && fatal "Usage: $0 <remote>"
-
-ssh -T ${REMOTE} <<EOF
-#!/bin/bash
-
-set -o errexit
-if [[ -n "${TRACE}" ]]; then
-    set -o xtrace
-fi
-
-IMAGE=\$(mdata-get sdc:image_uuid || true)
-if [[ \${IMAGE} == "fd2cc906-8938-11e3-beab-4359c665ac99" ]]; then
-    VERSION="1.6.3"
-elif [[ \${IMAGE} == "b4bdc598-8939-11e3-bea4-8341f6861379" ]]; then
-    VERSION="13.3.1"
-else
-    echo "Unknown image. Try again with a zone running a known image." >&2
-    exit 1
-fi
-
-echo "Found image version \${VERSION}"
-
-echo "Installing packages..."
-
-if [[ \${VERSION} == "1.6.3" ]]; then
-    SDCNODE_BUILD="https://download.joyent.com/pub/build/sdcnode/fd2cc906-8938-11e3-beab-4359c665ac99/master-20140830T012655Z/sdcnode/sdcnode-v0.10.26-zone-fd2cc906-8938-11e3-beab-4359c665ac99-master-20140829T232408Z-g649a9b0.tgz"
-    pkgin -y install binutils
-    pkgin -y install scmgit gcc-compiler gmake python26 \
-        gcc-runtime gcc-tools png GeoIP GeoLiteCity ghostscript \
-        zookeeper-client postgresql91-client-9.1.2 gsharutils
-    [[ ! -e /opt/local/bin/gld ]] && (cd /opt/local/bin && ln -s ld gld)
-else
-    SDCNODE_BUILD="https://download.joyent.com/pub/build/sdcnode/b4bdc598-8939-11e3-bea4-8341f6861379/master-20140830T005350Z/sdcnode/sdcnode-v0.10.26-zone-b4bdc598-8939-11e3-bea4-8341f6861379-master-20140829T232139Z-g649a9b0.tgz"
-    pkgin -y install binutils
-    pkgin -y install gcc47 gmake \
-        scmgit python26 png GeoIP GeoLiteCity ghostscript zookeeper-client \
-        gsharutils build-essential postgresql91-client
-fi
-
-echo "Fixing ~/.bashrc..."
-
-grep "^MANTA_USER=" ~/.bashrc \
-    || echo "export MANTA_USER=\"${MANTA_USER}\"" >> ~/.bashrc
-grep "^MANTA_KEY_ID=" ~/.bashrc \
-    || echo "export MANTA_KEY_ID=\"${MANTA_KEY_ID}\"" >> ~/.bashrc
-grep "^MANTA_URL=" ~/.bashrc \
-    || echo "export MANTA_URL=\"${MANTA_URL}\"" >> ~/.bashrc
-grep "^SDC_ACCOUNT=" ~/.bashrc \
-    || echo "export SDC_ACCOUNT=\"${SDC_ACCOUNT}\"" >> ~/.bashrc
-grep "^SDC_KEY_ID=" ~/.bashrc \
-    || echo "export SDC_KEY_ID=\"${SDC_KEY_ID}\"" >> ~/.bashrc
-grep "^SDC_URL=" ~/.bashrc \
-    || echo "export SDC_URL=\"${SDC_URL}\"" >> ~/.bashrc
-grep "^PATH=.*/root/opt/node/bin" ~/.bashrc \
-    || echo "export PATH=\"/opt/local/bin:/opt/local/sbin:/usr/bin:/usr/sbin:/root/opt/node/bin\"" >> ~/.bashrc
-
-echo "Ensuring we have ~/opt/node..."
-
-if [[ ! -f /root/opt/node/bin/node ]]; then
-   mkdir -p ~/opt && (cd ~/opt && curl \${SDCNODE_BUILD} | tar -zxvf -)
-   /root/opt/node/bin/node /root/opt/node/lib/node_modules/npm/cli.js install -gf npm
-fi
-
-echo "Setting ~/.npmrc..."
-
-echo "registry = http://registry.npmjs.org/" > ~/.npmrc
-
-touch /opt/local/.dlj_license_accepted
-
-echo 'DONE!'
-
-exit 0
-EOF
-
-exit 0
diff --git a/tools/targets-1.6.3.sh b/tools/targets-1.6.3.sh
deleted file mode 100755
index e5779f2..0000000
--- a/tools/targets-1.6.3.sh
+++ /dev/null
@@ -1,36 +0,0 @@
-#/bin/bash
-#
-# This Source Code Form is subject to the terms of the Mozilla Public
-# License, v. 2.0. If a copy of the MPL was not distributed with this
-# file, You can obtain one at http://mozilla.org/MPL/2.0/.
-#
-
-#
-# Copyright (c) 2018, Joyent, Inc.
-#
-
-#
-# This script outputs a list of targets that can be built with MG in a
-# sdc-smartos 1.6.3 build zone, in the order the should be built.
-#
-
-BASE=$(cd $(dirname $0) && pwd)/../
-
-# These are dependencies for other builds below
-echo "registrar"
-echo "config-agent"
-echo "amon"
-echo "minnow"
-
-# The bulk of the builds come from here. Anything that's not requiring 13.3.1
-# should be built on 1.6.3.
-cd ${BASE}
-if [[ ! -f "targets.json" ]]; then
-    bash < targets.json.in | json > targets.json
-fi
-node -e "var targets = require('./targets.json'); Object.keys(targets).forEach(function (t) { if (['all', 'usbheadnode', 'usbheadnode-debug', 'platform', 'platform-debug', 'agentsshar', 'minnow', 'amon', 'registar', 'config-agent', 'mockcloud'].indexOf(t) !== -1) { return; }; if (targets[t].image_uuid !== 'b4bdc598-8939-11e3-bea4-8341f6861379') { console.log(t); } });" | sort
-
-# This comes last as it depends on all the individual agents to be built first.
-echo "agentsshar"
-
-exit 0
diff --git a/tools/targets-13.3.1.sh b/tools/targets-13.3.1.sh
deleted file mode 100755
index 9f4e225..0000000
--- a/tools/targets-13.3.1.sh
+++ /dev/null
@@ -1,27 +0,0 @@
-#/bin/bash
-#
-# This Source Code Form is subject to the terms of the Mozilla Public
-# License, v. 2.0. If a copy of the MPL was not distributed with this
-# file, You can obtain one at http://mozilla.org/MPL/2.0/.
-#
-
-#
-# Copyright (c) 2014, Joyent, Inc.
-#
-
-#
-# This script outputs a list of targets that can be built with MG in a
-# sdc-multiarch 13.3.1 build zone, in the order the should be built.
-#
-
-BASE=$(cd $(dirname $0) && pwd)/../
-
-cd ${BASE}
-if [[ ! -f "targets.json" ]]; then
-    bash < targets.json.in | json > targets.json
-fi
-
-# Anything with the image_uuid set to the 13.3.1 image should be included.
-node -e "var targets = require('./targets.json'); Object.keys(targets).forEach(function (t) { if (targets[t].image_uuid === 'b4bdc598-8939-11e3-bea4-8341f6861379') { console.log(t); } });" | sort
-
-exit 0
diff --git a/tools/validate-dc-for-image-creation.sh b/tools/validate-dc-for-image-creation.sh
deleted file mode 100755
index 807b1ec..0000000
--- a/tools/validate-dc-for-image-creation.sh
+++ /dev/null
@@ -1,176 +0,0 @@
-#!/bin/bash
-#
-# This Source Code Form is subject to the terms of the Mozilla Public
-# License, v. 2.0. If a copy of the MPL was not distributed with this
-# file, You can obtain one at http://mozilla.org/MPL/2.0/.
-#
-
-#
-# Copyright (c) 2017, Joyent, Inc.
-#
-
-#
-# Validate a given Triton CLI profile for usage as the target DC (and account)
-# for image creation (i.e. what `tools/prep_dataset_in_jpc.sh` does).
-#
-# Requirements:
-# - the profile's account (typically this is Joyent_Dev for core builds) has
-#   access to all origin images (per "image_uuid" entries in "targets.json.in")
-# - capacity for N instances
-#
-# Warning: To test capacity, this *provisions a number of instances*.
-#
-
-if [[ -n "$TRACE" ]]; then
-    export PS4='[\D{%FT%TZ}] ${BASH_SOURCE}:${LINENO}: ${FUNCNAME[0]:+${FUNCNAME[0]}(): }'
-    set -o xtrace
-fi
-set -o errexit
-set -o pipefail
-
-#---- globals, config
-
-TOP=$(cd $(dirname $0)/../; pwd)
-
-# The number of simultaneous instances we use to test the DCs capacity.
-# Theoretically this means we could do N parallel image builds in this DC.
-# Eventually I think this number should approach the number of images we
-# build so that we could do the entire build suite in parallel.
-N=5
-
-INST_NAME_PREFIX=TMP-validate-dc-for-image-creation-$(hostname)
-PROFILE=
-
-
-#---- functions
-
-function usage() {
-    if [[ -n "$1" ]]; then
-        echo "$(basename $0): error: $1"
-        echo ""
-    fi
-    echo "Usage:"
-    echo "    validate-dc-for-image-creation.sh [<options>] TRITON-CLI-PROFILE"
-    echo ""
-    echo "Options:"
-    echo "    -h          Print this help and exit."
-    if [[ -n "$1" ]]; then
-        exit 1
-    else
-        exit 0
-    fi
-}
-
-function fatal {
-    echo "$(basename $0): error: $1"
-    clean_insts
-    exit 1
-}
-
-function clean_insts {
-    inst_ids=$(triton -p $PROFILE ls -j \
-        | json -ga -c "this.name.indexOf('$INST_NAME_PREFIX') === 0 && this.tags.pid === $$" id \
-        | xargs)
-    if [[ -n "$inst_ids" ]]; then
-        echo ""
-        echo "Cleaning up instances:"
-        triton -p $PROFILE inst rm -w $inst_ids
-    fi
-}
-
-
-#---- mainline
-
-while getopts "h" ch; do
-    case "$ch" in
-    h)
-        usage
-        ;;
-    *)
-        usage "illegal option -- $OPTARG"
-        ;;
-    esac
-done
-shift $((OPTIND - 1))
-
-PROFILE=$1
-[[ -n "$PROFILE" ]] || fatal "missing TRITON-CLI-PROFILE arg"
-
-# First clear out provisions from earlier runs.
-earlier_insts=$(triton -p $PROFILE ls -j \
-    | json -ga -c "this.name.indexOf('$INST_NAME_PREFIX') === 0")
-if [[ -n "$earlier_insts" ]]; then
-    echo "There are insts from earlier runs:"
-    echo "$earlier_insts" | json -ga shortid name age | sed -e 's/^/    /'
-    echo "Deleting them first:"
-    triton -p $PROFILE inst rm -w $(echo "$earlier_insts" | json -ga id | xargs)
-    echo ""
-fi
-
-# First test that we have access to all of the images.
-images=$(bash $TOP/targets.json.in 2>/dev/null \
-    | json -M -a value.image_uuid -C 'this.value.image_uuid' \
-    | sort | uniq)
-echo "# Validating access to origin images"
-for image in $images; do
-    set +o errexit
-    output=$(triton -p $PROFILE image get $image 2>&1)
-    retval=$?
-    set -o errexit
-    case "$retval" in
-    0)
-        echo "- Have access to image '$image'"
-        ;;
-    3)
-        fatal "cannot find image $image with triton profile '$PROFILE'"
-        ;;
-    *)
-        fatal "unexpected error looking up image $image: $output"
-    esac
-done
-
-# Test capacity by provisioning N.
-package=$(triton -p $PROFILE pkgs memory=4096 -H -o name | head -1)
-[[ -n "$package" ]] || fatal "could not find a package with memory=4096"
-image=$(echo "$images" | tail -1)
-
-echo ""
-echo "# Validating capacity with $N provisions"
-for i in $(seq 0 $(($N - 1))); do
-    triton -p $PROFILE create -n ${INST_NAME_PREFIX}-$(printf "%03d" $i) \
-        -t pid=$$ \
-        $image $package
-done
-
-# Wait for those provisions to complete.
-# Doing a `triton ls` search immediately often misses the last "triton create".
-# That sucks. Solution: retry a few times until we get the expected number.
-insts=
-attempts=5
-while true; do
-    if [[ $attempts -le 0 ]]; then
-        fatal "'triton -p $PROFILE ls | ...' is not resolving to $N instances: $insts"
-    fi
-    attempts=$(( $attempts - 1 ))
-
-    insts=$(triton -p $PROFILE ls -j | json -ga -c "this.tags && this.tags.pid === $$")
-    if [[ $(echo "$insts" | json -gA length) == "$N" ]]; then
-        break
-    fi
-    sleep 1
-done
-triton -p $PROFILE inst wait $(echo "$insts" | json -ga id | xargs)
-
-not_running=$(triton -p $PROFILE ls -j \
-    | json -ga -c "this.tags && this.tags.pid === $$ && this.state !== 'running'")
-if [[ -n "$not_running" ]]; then
-    echo "$(basename $0): capacity error: the following instance provisions failed:" >&2
-    echo "$not_running" | json -ga id name state | sed -e 's/^/    /'
-    clean_insts
-    exit 1
-else
-    clean_insts
-fi
-
-echo ""
-echo "Success. It looks like this DC (profile $PROFILE) could work."
-- 
2.21.0

