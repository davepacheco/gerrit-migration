commit 3330572984352bd6810a167eb60e96119db196d6 (refs/changes/93/4993/8)
Author: Jan Wyszynski <jan.wyszynski@joyent.com>
Date:   2019-01-11T00:56:49+00:00 (9 months ago)
    
    MANTA-3977 improve manta-garbage-collector documentation
    Reviewed by: Robert Bogart <robert.bogart@joyent.com>
    Approved by: Robert Bogart <robert.bogart@joyent.com>

diff --git a/docs/operator-guide.md b/docs/operator-guide.md
index b4c2f74..7093c9b 100644
--- a/docs/operator-guide.md
+++ b/docs/operator-guide.md
@@ -118,6 +118,8 @@ In order to make all this work, there are several other pieces:
   contents of the metadata tier with the contents of the storage tier to
   identify deleted objects, remove them, and verify that all other objects are
   replicated as they should be.
+* An **accelerated garbage collection** system that leverages some additional
+  constraints on object uploads to allow for faster space reclamation.
 * A **metering** system periodically processes log files generated by the rest
   of the system to produce reports that are ultimately turned into invoices.
 * A couple of **dashboards** provide visibility into what the system is doing at
@@ -560,6 +562,41 @@ used), storage metering (how much storage is used), request metering, and
 bandwidth metering.  These are compute jobs run over the compute logs (produced
 by the marlin agent), the metadata dumps, and the muskie request logs.
 
+## Accelerated Garbage Collection
+
+Accelerated garbage-collection is a mechanism for lower latency storage space
+reclamation for objects owned by accounts whose data is guaranteed not to be
+snaplinked. Unlike traditional garbage-collection, accelerated garbage-collection
+does not rely on the jobs-tier or backups of the metadata tier. However, the
+actual object file deletion is still done with the cron-scheduled `mako_gc.sh`
+script.
+
+Manta's offline garbage-collection system relies on a Manta job that periodically
+processes database dumps from all shards to locate unreferenced object data and
+mark it for cleanup. Objects which are found to only be referenced in the
+`manta_delete_log` after a system-wide grace-period are marked for deletion.
+This grace-period is intended to reduce the probability of deleting a snaplinked
+object. The reason the system processes database dumps from all shards is that,
+in a traditional setting, a single object may have multiple references on different
+metadata nodes.
+
+Accelerated garbage-collection processes only objects that are owned by
+snaplink-disabled accounts. Such objects are guaranteed to have a single
+metadata reference, and the delete records generated for them are added to the
+`manta_fastdelete_queue` as opposed to the `manta_delete_log`. A
+"garbage-collector" reads records from the queue and marks the corresponding
+objects for deletion. Since metadata records are removed from the `manta` table
+and added to the `manta_fastdelete_queue` in the same database transaction, rows
+in the `manta_fastdelete_queue` refer to objects that are no longer visible to
+Manta users.
+
+For Manta users whose data has never been snaplinked, the system provides an
+option to trade Manta's snaplink functionality for lower-latency storage space
+reclamation. While accelerated GC can be deployed in conjuction with the
+traditional offline GC system, it will only process objects owned by
+snaplink-disabled accounts. Objects owned by other accounts will continue to be
+processed by the traditional garbage-collection pipeline.
+
 ## Manta Scalability
 
 There are many dimensions to scalability.
@@ -699,6 +736,10 @@ but for reference, here are the principles to keep in mind:
 * For **moray**, there should be at least two instances per shard in the entire
   deployment, and these instances should reside on separate compute nodes (and
   preferably separate datacenters).
+* **garbage-collector** zones should not be configured to poll more than 6
+  shards. Further, they should not be co-located with instances of other
+  CPU-intensive Manta components (e.g. loadbalancer)  to avoid interference
+  with the data path.
 
 Most of these constraints are required in order to maintain availability in the
 event of failure of any component, server, or datacenter.  Below are some
@@ -1040,6 +1081,232 @@ Lastly test the CLI tools from your development machine:
     $ mget ~~/stor/hello-foo
     Hello, Manta
 
+### Deploy Accelerated Garbage Collection Components
+
+While the `manta-init` deployment script creates the SAPI `garbage-collector`
+service, none of the default deployment layouts include any garbage-collector
+instances. The accelerated gc functionality is intended to be an extension to
+Manta, so it is deployed and managed with a separate `manta-adm` mode after the
+core Manta components are deployed.
+
+#### Verify objects have no snaplinks
+
+Accelerated garbage-collection can only be used on objects that have no
+snaplinks. Part of the deployment sequence involves disabling snaplinks for a
+given user that owns the objects to be deleted. In order to prevent dangling
+metadata references, it is important first verify that the user does not own
+any snaplinked objects. Manta does not have tools to run this check, but one
+method for doing it might be to scan the Manta table on each shard primary to
+generate a mapping from `objectid` to `_key` and search for collisions.
+
+#### Get the latest garbage-collector zone image
+
+    headnode$ updates-imgadm import name=manta-garbage-collector --latest
+
+#### Enable Accelerated Garbage-collection (Disable snaplinks)
+
+In order to leverage accelerated garbage-collection, there must be at least one
+snaplink-disabled user in the Manta deployment. Snaplinks can be disabled
+using the following command:
+
+    headnode$ manta-adm accel-gc enable <ACCOUNT-LOGIN>
+
+To list the accounts for which accelerated garbage-collection is enabled run:
+
+    headnode$ manta-adm accel-gc accounts
+
+To disable accelerated garbage-collection (re-enabling snaplinks) for an
+account:
+
+    headnode$ manta-adm accel-gc disable <ACCOUNT-LOGIN>
+
+Since the snaplink-disabled bit for each account is stored as SAPI application
+metadata, enabling/disabling snaplinks requires restarting all Muskies in the
+Manta deployment:
+
+    headnode$ manta-oneach -s webapi "svcadm restart
+        svc:/manta/application/muskie:muskie-*"
+
+It is also necessary to restart garbage-collectors for the same reason.
+
+    headnode$ manta-oneach -s garbage-collector
+        "svcadm restart garbage-collector"
+
+After the restarts are complete any requests to snaplink objects owned by
+`<ACCOUNT-LOGIN>` will fail with a 403 error, and all successful requests to delete
+objects created and/or owned by the snaplink-disabled account will result in new
+records added to the `manta_fastdelete_queue` (and not the `manta_delete_log`).
+
+To summarize, objects owned by a snaplink-disabled account will not be
+garbage-collected until:
+
+* Shards storing those objects are assigned to some garbage-collector
+* The account has been marked snaplink-disabled, and that bit of configuration
+is picked up by all the Muskies and Garbage-collectors in the delpoyment.
+
+A simple check that the accelerated GC pipeline is working can be done by
+logging into an index shard and continuously monitoring the size of the
+`manta_fastdelete_queue` while issuing requests to delete objects owned by the
+snaplink-disabled account. Depending on the batch size configuration of the
+garbage-collectors in the local Manta deployment, it could take roughly 30
+seconds for records inserted into the queue to be removed, but **the queue
+should always eventually be cleared**.
+
+For more sophisticated progress monitoring, see the node-artedi probes described
+in the garbage-collector
+[readme](https://github.com/joyent/manta-garbage-collector/blob/master/README.md).
+
+#### Deploy garbage-collector zones
+
+garbage-collector instances are deployed just like any other Manta service: by
+including them in the service layout configuration and then passing the config to
+`manta-adm update`. The `manta-adm accel-gc` mode includes its own genconfig-like
+functionality that layers any number of garbage-collector instances onto the
+current deployment configuration. To generate a configuration with `N`
+garbage-collectors, run:
+
+    headnode$ IMAGEUUID=$(sdc-imgadm list -H -o uuid name=manta-garbage-collector --latest)
+    headnode$ manta-adm accel-gc genconfig $IMAGEUUID N &> config-with-gc.json
+
+`manta-adm accel-gc genconfig` functions similarly to `manta-adm genconfig` which
+print a json-formatted deployment layout. Like `manta-adm genconfig`, the 'gc'
+variant also enforces a set of criteria for choosing which compute nodes to add
+garbage-collector instance to:
+
+- `manta-adm accel-gc` will avoid co-locating garbage-collector zones with either
+  loadbalancer zones or nameservice zones. For delete-heavy workloads,
+  garbage-collector instances will generally use more CPU and memory resources.
+  Separating theses process from system-critical services is a precaution against
+  catastrophic service degradation.
+- `manta-adm accel-gc` will evenly distribute the garbage-collector zones among the
+  CNs with least number of deployed instances that also meet the first criterion
+
+If the above constraints cannot be met in a deployment (this will likely be
+the case for most non-production deployments), the operator may choose to ignore
+them with the `--ignore-criteria` or `-i` switch:
+
+    headnode$ manta-adm accel-gc genconfig -i $IMAGEUUID N &> config-with-gc.json
+
+Once you are happy with configuration, run:
+
+    headnode$ manta-adm update config-with-gc.json
+
+to deploy the new zones.
+
+#### Verify garbage-collectors are running
+
+Each garbage-collector instance runs an administration server that can be used
+to extract information about its configuration. A good check to run to ensure
+that the previous step went well is:
+
+    headnode$ manta-oneach -s garbage-collector \
+        "curl -X GET localhost:2020/shards"
+
+Here is an example of what the output of the previous command might look like in
+a deployment with two garbage-collectors:
+
+    SERVICE          ZONE     OUTPUT
+    garbage-collector 2bd87578 []
+    garbage-collector 756e17e4 []
+
+The output indicates that the collectors are healthy, but they haven't been
+tasked with processing records from any shards yet.
+
+#### Assign shards to collectors
+
+By default, garbage-collectors come up idle because they are not configured to
+read records from any metadata shards. The configuration that indicates which
+collector should process records from which shard is stored in each collector's
+SAPI instance object. The benefit of storing this mapping in each instance object
+is that the configuration survives reprovisions and can be managed with the
+existing SAPI interface.
+
+Each garbage-collector instance object stores a list of index shard identifiers
+in a metadata array called `GC_ASSIGNED_SHARDS`. This array can be modified by
+hand with tools like `sapiadm` and `sdc-sapi`, but it is simpler to manage this
+configuration with `manta-adm accel-gc`. Once the collectors have been deployed,
+run:
+
+    headnode$ manta-adm accel-gc gen-shard-assignment &> gc-shard-assignment.json
+
+to generate a mapping from garbage-collector instances to index shards. The
+operation will distribute all of the index shards in the Manta deployment
+to the garbage-collectors as evenly as possible. An example of how such a
+configuration would look on a deployment with two index shards is:
+```
+{
+    "756e17e4-bfd2-4bc5-b3b6-166480dd0614": {
+        "GC_ASSIGNED_SHARDS": [
+            {
+                "host": "2.moray.orbit.example.com",
+                "last": true
+            }
+        ]
+    },
+    "2bd87578-285f-4937-aff1-181a7f32dcc0": {
+        "GC_ASSIGNED_SHARDS": [
+            {
+                "host": "1.moray.orbit.example.com",
+                "last": true
+            }
+        ]
+    }
+}
+```
+Note that data in the `GC_ASSIGNED_SHARDS` array comes from the SAPI
+Manta application's `INDEX_MORAY_SHARDS` metadata field.
+
+The keys of the object are uuids of garbage-collector instances, and the values
+are changes that will be applied to the `metadata` blob of the corresponding
+instance's SAPI object. The last object in the `GC_ASSIGNED_SHARDS` array must
+have a "last" field set to true. This is a convention used by Manta's templating
+engine.
+
+#### Update garbage-collector metadata
+
+Once you've generated a mapping from collectors to shards and saved it in
+`gc-shard-assignment.json`, run:
+
+    headnode$ manta-adm accel-gc update gc-shard-assignment.json
+
+to update the corresponding SAPI metadata. There are a few points to stress
+here:
+
+- This operation **replaces** any existing mapping from garbage-collectors
+to shards. This behavior makes the assignment idempotent, which mimics the
+semantics of `manta-adm update`.
+- Running this operation does not immediately cause the garbage-collectors
+to start reading records from the newly assigned shards. That requires
+restarting all the modified collectors.
+
+If new shards or new garbage-collectors are added to the Manta deployment,
+you may run `manta-adm accel-gc gen-shard-assignment` followed by `manta-adm
+accel-gc update` to re-assign the new pool of shards to the new pool of
+collectors.
+
+
+#### Restart garbage-collectors
+
+In order for the configuration change in the previous step to take effect,
+you will need to restart the garbage-collectors. This can be done with the
+following manta-oneach query:
+
+    headnode$ manta-oneach -s garbage-collector
+        "svcadm restart config-agent; sleep 5; svcadm restart garbage-collector"
+
+Restarting the config-agent ensures that the changes have been picked up and that
+the local process configuration file has been re-rendered. After the
+garbage-collectors have been restarted, you can verify the shard assignment took
+effect with:
+
+    headnode$ manta-oneach -s garbage-collector "curl -X GET localhost:2020/shards"
+
+Which should now show the following output:
+
+    SERVICE          ZONE     OUTPUT
+    garbage-collector 2bd87578 [{"host":"1.moray.orbit.example.com"}]
+    garbage-collector 756e17e4 [{"host":"2.moray.orbit.example.com"}]
+
 
 ## Networking configuration
 
@@ -2097,6 +2364,188 @@ directories and allows for about 256 million multipart uploads, but only up to
 account.
 
 
+## Accelerated Garbage Collection
+
+### Overview
+
+The Manta Accelerated Garbage Collection pipeline consists of a number of
+'garbage-collector' instances which read delete records from metadata-tier
+database tables and transform them into instructions that are stored in a special
+(and configurable) location in the Manta object namespace. These instructions are
+processed by a cron job that runs on each of the Manta storage servers.
+
+### Tunables
+
+The 'garbage-collector' instances deployed in each Manta leveraging accelerated
+gc were designed to be highly-configurable. However, reasonable defaults based on
+performance testing have been set for all three Manta deployment sizes so that
+they do not need to be modified out of the box.
+
+If needed, operators should use the SAPI tunables to modulate the impact
+accelerated garbage-collection has on the datapath. It is recommended that
+all collectors be configured consistently.
+
+These tunables are summarized here:
+
+#### General Tunables
+
+- `GC_ASSIGNED_SHARDS` is an array of objects describing the index shards that a
+  given garbage-collector is configured to search for records from. The default
+  value is `[]`. It is recommended that this value not exceed 6 or 7 shards.
+- `GC_ASSIGNED_BUCKETS` is an array of objects describing which buckets a
+  garbage-collector instance should attempt to read records from. By default,
+  this array contains only an object referring to the `manta_fastdelete_queue`.
+- `GC_CONCURRENCY` is a positive integer describing how many concurrent workers
+  should be used to read delete records, upload instructions, and delete records
+  **per shard assigned to a collector**. This means that setting
+  `GC_CONCURRENCY=2` on a garbage-collector configured to read records from 2
+  shards results in an instance with 4 workers total. The default value depends
+  on the size of the Manta deployment (1-2).
+- `GC_CACHE_CAPACITY` is a positive integer that serves as an upper bound on the
+  total number of cache entries a garbage-collector may hold at once. It is a
+  coarse limit that can be applied to lower the memory usage of the collector if
+  the CN on which it is deployed is experiencing above-average memory-pressure.
+
+#### Delete Record Ingest Tunables
+
+- `GC_RECORD_READ_BATCH_SIZE` is a positive integer describing how many records
+  the garbage-collector will ask for per metadata-tier request. The default
+  value varies with the deployment size, but should generally be pretty high
+  since the garbage-collector targets high-throughput.
+- `GC_RECORD_READ_WAIT_INTERVAL` is a positive integer describing the number of
+  milliseconds the garbage-collector should wait between metadata tier requests
+  to find new delete records.
+- `GC_RECORD_READ_SORT_ATTR` is a Moray field identifier that should be used
+  to decide what order records should be read in from the metadata tier. The
+  default value is `_mtime`, which ensures that delete records are processed in
+  the order that they were created.
+- `GC_RECORD_READ_SORT_ORDER` is one of `ASC|DESC|asc|desc` and decides which
+  order the records should be processed in. The default value is `ASC`.
+
+#### Delete Instruction Upload Tunables
+
+- `GC_INSTR_UPLOAD_BATCH_SIZE` is a positive integer describing how many
+  lines to include in each delete instruction object uploaded to Manta. Each
+  instruction object that the garbage-collector uploads has a fixed storage
+  overhead on the metadata-tier, so this field should be set high for production
+  deployments. Performance testing shows that the garbage-collector performs
+  well when this tunable is set to `GC_RECORD_READ_BATCH_SIZE/GC_CONCURRENCY`.
+- `GC_INSTR_UPLOAD_FLUSH_DELAY` is a positive integer indicating the number of
+  milliseconds to wait between each instruction object upload. It is recommended
+  that this value match `GC_RECORD_READ_WAIT_INTERVAL`, but can be safely set
+  otherwise as a form of backpressure.
+- `GC_INSTR_UPLOAD_PATH_PREFIX` is the path under which per-shrimp instructions
+  should be uploaded. Today, this value must be `/poseidon/stor/manta_gc/mako`,
+  which is where the current version of the mako image searches for delete
+  instructions. **This value should not be changed in production deployments**.
+
+#### Delete Record Cleaning Tunables
+
+- `GC_RECORD_DELETE_BATCH_SIZE` is a positive integer describing how many
+  records the garbage-collector should clean per metadata-tier request. This
+  value should generally be equal to `GC_RECORD_READ_BATCH_SIZE`. To minimize
+  impact on the metadata tier, this tunable should generally be set to optimize
+  for throughput in delete record cleaning.
+- `GC_RECORD_DELETE_DELAY` is a positive integer describing the number of
+  milliseconds the garbage-collector should wait between metadata-tier requests
+  to cleanup processed delete records. This tunable should be used to apply
+  backpressure on the garbage-collector in times of above-average metadata-tier
+  load. There is no harm in setting this value lower than
+  `GC_INSTR_UPLOAD_FLUSH_DELAY` or `GC_RECORD_READ_WAIT_INTERVAL` as the
+  garbage-collector will only actually issue delete requests to the metadata tier
+  if it knows that there are records to be deleted.
+
+### Reading Garbage-Collector Tunables
+
+All the tunables described in this section are stored either in the SAPI
+garbage-collector service object, or in the individual instance objects for each
+collector. Generally, `GC_ASSIGNED_SHARDS` will be set on a per-instance basis.
+Other tunables are likely to be set within the service object. One way to read
+the garbage-collector configuration is to look at the metadata values in the
+service object, and each of the instance objects:
+
+    headnode$ sdc-sapi /services?name=garbage-collector | json metadata
+    headnode$ sdc-sapi /instances?service_uuid=$(sdc-sapi \
+	/services?name=garbage-collector | json -Ha uuid) | json -Ha metadata
+
+Another way to retrieve tunables on a per instance basis is via the admin http
+interface that each garbage-collector runs on startup:
+
+    headnode$ manta-oneach -z <instance> "curl -X GET localhost:2020/tunables"
+
+The admin interface exposes a number of other endpoints. For an exhaustive list,
+see the garbage-collector
+[README](https://github.com/joyent/manta-garbage-collector/blob/master/README.md).
+
+The mapping between garbage-collector instances and shards can also be read via
+manta-adm:
+
+    headnode$ manta-adm accel-gc show -j
+
+### Modifying Garbage-Collector Tunables
+
+#### Deployment Wide Modifications
+
+Deployment-wide garbage-collector modifications should be made by modifying the
+garbage-collector SAPI service object. This can be done with the tools like
+sapiadm:
+
+    headnode$ sdc-sapi -H /services?name=garbage-collector > update.json
+    ... modify update.json ...
+    headnode$ sapiadm update $(json -f update.json uuid) -f update.json
+
+Propagating changes made this way will require restarting the garbage-collectors.
+
+#### Per-Instance Modifications
+
+In some cases, it may make sense to modify the tunables of only a subset of the
+deployed garbage-collectors. This can be done with SAPI tools directly, but
+`manta-adm` exposes an interface for this as well:
+
+    headnode$ manta-adm accel-gc show -j > pre.json
+
+The output, pre.json, will contain a json object mapping instance UUIDs to
+json objects corresponding to the SAPI instance metadata sub-object:
+
+    {
+        "<INSTANCE-UUID>": {
+            ... instance metadata ...
+        }
+    }
+
+Modifications to a subset of garbage-collector instances can be made to this one
+file and then applied with:
+
+    headnode$ manta-adm accel-gc update pre.json
+
+In order for the tunables to take effect, the garbage-collector instances
+themselves must be restarted:
+
+    manta-oneach -s garbage-collector "svcadm restart config-agent; sleep 5; svcadm restart garbage-collector"
+
+
+### Metrics
+
+The garbage-collector exposes metrics through a node-artedi metrics server that
+listens on port 3020. The following is a table of metrics exported (these do not
+include metrics exported by garbage-collector dependencies, such as the
+node-moray client):
+
+| name                       | type    | description                                |
+|:--------------------------:|:-------:|:------------------------------------------:|
+| gc_delete_records_read     | counter | records read from `manta_fastdelete_queue` |
+| gc_cache_entries           | gauge   | the number of entities cached              |
+| gc_mako_instrs_uploaded    | counter | number of delete instructions uploaded     |
+| gc_bytes_marked_for_delete | counter | bytes made ready to be reclaimed           |
+| gc_delete_records_cleaned  | counter | number of delete records removed           |
+
+Muskie metrics that may be relevant to operators deploying accelerated
+garbage-collection:
+
+| name                 | type    | description                                |
+|:--------------------:|:-------:|:------------------------------------------:|
+| muskie_deleted_bytes | counter | bytes for which delete APIs have completed |
+
 # Debugging: general tasks
 
 ## Locating servers
@@ -2809,6 +3258,332 @@ This case indicates that the supervisor is functioning normally.  If one
 supervisor takes over for another, you'll see an "operatedBy" field with the
 uuid of the supervisor that's taken over for this supervisor.
 
+# Troubleshooting Accelerated Garbage Collection
+
+This section serves as a checklist for common problems that might stall or break
+accelerated garbage collection pipeline.
+
+## Simple Checks
+
+The following are simple checks that can be run to verify that the
+garbage-collection pipeline in your Manta deployment is correctly configured.
+
+### Verify the garbage-collectors are running
+
+The simplest command checks whether each garbage-collector process is running:
+
+    headnode$ manta-oneach -s garbage-collector "svcs -xv garbage-collector"
+
+Check the logs from the most recent run if the output shows that the service is
+disabled:
+
+    garbage-collector$ less -R $(svcs -L garbage-collector)
+
+Another method for verifying that collectors are functional is:
+
+    headnode$ manta-oneach -s garbage-collector \
+        "curl -X GET localhost:2020/ping"
+
+### Verify that the correct shards are assigned to each garbage-collector
+
+The simplest way to verify that the garbage-collectors have the correct
+assignment at the current moment is:
+
+    headnode$ manta-oneach -s garbage-collector \
+        "curl -X GET localhost:2020/shards"
+
+The output of the above command may not necessary correspond with the SAPI
+instance metadata. If this is the case, the assignment may change after the next
+restart due to some garbage-collectors configuration being re-rendered by the
+config-agent. To verify that the SAPI instance metadata is correct:
+
+    headnode$ sdc-sapi /instances?service_uuid=$(sdc-sapi \
+        /services?name=garbage-collector | json -Ha uuid) | \
+	json -Ha uuid metadata.GC_ASSIGNED_SHARDS
+
+
+### Verify that the `manta_fastdelete_queue` exists on all index shards
+
+The output of this command should show a json object describing the
+`manta_fastdelete_queue` for all index shards.
+
+    manta-oneach -s moray \
+        "getbucket manta_fastdelete_queue && cat /var/tmp/metadata.json | json SERVICE_NAME"
+
+### Verify that the owning account is snaplink-disabled
+
+The accelerated garbage-collection pipeline will not operate on objects that
+are not created or owned by a snaplink-disabled account. To verify that the
+account whose objects are being deleted get garbage-collected:
+
+    headnode$ UUID=$(pfexec sdc-mahi /account/<login> | json -Ha account.uuid)
+    headnode$ sdc-sapi /applications?name=manta | \
+        json -Ha metadata.ACCOUNTS_SNAPLINKS_DISABLED | json -Ha uuid | \
+        grep $UUID
+
+## Progress Checks
+
+If all of the previous checks in your Manta deployment pass, but
+garbage-collection does not seem to be making progress:
+
+### Verify records are being added to the `manta_fastdelete_queue`
+
+A simple way to do this is to login to the Manatee primary and manually check
+the number of records in the `manta_fastdelete_queue` for some time. To find the
+Manatee primary peer for a given shard, query the status server on each postgres
+instance in your deployment:
+
+    headnode$ manta-oneach -s postgres \
+        "curl -X GET localhost:5433/state | json role"
+
+After determining the zone uuid, `manta-login` into the instance and run:
+
+    primary$ while true; psql -d moray -U postgres -P format=unaligned -t -c \
+        "select count(*) from manta_fastdelete_queue"; sleep 5; done;
+
+This command will sample the number of entries in the queue every 5 seconds.
+**Do not use this solution when making a large number of deletes or have any
+reason to think that the queue is large**. Running a table scan could further
+stall the garbage collection pipeline.
+
+If there is a `pgstatsmon` instance available, use the `pg_row_estimate` gauge
+instead. This gauge is known to not be accurate, but it should report a non-zero
+value if new records have been added recently.
+
+### Verify the garbage-collectors are reading records from `manta_fastdelete_queue`
+
+The garbage-collector runs `findObjects` every `GC_RECORD_READ_WAIT_INTERVAL`
+milliseconds. This findObjects queries the oldest (sorted by `_mtime`)
+`GC_RECORD_READ_BATCH_SIZE` records from the table. Every record the
+garbage-collector reads can be found by searching through the service log. Newly
+discovered records will be reported by the reader:
+
+    headnode$ tail -F $(svcs -L garbage-collector) | grep "MorayDeleteRecordReader" | bunyan -p
+
+### Verify Delete instructions are being uploaded
+
+By default, delete instructions are uploaded as objects to
+`/poseidon/stor/manta_gc/mako/STORAGE_ID`. Here, `STORAGE_ID` is the DNS name
+of the storage server for which the contained instructions are intended. Each
+instruction is named as follows:
+
+    DATE-INSTANCE_UUID-X-UUID-mako-STORAGE_ID
+
+Here:
+- `DATE` is a UTC timestamp indicating when the instruction was generated.
+- `INSTANCE_UUID` is the zone uuid of the garbage-collector instance that
+   uploaded the instruction.
+- `UUID` is a uuid generated for each instruction object.
+- `STORAGE_ID` is the DNS name of the shark for which the instruction is
+  intended.
+
+Today, the name of the instruction object is arbitrary. The cron job processing
+the instructions finds out about them using a listing of the parent directory,
+`/poseidon/stor/manta_gc/mako/STORAGE_ID`.
+
+Each instruction contains a variable number of lines referring to object
+backing-files on a storage server. These are formatted to match the expectations
+of the existing storage server cron job. For more about this format and running
+Mako cleaning steps manually, see
+[documentation](https://github.com/joyent/manta-mola/blob/master/docs/gc-manual-coal.md#cleaning-makos)
+in the Mola consolidation.
+
+To check the number of instruction objects for a particular shark:
+
+    ops$ minfo /poseidon/stor/manta_gc/mako/SHARK | grep "result-set-size" | \
+        cut -d' ' -f2
+
+If there are snaplink-disabled deletes happening in Manta, the number printed by
+this command should increase. Instructions will not be removed from this
+directory until the next time the gc cron job on the corresponding shark runs.
+If no deletes have happened in a while, and the instruction directory is large,
+there could be a problem with the GC cron job running on the shark.
+
+To check the timestamp of the most recently uploaded instruction for shark
+`STORAGE_ID`:
+
+    ops$ mls -lt /poseidon/stor/manta_gc/mako/STORAGE_ID
+
+If there are deletes happening in Manta and the most recently uploaded object
+was modified too far in the past, it may indicate a problem with the
+garbage-collectors (which are responsible for uploading new instruction
+objects as user objects are deleted).
+
+### Verify Delete records are being cleaned
+
+Delete records are removed from the `manta_fastdelete_queue` after the object to
+which they refer has been included in an instruction object. The garbage-collector
+uses the Moray `batch` RPC to delete records from this queue. node-artedi
+metrics exposed by the node-moray client can be used to determine whether the
+requests are successful. The garbage-collector also exposes a node-artedi
+metric called `gc_delete_records_cleaned`, which is a count of the total number
+of rows deleted from a `manta_fastdelete_queue` by shard.
+
+Operators can also check record cleaning progress by inspecting the
+`manta_fastdelete_queue` table directly:
+
+    postgres$ while true; do psql -d moray -U postgres -c \
+        "select count(*) from manta_fastdelete_queue"; sleep 5; done;
+
+**It is not, in general, safe to run this command in production where the
+manta_fastdelete_queue could be large**. Once deletes stop, the value printed by
+this command should steadily drift down to 0. If it does not, this could indicate
+a problem with one of the garbage-collectors processing records from this shard.
+
+As before, if pgstatsmon is available, the `pg_row_estimate` metric for the
+`manta_fastdelete_queue` relation is a safer alternative to the above command
+(though it is less precise and may indicate that some rows exist when they
+really do not).
+
+### Check for bloated garbage-collector processes
+
+The garbage-collector caches instructions to upload to Manta, and records to delete
+from the `manta_fastdelete_queue`. The purpose of this caching is to enable
+these requests to be done in batches. The garbage-collector exposes an http
+admin endpoint to give operators some sense of how large each of these caches is:
+
+    garbage-collector$ curl -X GET localhost:2020/workers/get
+	{
+	  "1.moray.orbit.example.com": {
+	    "manta_fastdelete_queue": [
+	      {
+		"component": "worker",
+		"state": "running",
+		"actors": [
+		  {
+		    "component": "reader",
+		    "bucket": "manta_fastdelete_queue",
+		    "state": "waiting"
+		  },
+		  {
+		    "component": "transformer",
+		    "state": "running",
+		    "cached": 128
+		  },
+		  {
+		    "component": "instruction uploader",
+		    "state": "running"
+		  },
+		  {
+		    "component": "cleaner",
+		    "state": "running",
+		    "cached": 128
+		  }
+		]
+	      },
+	      {
+		"component": "worker",
+		"state": "running",
+		"actors": [
+		  {
+		    "component": "reader",
+		    "bucket": "manta_fastdelete_queue",
+		    "state": "waiting"
+		  },
+		  {
+		    "component": "transformer",
+		    "state": "running",
+		    "cached": 512
+		  },
+		  {
+		    "component": "instruction uploader",
+		    "state": "running"
+		  },
+		  {
+		    "component": "cleaner",
+		    "state": "running",
+		    "cached": 0
+		  }
+		]
+	      }
+	    ]
+	  }
+
+The output of the above command describes a collector reading records from the
+`manta_fastdelete_queue` from shard 1 of the Manta deployment. The concurrency
+of the collector corresponds with the number of workers (2).
+
+- The `cached` field of the `transformer` actor in each worker describes how
+  many instructions the collector is ready to upload to Manta, but has not yet.
+- The `cached` field of the `cleaner` actor in each workers describes how many
+  instructions the collector is ready to remove from the shard (`1.moray` in
+  this case).
+
+The caches may grow if:
+
+- A downstream component of the collector experiences an error
+- The rate of API delete calls grows
+
+The first case can usually be seen in the output of `GET /workers/get` above as
+the `state` field of some component being "error". Diagnosing the problem
+further will usually depend on log analysis, but this can be sped up by
+searching for the name of the component in the error state. For example, if the
+cleaner is in the "error" state:
+
+	      {
+		"component": "worker",
+		"state": "running",
+		"actors": [
+		  {
+		    "component": "reader",
+		    "bucket": "manta_fastdelete_queue",
+		    "state": "waiting"
+		  },
+		  {
+		    "component": "transformer",
+		    "state": "running",
+		    "cached": 512
+		  },
+		  {
+		    "component": "instruction uploader",
+		    "state": "running"
+		  },
+		  {
+		    "component": "cleaner",
+		    "state": "error",
+		    "cached": 0
+		  }
+		]
+	      }
+
+The particular error can be located faster in the service logs with:
+
+    garbage-collector$ cat $(svcs -L garbage-collector) | \
+	grep MorayDeleteRecordCleaner | bunyan -p
+
+See the garbage-collector documentation for a list of the full actor names.
+
+## Common Tweaks
+
+There are a couple of issues that may arise in the lifetime of a
+garbage-collector that can be solved by tweaking a tunable. This section
+covers a few of these.
+
+### Overlapping Workers
+
+Each garbage-collector stores an offset into the `manta_fastdelete_queue` of the
+shard it is reading from locally and allows it to wrap around to zero once the
+queue is exhausted. This issue can result in an overlap in which multiple
+garbage-collector workers, or multiple garbage-collector instances processing
+records from the same shard, operate in lock-step: reading, and attempting to
+delete the same records.
+
+If the rate of deletes in a given deployment is expected to be small, this
+problem can be mitigated by decreasing `GC_RECORD_READ_BATCH_SIZE` so that the
+collector processes the queue in smaller chunks.
+
+### Excessive Instruction Object Count
+
+garbage-collector instances upload instruction objects containing batches of
+object files to be removed from shrimps. A large number of such instruction
+objects results in metadata overhead and can slow down the rate at which
+`mako_gc.sh` processes them (`mako_gc.sh` will process at most 256 at a time).
+
+Increasing `GC_INSTR_UPLOAD_BATCH_SIZE` can mitigate this problem. If it does
+not, increasing `GC_INSTR_UPLOAD_FLUSH_DELAY` can also help by forcing the
+garbage-collectors to wait for more delete records to come in before uploading
+the next cleanup instruction for a given shrimp.
+
 # Debugging Marlin: storage nodes
 
 ## Figuring out what's running
