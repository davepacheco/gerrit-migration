From 7293f3e73ae71daf842cdf978c9bbd5626a3fe24 Mon Sep 17 00:00:00 2001
From: Julien Gilli <julien.gilli@joyent.com>
Date: Mon, 4 Apr 2016 13:10:37 -0700
Subject: [PATCH] DOCKER-793: First pass for NFS shared volumes DOCKER-863:
 Create shared volume when mounting nonexistent volume DOCKER-878: Make docker
 volume ls list only "active" volumes DOCKER-890: docker volume integration
 tests fail with docker version 1.12-rc2 DOCKER-880: add regression test
 DOCKER-895 duplicate volume name error at volume creation time should be
 properly communicated to docker client DOCKER-895 (fix make check) DOCKER-896
 invalid volume size error at volume creation time should be properly
 communicated to docker client DOCKER-906: support modes when mounting volumes
 DOCKER-911: tritonnfs volume mounted by container with docker-compose v2
 fails to mount VOLAPI-9 Implement asynchronous NFS shared volume creation
 VOLAPI-6: Deleting a tritonnfs volume used by an active Docker container
 should fail VOLAPI-23 docker run -v fails when the mount point is an existing
 folder in the image VOLAPI-25 Docker volume create does not honor the network
 selection PUBAPI-1300 Implement CRUD volumes endpoints VOLAPI-42 implement
 volumes renaming constraints DOCKER-1011 docker run -v option for local
 volume (non-tritonnfs) should continue to work VOLAPI-27 Divergence in
 `docker create -v` behavior; volume is not created until `docker start`
 DOCKER-1021 docker volume create should output error message when volume
 creation fails in storage VM's provisioning workflow DOCKER-1055 volume names
 for sdc-docker should match those from PUBAPI-1372 VOLAPI-49 DELETE should
 delete DOCKER-1030 restrain number of size unit suffixes supported when
 creating NFS volumes DOCKER-1066 sdc-docker should expose current config
 DOCKER-894 NFS shared volumes tests should check for "NFS shared volumes not
 supported" error in a separate tests suite VOLAPI-54 Custom errors' rest
 codes should follow engineering guide's guidelines DOCKER-1034 support docker
 volume ls -f dangling=true VOLAPI-65 rename required_nfs_volumes VM objects'
 property to volumes TOOLS-1830 replace sdcadm experimental nfs-volumes with
 cloudapi-nfs-volumes/docker-nfs-volumes subcommands TOOLS-1829 add sdcadm
 experimental docker-nfs-volumes-automount DOCKER-1088 sdc-docker should not
 modify VMs' internal docker:nfsvolumes metadata VOLAPI-72 creating volume
 with unsupported size should result in error, not in volume being created
 with closest matching size

---
 docs/api/features/volumes.md                  |  36 ++
 lib/backends/sdc/containers.js                | 412 +++++---------
 lib/backends/sdc/index.js                     |   9 +-
 lib/backends/sdc/networks.js                  | 260 ++++++++-
 lib/backends/sdc/volumes.js                   | 383 +++++++++++++
 lib/common.js                                 |   5 +-
 lib/containers.js                             |  28 +
 lib/docker.js                                 |  80 +--
 lib/endpoints/_ping.js                        |   2 +-
 lib/endpoints/auth.js                         |   2 +-
 lib/endpoints/build.js                        |   2 +-
 lib/endpoints/ca.js                           |   2 +-
 lib/endpoints/commit.js                       |   2 +-
 lib/endpoints/containers.js                   |   2 +-
 lib/endpoints/events.js                       |   2 +-
 lib/endpoints/exec.js                         |   2 +-
 lib/endpoints/images.js                       |   2 +-
 lib/endpoints/index.js                        |   7 +-
 lib/endpoints/info.js                         |   2 +-
 lib/endpoints/networks.js                     |  10 +-
 lib/endpoints/version.js                      |   2 +-
 lib/endpoints/volumes.js                      | 294 ++++++++++
 lib/errors.js                                 |  20 +-
 lib/ufds.js                                   |  37 ++
 lib/units.js                                  |  13 +
 lib/validate.js                               | 172 +++++-
 lib/volumes.js                                |  55 ++
 package.json                                  |   4 +-
 sapi_manifests/docker/template                |  13 +-
 ...fs-shared-volume-as-docker-compose.test.js | 165 ++++++
 .../api-rename-used-nfs-shared-volume.test.js | 229 ++++++++
 test/integration/cli-commit.test.js           |  10 +-
 test/integration/cli-filters.test.js          |   4 +-
 test/integration/cli-labels.test.js           |   6 +-
 test/integration/cli-local-volumes.test.js    |   4 +-
 ...-volume-created-with-docker-create.test.js | 143 +++++
 ...ared-volumes-create-duplicate-name.test.js | 107 ++++
 ...-nfs-shared-volumes-create-failure.test.js | 154 ++++++
 ...cli-nfs-shared-volumes-create-size.test.js | 152 ++++++
 ...i-nfs-shared-volumes-delete-in-use.test.js | 141 +++++
 .../cli-nfs-shared-volumes-disabled.test.js   | 231 ++++++++
 .../cli-nfs-shared-volumes-docker-880.test.js | 352 ++++++++++++
 ...nfs-shared-volumes-filter-dangling.test.js | 204 +++++++
 ...lumes-mount-overrides-local-volume.test.js | 131 +++++
 ...i-nfs-shared-volumes-mounting-mode.test.js | 281 ++++++++++
 .../cli-nfs-shared-volumes-networks.test.js   | 389 +++++++++++++
 .../cli-nfs-shared-volumes.test.js            | 514 ++++++++++++++++++
 test/integration/helpers.js                   |  55 +-
 test/lib/cli.js                               |   5 +-
 test/lib/common.js                            |   8 +-
 test/lib/volumes-api.js                       |  88 +++
 test/lib/volumes-cli.js                       | 216 ++++++++
 test/lib/volumes.js                           | 153 ++++++
 test/runtest.common                           |  15 +-
 54 files changed, 5245 insertions(+), 372 deletions(-)
 create mode 100644 lib/backends/sdc/volumes.js
 create mode 100644 lib/containers.js
 create mode 100644 lib/endpoints/volumes.js
 create mode 100644 lib/ufds.js
 create mode 100644 lib/units.js
 create mode 100644 lib/volumes.js
 create mode 100644 test/integration/api-mount-nfs-shared-volume-as-docker-compose.test.js
 create mode 100644 test/integration/api-rename-used-nfs-shared-volume.test.js
 create mode 100644 test/integration/cli-mount-nfs-volume-created-with-docker-create.test.js
 create mode 100644 test/integration/cli-nfs-shared-volumes-create-duplicate-name.test.js
 create mode 100644 test/integration/cli-nfs-shared-volumes-create-failure.test.js
 create mode 100644 test/integration/cli-nfs-shared-volumes-create-size.test.js
 create mode 100644 test/integration/cli-nfs-shared-volumes-delete-in-use.test.js
 create mode 100644 test/integration/cli-nfs-shared-volumes-disabled.test.js
 create mode 100644 test/integration/cli-nfs-shared-volumes-docker-880.test.js
 create mode 100644 test/integration/cli-nfs-shared-volumes-filter-dangling.test.js
 create mode 100644 test/integration/cli-nfs-shared-volumes-mount-overrides-local-volume.test.js
 create mode 100644 test/integration/cli-nfs-shared-volumes-mounting-mode.test.js
 create mode 100644 test/integration/cli-nfs-shared-volumes-networks.test.js
 create mode 100644 test/integration/cli-nfs-shared-volumes.test.js
 create mode 100644 test/lib/volumes-api.js
 create mode 100644 test/lib/volumes-cli.js
 create mode 100644 test/lib/volumes.js

diff --git a/docs/api/features/volumes.md b/docs/api/features/volumes.md
index 60af155..e59621a 100644
--- a/docs/api/features/volumes.md
+++ b/docs/api/features/volumes.md
@@ -17,3 +17,39 @@ different from Docker Inc's docker:
    specified (including those that this container is sharing from others) are
    ignored. Only volumes belonging to the specified container will be
    considered.
+
+## Experimental support for NFS shared volumes
+
+The NFS shared volumes feature is described in details by its corresponding [RFD
+document](https://github.com/joyent/rfd/blob/master/rfd/0026/README.md).
+
+To enable support for NFS shared volumes in Triton, run the following command
+line from the head node:
+
+```
+sdcadm post-setup volapi
+sdcadm experimental docker-nfs-volumes
+sdcadm experimental docker-nfs-volumes-automount
+```
+
+This command will create a new core zone that runs the VOLAPI service, which
+implements the Volumes API. It will also enable the
+`experimental_docker_nfs_shared_volumes` and
+`experimental_docker_automount_nfs_shared_volumes` metadata properties in SAPI.
+
+At this point, all `docker volume` commands are supported but only for the
+`'tritonnfs'` volume driver, which provides support for NFS shared volumes. Note
+that the `'tritonnfs'` volume driver is considered to be the default and thus
+does not need to be specified in the `docker volume create` command for it to
+work.
+
+The `experimental_docker_nfs_shared_volumes` SAPI flag can be set to `false` in
+SAPI to disable support for NFS shared volumes by running the following command
+line:
+
+```
+sdcadm experimental docker-nfs-volumes -d
+```
+
+After disabling this setting, running `docker volume` commands will result in an
+error message.
diff --git a/lib/backends/sdc/containers.js b/lib/backends/sdc/containers.js
index 516d192..92eda51 100644
--- a/lib/backends/sdc/containers.js
+++ b/lib/backends/sdc/containers.js
@@ -34,15 +34,13 @@ var common = require('../../../lib/common');
 var errors = require('../../../lib/errors');
 var images = require('./images');
 var Link = require('../../models/link');
+var mod_containers = require('../../containers');
 var mod_networks = require('./networks');
 var utils = require('./utils');
 var validate = require('../../validate');
 
-
 //---- globals
 
-var format = util.format;
-
 var _cnapiClientCache; // set in `getCnapiClient`
 var _fwapiClientCache; // set in `getFwapiClient`
 var _imgapiClientCache; // set in `getImgapiClient`
@@ -76,9 +74,6 @@ var MAX_VOLUMES_FROM = 2; // number of --volumes-from allowed
 // matches the value customers will see on their bills.
 var PACKAGE_SELECTION_LABEL = 'com.joyent.package';
 
-// Label name used to set the external (public) network for a container.
-var TRITON_PUBLIC_NETWORK_LABEL = 'triton.network.public';
-
 // These should match what PAPI uses for validating a package name
 var BAD_PKG_NAME_RE = /[\_\-\.][\_\-\.]/;
 var PKG_NAME_RE = /^[a-zA-Z0-9]([a-zA-Z0-9\_\-\.]+)?[a-zA-Z0-9]$/;
@@ -118,14 +113,6 @@ function getVmapiClient(config) {
     return _vmapiClientCache;
 }
 
-function getNapiClient(config) {
-    if (!_napiClientCache) {
-        // intentionally global
-        _napiClientCache = new NAPI(config);
-    }
-    return _napiClientCache;
-}
-
 function getPapiClient(config) {
     if (!_papiClientCache) {
         // intentionally global
@@ -134,7 +121,6 @@ function getPapiClient(config) {
     return _papiClientCache;
 }
 
-
 /**
  * Return rules that expose each specified port individually for a given
  * account and protocol.
@@ -209,16 +195,6 @@ function generateExposeRange(account, vm, proto, ports, cb) {
 }
 
 
-/**
- * List networks in NAPI, filtering by params
- */
-function listNetworks(opts, params, callback) {
-    var napi = getNapiClient(opts.config.napi);
-
-    napi.listNetworks(params, {headers: {'x-request-id': opts.req_id}},
-        callback);
-}
-
 /**
  * Add the "EXPOSE" firewall rules to the payload.  Note that we open up all
  * ports between docker hosts right now, since this is what other inter-host
@@ -355,7 +331,8 @@ function addPublishFirewallRules(opts, container, img, payload, callback) {
         udp: []
     };
 
-    if (!publishingPorts(container) && common.objEmpty(imageExposedPorts)) {
+    if (!mod_containers.publishingPorts(container)
+        && common.objEmpty(imageExposedPorts)) {
         // Nothing to publish externally and no ports exposed internally, so
         // there's no point in adding the firewall rules
         log.info('no ports used, so not adding publish firewall rules');
@@ -531,228 +508,11 @@ function addRulesToFWAPI(opts, rules, payload, callback) {
     });
 }
 
-/**
- * Add network configurations (fabrics and external) to the payload.
- *
- *  Fabrics:
- *
- *   When fabrics are enabled, the fabric network is selected in these ways:
- *    1. When 'bridge' or nothing specified, will use the user's default network
- *    2. Specifying a network name will provision to the named *fabric* network
- *    3. Specifying a network id (or portion) will provision to that *fabric*
- *       network
- *
- *   Docker resolves name/id collisions in the following way:
- *     - a name is preferred to a partial id
- *     - a full id is preferred to a name
- *
- *  External:
- *
- *   An external network is added in these cases:
- *    1. opts.requireExternalNetwork is set, or
- *    2. fabrics are *not* enabled, or
- *    3. fabrics are enabled and the user wants to expose ports
- *
- *   The user can specify which external network is used by setting the
- *   'triton.network.public' container label (tag), this specifies the external
- *   network *name*, all other cases will use the default external network,
- *   which for fabrics is opts.config.overlay.externalPool (uuid), or
- *   opts.config.externalNetwork (string name) when there are no fabrics.
- */
-function addNetworksToPayload(opts, container, payload, callback) {
-    assert.object(opts, 'opts');
-    assert.object(opts.config, 'opts.config');
-    assert.object(opts.config.napi, 'opts.config.napi');
-    assert.object(opts.log, 'opts.log');
-    assert.object(opts.config.overlay, 'opts.config.overlay');
-    assert.optionalString(opts.config.overlay.externalPool,
-        'opts.config.overlay.externalPool');
-    assert.optionalBool(opts.config.overlay.enabled,
-        'opts.config.overlay.enabled');
-    assert.optionalBool(opts.requireExternalNetwork,
-        'opts.requireExternalNetwork');
-    assert.func(callback, 'callback');
-
-    var log = opts.log;
-    var networkMode;
-
-    vasync.pipeline({ funcs: [
-        function addFabricNetworks(_, next) {
-            if (!opts.config.overlay.enabled) {
-                next();
-                return;
-            }
-            networkMode = container.HostConfig.NetworkMode;
-            if (!networkMode || networkMode === 'bridge'
-                    || networkMode === 'default') {
-                defaultFabricNetwork(opts, payload, next);
-            } else {
-                mod_networks.findNetworkByNameOrId(networkMode, opts,
-                    function (findErr, network)
-                {
-                    if (findErr) {
-                        next(findErr);
-                        return;
-                    }
-                    payload.networks = [ {uuid: network.uuid, primary: true} ];
-                    next();
-                });
-            }
-        },
-
-        function addExternalNetwork(_, next) {
-            if (!opts.requireExternalNetwork && opts.config.overlay.enabled
-                && !publishingPorts(container)) {
-                // DOCKER-1045: for fabrics, it is an error if the
-                // triton.network.public label is used and no ports are being
-                // published.
-                var labels = container.Labels || {};
-                if (Object.prototype.hasOwnProperty.call(labels,
-                        TRITON_PUBLIC_NETWORK_LABEL)) {
-                    next(new errors.ValidationError(format(
-                        '%s label requires a container with published ports',
-                        TRITON_PUBLIC_NETWORK_LABEL)));
-                    return;
-                }
-                next();
-                return;
-            }
-            externalNetworkByName(opts, container, payload, next);
-        }
-    ]}, function (err) {
-        if (!err) {
-            log.debug({ networks: payload.networks }, 'payload.networks');
-        }
-        callback(err);
-    });
-}
-
-/*
- * Add the required external network to the payload.networks.
- */
-function externalNetworkByName(opts, container, payload, callback) {
-    assert.object(opts.config, 'opts.config');
-    assert.optionalString(opts.config.externalNetwork,
-        'opts.config.externalNetwork');
-    assert.object(opts.account, 'opts.account');
-    assert.string(opts.account.uuid, 'opts.account.uuid');
-    assert.object(payload, 'payload');
-    assert.func(callback, 'callback');
-
-    var log = opts.log;
-
-    var externalNetworkName;
-    var labels = container.Labels || {};
-    if (Object.prototype.hasOwnProperty.call(labels,
-        TRITON_PUBLIC_NETWORK_LABEL))
-    {
-        externalNetworkName = labels[TRITON_PUBLIC_NETWORK_LABEL];
-    }
-
-    if (!payload.hasOwnProperty('networks')) {
-        payload.networks = [];
-    } else {
-        // Ensure the external network is the *only* primary network.
-        payload.networks.forEach(function (nw) {
-            delete nw.primary;
-        });
-    }
-
-    // When fabrics are enabled and no external name has been specified, use the
-    // opts.config.overlay.externalPool uuid for the default external network.
-    if (!externalNetworkName && opts.config.overlay.enabled) {
-        assert.string(opts.config.overlay.externalPool,
-            'opts.config.overlay.externalPool');
-        payload.networks.push(
-            { uuid: opts.config.overlay.externalPool, primary: true });
-        callback();
-        return;
-    }
-
-    // Find the external network using the given (or default) network name.
-    var listParams = {
-        name: externalNetworkName || opts.config.externalNetwork || 'external',
-        fabric: false,
-        provisionable_by: opts.account.uuid
-    };
-
-    log.debug({ listParams: listParams },
-        format('Networks: fabrics not configured, using network %s',
-        listParams.name));
-
-    listNetworks(opts, listParams, function (err, networks) {
-        log.debug({ err: err, res: networks },
-            format('Networks: listNetworks result for %s', listParams.name));
-
-        if (err) {
-            callback(errors.napiErrorWrap(err,
-                format('Networks: problem listing network %s',
-                    listParams.name)));
-            return;
-        }
-
-        if (networks.length < 1) {
-            log.error({ networks: networks, params: listParams },
-                format('Networks: network %s provisionable by %s not found',
-                    listParams.name, listParams.provisionable_by));
-            callback(new errors.NetworkNotFoundError(listParams.name));
-            return;
-        }
-
-        payload.networks.push({uuid: networks[0].uuid, primary: true});
-
-        callback();
-        return;
-    });
-}
-
-/*
- * When fabrics are configured, but no specific network is supplied,
- * or if 'bridge' is supplied, we will use the user's default fabric
- * network, stored in UFDS.
- *
- * Additionally, if the user is publishing ports or we have the
- * `requireExternalNetwork` option (currently used by docker build,
- * see DOCKER-705), we will also attach to configured network pool
- * (typically the external/public pool).
- */
-function defaultFabricNetwork(opts, payload, callback) {
-    assert.object(opts, 'opts');
-    assert.object(opts.app, 'opts.app');
-    assert.object(opts.app.config, 'opts.app.config');
-    assert.string(opts.app.config.datacenterName,
-        'opts.app.config.datacenterName');
-    assert.object(opts.log, 'opt.log');
-    assert.object(payload, 'payload');
-    assert.func(callback, 'callback');
-
-    var dc = opts.app.config.datacenterName;
-    var log = opts.log;
-
-    log.debug('Networks: using default fabric network');
-
-    opts.app.ufds.getDcLocalConfig(opts.account.uuid, dc, function (err, conf) {
-        log.debug({err: err, conf: conf, account: opts.account.uuid},
-            'Networks: get DC local config');
-
-        if (err || !conf || !conf.defaultnetwork) {
-            callback(errors.ufdsErrorWrap(err,
-                'Networks: could not get default network'));
-            return;
-        }
-
-        payload.networks = [ { uuid: conf.defaultnetwork, primary: true } ];
-
-        callback();
-        return;
-    });
-}
-
 /**
  * Add a rule or rules to payload.firewall_rules
  */
 function addRulesToPayload(payload, rules) {
-    var rulesArr = util.isArray(rules) ? rules : [ rules ];
+    var rulesArr = Array.isArray(rules) ? rules : [ rules ];
 
     if (!payload.firewall_rules) {
         payload.firewall_rules = [];
@@ -1566,17 +1326,57 @@ function ltrim(str, chars)
     return str.replace(new RegExp('^[' + chars + ']+', 'g'), '');
 }
 
-/**
- * Returns true if the container is publishing ports
+/*
+ * Given a string that represents a Docker volume bind, returns an object with
+ * the following properties:
+ *
+ * - "name": a string that represents the name of that volume.
+ *
+ * - "mountpoint": a string that represents the mount point where that volume
+ *   would be mounted.
+ *
+ * - "mode": a string representing the permissions this container will have when
+ *   attempting to read from or write to the volume (default: rw, read and write
+ *   are both allowed). Values can be 'rw', 'ro' or undefined.
  */
-function publishingPorts(container) {
-    var hostConf = container.HostConfig;
+function getVolumeInfoFromBindString(dockerVolumeBindString) {
+    assert.string(dockerVolumeBindString, 'dockerVolumeBindString');
+
+    var volumeInfo;
+    var volumeName;
+    var volumeMountPoint;
+    var volumeMode;
+
+    // "bind" is expected to be of the form: volumeName:/mount/point[:flags]
+    var volumeComponents = dockerVolumeBindString.split(':');
+    assert.ok(volumeComponents.length === 2 || volumeComponents.length === 3,
+        'bind string should be of the form volumeName:/mount/point[:flags]');
+
+    volumeName = volumeComponents[0];
+    volumeMountPoint = volumeComponents[1];
+    if (volumeComponents.length === 3) {
+        volumeMode = volumeComponents[2];
+    }
+
+    volumeInfo = {
+        name: volumeName,
+        mountpoint: volumeMountPoint
+    };
 
-    if (hostConf.PublishAllPorts || !common.objEmpty(hostConf.PortBindings)) {
-        return true;
+    if (volumeMode) {
+        volumeInfo.mode = volumeMode;
     }
 
-    return false;
+    return volumeInfo;
+}
+
+function addNfsVolumesToPayload(opts, payload, binds) {
+    assert.object(opts, 'opts');
+    assert.object(payload, 'payload');
+    assert.arrayOfString(binds, 'binds');
+
+    var volumeNamesToMountInfo = binds.map(getVolumeInfoFromBindString);
+    payload.volumes = volumeNamesToMountInfo;
 }
 
 /**
@@ -1818,19 +1618,26 @@ function buildVmPayload(opts, container, callback) {
     }
 
     /*
-     * "Host" volumes created using -v http[s]://example/file:/container/file
-     * We'll download http[s]://example/file and write it to a new directory in
-     * the zone's dataset but not in the zoneroot, and we'll mount that into the
-     * container at /container/file.
+     * If support for docker volumes is enabled, don't consider that "Binds"
+     * represent host volumes, otherwise send an error that communicates that
+     * host volumes are not supported.
      */
-    binds = container.Binds
-        || container.HostConfig && container.HostConfig.Binds;
-    if (binds && binds.length > 0) {
-        log.error({host_volumes: binds},
-            'host volumes are not supported');
-        callback(new errors.DockerError(
-            'host volumes are not supported'));
-        return;
+    if (opts.config.experimental_docker_nfs_shared_volumes !== true) {
+       /*
+        * "Host" volumes created using -v http[s]://example/file:/container/file
+        * We'll download http[s]://example/file and write it to a new directory
+        * in the zone's dataset but not in the zoneroot, and we'll mount that
+        * into the container at /container/file.
+        */
+        binds = container.Binds
+            || container.HostConfig && container.HostConfig.Binds;
+        if (binds && binds.length > 0) {
+            log.error({host_volumes: binds},
+                'host volumes are not supported');
+            callback(new errors.DockerError(
+                'host volumes are not supported'));
+            return;
+        }
     }
 
     /*
@@ -1874,7 +1681,8 @@ function buildVmPayload(opts, container, callback) {
 
     vasync.pipeline({funcs: [
         function handleNetworks(_, cb) {
-            addNetworksToPayload(opts, container, payload, cb);
+            mod_networks.addNetworksToContainerPayload(opts, container, payload,
+                cb);
         },
 
         function handleVolumesFrom(_, cb) {
@@ -1987,8 +1795,36 @@ function buildVmPayload(opts, container, callback) {
             cb();
         },
 
+        function handleSharedVolumes(_, cb) {
+            var dockerNfsVolumes =
+                opts.config.experimental_docker_nfs_shared_volumes;
+            var dockerNfsVolumesAutomount =
+                opts.config.experimental_docker_automount_nfs_shared_volumes;
+
+            log.debug('handling shared volumes');
+
+            binds = container.Binds
+                || container.HostConfig && container.HostConfig.Binds;
+
+            if (binds && binds.length > 0) {
+                if (dockerNfsVolumes !== true) {
+                    cb(new Error('Support for NFS volumes is not enabled'));
+                    return;
+                } else if (dockerNfsVolumesAutomount !== true) {
+                    cb(new Error('Support for automounting NFS volumes is not '
+                        + 'enabled'));
+                    return;
+                } else {
+                    addNfsVolumesToPayload(opts, payload, binds);
+                }
+            }
+
+            cb();
+        },
+
         function handleVolumes(_, cb) {
             var existing = {};
+            var nfsVolumesMetadata, nfsVolumesInfo;
 
             /*
              * Regular -v /data volumes, we'll create a new ZFS dataset for
@@ -2002,7 +1838,31 @@ function buildVmPayload(opts, container, callback) {
 
             if (payload.filesystems) {
                 payload.filesystems.forEach(function (f) {
-                    existing[f.target] = true;
+                    existing[f.target] = {
+                        name: f.source + ':' + f.target,
+                        type: f.type
+                    };
+                });
+            }
+
+            /*
+             * Filter out NFS shared volumes: docker-compose uses both the
+             * "Volumes" and "HostConfig.Binds" properties of the container
+             * creation request's payload when creating a non-local volume. Thus
+             * if both properties are used, we consider that the volume to
+             * create is not local and shouldn't be handled here.
+             */
+            nfsVolumesMetadata = payload.internal_metadata['docker:nfsvolumes'];
+            if (nfsVolumesMetadata !== undefined) {
+                nfsVolumesInfo = JSON.parse(nfsVolumesMetadata);
+                nfsVolumesInfo.forEach(function markNfsVolumeExisting(volume) {
+                    assert.object(volume, 'volume');
+                    assert.string(volume.name, 'volume.name');
+
+                    existing[volume.mountpoint] = {
+                        name: volume.name,
+                        type: 'tritonnfs'
+                    };
                 });
             }
 
@@ -2017,10 +1877,14 @@ function buildVmPayload(opts, container, callback) {
             Object.keys(container.Volumes).forEach(function (v) {
                 // v will be something like: `/dir` and container.Volumes[v]
                 // will be an object with options.
-
-                if (existing[v]) {
-                    log.warn(v + ' already added by VolumesFrom or HostVolume '
-                        + 'not adding new volume');
+                var existingVolume = existing[v];
+                if (existingVolume) {
+                    assert.string(existingVolume.name, 'existingVolume.name');
+                    assert.string(existingVolume.type, 'existingVolume.type');
+
+                    log.warn(v + ' already added for volume with name: '
+                        + existingVolume.name + ', of type: '
+                        + existingVolume.type + ', not adding new volume');
                     return;
                 }
                 _addDataVolume(v);
@@ -2081,7 +1945,7 @@ function buildVmPayload(opts, container, callback) {
                     var val = label[1];
 
                     if (typeof (val) !== 'string') {
-                        labelErrs.push(new errors.ValidationError(format(
+                        labelErrs.push(new errors.ValidationError(util.format(
                             'label "%s" value is not a string: %j',
                             key, val)));
                         next();
@@ -2180,6 +2044,7 @@ function buildVmPayload(opts, container, callback) {
             if (imgConfig.Volumes) {
                 Object.keys(imgConfig.Volumes).forEach(function (v) {
                     var exists = false;
+                    var sharedVols = payload.volumes;
 
                     payload.filesystems.forEach(function (f) {
                         if (f.target === v) {
@@ -2187,6 +2052,12 @@ function buildVmPayload(opts, container, callback) {
                         }
                     });
 
+                    sharedVols.forEach(function checkSameMountpoint(vol) {
+                        if (vol.mountpoint === v) {
+                            exists = true;
+                        }
+                    });
+
                     if (exists) {
                         log.warn({volume: v}, 'volume specified both in payload'
                             + ' and image, ignoring volume from image');
@@ -2864,6 +2735,7 @@ function createContainer(opts, callback) {
     function _buildPayload(cb) {
         // XXX check that "name" is not already used? VMAPI also does that.
         container.Name = name;
+
         buildVmPayload({
             app: opts.app,
             config: config,
diff --git a/lib/backends/sdc/index.js b/lib/backends/sdc/index.js
index afb8fc7..e9c8c3d 100644
--- a/lib/backends/sdc/index.js
+++ b/lib/backends/sdc/index.js
@@ -19,7 +19,7 @@ var containers = require('./containers');
 var sysinfo = require('./sysinfo');
 var images = require('./images');
 var networks = require('./networks');
-
+var volumes = require('./volumes');
 
 
 function SdcBackend(opts) {
@@ -82,7 +82,12 @@ SdcBackend.prototype.commitImage = build.commitImage;
 // networks.js
 SdcBackend.prototype.findNetworkByNameOrId = networks.findNetworkByNameOrId;
 SdcBackend.prototype.inspectNetwork = networks.inspectNetwork;
-SdcBackend.prototype.listNetworks = networks.listNetworks;
+SdcBackend.prototype.listNetworksForAccount = networks.listNetworksForAccount;
 
+// volumes.js
+SdcBackend.prototype.createVolume = volumes.createVolume;
+SdcBackend.prototype.listVolumes = volumes.listVolumes;
+SdcBackend.prototype.deleteVolume = volumes.deleteVolume;
+SdcBackend.prototype.inspectVolume = volumes.inspectVolume;
 
 module.exports = SdcBackend;
diff --git a/lib/backends/sdc/networks.js b/lib/backends/sdc/networks.js
index 409f59a..bec9398 100644
--- a/lib/backends/sdc/networks.js
+++ b/lib/backends/sdc/networks.js
@@ -19,6 +19,7 @@ var NAPI = require('sdc-clients').NAPI;
 var util = require('util');
 var vasync = require('vasync');
 
+var containers = require('../../containers');
 var errors = require('../../errors');
 var utils = require('./utils');
 
@@ -26,6 +27,9 @@ var utils = require('./utils');
 
 var ADMIN_NIC_TAG = 'admin';
 
+// Label name used to set the external (public) network for a container.
+var TRITON_PUBLIC_NETWORK_LABEL = 'triton.network.public';
+
 var _napiClientCache; // set in `getNapiClient`
 
 
@@ -215,20 +219,19 @@ function getNapiNetworksForAccount(opts, callback) {
  * @param {Object} opts Options.
  * @param {Function} callback (err, networks) Array of docker network objects.
  */
-function listNetworks(opts, callback) {
+function listNetworksForAccount(opts, callback) {
     assert.object(opts, 'opts');
-    assert.object(opts.req, 'opts.req');
-    assert.object(opts.req.account, 'opts.req.account');
-    assert.object(opts.req.app, 'opts.req.app');
-    assert.object(opts.req.app.config, 'opts.req.app.config');
-    assert.object(opts.req.log, 'opts.req.log');
+    assert.object(opts.account, 'opts.account');
+    assert.object(opts.config, 'opts.config');
+    assert.object(opts.log, 'opts.log');
+    assert.uuid(opts.reqId, 'opts.reqId');
+
+    var reqId = opts.reqId;
+    var log = opts.log;
 
-    var req = opts.req;
-    var reqId = req.getId();
-    var log = req.log;
     var params = {
-        accountUuid: req.account.uuid,
-        config: req.app.config,
+        accountUuid: opts.account.uuid,
+        config: opts.config,
         log: log,
         reqId: reqId
     };
@@ -280,6 +283,7 @@ function inspectNetwork(opts, callback) {
  * @param {Function} callback (err, network) Called with the found network.
  */
 function findNetworkByNameOrId(name, opts, callback) {
+    assert.string(name, 'name');
     assert.object(opts, 'opts');
     assert.object(opts.account, 'opts.account');
     assert.string(opts.account.uuid, 'opts.account.uuid');
@@ -434,9 +438,241 @@ function findNetworkByNameOrId(name, opts, callback) {
     });
 }
 
+/*
+ * When fabrics are configured, but no specific network is supplied,
+ * or if 'bridge' is supplied, we will use the user's default fabric
+ * network, stored in UFDS.
+ *
+ * Additionally, if the user is publishing ports or we have the
+ * `requireExternalNetwork` option (currently used by docker build,
+ * see DOCKER-705), we will also attach to configured network pool
+ * (typically the external/public pool).
+ */
+function getDefaultFabricNetwork(opts, callback) {
+    assert.object(opts, 'opts');
+    assert.object(opts.app, 'opts.app');
+    assert.object(opts.app.config, 'opts.app.config');
+    assert.string(opts.app.config.datacenterName,
+        'opts.app.config.datacenterName');
+    assert.object(opts.log, 'opt.log');
+    assert.func(callback, 'callback');
+
+    var dc = opts.app.config.datacenterName;
+    var log = opts.log;
+
+    log.debug('Networks: using default fabric network');
+
+    opts.app.ufds.getDcLocalConfig(opts.account.uuid, dc, function (err, conf) {
+        log.debug({err: err, conf: conf, account: opts.account.uuid},
+            'Networks: get DC local config');
+
+        if (err || !conf || !conf.defaultnetwork) {
+            callback(errors.ufdsErrorWrap(err,
+                'Networks: could not get default network'));
+            return;
+        }
+
+        callback(null, conf.defaultnetwork);
+
+
+        return;
+    });
+}
+
+/*
+ * Add the required external network to the payload.networks.
+ */
+function externalNetworkByName(opts, container, payload, callback) {
+    assert.object(opts.config, 'opts.config');
+    assert.object(opts.config.napi, 'opts.config.napi');
+    assert.optionalString(opts.config.externalNetwork,
+        'opts.config.externalNetwork');
+    assert.object(opts.account, 'opts.account');
+    assert.string(opts.account.uuid, 'opts.account.uuid');
+    assert.object(payload, 'payload');
+    assert.func(callback, 'callback');
+
+    var externalNetworkName;
+    var labels = container.Labels || {};
+    var log = opts.log;
+    var napiClient = getNapiClient(opts.config.napi);
+
+    if (Object.prototype.hasOwnProperty.call(labels,
+        TRITON_PUBLIC_NETWORK_LABEL))
+    {
+        externalNetworkName = labels[TRITON_PUBLIC_NETWORK_LABEL];
+    }
+
+    if (!payload.hasOwnProperty('networks')) {
+        payload.networks = [];
+    } else {
+        // Ensure the external network is the *only* primary network.
+        payload.networks.forEach(function (nw) {
+            delete nw.primary;
+        });
+    }
+
+    // When fabrics are enabled and no external name has been specified, use the
+    // opts.config.overlay.externalPool uuid for the default external network.
+    if (!externalNetworkName && opts.config.overlay.enabled) {
+        assert.string(opts.config.overlay.externalPool,
+            'opts.config.overlay.externalPool');
+        payload.networks.push(
+            { uuid: opts.config.overlay.externalPool, primary: true });
+        callback();
+        return;
+    }
+
+    // Find the external network using the given (or default) network name.
+    var listParams = {
+        name: externalNetworkName || opts.config.externalNetwork || 'external',
+        fabric: false,
+        provisionable_by: opts.account.uuid
+    };
+
+    log.debug({ listParams: listParams },
+        util.format('Networks: fabrics not configured, using network %s',
+            listParams.name));
+
+    napiClient.listNetworks(listParams, opts, function (err, networks) {
+        log.debug({ err: err, res: networks },
+            util.format('Networks: listNetworks result for %s',
+                listParams.name));
+
+        if (err) {
+            callback(errors.napiErrorWrap(err,
+                util.format('Networks: problem listing network %s',
+                    listParams.name)));
+            return;
+        }
+
+        if (networks.length < 1) {
+            log.error({ networks: networks, params: listParams },
+                util.format('Networks: network %s provisionable by %s not '
+                    + 'found', listParams.name, listParams.provisionable_by));
+            callback(new errors.NetworkNotFoundError(listParams.name));
+            return;
+        }
+
+        payload.networks.push({uuid: networks[0].uuid, primary: true});
+
+        callback();
+        return;
+    });
+}
+
+/**
+ * Add network configurations (fabrics and external) to the payload.
+ *
+ *  Fabrics:
+ *
+ *   When fabrics are enabled, the fabric network is selected in these ways:
+ *    1. When 'bridge' or nothing specified, will use the user's default network
+ *    2. Specifying a network name will provision to the named *fabric* network
+ *    3. Specifying a network id (or portion) will provision to that *fabric*
+ *       network
+ *
+ *   Docker resolves name/id collisions in the following way:
+ *     - a name is preferred to a partial id
+ *     - a full id is preferred to a name
+ *
+ *  External:
+ *
+ *   An external network is added in these cases:
+ *    1. opts.requireExternalNetwork is set, or
+ *    2. fabrics are *not* enabled, or
+ *    3. fabrics are enabled and the user wants to expose ports
+ *
+ *   The user can specify which external network is used by setting the
+ *   'triton.network.public' container label (tag), this specifies the external
+ *   network *name*, all other cases will use the default external network,
+ *   which for fabrics is opts.config.overlay.externalPool (uuid), or
+ *   opts.config.externalNetwork (string name) when there are no fabrics.
+ */
+function addNetworksToContainerPayload(opts, container, payload, callback) {
+    assert.object(opts, 'opts');
+    assert.object(opts.config, 'opts.config');
+    assert.object(opts.config.napi, 'opts.config.napi');
+    assert.object(opts.log, 'opts.log');
+    assert.object(opts.config.overlay, 'opts.config.overlay');
+    assert.optionalString(opts.config.overlay.externalPool,
+        'opts.config.overlay.externalPool');
+    assert.optionalBool(opts.config.overlay.enabled,
+        'opts.config.overlay.enabled');
+    assert.optionalBool(opts.requireExternalNetwork,
+        'opts.requireExternalNetwork');
+    assert.func(callback, 'callback');
+
+    var log = opts.log;
+    var networkMode;
+
+    vasync.pipeline({ funcs: [
+        function addFabricNetworks(_, next) {
+            if (!opts.config.overlay.enabled) {
+                next();
+                return;
+            }
+            networkMode = container.HostConfig.NetworkMode;
+            if (!networkMode || networkMode === 'bridge'
+                    || networkMode === 'default') {
+                getDefaultFabricNetwork(opts,
+                    function onGetDefaultFabricNet(getDefaultFabricNetErr,
+                        defaultFabricNet) {
+                            payload.networks =
+                                [ {uuid: defaultFabricNet, primary: true} ];
+                            next(getDefaultFabricNetErr);
+                        });
+            } else {
+                findNetworkByNameOrId(networkMode, opts,
+                    function (findErr, network)
+                {
+                    if (findErr) {
+                        next(findErr);
+                        return;
+                    }
+                    payload.networks = [ {uuid: network.uuid, primary: true} ];
+                    next();
+                });
+            }
+        },
+
+        function addExternalNetwork(_, next) {
+            if (!opts.requireExternalNetwork && opts.config.overlay.enabled
+                && !containers.publishingPorts(container)) {
+                // DOCKER-1045: for fabrics, it is an error if the
+                // triton.network.public label is used and no ports are being
+                // published.
+                var labels = container.Labels || {};
+                if (Object.prototype.hasOwnProperty.call(labels,
+                        TRITON_PUBLIC_NETWORK_LABEL)) {
+                    next(new errors.ValidationError(util.format(
+                        '%s label requires a container with published ports',
+                        TRITON_PUBLIC_NETWORK_LABEL)));
+                    return;
+                }
+                next();
+                return;
+            }
+            externalNetworkByName(opts, container, payload, next);
+        }
+    ]}, function (err) {
+        if (!err) {
+            log.debug({ networks: payload.networks }, 'payload.networks');
+        } else {
+            log.error({error: err},
+                'Error when adding networks to container payload');
+        }
+
+        callback(err);
+    });
+}
+
+
 
 module.exports = {
+    addNetworksToContainerPayload: addNetworksToContainerPayload,
     findNetworkByNameOrId: findNetworkByNameOrId,
+    getDefaultFabricNetwork: getDefaultFabricNetwork,
     inspectNetwork: inspectNetwork,
-    listNetworks: listNetworks
+    listNetworksForAccount: listNetworksForAccount
 };
diff --git a/lib/backends/sdc/volumes.js b/lib/backends/sdc/volumes.js
new file mode 100644
index 0000000..0c37874
--- /dev/null
+++ b/lib/backends/sdc/volumes.js
@@ -0,0 +1,383 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+var assert = require('assert-plus');
+var jsprim = require('jsprim');
+var utils = require('./utils');
+var vasync = require('vasync');
+var verror = require('verror');
+
+var errors = require('../../errors');
+var mod_networks = require('./networks');
+
+/*
+ * This function polls VOLAPI for the volume specified by volumeUuid.
+ *
+ * It ignores any VOLAPI errors, and retries every second until either one of
+ * the states (array of string states e.g. ['failed', 'ready']) is seen as the
+ * volume's state, or until 100 attempts have been made.
+ *
+ * Calls callback(err, volume) with volume being the last-loaded state (if any)
+ * and err being either an Error object or null.
+ */
+function pollVolumeState(volumeUuid, volapiClient, states, callback) {
+    assert.uuid(volumeUuid, 'volumeUuid');
+    assert.object(volapiClient, 'volapiClient');
+    assert.arrayOfString(states, 'states');
+    assert.func(callback, 'callback');
+
+    var nbVolumeStatePolled = 0;
+    var MAX_NB_VOLUME_STATE_POLLS = 120;
+    var VOLUME_STATE_POLL_INTERVAL_IN_MS = 1000;
+
+    function doPollVolumeStateChange() {
+        ++nbVolumeStatePolled;
+
+        volapiClient.getVolume({
+            uuid: volumeUuid
+        }, function onGetVolume(err, updatedVolume) {
+            if (err && verror.hasCauseWithName(err, 'VolumeNotFoundError')) {
+                // If the VM is not found, no use in polling further and we
+                // return the error in case the caller wanted to know when the
+                // volume disappeared.
+                callback(err);
+                return;
+            }
+
+            if (updatedVolume && states.indexOf(updatedVolume.state) !== -1) {
+                callback(null, updatedVolume);
+            } else {
+                if (nbVolumeStatePolled > MAX_NB_VOLUME_STATE_POLLS) {
+                    callback(new Error('Timed out polling for '
+                        + 'volume state change'), updatedVolume);
+                } else {
+                    setTimeout(doPollVolumeStateChange,
+                        VOLUME_STATE_POLL_INTERVAL_IN_MS);
+                }
+            }
+        });
+    }
+
+    doPollVolumeStateChange();
+}
+
+function createVolume(volumeParams, options, callback) {
+    assert.object(volumeParams, 'params');
+    assert.object(options, 'options');
+    assert.object(options.log, 'options.log');
+    assert.object(options.app, 'options.app');
+    assert.func(callback, 'callback');
+
+    var self = this;
+    var log = options.log;
+    var volapiClient = options.app.volapi;
+
+    var payload = {
+        name: volumeParams.name,
+        size: volumeParams.size,
+        type: volumeParams.type
+    };
+
+    payload.owner_uuid = options.account.uuid;
+
+    log.debug({payload: volumeParams},
+        'Sending request to VOLAPI for volume creation');
+
+    var context = {};
+    vasync.pipeline({funcs: [
+        function setupNetwork(ctx, done) {
+            var networkOpts = {
+                config: self.config,
+                account: options.account,
+                app: options.app,
+                log: log,
+                req_id: options.reqId
+            };
+
+            if (volumeParams.network === undefined) {
+                mod_networks.getDefaultFabricNetwork(networkOpts,
+                    function onGetDefaultFabricNet(getDefaultFabricNetErr,
+                        defaultFabricNetwork) {
+                        if (!getDefaultFabricNetErr) {
+                            payload.networks = [defaultFabricNetwork];
+                        }
+                        done(getDefaultFabricNetErr);
+                    });
+            } else {
+                mod_networks.findNetworkByNameOrId(volumeParams.network,
+                    networkOpts,
+                    function onGetNamedNetwork(getNetworkErr, network) {
+                        payload.networks = [network.uuid];
+                        done(getNetworkErr);
+                    });
+            }
+        },
+        function doCreateVolume(ctx, done) {
+            volapiClient.createVolume(payload, {
+                headers: {'x-request-id': options.reqId}
+            }, function onVolumeCreated(err, volume) {
+                log.debug({err: err, volume: volume},
+                    'Got response from VOLAPI');
+
+                if (err) {
+                    callback(errors.volapiErrorWrap(err,
+                        'problem creating volume'));
+                    return;
+                }
+
+                ctx.volume = volume;
+                pollVolumeState(volume.uuid, volapiClient,
+                    ['ready', 'failed'],
+                    function _onPollComplete(pollErr, vol) {
+                        var createErr = pollErr;
+
+                        if (!pollErr && vol && vol.state === 'failed') {
+                            createErr = new errors.InternalError(
+                                'volume creation failed');
+                        }
+
+                        ctx.volume = vol;
+                        done(createErr);
+                    }
+                );
+            });
+        }
+    ], arg: context
+    }, function allDone(err) {
+        callback(err, context.volume);
+    });
+}
+
+function listVolumes(params, options, callback) {
+    assert.object(params, 'params');
+    assert.object(options, 'options');
+    assert.func(callback, 'callback');
+
+    var filterName;
+    var filters;
+    var filterValue;
+    var log = options.log;
+    var reqParams = {};
+    var valIdx;
+
+    /*
+     * By default, we only list volumes that are ready. As far as I know, docker
+     * doesn't have a concept of a volume's state, and so doesn't provide any
+     * ability to filter on such a property.
+     */
+    var predicate = {
+        eq: ['state', 'ready']
+    };
+    var volapiClient = options.app.volapi;
+
+    log.info({
+        params: params
+    }, 'params');
+
+    if (params.filters) {
+        filters = JSON.parse(params.filters);
+        filters = utils.getNormalizedFilters(filters);
+        if (filters instanceof Error) {
+            callback(new errors.DockerError('invalid filters: ' + filters));
+            return;
+        }
+        log.debug({filters: filters}, 'listVolumes: filters');
+    }
+
+    assert.optionalObject(filters, 'filters');
+
+    if (filters !== undefined) {
+        log.info({
+            filters: filters
+        }, 'filters');
+
+        for (filterName in filters) {
+            var filterValues = filters[filterName];
+            for (valIdx in filterValues) {
+                filterValue = filterValues[valIdx];
+                if (filterName === 'dangling') {
+                    filterValue = (filterValue === 'true' ? true : false);
+                }
+
+                var newPredicateComponent = {eq: [filterName, filterValue]};
+
+                if (!predicate.hasOwnProperty('and')) {
+                    predicate = {
+                        and: [
+                            predicate,
+                            newPredicateComponent
+                        ]
+                    };
+                } else {
+                    predicate.and.push(newPredicateComponent);
+                }
+            }
+        }
+    }
+
+    reqParams.owner_uuid = options.account.uuid;
+
+    assert.object(predicate, 'predicate');
+    reqParams.predicate = JSON.stringify(predicate);
+
+    log.debug({
+        reqParams: reqParams
+    }, 'sending ListVolumes request to VOLAPI');
+
+    volapiClient.listVolumes(reqParams, {
+        headers: {
+            'x-request-id': options.reqId
+        }
+    }, function onVolumesListed(err, volumes) {
+        log.debug({err: err, volumes: volumes},
+            'Response from volapi ListVolumes');
+        if (err) {
+            callback(errors.volapiErrorWrap(err,
+                'problem listing volumes'));
+            return;
+        }
+
+        callback(null, volumes);
+    });
+}
+
+function deleteVolume(params, options, callback) {
+    assert.object(params, 'params');
+    assert.string(params.name, 'params.name');
+    assert.object(options, 'options');
+    assert.object(options.account, 'options.account');
+    assert.uuid(options.account.uuid, 'options.account.uuid');
+    assert.func(callback, 'callback');
+
+    var log = options.log;
+    var volapiClient = options.app.volapi;
+
+    var volumeName = params.name;
+    var volumeOwnerUuid = options.account.uuid;
+
+    var context = {};
+
+    vasync.pipeline({funcs: [
+        function _findVolumeByName(ctx, next) {
+            var volumeReadyPredicate = {
+                eq: ['state', 'ready']
+            };
+
+            var listVolumesParams = {
+                name: volumeName,
+                owner_uuid: volumeOwnerUuid,
+                predicate: JSON.stringify(volumeReadyPredicate)
+            };
+
+            volapiClient.listVolumes(listVolumesParams, {
+                headers: {'x-request-id': options.reqId}
+            }, function volumesListed(volumesListErr, volumes) {
+                var err;
+
+                if (!volumesListErr && volumes) {
+                    if (volumes.length === 0) {
+                        err = new errors.DockerError('Could not find volume '
+                            + 'with name: ' + params.name);
+                    } else if (volumes.length !== 1) {
+                        err = new errors.DockerError('More than one volume '
+                            + 'with name: ' + params.name);
+                    } else {
+                        ctx.volumeToDeleteUuid = volumes[0].uuid;
+                    }
+                }
+
+                next(err);
+            });
+        },
+        function _deleteVolume(ctx, next) {
+            assert.uuid(ctx.volumeToDeleteUuid, 'ctx.volumeToDeleteUuid');
+
+            var volumeUuid = ctx.volumeToDeleteUuid;
+
+            var deleteVolumeParams = {
+                uuid: volumeUuid,
+                owner_uuid: volumeOwnerUuid
+            };
+
+            volapiClient.deleteVolume(deleteVolumeParams, {
+                headers: {'x-request-id': options.reqId}
+            }, function onVolumeDeleted(volumeDeletionErr) {
+                var err;
+
+                log.debug({err: volumeDeletionErr},
+                    'Response from volapi.deleteVolume');
+
+                if (volumeDeletionErr) {
+                    err = errors.volapiErrorWrap(volumeDeletionErr,
+                        'problem deleting volume');
+                    next(err);
+                    return;
+                }
+
+                pollVolumeState(volumeUuid, volapiClient, ['failed'],
+                    function _onPollComplete(pollErr, vol) {
+                        var deleteErr = pollErr;
+
+                        if (pollErr && verror.hasCauseWithName(pollErr,
+                            'VolumeNotFoundError')) {
+                            // The volume disappeared, which is what we wanted,
+                            // so nothing further to do.
+                            next();
+                            return;
+                        }
+
+                        if (!pollErr && vol && vol.state === 'failed') {
+                            deleteErr = new errors.InternalError(
+                                'volume deletion failed');
+                        }
+
+                        next(deleteErr);
+                    }
+                );
+            });
+        }
+    ],
+    arg: context
+    }, callback);
+}
+
+function inspectVolume(params, options, callback) {
+    assert.object(params, 'params');
+    assert.string(params.name, 'params.name');
+    assert.object(options, 'options');
+    assert.func(callback, 'callback');
+
+    var err;
+    var log = options.log;
+    var volapiClient = options.app.volapi;
+
+    params.owner_uuid = options.account.uuid;
+
+    volapiClient.listVolumes(params, {headers: {'x-request-id': options.reqId}},
+        function onVolume(volapiErr, volumes) {
+            var volume = volumes[0];
+
+            log.debug({err: err}, 'Response from volapi.getVolume');
+
+            if (volapiErr) {
+                err = errors.volapiErrorWrap(volapiErr,
+                    'problem getting volume');
+            }
+
+            callback(err, volume);
+        });
+}
+
+module.exports = {
+    createVolume: createVolume,
+    listVolumes: listVolumes,
+    deleteVolume: deleteVolume,
+    inspectVolume: inspectVolume
+};
diff --git a/lib/common.js b/lib/common.js
index 4a24015..1c11293 100644
--- a/lib/common.js
+++ b/lib/common.js
@@ -375,6 +375,9 @@ function filteredAuditLog(req, res, route, err) {
  * Returns a handler that will log uncaught exceptions properly
  */
 function uncaughtHandler(req, res, route, err) {
+    req.log.error({err: err, route: route && route.name,
+        req: req}, 'Uncaught exception');
+
     res.send(new restify.InternalError(err, 'Internal error'));
     /**
      * We don't bother logging the `res` here because it always looks like
@@ -390,8 +393,6 @@ function uncaughtHandler(req, res, route, err) {
      *
      *      {"code":"InternalError","message":"Internal error"}
      */
-    req.log.error({err: err, route: route && route.name,
-        req: req}, 'Uncaught exception');
 }
 
 
diff --git a/lib/containers.js b/lib/containers.js
new file mode 100644
index 0000000..c5eaa1d
--- /dev/null
+++ b/lib/containers.js
@@ -0,0 +1,28 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+var common = require('./common');
+
+/**
+ * Returns true if the container is publishing ports
+ */
+function publishingPorts(container) {
+    var hostConf = container.HostConfig;
+
+    if (hostConf.PublishAllPorts || !common.objEmpty(hostConf.PortBindings)) {
+        return true;
+    }
+
+    return false;
+}
+
+module.exports = {
+    publishingPorts: publishingPorts
+};
\ No newline at end of file
diff --git a/lib/docker.js b/lib/docker.js
index d1d05f7..a7b814e 100644
--- a/lib/docker.js
+++ b/lib/docker.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright (c) 2016, Joyent, Inc.
+ * Copyright (c) 2017, Joyent, Inc.
  */
 
 /*
@@ -19,6 +19,7 @@ var EffluentLogger = require('effluent-logger');
 var fmt = require('util').format;
 var fs = require('fs');
 var http = require('http');
+var jsprim = require('jsprim');
 var LRU = require('lru-cache');
 var moray = require('moray');
 var sbs = require('sdc-bunyan-serializers');
@@ -26,12 +27,12 @@ var os = require('os');
 var path = require('path');
 var restify = require('restify');
 var formatJSON = require('restify/lib/formatters/json');
-var UFDS = require('ufds');
 var vasync = require('vasync');
 var verror = require('verror');
 var CNAPI = require('sdc-clients').CNAPI;
 var IMGAPI = require('sdc-clients').IMGAPI;
 var VMAPI = require('sdc-clients').VMAPI;
+var VOLAPI = require('sdc-clients').VOLAPI;
 
 var adminEndpoints = require('./endpoints/admin');
 var auth = require('./auth');
@@ -43,6 +44,7 @@ var errors = require('./errors');
 var hijack = require('./hijack');
 var models = require('./models');
 var SocketManager = require('./socket-manager');
+var ufds = require('./ufds');
 var wfapi = require('./wfapi');
 var configLoader = require('./config-loader');
 
@@ -61,11 +63,13 @@ var request_seq_id = 0;
 //---- the App
 
 function App(opts) {
-    var self = this;
     assert.object(opts, 'opts');
     assert.object(opts.log, 'opts.log');
     assert.object(opts.config, 'opts.config');
 
+    var server;
+    var self = this;
+
     self.version = VERSION;
     self.log = opts.log;
     self.config = opts.config;
@@ -73,6 +77,8 @@ function App(opts) {
     var Backend = require('./backends/' + self.config.backend);
     self.backend = new Backend({log: self.log, config: self.config});
 
+    server = self.server = self.setupServer();
+
     self.setupConnections();
 
     // TODO make the other clients accessible via req.app
@@ -85,7 +91,7 @@ function App(opts) {
             'starting in read-only mode');
     }
 
-    var server = self.server = self.setupServer();
+    server = self.server = self.setupServer();
 
     server.server.on('upgrade', function (oldreq, socket, body) {
         self.log.info('Socket has been upgraded');
@@ -123,6 +129,7 @@ function App(opts) {
     });
 
     server.use(restify.requestLogger());
+
     server.use(function (req, res, next) {
         req.trace = trace_event.createBunyanTracer({
             log: req.log
@@ -142,7 +149,7 @@ function App(opts) {
 
     server.on('after', common.filteredAuditLog);
     server.on('uncaughtException', common.uncaughtHandler);
-    endpoints.register(server, self.log, [
+    endpoints.register(self.config, server, self.log, [
         common.checkReadonlyMode,
         common.checkServices,
         auth.auth(self.config, self.log),  /* sets `req.account` */
@@ -154,6 +161,9 @@ function App(opts) {
 
 App.prototype.setupConnections = function setupConnections() {
     var self = this;
+    var ufdsOptions = jsprim.deepCopy(self.config.ufds);
+
+    assert.object(self.server, 'self.server');
 
     self.connWatcher = new ConnectionStatusWatcher({
         app: self
@@ -196,6 +206,31 @@ App.prototype.setupConnections = function setupConnections() {
         }
     });
 
+    if (self.config.experimental_docker_nfs_shared_volumes === true) {
+        self.connWatcher.register({
+            name: 'volapi',
+            init: function (cb) {
+                var volapiConfig = jsprim.deepCopy(self.config.volapi);
+                var volapi;
+
+                volapiConfig.version = '^1';
+                volapiConfig.userAgent = self.server.name;
+                volapi = new VOLAPI(volapiConfig);
+                cb(null, volapi);
+            },
+            pingIntervalSecs: 10,
+            ping: function (volapi, cb) {
+                volapi.ping(function (err) {
+                    if (err) {
+                        cb(new verror.VError(err, 'could not ping VOLAPI'));
+                        return;
+                    }
+                    cb();
+                });
+            }
+        });
+    }
+
     self.connWatcher.register({
         name: 'imgapi',
         init: function (cb) {
@@ -240,12 +275,13 @@ App.prototype.setupConnections = function setupConnections() {
         }
     });
 
-    self.createUfdsClient(self.config.ufds, function (err, ufds) {
+    ufdsOptions.log = self.log;
+    ufds.createUfdsClient(ufdsOptions, function (err, ufdsClient) {
         if (err) {
-            self.log.error({ err: err }, 'ufds error');
+            self.log.error({ err: err }, 'Failed to create UFDS client');
             return;
         }
-        self.ufds = ufds;
+        self.ufds = ufdsClient;
     });
 };
 
@@ -426,34 +462,6 @@ App.prototype.createMorayClient = function createMorayClient() {
     return client;
 };
 
-/**
- * Creates a UFDS client instance pointing to the UFDS server provided
- * in options. callback will be called either with Error - cb(err) - or
- * with the recently instantiated client object: cb(null, ufds_client)
- */
-App.prototype.createUfdsClient = function (options, callback) {
-    options.log = this.log;
-    var ufds = new UFDS(options);
-
-    ufds.once('connect', function () {
-        ufds.removeAllListeners('error');
-        ufds.on('error', function (err) {
-            options.log.error(err, 'UFDS disconnected');
-        });
-        ufds.on('connect', function () {
-            options.log.info('UFDS reconnected');
-        });
-        callback(null, ufds);
-    });
-
-    ufds.once('error', function (err) {
-        // You are screwed. It's likely that the bind credentials were bad.
-        // Treat this as fatal and move on:
-        options.log.error({err: err}, 'UFDS connection error');
-        callback(err);
-    });
-};
-
 /*
  * Gets the admin IP address for the sdc-docker server
  */
diff --git a/lib/endpoints/_ping.js b/lib/endpoints/_ping.js
index f7112b4..074167a 100644
--- a/lib/endpoints/_ping.js
+++ b/lib/endpoints/_ping.js
@@ -31,7 +31,7 @@ function ping(req, res, next) {
 /**
  * Register all endpoints with the restify server
  */
-function register(http, before) {
+function register(config, http, before) {
     // Note: '_ping' ignores the 'before' argument to avoid running the
     //       authentication methods.
     http.get({ path: /^(\/v[^\/]+)?\/_ping$/, name: 'Ping' }, ping);
diff --git a/lib/endpoints/auth.js b/lib/endpoints/auth.js
index 457ecb0..547e32a 100644
--- a/lib/endpoints/auth.js
+++ b/lib/endpoints/auth.js
@@ -80,7 +80,7 @@ function auth(req, res, next) {
 /**
  * Register all endpoints with the restify server
  */
-function register(http, before) {
+function register(config, http, before) {
     http.post({ path: /^(\/v[^\/]+)?\/auth$/, name: 'Auth' },
         before, restify.bodyParser(), auth);
 }
diff --git a/lib/endpoints/build.js b/lib/endpoints/build.js
index a5ad4b2..42d1292 100644
--- a/lib/endpoints/build.js
+++ b/lib/endpoints/build.js
@@ -118,7 +118,7 @@ function build(req, res, next) {
 /**
  * Register all endpoints with the restify server
  */
-function register(http, before) {
+function register(config, http, before) {
     http.post({ path: /^(\/v[^\/]+)?\/build$/, name: 'Build' },
         before, restify.queryParser({mapParams: false}), build);
 }
diff --git a/lib/endpoints/ca.js b/lib/endpoints/ca.js
index ae83d8b..15c30df 100644
--- a/lib/endpoints/ca.js
+++ b/lib/endpoints/ca.js
@@ -27,7 +27,7 @@ function caPem(req, res, next) {
 /**
  * Register all endpoints with the restify server.
  */
-function register(http, before) {
+function register(config, http, before) {
     // Note: 'ca.pem' ignores the 'before' argument to avoid performing auth.
     http.get({ path: '/ca.pem', name: 'CA' }, caPem);
 }
diff --git a/lib/endpoints/commit.js b/lib/endpoints/commit.js
index 5ce1ca5..88391c1 100644
--- a/lib/endpoints/commit.js
+++ b/lib/endpoints/commit.js
@@ -74,7 +74,7 @@ function commit(req, res, next) {
 /**
  * Register all endpoints with the restify server
  */
-function register(http, before) {
+function register(config, http, before) {
 
     function reqParamsId(req, res, next) {
         req.params.id = req.query.container;
diff --git a/lib/endpoints/containers.js b/lib/endpoints/containers.js
index 0819881..f614a55 100644
--- a/lib/endpoints/containers.js
+++ b/lib/endpoints/containers.js
@@ -934,7 +934,7 @@ function containerRename(req, res, next) {
 /**
  * Register all endpoints with the restify server
  */
-function register(http, before) {
+function register(config, http, before) {
 
     function reqParamsId(req, res, next) {
         req.params.id = unescape(req.params[1]);
diff --git a/lib/endpoints/events.js b/lib/endpoints/events.js
index 5fb439d..ac4b956 100644
--- a/lib/endpoints/events.js
+++ b/lib/endpoints/events.js
@@ -25,7 +25,7 @@ function events(req, res, next) {
 /**
  * Register all endpoints with the restify server
  */
-function register(http, before) {
+function register(config, http, before) {
     http.get({ path: /^(\/v[^\/]+)?\/events$/, name: 'Events' }, events);
 }
 
diff --git a/lib/endpoints/exec.js b/lib/endpoints/exec.js
index ab071c8..f500bce 100644
--- a/lib/endpoints/exec.js
+++ b/lib/endpoints/exec.js
@@ -133,7 +133,7 @@ function execInspect(req, res, next) {
 /**
  * Register all endpoints with the restify server
  */
-function register(http, before) {
+function register(config, http, before) {
 
     function reqParamsId(req, res, next) {
         req.params.id = unescape(req.params[1]);
diff --git a/lib/endpoints/images.js b/lib/endpoints/images.js
index e553ea3..bef797b 100644
--- a/lib/endpoints/images.js
+++ b/lib/endpoints/images.js
@@ -319,7 +319,7 @@ function imageLoad(req, res, next) {
 /**
  * Register all endpoints with the restify server
  */
-function register(http, before) {
+function register(config, http, before) {
 
     function reqParamsName(req, res, next) {
         req.params.name = unescape(req.params[1]);
diff --git a/lib/endpoints/index.js b/lib/endpoints/index.js
index 18141cf..5865f93 100644
--- a/lib/endpoints/index.js
+++ b/lib/endpoints/index.js
@@ -26,7 +26,8 @@ var toRegister = {
     '/images': require('./images'),
     '/info': require('./info'),
     '/networks': require('./networks'),
-    '/version': require('./version')
+    '/version': require('./version'),
+    '/volumes': require('./volumes')
 };
 
 
@@ -38,10 +39,10 @@ var toRegister = {
 /*
  * Register all endpoints with the restify server
  */
-function registerEndpoints(http, log, before) {
+function registerEndpoints(config, http, log, before) {
     for (var t in toRegister) {
         log.debug('Registering endpoints for "%s"', t);
-        toRegister[t].register(http, before);
+        toRegister[t].register(config, http, before);
     }
 }
 
diff --git a/lib/endpoints/info.js b/lib/endpoints/info.js
index 5dc7fbc..5836e1a 100644
--- a/lib/endpoints/info.js
+++ b/lib/endpoints/info.js
@@ -33,7 +33,7 @@ function info(req, res, next) {
 /**
  * Register all endpoints with the restify server
  */
-function register(http, before) {
+function register(config, http, before) {
     http.get({ path: /^(\/v[^\/]+)?\/info$/, name: 'Info' },
         before, info);
 }
diff --git a/lib/endpoints/networks.js b/lib/endpoints/networks.js
index cd71323..165ccd4 100644
--- a/lib/endpoints/networks.js
+++ b/lib/endpoints/networks.js
@@ -21,9 +21,13 @@ var errors = require('../errors');
  */
 function networkList(req, res, next) {
     var log = req.log;
-    var opts = {req: req};
 
-    req.backend.listNetworks(opts, function (err, networks) {
+    req.backend.listNetworksForAccount({
+        account: req.account,
+        config: req.app.config,
+        log: req.log,
+        reqId: req.getId()
+    }, function (err, networks) {
         if (err) {
             if (!(err instanceof errors.DockerError)) {
                 log.error({err: err}, 'Problem loading networks');
@@ -106,7 +110,7 @@ function networkPrune(req, res, next) {
 /**
  * Register all endpoints with the restify server
  */
-function register(http, before) {
+function register(config, http, before) {
 
     function reqParamsId(req, res, next) {
         req.params.id = unescape(req.params[1]);
diff --git a/lib/endpoints/version.js b/lib/endpoints/version.js
index c39322a..39de708 100644
--- a/lib/endpoints/version.js
+++ b/lib/endpoints/version.js
@@ -71,7 +71,7 @@ function version(req, res, next) {
 /**
  * Register all endpoints with the restify server
  */
-function register(http, before) {
+function register(config, http, before) {
     http.get({ path: /^(\/v[^\/]+)?\/version$/, name: 'Version' },
         before, version);
 }
diff --git a/lib/endpoints/volumes.js b/lib/endpoints/volumes.js
new file mode 100644
index 0000000..c796c03
--- /dev/null
+++ b/lib/endpoints/volumes.js
@@ -0,0 +1,294 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+var assert = require('assert-plus');
+var restify = require('restify');
+var vasync = require('vasync');
+var VError = require('verror');
+
+var mod_volumes = require('../volumes');
+var validate = require('../validate');
+
+function getErrorFromVolSizeNotAvailableError(err) {
+    assert.object(err, 'err');
+
+    var availableSizes;
+    var availableSizesInGiB;
+    var volSizeNotAvailableErr = VError.findCauseByName(err,
+        'VolumeSizeNotAvailableError');
+
+    availableSizes = volSizeNotAvailableErr.body.availableSizes;
+    availableSizesInGiB = availableSizes.map(function convertToGib(size) {
+        return size / 1024 + 'G';
+    });
+
+    return new Error('Volume size not available. Available sizes: '
+        + availableSizesInGiB.join(', '));
+}
+
+function createVolume(req, res, next) {
+    assert.object(req, 'object');
+
+    assert.object(req.params, 'req.params');
+    assert.string(req.params.Name, 'req.params.Name');
+    assert.string(req.params.Driver, 'req.params.Driver');
+    assert.optionalObject(req.params.DriverOpts, 'req.params.DriverOpts');
+
+    assert.object(req.log, 'req.log');
+    assert.object(res, 'object');
+    assert.func(next, 'next');
+
+    var log = req.log;
+
+    var options = {
+        account: req.account,
+        app: req.app,
+        log: log,
+        reqId: req.getId()
+    };
+
+    var sizeParams;
+    var networkParams;
+
+    if (req.params.DriverOpts) {
+        /*
+         * The "size" input parameter was validated by a previous restify
+         * handler, so it's safe to call "parseVolumeSize" here, even though it
+         * throws on invalid input.
+         */
+        sizeParams = mod_volumes.parseVolumeSize(req.params.DriverOpts.size);
+        networkParams = req.params.DriverOpts.network;
+    }
+
+    var volumeParams = {
+        name: req.params.Name,
+        size: sizeParams,
+        network: networkParams,
+        type: req.params.Driver
+    };
+
+    // The 'local' volume driver is used by default and implicitly by at least
+    // several Docker clients when creating a volume (including the docker
+    // binary and docker-compose). Since "local" volumes don't make much sense
+    // on Triton (different containers may end up on different servers), the
+    // implicitly set 'local' driver is overriden to Triton's own default volume
+    // driver.
+    if (volumeParams.type === 'local') {
+        volumeParams.type = mod_volumes.DEFAULT_DRIVER;
+    }
+
+    req.backend.createVolume(volumeParams, options,
+        function onVolumeCreated(err, volume) {
+            log.debug({err: err}, 'result from backend');
+            if (err) {
+                log.error({volume: volume, err: err}, 'createVolume error');
+                /*
+                 * VolumeSizeNotAvailable errors include some extra information
+                 * in their body: a list of available volume sizes in mebibytes.
+                 * However, users of the docker client specify volume sizes in
+                 * gibibytes, so we need to generate a new error with a message
+                 * that mentions available volume sizes in mebibytes.
+                 */
+                if (err.restCode === 'VolumeSizeNotAvailable') {
+                    next(getErrorFromVolSizeNotAvailableError(err));
+                    return;
+                }
+                next(err);
+                return;
+            }
+
+            res.send({
+                Name: volume.name
+            });
+
+            next();
+        });
+}
+
+function volapiToDockerNfsVolume(volapiVolume) {
+    assert.object(volapiVolume, 'volapiVolume');
+
+    return {
+        Name: volapiVolume.name,
+        Driver: volapiVolume.type,
+        Mountpoint: volapiVolume.filesystem_path
+    };
+}
+
+function volapiToDockerVolume(volapiVolume) {
+    assert.object(volapiVolume, 'volapiVolume');
+
+    var volumesFormatters = {
+        tritonnfs: volapiToDockerNfsVolume
+    };
+
+    var volumeFormatter = volumesFormatters[volapiVolume.type];
+    if (volumeFormatter) {
+        return volumeFormatter(volapiVolume);
+    } else {
+        return volapiVolume;
+    }
+}
+
+function volapiToDockerVolumes(volapiVolumes) {
+    assert.arrayOfObject(volapiVolumes, 'volapiVolumes');
+
+    return volapiVolumes.map(volapiToDockerVolume);
+}
+
+function listVolumes(req, res, next) {
+    assert.object(req, 'req');
+    assert.object(res, 'res');
+    assert.func(next, 'next');
+
+    var log = req.log;
+
+    var params = {};
+    params.filters = req.query.filters;
+
+    var options = {
+        account: req.account,
+        app: req.app,
+        log: log,
+        reqId: req.getId()
+    };
+
+    req.backend.listVolumes(params, options,
+        function onVolumesListed(err, volapiVolumes) {
+            var dockerVolumes = [];
+
+            if (err) {
+                log.error({err: err, volapiVolumes: volapiVolumes},
+                    'Error when listing volumes');
+                next(err);
+            } else {
+                dockerVolumes = volapiToDockerVolumes(volapiVolumes);
+                res.send(200, {
+                    Volumes: dockerVolumes
+                });
+                next();
+            }
+        });
+}
+
+function deleteVolume(req, res, next) {
+    assert.object(req, 'req');
+    assert.object(res, 'res');
+    assert.func(next, 'next');
+
+    var log = req.log;
+    var volumeName = req.params.name;
+
+    var options = {
+        account: req.account,
+        app: req.app,
+        log: log,
+        reqId: req.getId()
+    };
+
+    req.backend.deleteVolume({
+        name: volumeName
+    }, options, function volumeDeleted(err) {
+        if (err) {
+            log.error({err: err}, 'Error when deleting volume');
+            next(err);
+        } else {
+            res.send(204);
+            next();
+        }
+    });
+}
+
+function inspectVolume(req, res, next) {
+    assert.object(req, 'req');
+    assert.object(res, 'res');
+    assert.func(next, 'next');
+
+    var log = req.log;
+    var options = {
+        account: req.account,
+        app: req.app,
+        log: log,
+        reqId: req.getId()
+    };
+
+    req.backend.inspectVolume({
+        name: req.params.name
+    }, options, function onVolumeInspected(err, volume) {
+        if (err) {
+            log.error({err: err}, 'Error when inspecting volume');
+            next(err);
+        } else {
+            if (volume === undefined) {
+                res.send(404);
+            } else {
+                res.send(200, volapiToDockerVolume(volume));
+            }
+
+            next();
+        }
+    });
+}
+
+/**
+ * Register all endpoints with the restify server
+ */
+function register(config, http, before) {
+    function reqParamsName(req, res, next) {
+        assert.object(req, 'req');
+        assert.object(res, 'res');
+        assert.func(next, 'next');
+
+        req.params.name = decodeURIComponent(req.params[1]);
+        next();
+    }
+
+    function volumesSupported(req, res, next) {
+        assert.object(req, 'req');
+        assert.object(res, 'res');
+        assert.func(next, 'next');
+
+        var err;
+        if (config.experimental_docker_nfs_shared_volumes !== true) {
+            err = new Error('Volumes are not supported');
+        }
+
+        next(err);
+    }
+
+    http.post({
+        path: /^(\/v[^\/]+)?\/volumes\/create$/,
+        name: 'CreateVolume'
+    }, before, volumesSupported, restify.bodyParser(),
+        validate.createVolume, createVolume);
+
+    http.get({
+        path: /^(\/v[^\/]+)?\/volumes$/,
+        name: 'ListVolumes'
+    }, before, volumesSupported, restify.queryParser({mapParams: false}),
+        listVolumes);
+
+    http.del({
+        path: /^(\/v[^\/]+)?\/volumes\/([^\/]+)$/,
+        name: 'DeleteVolume'
+    }, before, volumesSupported, reqParamsName,
+        restify.queryParser({mapParams: false}),
+        validate.deleteVolume, deleteVolume);
+
+    http.get({
+        path: /^(\/v[^\/]+)?\/volumes\/([^\/]+)$/,
+        name: 'InspectVolume'
+    }, before, volumesSupported, reqParamsName, validate.inspectVolume,
+        inspectVolume);
+}
+
+module.exports = {
+    register: register
+};
diff --git a/lib/errors.js b/lib/errors.js
index 6003ac4..ed3190f 100644
--- a/lib/errors.js
+++ b/lib/errors.js
@@ -706,7 +706,24 @@ function ufdsErrorWrap(cause, message) {
     }
 }
 
+function volapiErrorWrap(cause, message) {
+    if (!cause) {
+        return cause;
+    } else if (!cause.restCode) {
+        return new DockerError(cause, message);
+    }
+
+    switch (cause.restCode) {
+        case 'ValidationError':
+        case 'VolumeAlreadyExists':
+        case 'VolumeInUse':
+        case 'VolumeSizeNotAvailable':
+            return new ExposedSDCError(cause, message);
 
+        default:
+            return new DockerError(cause, message);
+    }
+}
 
 // ---- exports
 
@@ -743,5 +760,6 @@ module.exports = {
     napiErrorWrap: napiErrorWrap,
     papiErrorWrap: papiErrorWrap,
     ufdsErrorWrap: ufdsErrorWrap,
-    vmapiErrorWrap: vmapiErrorWrap
+    vmapiErrorWrap: vmapiErrorWrap,
+    volapiErrorWrap: volapiErrorWrap
 };
diff --git a/lib/ufds.js b/lib/ufds.js
new file mode 100644
index 0000000..f5aa56b
--- /dev/null
+++ b/lib/ufds.js
@@ -0,0 +1,37 @@
+var assert = require('assert-plus');
+var UFDS = require('ufds');
+
+/**
+ * Creates a UFDS client instance pointing to the UFDS server provided
+ * in options. callback will be called either with Error - cb(err) - or
+ * with the recently instantiated client object: cb(null, ufds_client)
+ */
+function createUfdsClient(options, callback) {
+    assert.object(options, 'options');
+    assert.object(options.log, 'options.log');
+    assert.func(callback, 'callback');
+
+    var ufds = new UFDS(options);
+
+    ufds.once('connect', function () {
+        ufds.removeAllListeners('error');
+        ufds.on('error', function (err) {
+            options.log.error(err, 'UFDS disconnected');
+        });
+        ufds.on('connect', function () {
+            options.log.info('UFDS reconnected');
+        });
+        callback(null, ufds);
+    });
+
+    ufds.once('error', function (err) {
+        // You are screwed. It's likely that the bind credentials were bad.
+        // Treat this as fatal and move on:
+        options.log.error({err: err}, 'UFDS connection error');
+        callback(err);
+    });
+}
+
+module.exports = {
+    createUfdsClient: createUfdsClient
+};
\ No newline at end of file
diff --git a/lib/units.js b/lib/units.js
new file mode 100644
index 0000000..702c1fd
--- /dev/null
+++ b/lib/units.js
@@ -0,0 +1,13 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+module.exports = {
+    MIBS_IN_GB: 1024
+};
\ No newline at end of file
diff --git a/lib/validate.js b/lib/validate.js
index 33b3b9a..268c9e2 100644
--- a/lib/validate.js
+++ b/lib/validate.js
@@ -13,6 +13,8 @@ var common = require('./common');
 var errors = require('./errors');
 var fmt = require('util').format;
 
+var mod_volumes = require('./volumes');
+
 /*
  * This determines how large the Config object passed for a log driver can be
  * before we'll reject it as too large. The size is compared after running the
@@ -25,6 +27,8 @@ var fmt = require('util').format;
  */
 var MAX_LOG_CONFIG_LEN = 1024;
 
+var VALID_VOLUME_NAME_REGEXP = /^[a-zA-Z0-9][a-zA-Z0-9_\.\-]+$/;
+var VALID_VOLUME_DRIVERS = ['tritonnfs'];
 
 // --- Internal
 
@@ -404,18 +408,64 @@ function assertString(arg, name) {
     }
 }
 
+function validateBinds(binds) {
+    assert.optionalArrayOfString(binds, 'binds');
+
+    var invalidBinds = [];
+    binds.forEach(function validateBind(bind) {
+        assert.string(bind, 'bind');
+        // Each "bind" is expected to be of the form:
+        // volumeName:/mount/point[:flags]
+        var bindComponents = bind.split(':');
+        var volumeName, mountPoint, flags;
+
+        if (bindComponents.length != 2 && bindComponents.length != 3) {
+            invalidBinds.push(bind + ': ' + bind + ' is not of the form: '
+                + 'volumeName:/mount/point[:flags]');
+            return;
+        }
 
+        volumeName = bindComponents[0];
+        mountPoint = bindComponents[1];
+
+        if (bindComponents.length === 3) {
+            flags = bindComponents[2];
+        }
+
+        if (!isValidVolumeName(volumeName)) {
+            invalidBinds.push(bind + ': ' + volumeName
+                + ' is not a valid volume name');
+        }
+
+        if (mountPoint.length === 0) {
+            invalidBinds.push(bind + ': ' + mountPoint
+                + ' must be a non-empty string');
+        }
+
+        if (flags !== undefined && flags !== 'ro' && flags !== 'rw') {
+            if (flags === '') {
+                invalidBinds.push(bind + ': an empty flag is not a valid flag');
+            } else {
+                invalidBinds.push(bind + ': "' + flags + '" is not a valid '
+                    + 'flag');
+            }
+        }
+    });
+
+    return invalidBinds;
+}
 
 // --- Exports
 
 
 
 function validateCreateContainer(req, res, next) {
+    var binds;
     var config = req.app.config;
     var container = req.body;
+    var invalidBinds;
     var volumeNames = [];
 
-
     try {
         // -- Required --
 
@@ -480,6 +530,18 @@ function validateCreateContainer(req, res, next) {
         return next(assertErr);
     }
 
+    binds = container.Binds
+        || container.HostConfig && container.HostConfig.Binds;
+
+    if (binds) {
+        invalidBinds = validateBinds(binds);
+    }
+
+    if (invalidBinds && invalidBinds.length > 0) {
+        return next(new errors.ValidationError('Invalid binds: '
+            + invalidBinds.join()));
+    }
+
     return next();
 }
 
@@ -506,7 +568,110 @@ function validateArchiveWriteStream(req, res, next) {
     next();
 }
 
+function isSupportedVolumeDriver(driver) {
+    assert.string(driver, 'driver');
+
+    if (driver !== 'tritonnfs') {
+        return false;
+    } else {
+        return true;
+    }
+}
+
+function isValidVolumeName(name) {
+    assert.string(name, 'name');
+
+    return name.match(VALID_VOLUME_NAME_REGEXP);
+}
+
+function validateCreateVolume(req, res, next) {
+    assert.object(req, 'req');
+    assert.object(res, 'res');
+    assert.func(next, 'next');
+    assert.object(req.params, 'req.params');
+
+    var validVolumeSize = false;
+    var volumeDriver = req.params.Driver;
+    var volumeName = req.params.Name;
+    var volumeSize = req.params.DriverOpts.size;
+
+    if (typeof (volumeDriver) !== 'string') {
+        next(new errors.ValidationError('volume driver must be a string'));
+        return;
+    }
+
+    if (volumeDriver !== 'local'
+        && VALID_VOLUME_DRIVERS.indexOf(volumeDriver) === -1) {
+        next(new errors.ValidationError(volumeDriver
+            + ' is not a supported volume driver. Supported volume drivers '
+            + 'are: ' + VALID_VOLUME_DRIVERS.join(', ')));
+        return;
+    }
+
+    if (typeof (volumeName) !== 'string') {
+        next(new errors.ValidationError('volume name must be a string'));
+        return;
+    }
+
+    if (volumeName !== '' && !isValidVolumeName(volumeName)) {
+        next(new errors.ValidationError(volumeName
+            + ' is not a valid volume name'));
+        return;
+    }
+
+    if (volumeSize !== undefined) {
+        if (typeof (volumeSize) === 'string') {
+            try {
+                mod_volumes.parseVolumeSize(volumeSize);
+                validVolumeSize = true;
+            } catch (parseVolumeSizeErr) {
+            }
+        }
+    } else {
+        validVolumeSize = true;
+    }
+
+    if (!validVolumeSize) {
+        next(new errors.ValidationError('Volume size: "' + volumeSize + '" is '
+            + 'not a valid volume size'));
+    }
+
+    next();
+}
+
+function validateDeleteVolume(req, res, next) {
+    assert.object(req, 'req');
+    assert.object(res, 'res');
+    assert.func(next, 'next');
+    assert.object(req.params, 'req.params');
 
+    var volumeName = req.params.name;
+
+    if (!isValidVolumeName(volumeName)) {
+        next(new errors.ValidationError(volumeName
+            + ' is not a valid volume name'));
+        return;
+    }
+
+    next();
+}
+
+function validateInspectVolume(req, res, next) {
+    assert.object(req, 'req');
+    assert.object(res, 'res');
+    assert.func(next, 'next');
+    assert.object(req.params, 'req.params');
+
+    var volumeName = req.params.name;
+
+    if (!isValidVolumeName(volumeName)) {
+        next(new errors.ValidationError(volumeName
+            + ' is not a valid volume name'));
+        return;
+    }
+
+    next();
+}
 
 module.exports = {
     assert: {
@@ -514,5 +679,8 @@ module.exports = {
     },
     createContainer: validateCreateContainer,
     archiveReadStream: validateArchiveReadStream,
-    archiveWriteStream: validateArchiveWriteStream
+    archiveWriteStream: validateArchiveWriteStream,
+    createVolume: validateCreateVolume,
+    deleteVolume: validateDeleteVolume,
+    inspectVolume: validateInspectVolume
 };
diff --git a/lib/volumes.js b/lib/volumes.js
new file mode 100644
index 0000000..07db63f
--- /dev/null
+++ b/lib/volumes.js
@@ -0,0 +1,55 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+var assert = require('assert-plus');
+var libuuid = require('libuuid');
+
+var units = require('./units');
+
+var DEFAULT_DRIVER = 'tritonnfs';
+
+function throwInvalidSize(size) {
+    assert.string(size, 'size');
+
+    throw new Error('size "' + size + '" is not a valid volume size');
+}
+
+/*
+ * Returns the number of MiBs (Mebibytes) represented by the string "size". That
+ * string must be of the form: value<unit-suffix>, where "unit-suffix" can only
+ * be "G" for "gibibytes, such as "100G" for 100 gibibytes. If "size" is not a
+ * valid size string, an error is thrown.
+ */
+function parseVolumeSize(size) {
+    assert.optionalString(size, 'size');
+
+    var baseValue;
+
+    if (size === undefined) {
+        return undefined;
+    }
+
+    var matches = size.match(/^(\d+)G$/);
+    if (!matches) {
+        throwInvalidSize(size);
+    }
+
+    baseValue = Number(matches[1]);
+    if (isNaN(baseValue)) {
+        throwInvalidSize(size);
+    }
+
+    return baseValue * units.MIBS_IN_GB;
+}
+
+module.exports = {
+    DEFAULT_DRIVER: DEFAULT_DRIVER,
+    parseVolumeSize: parseVolumeSize
+};
diff --git a/package.json b/package.json
index 9a9c30f..e808182 100644
--- a/package.json
+++ b/package.json
@@ -20,8 +20,8 @@
     "once": "1.3.1",
     "openssl-wrapper": "0.2.1",
     "restify": "4.3.0",
-    "sdc-bunyan-serializers": "git+https://github.com/joyent/sdc-bunyan-serializers.git#a3e7780",
-    "sdc-clients": "10.0.3",
+    "sdc-bunyan-serializers": "git+https://github.com/joyent/sdc-bunyan-serializers.git#a3e7780",    
+    "sdc-clients": "git+https://github.com/joyent/node-sdc-clients.git#tritonnfs",
     "sprintf": "0.1.5",
     "sshpk": "1.11.0",
     "strsplit": "1.0.0",
diff --git a/sapi_manifests/docker/template b/sapi_manifests/docker/template
index 652cf95..1fa5e28 100644
--- a/sapi_manifests/docker/template
+++ b/sapi_manifests/docker/template
@@ -57,7 +57,11 @@
         "workflows": ["pull-image-v2"],
         "url": "http://workflow.{{{datacenter_name}}}.{{{dns_domain}}}"
     },
-
+{{#experimental_docker_nfs_shared_volumes}}
+    "volapi": {
+        "url": "http://volapi.{{{datacenter_name}}}.{{{dns_domain}}}"
+    },
+{{/experimental_docker_nfs_shared_volumes}}
     "httpProxy": "{{{http_proxy}}}",
     "dockerRegistryInsecure": {{^docker_registry_insecure}}false{{/docker_registry_insecure}}{{#docker_registry_insecure}}{{{docker_registry_insecure}}}{{/docker_registry_insecure}},
 
@@ -77,7 +81,12 @@
 {{#experimental_fluentd_host}}
     "fluentd_host": "{{{experimental_fluentd_host}}}",
 {{/experimental_fluentd_host}}
-
+{{#experimental_docker_nfs_shared_volumes}}
+    "experimental_docker_nfs_shared_volumes": {{{experimental_docker_nfs_shared_volumes}}},
+{{/experimental_docker_nfs_shared_volumes}}
+{{#experimental_docker_automount_nfs_shared_volumes}}
+    "experimental_docker_automount_nfs_shared_volumes": {{{experimental_docker_automount_nfs_shared_volumes}}},
+{{/experimental_docker_automount_nfs_shared_volumes}}
 {{#FWRULE_VERSION}}
     "fwrule_version": {{{FWRULE_VERSION}}},
 {{/FWRULE_VERSION}}
diff --git a/test/integration/api-mount-nfs-shared-volume-as-docker-compose.test.js b/test/integration/api-mount-nfs-shared-volume-as-docker-compose.test.js
new file mode 100644
index 0000000..b24de95
--- /dev/null
+++ b/test/integration/api-mount-nfs-shared-volume-as-docker-compose.test.js
@@ -0,0 +1,165 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright 2017, Joyent, Inc.
+ */
+
+/*
+ * Integration test for https://smartos.org/bugview/DOCKER-911. Ideally, this
+ * should be a CLI integration test using the real docker-compose CLI program,
+ * but at the time this test was written, using docker-compose in integration
+ * tests wasn't supported (see https://smartos.org/bugview/DOCKER-916). As a
+ * result, this test was written to use Docker's engine's API the same way that
+ * the docker-compose CLI uses it when mounting NFS shared volumes. Hopefully,
+ * when support for using docker-compose in integration tests lands, this can be
+ * rewritten as a CLI integration test.
+ */
+
+
+var jsprim = require('jsprim');
+
+var common = require('../lib/common');
+var dockerTestHelpers = require('./helpers');
+var mod_testVolumes = require('../lib/volumes');
+var volumesApi = require('../lib/volumes-api');
+
+var test = mod_testVolumes.testIfEnabled;
+
+var ALICE_ACCOUNT;
+var ALICE_DOCKER_API_CLIENT;
+var VMAPI_CLIENT;
+
+var STATE = {
+    log: require('../lib/log')
+};
+
+var NFS_SHARED_VOLUME_NAMES_PREFIX =
+    mod_testVolumes.getNfsSharedVolumesNamePrefix();
+
+test('setup', function (tt) {
+
+    tt.test('docker env', function (t) {
+        dockerTestHelpers.getDockerEnv(t, STATE, {
+            account: 'sdcdockertest_alice'
+        }, function (err, env) {
+            t.ifErr(err, 'docker env: alice');
+            t.ok(env, 'have a DockerEnv for alice');
+
+            ALICE_ACCOUNT = env;
+
+            t.end();
+        });
+    });
+
+
+    tt.test('docker client init', function (t) {
+        dockerTestHelpers.createDockerRemoteClient({
+            user: ALICE_ACCOUNT
+        }, function (err, client) {
+            t.ifErr(err, 'docker client init');
+
+            ALICE_DOCKER_API_CLIENT = client;
+
+            t.end();
+        });
+    });
+
+    tt.test('vmapi client init', function (t) {
+        dockerTestHelpers.createVmapiClient(function (err, client) {
+            t.ifErr(err, 'vmapi client');
+
+            VMAPI_CLIENT = client;
+
+            t.end();
+        });
+    });
+
+});
+
+test('mounting volume as docker-compose', function (tt) {
+    var testVolumeName =
+        common.makeResourceName(NFS_SHARED_VOLUME_NAMES_PREFIX);
+    var mountingContainer;
+
+    tt.test('creating volume with name ' + testVolumeName + ' should succeed',
+        function (t) {
+            volumesApi.createDockerVolume({
+                dockerClient: ALICE_DOCKER_API_CLIENT,
+                name: testVolumeName
+            }, function volumeCreated(err, volume) {
+                t.ifErr(err, 'creating volume with name ' + testVolumeName
+                    + ' should not error');
+                t.equal(volume.Name, testVolumeName,
+                    'created volume name should be: ' + testVolumeName);
+
+                t.end();
+            });
+        }
+    );
+
+    tt.test('mounting volume ' + testVolumeName + ' with payload similar to '
+        + 'docker-compose should succeed', function (t) {
+        var volumesPayload = {};
+        var volumeMountPoint = '/data';
+
+        volumesPayload[volumeMountPoint] = {};
+
+        dockerTestHelpers.createDockerContainer({
+            imageName: 'busybox:latest',
+            dockerClient: ALICE_DOCKER_API_CLIENT,
+            vmapiClient: VMAPI_CLIENT,
+            test: t,
+            extra: {
+                Volumes: volumesPayload,
+                'HostConfig.Binds': [testVolumeName + ':' + volumeMountPoint]
+            },
+            start: true,
+            wait: true
+        }, function onContainerCreated(err, response) {
+            var expectedExitCode = 0;
+
+            t.ifErr(err,
+                'creating and starting mounting container should succeed');
+            if (!err) {
+                mountingContainer = response.inspect;
+                t.equal(mountingContainer.State.ExitCode, expectedExitCode,
+                    'exit code of mounting container should be: '
+                        + expectedExitCode);
+            }
+
+            t.end();
+        });
+    });
+
+    tt.test('deleting newly created container should succeed', function (t) {
+        if (mountingContainer !== undefined) {
+            ALICE_DOCKER_API_CLIENT.del('/containers/'
+                + mountingContainer.Id,
+                function ondel(err, res, req, body) {
+                    t.ifErr(err, 'removing container '
+                        + mountingContainer.Id + ' should not err');
+
+                    t.end();
+                });
+        } else {
+            t.ok(false, 'no mounting container to delete because it was '
+                + 'not created successfully');
+            t.end();
+        }
+    });
+
+    tt.test('deleting test volume with name ' + testVolumeName + 'should '
+        + 'succeed', function (t) {
+            ALICE_DOCKER_API_CLIENT.del('/volumes/' + testVolumeName,
+                function onVolumeDeleted(err) {
+                    t.ifErr(err, 'deleting volume with name ' + testVolumeName
+                        + ' should not error');
+
+                    t.end();
+                });
+    });
+});
diff --git a/test/integration/api-rename-used-nfs-shared-volume.test.js b/test/integration/api-rename-used-nfs-shared-volume.test.js
new file mode 100644
index 0000000..470ae31
--- /dev/null
+++ b/test/integration/api-rename-used-nfs-shared-volume.test.js
@@ -0,0 +1,229 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright 2017, Joyent, Inc.
+ */
+
+/*
+ * Integration test for https://smartos.org/bugview/VOLAPI-42.
+ */
+
+var assert = require('assert-plus');
+var vasync = require('vasync');
+
+var common = require('../lib/common');
+var dockerTestHelpers = require('./helpers');
+var mod_testVolumes = require('../lib/volumes');
+var volumesApi = require('../lib/volumes-api');
+
+var test = mod_testVolumes.testIfEnabled;
+
+var ALICE_ACCOUNT;
+var ALICE_DOCKER_API_CLIENT;
+var VMAPI_CLIENT;
+var VOLAPI_CLIENT;
+
+var STATE = {
+    log: require('../lib/log')
+};
+
+var NFS_SHARED_VOLUME_NAMES_PREFIX =
+    mod_testVolumes.getNfsSharedVolumesNamePrefix();
+
+test('setup', function (tt) {
+
+    tt.test('docker env', function (t) {
+        dockerTestHelpers.getDockerEnv(t, STATE, {
+            account: 'sdcdockertest_alice'
+        }, function (err, env) {
+            t.ifErr(err, 'docker env: alice');
+            t.ok(env, 'have a DockerEnv for alice');
+
+            ALICE_ACCOUNT = env;
+
+            t.end();
+        });
+    });
+
+    tt.test('docker client init', function (t) {
+        dockerTestHelpers.createDockerRemoteClient({
+            user: ALICE_ACCOUNT
+        }, function (err, client) {
+            t.ifErr(err, 'docker client init');
+
+            ALICE_DOCKER_API_CLIENT = client;
+
+            t.end();
+        });
+    });
+
+    tt.test('vmapi client init', function (t) {
+        dockerTestHelpers.createVmapiClient(function (err, client) {
+            t.ifErr(err, 'vmapi client');
+
+            VMAPI_CLIENT = client;
+
+            t.end();
+        });
+    });
+
+    tt.test('volapi client init', function (t) {
+        dockerTestHelpers.createVolapiClient(function (err, client) {
+            t.ifErr(err, 'volapi client');
+
+            VOLAPI_CLIENT = client;
+
+            t.end();
+        });
+    });
+
+});
+
+test('renaming mounted volume', function (tt) {
+    var testVolumeName =
+        common.makeResourceName(NFS_SHARED_VOLUME_NAMES_PREFIX);
+    var testVolume;
+    var mountingContainer;
+
+    tt.test('creating volume with name ' + testVolumeName + ' should succeed',
+        function (t) {
+            volumesApi.createDockerVolume({
+                dockerClient: ALICE_DOCKER_API_CLIENT,
+                name: testVolumeName
+            }, function volumeCreated(err, volume) {
+                t.ifErr(err, 'creating volume with name ' + testVolumeName
+                    + ' should not error');
+                t.equal(volume.Name, testVolumeName,
+                    'created volume name should be: ' + testVolumeName);
+
+                testVolume = volume;
+
+                t.end();
+            });
+        }
+    );
+
+    tt.test('mounting volume ' + testVolumeName + ' should succeed',
+        function (t) {
+        var volumeMountPoint = '/data';
+
+        dockerTestHelpers.createDockerContainer({
+            imageName: 'busybox:latest',
+            dockerClient: ALICE_DOCKER_API_CLIENT,
+            vmapiClient: VMAPI_CLIENT,
+            test: t,
+            extra: {
+                'HostConfig.Binds': [testVolumeName + ':' + volumeMountPoint]
+            },
+            start: true,
+            wait: true
+        }, function onContainerCreated(err, response) {
+            var expectedExitCode = 0;
+
+            t.ifErr(err,
+                'creating and starting mounting container should succeed');
+            if (!err) {
+                mountingContainer = response.inspect;
+                t.equal(mountingContainer.State.ExitCode, expectedExitCode,
+                    'exit code of mounting container should be: '
+                        + expectedExitCode);
+            }
+
+            t.end();
+        });
+    });
+
+    tt.test('attempting to rename mounted volume should fail', function (t) {
+        var context = {};
+        vasync.pipeline({funcs: [
+            function getVolume(ctx, next) {
+                VOLAPI_CLIENT.listVolumes({
+                    owner_uuid: ALICE_ACCOUNT.account.uuid,
+                    name: testVolumeName
+                }, function onListVolumes(listVolumeErr, volumes) {
+                    t.ok(!listVolumeErr, 'listing volumes should not error');
+                    if (volumes && Array.isArray(volumes)) {
+                        t.equal(volumes.length, 1,
+                            'only one active volume with name '
+                                + testVolumeName + ' should be listed');
+                        testVolume = volumes[0];
+                    } else {
+                        t.ok(false, 'volumes should be an array');
+                    }
+                    next(listVolumeErr);
+                });
+            },
+            function renameVolume(ctx, next) {
+                assert.object(testVolume, 'testVolume');
+
+                VOLAPI_CLIENT.updateVolume({
+                    name: testVolumeName + '-renamed',
+                    uuid: testVolume.uuid
+                }, function onVolUpdated(volUpdateErr) {
+                    var expectedErrCode = 'VolumeInUse';
+                    var expectedErrMsg = 'Volume with name ' + testVolumeName
+                        + ' is used';
+
+                    t.ok(volUpdateErr, 'renaming mounting volume should fail');
+                    t.equal(volUpdateErr.body.code, expectedErrCode,
+                        'error code should be: ' + expectedErrCode);
+                    t.equal(volUpdateErr.body.message, expectedErrMsg,
+                        'error message should be: ' + expectedErrMsg);
+
+                    next();
+                });
+            }
+        ], arg: context}, function onVolumeRenamed(err) {
+            t.end();
+        });
+    });
+
+    tt.test('deleting newly created container should succeed', function (t) {
+        if (mountingContainer !== undefined) {
+            ALICE_DOCKER_API_CLIENT.del('/containers/'
+                + mountingContainer.Id,
+                function ondel(err, res, req, body) {
+                    t.ifErr(err, 'removing container '
+                        + mountingContainer.Id + ' should not err');
+
+                    t.end();
+                });
+        } else {
+            t.ok(false, 'no mounting container to delete because it was '
+                + 'not created successfully');
+            t.end();
+        }
+    });
+
+    tt.test('attempting to rename mounted volume should succeed', function (t) {
+        testVolumeName = testVolumeName + '-renamed';
+
+        if (testVolume) {
+            VOLAPI_CLIENT.updateVolume({
+                name: testVolumeName,
+                uuid: testVolume.uuid
+            }, function onVolUpdated(volUpdateErr) {
+                t.ok(!volUpdateErr, 'renaming mounting volume should succeed');
+                t.end();
+            });
+        } else {
+            t.ok(false, 'failed to get volume object');
+            t.end();
+        }
+    });
+
+    tt.test('deleting test volume with name ' + testVolumeName + 'should '
+        + 'succeed', function (t) {
+            ALICE_DOCKER_API_CLIENT.del('/volumes/' + testVolumeName,
+                function onVolumeDeleted(err) {
+                    t.ifErr(err, 'deleting volume with name ' + testVolumeName
+                        + ' should not error');
+
+                    t.end();
+                });
+    });
+});
diff --git a/test/integration/cli-commit.test.js b/test/integration/cli-commit.test.js
index 16286e9..768bfe0 100644
--- a/test/integration/cli-commit.test.js
+++ b/test/integration/cli-commit.test.js
@@ -31,7 +31,9 @@ var STATE = {
     log: require('../lib/log')
 };
 
-var CONTAINER_PREFIX = 'sdcdockertest_commit_';
+var COMMIT_IMAGE_TAG_PREFIX = 'sdcdockertest-commit-image-tag';
+var CONTAINER_PREFIX = 'sdcdockertest-commit-container';
+
 var IMAGE_NAME = 'busybox';
 var IMAGE_PREFIX = 'sdcdockertest-commit';
 var TP = 'api: commit: ';  // Test prefix.
@@ -51,8 +53,8 @@ test(TP + 'setup', function (tt) {
 
 test(TP + 'test add file', function (tt) {
 
-    var commitImageTag = common.makeImageName(IMAGE_PREFIX);
-    var containerName = common.makeContainerName(CONTAINER_PREFIX);
+    var commitImageTag = common.makeResourceName(IMAGE_PREFIX);
+    var containerName = common.makeResourceName(CONTAINER_PREFIX);
 
     tt.test('run ' + IMAGE_NAME + ' container', function (t) {
         var runArgs = format('--name %s %s sh -c "echo hello > '
@@ -88,7 +90,7 @@ test(TP + 'test add file', function (tt) {
     // Run the committed image and verify the 'newfile.txt' contents.
     tt.test('verify created image', function (t) {
         var runArgs = format('--rm --name %s %s sh -c "cat /newfile.txt"',
-            common.makeContainerName(CONTAINER_PREFIX + 'verify_'),
+            common.makeResourceName(CONTAINER_PREFIX + 'verify_'),
             commitImageTag);
         cli.run(t, {args: runArgs}, function (err, result) {
             // err is already tested in cli.run() call
diff --git a/test/integration/cli-filters.test.js b/test/integration/cli-filters.test.js
index 665abd7..94e38e1 100644
--- a/test/integration/cli-filters.test.js
+++ b/test/integration/cli-filters.test.js
@@ -60,8 +60,8 @@ function checkContainerFiltering(tt, args, expectedNames) {
 
 
 test('container filters', function (tt) {
-    var containerName1 = common.makeContainerName(CONTAINER_PREFIX);
-    var containerName2 = common.makeContainerName(CONTAINER_PREFIX);
+    var containerName1 = common.makeResourceName(CONTAINER_PREFIX);
+    var containerName2 = common.makeResourceName(CONTAINER_PREFIX);
 
     tt.test('create container 1', function (t) {
         var runArgs = format('-d --name %s --label fishing=true %s sleep 3600',
diff --git a/test/integration/cli-labels.test.js b/test/integration/cli-labels.test.js
index 5b7670f..8f857a7 100644
--- a/test/integration/cli-labels.test.js
+++ b/test/integration/cli-labels.test.js
@@ -45,7 +45,7 @@ test('labels', function (tt) {
     tt.test('simple label', function (t) {
         var runArgs = format('-d --label foo=bar --name %s '
             + '--label "elem=something with a space" busybox sleep 3600',
-            common.makeContainerName(CONTAINER_PREFIX));
+            common.makeResourceName(CONTAINER_PREFIX));
         cli.run(t, {args: runArgs}, function (err, id) {
             t.ifErr(err, 'docker run --label foo=bar busybox');
             containerId = id;
@@ -82,7 +82,7 @@ test('labels on container', function (tt) {
 
     tt.test('container label', function (t) {
         var runArgs = format('-d --name %s --label foo=bar %s sleep 3600',
-            common.makeContainerName(CONTAINER_PREFIX), IMAGE_NAME);
+            common.makeResourceName(CONTAINER_PREFIX), IMAGE_NAME);
         cli.run(t, {args: runArgs}, function (err, id) {
             t.ifErr(err, 'docker run --label foo=bar ' + IMAGE_NAME);
             containerId = id;
@@ -128,7 +128,7 @@ test('labels conflict', function (tt) {
 
     tt.test('conflicting label', function (t) {
         var runArgs = format('-d --name %s --label todd=notcool %s sleep 3600',
-            common.makeContainerName(CONTAINER_PREFIX), IMAGE_NAME);
+            common.makeResourceName(CONTAINER_PREFIX), IMAGE_NAME);
         cli.run(t, {args: runArgs}, function (err, id) {
             t.ifErr(err, 'docker run --label todd=notcool ' + IMAGE_NAME);
             containerId = id;
diff --git a/test/integration/cli-local-volumes.test.js b/test/integration/cli-local-volumes.test.js
index 8a0b5d6..caca356 100644
--- a/test/integration/cli-local-volumes.test.js
+++ b/test/integration/cli-local-volumes.test.js
@@ -53,9 +53,9 @@ test('setup', function (tt) {
 
 test('docker local volumes', function (tt) {
     var containerWithLocalVolName =
-        common.makeContainerName('local-volume-test-container-with-local-vol');
+        common.makeResourceName('local-volume-test-container-with-local-vol');
     var mountingContainerName =
-        common.makeContainerName('local-volume-test-mounting-container');
+        common.makeResourceName('local-volume-test-mounting-container');
 
     tt.test('creating container with non-absolute mount should fail',
         function (t) {
diff --git a/test/integration/cli-mount-nfs-volume-created-with-docker-create.test.js b/test/integration/cli-mount-nfs-volume-created-with-docker-create.test.js
new file mode 100644
index 0000000..05e45c7
--- /dev/null
+++ b/test/integration/cli-mount-nfs-volume-created-with-docker-create.test.js
@@ -0,0 +1,143 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+var assert = require('assert-plus');
+var vasync = require('vasync');
+
+var cli = require('../lib/cli');
+var common = require('../lib/common');
+var testVolumes = require('../lib/volumes');
+var volumesCli = require('../lib/volumes-cli');
+
+if (!testVolumes.dockerClientSupportsVolumes(process.env.DOCKER_CLI_VERSION)) {
+    console.log('Skipping volume tests: volumes are not supported in Docker '
+        + 'versions < 1.9');
+    process.exit(0);
+}
+
+var test = testVolumes.testIfEnabled;
+
+var NFS_SHARED_VOLUME_NAMES_PREFIX =
+    testVolumes.getNfsSharedVolumesNamePrefix();
+
+var MOUNTING_CONTAINER_NAMES_PREFIX = 'test-nfs-mounting-container';
+
+var ALICE_USER;
+
+test('setup', function (tt) {
+    tt.test('DockerEnv: alice init', function (t) {
+        cli.init(t, function onCliInit(err, env) {
+            t.ifErr(err, 'Docker environment initialization should not err');
+            if (env) {
+                ALICE_USER = env.user;
+            }
+        });
+    });
+
+    // Ensure the busybox image is around.
+    tt.test('pull busybox image', function (t) {
+        cli.pull(t, {
+            image: 'busybox:latest'
+        });
+    });
+});
+
+test('docker volume created with docker create', function (tt) {
+    var containerName;
+    var volumeName;
+
+    tt.test('creating volume should succeed', function (t) {
+        volumeName =
+            common.makeResourceName(NFS_SHARED_VOLUME_NAMES_PREFIX);
+        containerName =
+            common.makeResourceName(MOUNTING_CONTAINER_NAMES_PREFIX);
+
+        vasync.pipeline({arg: {}, funcs: [
+            function createVolume(ctx, next) {
+                cli.create(t, {
+                    args: '--name ' + containerName + ' -v ' + volumeName
+                        + ':/data busybox:latest /bin/sh -c '
+                        + '"touch /data/foo.txt && ls /data"'
+                }, function onContainerRun(err, output) {
+                    t.ifErr(err,
+                        'Creating a volume via docker create -v should not '
+                            + 'error');
+                    next();
+                });
+            },
+            function checkVolumeExists(ctx, next) {
+                var volumes;
+
+                volumesCli.inspectVolume({
+                    user: ALICE_USER,
+                    args: volumeName
+                }, function onInspectedVol(inspectErr, stdout, stderr) {
+                    t.ifErr(inspectErr, 'volume inspect should not error');
+
+                    if (!inspectErr) {
+                        try {
+                            volumes = JSON.parse(stdout);
+                        } catch (volParseErr) {
+                            t.ifErr(volParseErr,
+                                'volume inspect\'s output should be valid '
+                                    + 'JSON');
+                        }
+                    }
+
+                    if (volumes) {
+                        t.equal(volumes.length, 1,
+                            'volume inspect should return only 1 volume');
+                        t.equal(volumes[0].Name, volumeName,
+                            'inspected volume\'s name should be: '
+                                + volumeName);
+                    } else {
+                        t.ok(false, 'volume inspect should return 1 volume');
+                    }
+
+                    next();
+                });
+            }
+        ]}, function onDone(err) {
+            t.end();
+        });
+    });
+
+    tt.test('starting container should mount volume succesffuly', function (t) {
+        cli.start(t, {
+                args: '-a -i ' + containerName
+            }, function onContainerStart(err, stdout, stderr) {
+                t.ifErr(err,
+                    'Mounting a valid volume via docker start should not '
+                        + 'error');
+                t.equal(stdout, 'foo.txt\n', 'Output should equal '
+                    + 'file name created in mounted volume');
+                t.end();
+            });
+    });
+
+    tt.test('deleting mounting container', function (t) {
+        cli.rm(t, {args: containerName}, function onContainerDeleted(err) {
+            t.ifErr(err, 'deleting container ' + containerName
+                + ' should not error');
+            t.end();
+        });
+    });
+
+    tt.test('deleting volume', function (t) {
+        volumesCli.rmVolume({
+            user: ALICE_USER,
+            t: t,
+            args: volumeName
+        }, function onVolumeDeleted(err, stdout, stderr) {
+            t.ifErr(err, 'deleting volume ' + volumeName + ' should not error');
+            t.end();
+        });
+    });
+});
diff --git a/test/integration/cli-nfs-shared-volumes-create-duplicate-name.test.js b/test/integration/cli-nfs-shared-volumes-create-duplicate-name.test.js
new file mode 100644
index 0000000..711495e
--- /dev/null
+++ b/test/integration/cli-nfs-shared-volumes-create-duplicate-name.test.js
@@ -0,0 +1,107 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+var assert = require('assert-plus');
+
+var cli = require('../lib/cli');
+var common = require('../lib/common');
+var log = require('../lib/log');
+var mod_testVolumes = require('../lib/volumes');
+var volumesCli = require('../lib/volumes-cli');
+
+var dockerVersion = common.parseDockerVersion(process.env.DOCKER_CLI_VERSION);
+if (dockerVersion.major < 1 || dockerVersion.minor < 9) {
+    console.log('Skipping volume tests: volumes are not supported in Docker '
+        + 'versions < 1.9');
+    process.exit(0);
+}
+
+var createTestVolume = mod_testVolumes.createTestVolume;
+var test = mod_testVolumes.testIfEnabled;
+
+var NFS_SHARED_VOLUME_NAMES_PREFIX =
+    mod_testVolumes.getNfsSharedVolumesNamePrefix();
+
+var ALICE_USER;
+
+test('setup', function (tt) {
+    tt.test('DockerEnv: alice init', function (t) {
+        cli.init(t, function onCliInit(err, env) {
+            t.ifErr(err, 'Docker environment initialization should not err');
+            if (env) {
+                ALICE_USER = env.user;
+            }
+        });
+    });
+
+    // Ensure the busybox image is around.
+    tt.test('pull busybox image', function (t) {
+        cli.pull(t, {
+            image: 'busybox:latest'
+        });
+    });
+});
+
+test('Volume creation with same name as existing volume', function (tt) {
+    var testVolumeName =
+        common.makeResourceName(NFS_SHARED_VOLUME_NAMES_PREFIX);
+
+    tt.test('creating volume with name ' + testVolumeName + ' should succeed',
+        function (t) {
+            volumesCli.createTestVolume(ALICE_USER, {
+                name: testVolumeName
+            }, function volumeCreated(err, stdout, stderr) {
+                t.ifErr(err,
+                    'volume should have been created successfully');
+                t.equal(stdout, testVolumeName + '\n',
+                    'output is newly created volume\'s name');
+
+                t.end();
+            });
+        }
+    );
+
+    tt.test('creating second volume with name ' + testVolumeName + ' should '
+        + 'fail with appropriate error message',
+        function (t) {
+            var expectedErrMsg = '(VolumeAlreadyExists) problem creating '
+                + 'volume: Volume with name ' + testVolumeName
+                + ' already exists';
+
+            volumesCli.createTestVolume(ALICE_USER, {
+                name: testVolumeName
+            }, function volumeCreated(err, stdout, stderr) {
+                t.ok(err, 'volume creation should not succeed');
+                t.ok(stderr.indexOf(expectedErrMsg) !== -1,
+                    'Error message should include: ' + expectedErrMsg);
+
+                t.end();
+            });
+        }
+    );
+
+    tt.test('removing volume with name ' + testVolumeName + ' should succeed',
+        function (t) {
+            volumesCli.rmVolume({
+                user: ALICE_USER,
+                args: testVolumeName
+            }, function onVolumeDeleted(err, stdout, stderr) {
+                    var dockerVolumeOutput = stdout;
+
+                    t.ifErr(err,
+                        'Removing an existing shared volume should not '
+                            + 'error');
+                    t.equal(dockerVolumeOutput, testVolumeName + '\n',
+                        'Output should be shared volume\'s name');
+
+                    t.end();
+                });
+        });
+});
diff --git a/test/integration/cli-nfs-shared-volumes-create-failure.test.js b/test/integration/cli-nfs-shared-volumes-create-failure.test.js
new file mode 100644
index 0000000..a3e3fb4
--- /dev/null
+++ b/test/integration/cli-nfs-shared-volumes-create-failure.test.js
@@ -0,0 +1,154 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+//
+// Summary:
+//
+// These tests were added for DOCKER-1021 and ensure that when provisioning
+// fails for a volapi storage VM, we get an error message from sdc-docker rather
+// than a message telling us the creation was successful. We "break"
+// provisioning for the 10g package by setting at trait which no CNs will have.
+// The provision will then fail at the workflow job when we're trying to
+// allocate a CN.
+//
+// In the future, when we have a better mechanism for forcing a provision to
+// fail, we should use that so we don't impact other tests that might run in
+// parallel.
+//
+
+var assert = require('assert-plus');
+var common = require('../lib/common');
+var mod_testVolumes = require('../lib/volumes');
+
+var cli = require('../lib/cli');
+var h = require('./helpers');
+var volumesCli = require('../lib/volumes-cli');
+
+assert.string(process.env.DOCKER_CLI_VERSION, 'process.env.DOCKER_CLI_VERSION');
+
+var dockerVersion = common.parseDockerVersion(process.env.DOCKER_CLI_VERSION);
+if (dockerVersion.major < 1 || dockerVersion.minor < 9) {
+    console.log('Skipping volume tests: volumes are not supported in Docker '
+        + 'versions < 1.9');
+    process.exit(0);
+}
+
+var createTestVolume = mod_testVolumes.createTestVolume;
+var test = mod_testVolumes.testIfEnabled;
+
+var NFS_SHARED_VOLUME_NAMES_PREFIX =
+    mod_testVolumes.getNfsSharedVolumesNamePrefix();
+
+var ALICE_USER;
+var PAPI;
+var PAPI_PACKAGE;
+var PAPI_ORIGINAL_TRAITS;
+
+test('setup', function (tt) {
+    tt.test('DockerEnv: alice init', function (t) {
+        cli.init(t, function onCliInit(err, env) {
+            t.ifErr(err, 'Docker environment initialization should not err');
+            if (env) {
+                ALICE_USER = env.user;
+            }
+        });
+    });
+
+    tt.test('setup PAPI client', function (t) {
+        h.createPapiClient(function (err, _papi) {
+            t.ifErr(err, 'create PAPI client');
+            PAPI = _papi;
+            t.end();
+        });
+    });
+
+    tt.test('getting 10g PAPI package', function (t) {
+        PAPI.list('(&(name=sdc_volume_nfs_10)(active=true))',
+            {},
+            function _onResults(err, pkgs, count) {
+                t.ifErr(err, 'get PAPI package');
+
+                // Ensure that if there are multiple results, the output
+                // includes the list of uuids so we can investigate, hence
+                // the pkgs.map() here when pkgs is defined.
+                t.equal(count, 1, 'should be 1 result '
+                    + JSON.stringify(pkgs ? pkgs.map(function mapUuids(pkg) {
+                        return pkg.uuid;
+                    }) : []));
+
+                if (count === 1 && pkgs && pkgs.length === 1) {
+                    PAPI_PACKAGE = pkgs[0].uuid;
+                    PAPI_ORIGINAL_TRAITS = pkgs[0].traits;
+                }
+                t.end();
+            }
+        );
+    });
+
+    tt.test('breaking provisioning w/ 10g package', function (t) {
+        PAPI.update(PAPI_PACKAGE, {traits: {broken_by_docker_tests: true}}, {},
+            function onUpdated(err) {
+                t.ifErr(err, 'update PAPI setting broken traits');
+                t.end();
+            }
+        );
+    });
+});
+
+test('Volume creation should fail when provision fails', function (tt) {
+    var testVolumeName =
+        common.makeResourceName(NFS_SHARED_VOLUME_NAMES_PREFIX);
+
+    tt.test('creating volume ' + testVolumeName + ' should fail with '
+        + 'appropriate error message',
+        function (t) {
+            volumesCli.createTestVolume(ALICE_USER, {
+                size: '10g',
+                name: testVolumeName
+            }, function volumeCreated(err, stdout, stderr) {
+                var expectedErr = 'Error response from daemon: (InternalError) '
+                    + 'volume creation failed';
+                var matches;
+
+                // Make a RegExp from the expectedErr but we need to escape the
+                // '(' and ')' characters to '\(' and '\)' so that the regex
+                // will not treat that as a grouping.
+                var re = new RegExp(expectedErr.replace(/[()]/g, '\\$&'));
+
+                matches = stderr.match(re);
+
+                t.ok(err, 'volume creation should not succeed');
+                // with this, we get the actual error message if it fails
+                t.equal((matches ? matches[0] : stderr), expectedErr,
+                    'expected InternalError');
+
+                t.end();
+            });
+        }
+    );
+});
+
+test('teardown', function (tt) {
+    tt.test('un-breaking provisioning w/ 10g package', function (t) {
+        var newTraits = {};
+
+        if (PAPI_ORIGINAL_TRAITS) {
+            newTraits = PAPI_ORIGINAL_TRAITS;
+        }
+
+        PAPI.update(PAPI_PACKAGE, {traits: newTraits}, {},
+            function onUpdated(err) {
+                t.ifErr(err, 'update PAPI setting original traits: '
+                    + JSON.stringify(newTraits));
+                t.end();
+            }
+        );
+    });
+});
diff --git a/test/integration/cli-nfs-shared-volumes-create-size.test.js b/test/integration/cli-nfs-shared-volumes-create-size.test.js
new file mode 100644
index 0000000..02a8107
--- /dev/null
+++ b/test/integration/cli-nfs-shared-volumes-create-size.test.js
@@ -0,0 +1,152 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+var assert = require('assert-plus');
+var vasync = require('vasync');
+
+var cli = require('../lib/cli');
+var common = require('../lib/common');
+var helpers = require('./helpers');
+var log = require('../lib/log');
+var mod_testVolumes = require('../lib/volumes');
+var volumesCli = require('../lib/volumes-cli');
+
+var dockerVersion = common.parseDockerVersion(process.env.DOCKER_CLI_VERSION);
+if (dockerVersion.major < 1 || dockerVersion.minor < 9) {
+    console.log('Skipping volume tests: volumes are not supported in Docker '
+        + 'versions < 1.9');
+    process.exit(0);
+}
+
+var createTestVolume = mod_testVolumes.createTestVolume;
+var test = mod_testVolumes.testIfEnabled;
+
+var ALICE_USER;
+var MIBS_IN_GIB = 1024;
+var NFS_SHARED_VOLUMES_DRIVER_NAME =
+    mod_testVolumes.getNfsSharedVolumesDriverName();
+var NFS_SHARED_VOLUME_NAMES_PREFIX =
+    mod_testVolumes.getNfsSharedVolumesNamePrefix();
+var VOLAPI_CLIENT;
+
+test('setup', function (tt) {
+    tt.test('DockerEnv: alice init', function (t) {
+        cli.init(t, function onCliInit(err, env) {
+            t.ifErr(err, 'Docker environment initialization should not err');
+            if (env) {
+                ALICE_USER = env.user;
+            }
+        });
+    });
+
+    // Ensure the busybox image is around.
+    tt.test('pull busybox image', function (t) {
+        cli.pull(t, {
+            image: 'busybox:latest'
+        });
+    });
+
+    tt.test('volapi client init', function (t) {
+        helpers.createVolapiClient(function (err, client) {
+            t.ifErr(err, 'volapi client');
+            VOLAPI_CLIENT = client;
+            t.end();
+        });
+    });
+});
+
+test('Volume creation with invalid size', function (tt) {
+    tt.test('creating volume with invalid sizes should fail', function (t) {
+        var INVALID_SIZES = [
+            'invalid-size',
+            '$%#%',
+            '',
+            '10GB',
+            '10MB',
+            '100gb',
+            '100mb'
+        ];
+
+        vasync.forEachParallel({
+            func: createVolumeWithInvalidSize,
+            inputs: INVALID_SIZES
+        }, function invalidSizesTested(err, results) {
+            t.end();
+        });
+
+        function createVolumeWithInvalidSize(invalidSize, callback) {
+            assert.string(invalidSize, 'invalidSize');
+            assert.func(callback, 'callback');
+
+            var expectedErrMsg = '(Validation) Volume size: "' + invalidSize
+                + '" is not a valid volume size';
+
+            volumesCli.createTestVolume(ALICE_USER, {
+                size: invalidSize
+            }, function volumeCreated(err, stdout, stderr) {
+                t.ok(err, 'volume creation should result in an error');
+                t.ok(stderr.indexOf(expectedErrMsg) !== -1,
+                    'Error message should include: ' + expectedErrMsg);
+
+                callback();
+            });
+        }
+    });
+});
+
+test('Volume creation with unavailable size', function (tt) {
+    tt.test('creating volume with unavailable size should fail', function (t) {
+        var largestVolSize;
+
+        vasync.pipeline({arg: {}, funcs: [
+            function getAvailableSizes(ctx, next) {
+                VOLAPI_CLIENT.listVolumeSizes(
+                    function onListVolSizes(listVolSizesErr, sizes) {
+                        t.ifErr(listVolSizesErr,
+                            'listing volume sizes should not error');
+                        if (listVolSizesErr) {
+                            next(listVolSizesErr);
+                            return;
+                        }
+
+                        t.ok(sizes,
+                            'listing volume sizes should return a non-empty '
+                                + 'response');
+                        if (sizes) {
+                            t.ok(sizes.length > 0,
+                                'listing volume sizes should return a '
+                                    + 'non-empty list of sizes');
+                        }
+
+                        largestVolSize = sizes[sizes.length - 1].size;
+
+                        next();
+                    });
+            },
+            function createVolWithUnavailableSize(ctx, next) {
+                var unavailableSize = (largestVolSize / MIBS_IN_GIB + 1) + 'G';
+
+                volumesCli.createTestVolume(ALICE_USER, {
+                    size: unavailableSize
+                }, function volumeCreated(err, stdout, stderr) {
+                    var expectedErrMsg = 'Volume size not available';
+                    t.ok(err, 'volume creation should result in an error');
+                    t.ok(stderr.indexOf(expectedErrMsg) !== -1,
+                        'Error message should include: ' + expectedErrMsg
+                            + ' and was: ' + stderr);
+
+                    next();
+                });
+            }
+        ]}, function onTestDone(err) {
+            t.end();
+        });
+    });
+});
diff --git a/test/integration/cli-nfs-shared-volumes-delete-in-use.test.js b/test/integration/cli-nfs-shared-volumes-delete-in-use.test.js
new file mode 100644
index 0000000..6ead85e
--- /dev/null
+++ b/test/integration/cli-nfs-shared-volumes-delete-in-use.test.js
@@ -0,0 +1,141 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+var assert = require('assert-plus');
+
+var cli = require('../lib/cli');
+var common = require('../lib/common');
+var log = require('../lib/log');
+var mod_testVolumes = require('../lib/volumes');
+var mod_testVolumesCli = require('../lib/volumes-cli');
+
+var dockerVersion = common.parseDockerVersion(process.env.DOCKER_CLI_VERSION);
+if (dockerVersion.major < 1 || dockerVersion.minor < 9) {
+    console.log('Skipping volume tests: volumes are not supported in Docker '
+        + 'versions < 1.9');
+    process.exit(0);
+}
+
+var createTestVolume = mod_testVolumesCli.createTestVolume;
+var test = mod_testVolumes.testIfEnabled;
+
+var NFS_SHARED_VOLUMES_DRIVER_NAME =
+    mod_testVolumes.getNfsSharedVolumesDriverName();
+var NFS_SHARED_VOLUME_NAMES_PREFIX =
+    mod_testVolumes.getNfsSharedVolumesNamePrefix();
+
+var MOUNTING_CONTAINER_NAMES_PREFIX =
+    'test-nfs-mounting-container-volume-in-use';
+
+var ALICE_USER;
+
+test('setup', function (tt) {
+    tt.test('DockerEnv: alice init', function (t) {
+        cli.init(t, function onCliInit(err, env) {
+            t.ifErr(err, 'Docker environment initialization should not err');
+            if (env) {
+                ALICE_USER = env.user;
+            }
+        });
+    });
+
+    // Ensure the busybox image is around.
+    tt.test('pull busybox image', function (t) {
+        cli.pull(t, {
+            image: 'busybox:latest'
+        });
+    });
+});
+
+test('Volume deletion when volume in use', function (tt) {
+    var testVolumeName =
+        common.makeResourceName(NFS_SHARED_VOLUME_NAMES_PREFIX);
+    var containerName =
+        common.makeResourceName(MOUNTING_CONTAINER_NAMES_PREFIX);
+
+    tt.test('creating volume with name ' + testVolumeName + ' should succeed',
+        function (t) {
+            createTestVolume(ALICE_USER, {
+                name: testVolumeName
+            }, function volumeCreated(err, stdout, stderr) {
+                t.ifErr(err,
+                    'volume should have been created successfully');
+                t.equal(stdout, testVolumeName + '\n',
+                    'output is newly created volume\'s name');
+
+                t.end();
+            });
+        }
+    );
+
+    tt.test('mounting the newly created volume from a container should succeed',
+        function (t) {
+            cli.run(t, {
+                args: '--name ' + containerName + ' -v ' + testVolumeName
+                    + ':/data busybox:latest /bin/sh -c '
+                    + '"touch /data/foo.txt && ls /data"'
+            }, function onContainerRun(err, output) {
+                t.ifErr(err, 'Mounting a valid volume should not error');
+                t.equal(output.stdout, 'foo.txt\n', 'Output should equal '
+                    + 'newly created container\'s name');
+                t.end();
+            });
+        });
+
+    tt.test('removing volume with name ' + testVolumeName + ' should fail',
+        function (t) {
+            mod_testVolumesCli.rmVolume({
+                user: ALICE_USER,
+                args: testVolumeName
+            }, function onVolumeDeleted(err, stdout, stderr) {
+                var dockerVolumeOutput = stderr;
+                var expectedErrMsg = 'problem deleting volume: Volume '
+                    + 'with name ' + testVolumeName + ' is used';
+
+                t.ok(err,
+                    'Removing an existing shared volume in use should '
+                        + 'error');
+                t.ok(dockerVolumeOutput
+                    && dockerVolumeOutput.indexOf(expectedErrMsg) !== -1,
+                    'Error message should include: ' + expectedErrMsg);
+
+                t.end();
+            });
+        });
+
+    tt.test('removing container mounting volume should succeed', function (t) {
+        cli.rm(t, {args: containerName},
+            function onContainerDeleted(err, stdout, stderr) {
+                t.ifErr(err,
+                    'deleting container mounting NFS shared volume '
+                        + 'should succeed');
+                t.end();
+            });
+    });
+
+    tt.test('after deleting mounting container, deleting volume should succeed',
+        function (t) {
+            mod_testVolumesCli.rmVolume({
+                user: ALICE_USER,
+                args: testVolumeName
+            }, function onVolumeDeleted(err, stdout, stderr) {
+                var dockerVolumeOutput = stdout;
+
+                t.ifErr(err,
+                    'Removing an existing shared volume not in use should '
+                        + 'succeed');
+                t.equal(dockerVolumeOutput, testVolumeName + '\n',
+                    'Output should be volume\'s name: ' + testVolumeName);
+
+                t.end();
+            });
+        });
+
+});
diff --git a/test/integration/cli-nfs-shared-volumes-disabled.test.js b/test/integration/cli-nfs-shared-volumes-disabled.test.js
new file mode 100644
index 0000000..7b5d513
--- /dev/null
+++ b/test/integration/cli-nfs-shared-volumes-disabled.test.js
@@ -0,0 +1,231 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+/*
+ * Tests that clients get expected messages when NFS volumes disabled.
+ */
+
+var restify = require('restify');
+var test = require('tape');
+var vasync = require('vasync');
+
+var cli = require('../lib/cli');
+var common = require('../lib/common');
+var testVolumes = require('../lib/volumes');
+var volumesCli = require('../lib/volumes-cli');
+
+if (!testVolumes.dockerClientSupportsVolumes(process.env.DOCKER_CLI_VERSION)) {
+    console.log('Skipping volume tests: volumes are not supported in Docker '
+        + 'versions < 1.9');
+    process.exit(0);
+}
+
+var errorMeansNFSSharedVolumeSupportDisabled =
+    testVolumes.errorMeansNFSSharedVolumeSupportDisabled;
+var nfsSharedVolumesSupported = testVolumes.nfsSharedVolumesSupported;
+
+var disabled_nfs_volumes = false;
+
+var ALICE_USER;
+var NFS_SHARED_VOLUME_NAMES_PREFIX =
+    testVolumes.getNfsSharedVolumesNamePrefix();
+var SAPI_CLIENT;
+var SAPI_APP;
+var STATE_RETRIES = 120;
+
+// wait for /admin/config to have state either 'enabled' or 'disabled' for
+// experimental_docker_nfs_shared_volumes
+function waitForState(tt, state, callback) {
+    var dockerAdminClient;
+    var retries = STATE_RETRIES;
+
+    tt.ok(process.env.DOCKER_ADMIN_URL, 'should have DOCKER_ADMIN_URL, got: '
+        + JSON.stringify(process.env.DOCKER_ADMIN_URL));
+
+    dockerAdminClient = restify.createJsonClient({
+        url: process.env.DOCKER_ADMIN_URL
+    });
+
+    function checkState() {
+        dockerAdminClient.get('/admin/config',
+            function onConfig(err, req, res, cfg) {
+                var enabled;
+
+                if (!err && cfg) {
+                    if (cfg.experimental_docker_nfs_shared_volumes === true) {
+                        enabled = true;
+                    } else {
+                        enabled = false;
+                    }
+                }
+
+                retries--;
+
+                if (!err && state === 'enabled' && enabled) {
+                    tt.comment('saw "enabled" after '
+                        + (STATE_RETRIES - retries) + ' seconds');
+                    callback(null, 'enabled');
+                    return;
+                } else if (!err && state === 'disabled' && !enabled) {
+                    tt.comment('saw "disabled" after '
+                        + (STATE_RETRIES - retries) + ' seconds');
+                    callback(null, 'disabled');
+                    return;
+                }
+
+                if (retries <= 0) {
+                    callback(new Error('Timed out waiting for NFS volumes to be'
+                        + ' ' + state));
+                    return;
+                }
+
+                // try again
+                setTimeout(checkState, 1000);
+            });
+    }
+
+    checkState();
+}
+
+test('setup', function (tt) {
+    tt.test('DockerEnv: alice init', function (t) {
+        cli.init(t, function onCliInit(err, env) {
+            t.ifErr(err, 'Docker environment initialization should not err');
+            if (env) {
+                ALICE_USER = env.user;
+            }
+        });
+    });
+
+    // Ensure the busybox image is around.
+    tt.test('pull busybox image', function (t) {
+        cli.pull(t, {
+            image: 'busybox:latest'
+        });
+    });
+
+    if (!nfsSharedVolumesSupported()) {
+        tt.ok(true, 'NFS volumes are already disabled, no need to do anything');
+        tt.end();
+        return;
+    }
+
+    tt.test('disable NFS volumes', function (t) {
+        t.ok(process.env.SAPI_URL, 'have SAPI_URL ('
+            + JSON.stringify(process.env.SAPI_URL) + ')');
+
+        SAPI_CLIENT = restify.createJsonClient({
+            url: process.env.SAPI_URL
+        });
+
+        SAPI_CLIENT.get('/applications?name=sdc',
+            function onApp(err, req, res, appList) {
+                var app;
+                var nfsVolumeSupportKey =
+                    'experimental_docker_nfs_shared_volumes';
+
+                t.ifErr(err, 'should succeed to GET app from SAPI');
+                t.ok(appList, 'should have an appList object');
+                t.ok(Array.isArray(appList), 'appList should be an array');
+                t.equal(appList.length, 1, 'should have one sdc app');
+
+                app = appList[0];
+                t.ok(app.uuid, 'app should have uuid, got: '
+                    + JSON.stringify(app.uuid));
+                SAPI_APP = app.uuid;
+                t.ok(app.metadata, 'app should have metadata');
+                if (app.metadata.hasOwnProperty(nfsVolumeSupportKey)) {
+                    t.comment('current value of '
+                        + 'experimental_docker_nfs_shared_volumes is: '
+                        + app.metadata.experimental_docker_nfs_shared_volumes);
+                }
+
+                SAPI_CLIENT.put('/applications/' + SAPI_APP, {
+                    action: 'update',
+                    metadata: {
+                        experimental_docker_nfs_shared_volumes: false
+                    }
+                }, function onPut(sapiPutErr, sapiPutReq, sapiPutRes, obj) {
+                    t.ifErr(sapiPutErr, 'should succeed to PUT app to SAPI');
+                    t.equal(sapiPutRes.statusCode, 200, 'expected 200');
+
+                    disabled_nfs_volumes = true;
+                    waitForState(t, 'disabled',
+                        function onDisabled(waitStateErr) {
+                            t.ifErr(waitStateErr,
+                                'expected state to be disabled');
+                            t.end();
+                        });
+                });
+            });
+        });
+});
+
+test('test with NFS volumes disabled', function (tt) {
+    var volumeName;
+
+    tt.test(' create docker volume should fail', function (t) {
+        volumeName = common.makeResourceName(NFS_SHARED_VOLUME_NAMES_PREFIX);
+
+        volumesCli.createVolume({
+            user: ALICE_USER,
+            args: '--name ' + volumeName
+        }, function onVolumeCreated(err, stdout, stderr) {
+            t.ok(errorMeansNFSSharedVolumeSupportDisabled(err, stderr),
+                'expected create to fail w/ volumes disabled');
+            t.end();
+        });
+    });
+
+    tt.test(' inspect docker volume should fail', function (t) {
+        volumesCli.inspectVolume({
+            user: ALICE_USER,
+            args: volumeName
+        }, function onInspect(err, stdout, stderr) {
+            t.ok(errorMeansNFSSharedVolumeSupportDisabled(err, stderr),
+                'expected inspect to fail w/ volumes disabled');
+            t.end();
+        });
+    });
+
+    tt.test(' listing volumes should fail', function (t) {
+        volumesCli.listVolumes({
+            user: ALICE_USER
+        }, function onVolumesListed(err, stdout, stderr) {
+            t.ok(errorMeansNFSSharedVolumeSupportDisabled(err, stderr),
+                'expected list to fail w/ volumes disabled');
+            t.end();
+        });
+    });
+});
+
+test('teardown', function (tt) {
+    if (!disabled_nfs_volumes) {
+        tt.ok(true, 'NFS volumes were already disabled, no need to re-enable');
+        tt.end();
+        return;
+    }
+
+    // re-enable NFS volumes if we disabled them
+    SAPI_CLIENT.put('/applications/' + SAPI_APP, {
+        action: 'update',
+        metadata: {
+            experimental_docker_nfs_shared_volumes: true
+        }
+    }, function onPut(sapiPutErr, req, res, obj) {
+        tt.ifErr(sapiPutErr, 'should succeed to PUT app to SAPI');
+        tt.equal(res.statusCode, 200, 'expected 200');
+
+        waitForState(tt, 'enabled', function onEnabled(waitStateErr) {
+            tt.ifErr(waitStateErr, 'expected state to be enabled');
+            tt.end();
+        });
+    });
+});
diff --git a/test/integration/cli-nfs-shared-volumes-docker-880.test.js b/test/integration/cli-nfs-shared-volumes-docker-880.test.js
new file mode 100644
index 0000000..4f0b9f0
--- /dev/null
+++ b/test/integration/cli-nfs-shared-volumes-docker-880.test.js
@@ -0,0 +1,352 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+/*
+ * Regression test for DOCKER-880: https://smartos.org/bugview/DOCKER-880.
+ *
+ * This test makes sure that when deleting a volume in state === 'ready' that
+ * has the same name as at least one volume in a state !== 'ready', the "docker
+ * volume rm" command will actually delete the volume in state === 'ready'.
+ *
+ * Put differently, if there's no exiting volume for a given sdc-docker account,
+ * the following sequence of commands:
+ *
+ * 1. docker volume create --name foo
+ * 2. docker volume rm foo
+ * 3. docker volume create --name foo
+ * 4. docker volume rm foo
+ * 5. docker volume ls
+ *
+ * will produce no output for the last command.
+ */
+
+var assert = require('assert-plus');
+var vasync = require('vasync');
+
+var cli = require('../lib/cli');
+var common = require('../lib/common');
+var log = require('../lib/log');
+var mod_testVolumes = require('../lib/volumes');
+var volumesCli = require('../lib/volumes-cli');
+
+var dockerVersion = common.parseDockerVersion(process.env.DOCKER_CLI_VERSION);
+if (dockerVersion.major < 1 || dockerVersion.minor < 9) {
+    console.log('Skipping volume tests: volumes are not supported in Docker '
+        + 'versions < 1.9');
+    process.exit(0);
+}
+
+var errorMeansNFSSharedVolumeSupportDisabled =
+    mod_testVolumes.errorMeansNFSSharedVolumeSupportDisabled;
+var test = mod_testVolumes.testIfEnabled;
+var VOLAPI_CLIENT = mod_testVolumes.getVolapiClient();
+
+var NFS_SHARED_VOLUMES_DRIVER_NAME =
+    mod_testVolumes.getNfsSharedVolumesDriverName();
+var NFS_SHARED_VOLUME_NAMES_PREFIX =
+    mod_testVolumes.getNfsSharedVolumesNamePrefix();
+
+var ALICE_USER;
+
+
+function makeKeepVolumeWithNameFn(volumeName) {
+    assert.string(volumeName, 'volumeName');
+
+    return function keepVolumeWithName(dockerVolumeLsOutputLine) {
+        assert.string(dockerVolumeLsOutputLine, 'dockerVolumeLsOutputLine');
+
+        var driverAndName = dockerVolumeLsOutputLine.trim().split(/\s+/);
+        var name = driverAndName[1];
+
+        if (name === volumeName) {
+            return true;
+        }
+
+        return false;
+    };
+}
+
+test('setup', function (tt) {
+    tt.test('DockerEnv: alice init', function (t) {
+        cli.init(t, function onCliInit(err, env) {
+            t.ifErr(err, 'Docker environment initialization should not err');
+            if (env) {
+                ALICE_USER = env.user;
+            }
+        });
+    });
+
+    // Ensure the busybox image is around.
+    tt.test('pull busybox image', function (t) {
+        cli.pull(t, {
+            image: 'busybox:latest'
+        });
+    });
+});
+
+
+test('cleanup leftover resources from previous tests run', function (tt) {
+
+    tt.test('leftover volumes should be cleaned up', function (t) {
+        volumesCli.deleteAllVolumes(ALICE_USER, function done(err, errMsg) {
+            if (mod_testVolumes.nfsSharedVolumesSupported()) {
+                t.ifErr(err, 'deleting leftover volumes should succeed');
+            } else {
+                t.ok(errorMeansNFSSharedVolumeSupportDisabled(err, errMsg));
+            }
+
+            t.end();
+        });
+    });
+});
+
+test('DOCKER-880', function (tt) {
+    var testVolumeName =
+        common.makeResourceName(NFS_SHARED_VOLUME_NAMES_PREFIX);
+    var filterTestVolumesFn = makeKeepVolumeWithNameFn(testVolumeName);
+    var firstVolumeUuid;
+
+    tt.test('creating volume with name ' + testVolumeName + ' should succeed',
+        function (t) {
+            volumesCli.createTestVolume(ALICE_USER, {
+                name: testVolumeName
+            }, function volumeCreated(err, stdout, stderr) {
+                if (mod_testVolumes.nfsSharedVolumesSupported()) {
+                    t.ifErr(err,
+                        'volume should have been created successfully');
+                    t.equal(stdout, testVolumeName + '\n',
+                        'output is newly created volume\'s name');
+                } else {
+                    t.notEqual(stderr.indexOf('Volumes are not supported'),
+                        -1);
+                }
+
+                t.end();
+            });
+        }
+    );
+
+    tt.test('listing volumes should output one volume with name: '
+        + testVolumeName, function (t) {
+            volumesCli.listVolumes({
+                user: ALICE_USER
+            }, function onVolumesListed(err, stdout, stderr) {
+                var outputLines;
+                var testVolumes;
+
+                if (mod_testVolumes.nfsSharedVolumesSupported()) {
+                    t.ifErr(err, 'listing volumes should not error');
+                    outputLines = stdout.trim().split(/\n/);
+                    // Remove header from docker volume ls' output.
+                    outputLines = outputLines.slice(1);
+
+                    testVolumes = outputLines.filter(filterTestVolumesFn);
+
+                    t.equal(testVolumes.length, 1, 'only one volume with name '
+                        + testVolumeName + ' should be listed');
+                } else {
+                    t.ok(errorMeansNFSSharedVolumeSupportDisabled(err, stderr));
+                }
+
+                t.end();
+            });
+        });
+
+    if (mod_testVolumes.nfsSharedVolumesSupported() && VOLAPI_CLIENT) {
+        tt.test('getting first created volume\'s UUID should succeed',
+            function (t) {
+                VOLAPI_CLIENT.listVolumes({
+                    name: testVolumeName,
+                    predicate: JSON.stringify({
+                        eq: ['state', 'ready']
+                    })
+                }, function volumesListed(err, volumes) {
+                    t.ifErr(err, 'list volumes should not error');
+                    t.equal(volumes.length, 1, 'only one volume with name '
+                        + testVolumeName + ' should be in state \'ready\'');
+                    firstVolumeUuid = volumes[0].uuid;
+
+                    t.end();
+                });
+            });
+    }
+
+    tt.test('removing volume with name ' + testVolumeName + ' should succeed',
+        function (t) {
+            volumesCli.rmVolume({
+                user: ALICE_USER,
+                args: testVolumeName
+            }, function onVolumeDeleted(err, stdout, stderr) {
+                var dockerVolumeOutput;
+                if (mod_testVolumes.nfsSharedVolumesSupported()) {
+                    dockerVolumeOutput = stdout;
+
+                    t.ifErr(err,
+                        'Removing an existing shared volume should not '
+                            + 'error');
+                    t.equal(dockerVolumeOutput, testVolumeName + '\n',
+                        'Output should be shared volume\'s name');
+                } else {
+                    t.ok(errorMeansNFSSharedVolumeSupportDisabled(err,
+                        stderr));
+                }
+
+                t.end();
+            });
+        });
+
+    tt.test('listing volumes should output no volume with name: '
+        + testVolumeName, function (t) {
+            volumesCli.listVolumes({
+                user: ALICE_USER
+            }, function onVolumesListed(err, stdout, stderr) {
+                var outputLines;
+                var testVolumes;
+
+                if (mod_testVolumes.nfsSharedVolumesSupported()) {
+                    t.ifErr(err, 'listing volumes should not error');
+                    outputLines = stdout.trim().split(/\n/);
+                    // Remove header from docker volume ls' output.
+                    outputLines = outputLines.slice(1);
+
+                    testVolumes = outputLines.filter(filterTestVolumesFn);
+
+                    t.equal(testVolumes.length, 0, 'no volume with name '
+                        + testVolumeName + ' should be listed');
+                } else {
+                    t.ok(errorMeansNFSSharedVolumeSupportDisabled(err, stderr));
+                }
+
+                t.end();
+            });
+        });
+
+    tt.test('creating second volume with name ' + testVolumeName + ' should '
+        + 'succeed', function (t) {
+            volumesCli.createTestVolume(ALICE_USER, {
+                name: testVolumeName
+            }, function volumeCreated(err, stdout, stderr) {
+                if (mod_testVolumes.nfsSharedVolumesSupported()) {
+                    t.ifErr(err,
+                        'volume should have been created successfully');
+                    t.equal(stdout, testVolumeName + '\n',
+                        'output is newly created volume\'s name');
+                } else {
+                    t.ok(errorMeansNFSSharedVolumeSupportDisabled(err, stderr));
+                }
+
+                t.end();
+            });
+        }
+    );
+
+    if (mod_testVolumes.nfsSharedVolumesSupported() && VOLAPI_CLIENT) {
+        tt.test('getting second created volume\'s UUID should succeed',
+            function (t) {
+                VOLAPI_CLIENT.listVolumes({
+                    name: testVolumeName,
+                    predicate: JSON.stringify({
+                        eq: ['state', 'ready']
+                    })
+                }, function volumesListed(err, volumes) {
+                    var volumeUuid;
+
+                    t.ifErr(err, 'list volumes should not error');
+                    t.equal(volumes.length, 1, 'only one volume with name '
+                        + testVolumeName + ' should be in state \'ready\'');
+
+                    volumeUuid = volumes[0].uuid;
+                    t.notEqual(volumeUuid, firstVolumeUuid,
+                        'UUID of volume with name ' + testVolumeName
+                            + ' should be different than the first created '
+                            + 'volume ('+ firstVolumeUuid + ')');
+                    t.end();
+                });
+            });
+    }
+
+    tt.test('listing volumes with name ' + testVolumeName + ' after second '
+        + 'volume created should output only one volume', function (t) {
+            volumesCli.listVolumes({
+                user: ALICE_USER
+            }, function onVolumesListed(err, stdout, stderr) {
+                var outputLines;
+                var testVolumes;
+
+                if (mod_testVolumes.nfsSharedVolumesSupported()) {
+                    t.ifErr(err, 'listing volumes should not error');
+                    outputLines = stdout.trim().split(/\n/);
+                    // Remove header from docker volume ls' output.
+                    outputLines = outputLines.slice(1);
+
+                    testVolumes = outputLines.filter(filterTestVolumesFn);
+
+                    t.equal(testVolumes.length, 1, 'only one volume with name '
+                        + testVolumeName + ' should be listed');
+                } else {
+                    t.ok(errorMeansNFSSharedVolumeSupportDisabled(err, stderr));
+                }
+
+                t.end();
+            });
+        });
+
+    tt.test('removing second volume with name ' + testVolumeName + ' should '
+        + 'succeed', function (t) {
+            volumesCli.rmVolume({
+                user: ALICE_USER,
+                args: testVolumeName
+            },
+                function onVolumeDeleted(err, stdout, stderr) {
+                    var dockerVolumeOutput;
+
+                    if (mod_testVolumes.nfsSharedVolumesSupported()) {
+                        dockerVolumeOutput = stdout;
+
+                        t.ifErr(err,
+                            'Removing an existing shared volume should not '
+                                + 'error');
+                        t.equal(dockerVolumeOutput, testVolumeName + '\n',
+                            'Output should be shared volume\'s name');
+                    } else {
+                        t.ok(errorMeansNFSSharedVolumeSupportDisabled(err,
+                            stderr));
+                    }
+
+                    t.end();
+                });
+        });
+
+    tt.test('listing volumes should output no volume with name after second '
+        + 'volume with name ' + testVolumeName + ' is deleted: ', function (t) {
+            volumesCli.listVolumes({
+                user: ALICE_USER
+            }, function onVolumesListed(err, stdout, stderr) {
+                var outputLines;
+                var testVolumes;
+
+                if (mod_testVolumes.nfsSharedVolumesSupported()) {
+                    t.ifErr(err, 'listing volumes should not error');
+                    outputLines = stdout.trim().split(/\n/);
+                    // Remove header from docker volume ls' output.
+                    outputLines = outputLines.slice(1);
+
+                    testVolumes = outputLines.filter(filterTestVolumesFn);
+
+                    t.equal(testVolumes.length, 0, 'no volume with name '
+                        + testVolumeName + ' should be listed');
+                } else {
+                    t.ok(errorMeansNFSSharedVolumeSupportDisabled(err, stderr));
+                }
+
+                t.end();
+            });
+        });
+});
diff --git a/test/integration/cli-nfs-shared-volumes-filter-dangling.test.js b/test/integration/cli-nfs-shared-volumes-filter-dangling.test.js
new file mode 100644
index 0000000..7e3476e
--- /dev/null
+++ b/test/integration/cli-nfs-shared-volumes-filter-dangling.test.js
@@ -0,0 +1,204 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+var assert = require('assert-plus');
+
+var cli = require('../lib/cli');
+var common = require('../lib/common');
+var log = require('../lib/log');
+var mod_testVolumes = require('../lib/volumes');
+var mod_testVolumesCli = require('../lib/volumes-cli');
+
+var dockerVersion = common.parseDockerVersion(process.env.DOCKER_CLI_VERSION);
+if (dockerVersion.major < 1 || dockerVersion.minor < 9) {
+    console.log('Skipping volume tests: volumes are not supported in Docker '
+        + 'versions < 1.9');
+    process.exit(0);
+}
+
+var createTestVolume = mod_testVolumesCli.createTestVolume;
+var test = mod_testVolumes.testIfEnabled;
+
+var NFS_SHARED_VOLUMES_DRIVER_NAME =
+    mod_testVolumes.getNfsSharedVolumesDriverName();
+var NFS_SHARED_VOLUME_NAMES_PREFIX =
+    mod_testVolumes.getNfsSharedVolumesNamePrefix();
+
+var MOUNTING_CONTAINER_NAMES_PREFIX =
+    'test-nfs-mounting-container-volume-in-use';
+
+var ALICE_USER;
+
+test('setup', function (tt) {
+    tt.test('DockerEnv: alice init', function (t) {
+        cli.init(t, function onCliInit(err, env) {
+            t.ifErr(err, 'Docker environment initialization should not err');
+            if (env) {
+                ALICE_USER = env.user;
+            }
+        });
+    });
+
+    // Ensure the busybox image is around.
+    tt.test('pull busybox image', function (t) {
+        cli.pull(t, {
+            image: 'busybox:latest'
+        });
+    });
+});
+
+test('Volume ls using dangling filter', function (tt) {
+    var testVolumeName =
+        common.makeResourceName(NFS_SHARED_VOLUME_NAMES_PREFIX);
+    var containerName =
+        common.makeResourceName(MOUNTING_CONTAINER_NAMES_PREFIX);
+
+    tt.test('creating volume with name ' + testVolumeName + ' should succeed',
+        function (t) {
+            createTestVolume(ALICE_USER, {
+                name: testVolumeName
+            }, function volumeCreated(err, stdout, stderr) {
+                t.ifErr(err,
+                    'volume should have been created successfully');
+                t.equal(stdout, testVolumeName + '\n',
+                    'output is newly created volume\'s name');
+
+                t.end();
+            });
+        }
+    );
+
+    tt.test('mounting the newly created volume from a container should succeed',
+        function (t) {
+            cli.run(t, {
+                args: '--name ' + containerName + ' -v ' + testVolumeName
+                    + ':/data busybox:latest /bin/sh -c '
+                    + '"touch /data/foo.txt && ls /data"'
+            }, function onContainerRun(err, output) {
+                t.ifErr(err, 'Mounting a valid volume should not error');
+                t.equal(output.stdout, 'foo.txt\n', 'Output should equal '
+                    + 'newly created container\'s name');
+                t.end();
+            });
+        });
+
+    tt.test('listing volumes using --filter dangling=true should NOT include '
+        + 'the volume', function (t) {
+        mod_testVolumesCli.listVolumes({
+                user: ALICE_USER,
+                args: '--filter dangling=true'
+            }, function onVolumesListed(volListErr, stdout, stderr) {
+                t.equal(stdout.indexOf(testVolumeName), -1,
+                    'newly created and mounted test volume ' + testVolumeName
+                        + ' should not be included in docker volume ls '
+                        + '--filter dangling=true output, got: ' + stdout);
+                t.end();
+            });
+    });
+
+    tt.test('listing volumes using --filter dangling=false should include the '
+        + 'volume', function (t) {
+        mod_testVolumesCli.listVolumes({
+                user: ALICE_USER,
+                args: '--filter dangling=false'
+            }, function onVolumesListed(volListErr, stdout, stderr) {
+                t.notEqual(stdout.indexOf(testVolumeName), -1,
+                    'newly created and mounted test volume ' + testVolumeName
+                        + ' should be included in docker volume ls --filter '
+                        + 'dangling=false output, got: ' + stdout);
+                t.end();
+            });
+    });
+
+    tt.test('listing volumes using no filter should include the volume',
+        function (t) {
+        mod_testVolumesCli.listVolumes({
+                user: ALICE_USER
+            }, function onVolumesListed(volListErr, stdout, stderr) {
+                t.notEqual(stdout.indexOf(testVolumeName), -1,
+                    'newly created and mounted test volume ' + testVolumeName
+                        + ' should be included in docker volume ls output, '
+                        + 'got: ' + stdout);
+                t.end();
+            });
+    });
+
+    tt.test('removing container mounting volume should succeed', function (t) {
+        cli.rm(t, {args: containerName},
+            function onContainerDeleted(err, stdout, stderr) {
+                t.ifErr(err,
+                    'deleting container mounting NFS shared volume '
+                        + 'should succeed');
+                t.end();
+            });
+    });
+
+
+    tt.test('listing volumes using --filter dangling=true should include the '
+        + 'volume', function (t) {
+        mod_testVolumesCli.listVolumes({
+                user: ALICE_USER,
+                args: '--filter dangling=true'
+            }, function onVolumesListed(volListErr, stdout, stderr) {
+                t.notEqual(stdout.indexOf(testVolumeName), -1,
+                    'newly created and mounted test volume ' + testVolumeName
+                        + ' should be included in docker volume ls '
+                        + '--filter dangling=true output after its last '
+                        + 'mounting container was removed, got: ' + stdout);
+                t.end();
+            });
+    });
+
+    tt.test('listing volumes using --filter dangling=false should not include '
+        + 'the volume', function (t) {
+        mod_testVolumesCli.listVolumes({
+                user: ALICE_USER,
+                args: '--filter dangling=false'
+            }, function onVolumesListed(volListErr, stdout, stderr) {
+                t.equal(stdout.indexOf(testVolumeName), -1,
+                    'newly created and mounted test volume ' + testVolumeName
+                        + ' should NOT be included in docker volume ls '
+                        + '--filter  dangling=false output after its last '
+                        + 'mounting container was removed, got: ' + stdout);
+                t.end();
+            });
+    });
+
+    tt.test('listing volumes using no filter should include the volume',
+        function (t) {
+        mod_testVolumesCli.listVolumes({
+                user: ALICE_USER
+            }, function onVolumesListed(volListErr, stdout, stderr) {
+                t.notEqual(stdout.indexOf(testVolumeName), -1,
+                    'newly created and mounted test volume ' + testVolumeName
+                        + ' should be included in docker volume ls output, '
+                        + 'got: ' + stdout);
+                t.end();
+            });
+    });
+
+    tt.test('after deleting mounting container, deleting volume should succeed',
+        function (t) {
+            mod_testVolumesCli.rmVolume({
+                user: ALICE_USER,
+                args: testVolumeName
+            }, function onVolumeDeleted(err, stdout, stderr) {
+                var dockerVolumeOutput = stdout;
+
+                t.ifErr(err,
+                    'Removing an existing shared volume not in use should '
+                        + 'succeed');
+                t.equal(dockerVolumeOutput, testVolumeName + '\n',
+                    'Output should be volume\'s name: ' + testVolumeName);
+
+                t.end();
+            });
+        });
+});
diff --git a/test/integration/cli-nfs-shared-volumes-mount-overrides-local-volume.test.js b/test/integration/cli-nfs-shared-volumes-mount-overrides-local-volume.test.js
new file mode 100644
index 0000000..5bd7734
--- /dev/null
+++ b/test/integration/cli-nfs-shared-volumes-mount-overrides-local-volume.test.js
@@ -0,0 +1,131 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+var assert = require('assert-plus');
+var vasync = require('vasync');
+
+var cli = require('../lib/cli');
+var common = require('../lib/common');
+var testVolumes = require('../lib/volumes');
+var volumesCli = require('../lib/volumes-cli');
+
+if (!testVolumes.dockerClientSupportsVolumes(process.env.DOCKER_CLI_VERSION)) {
+    console.log('Skipping volume tests: volumes are not supported in Docker '
+        + 'versions < 1.9');
+    process.exit(0);
+}
+
+var test = testVolumes.testIfEnabled;
+
+var NFS_SHARED_VOLUME_NAMES_PREFIX =
+    testVolumes.getNfsSharedVolumesNamePrefix();
+var NFS_SHARED_VOLUMES_DRIVER_NAME =
+    testVolumes.getNfsSharedVolumesDriverName();
+
+var MOUNTING_CONTAINER_NAMES_PREFIX = 'test-nfs-mounting-container';
+var IMAGE_WITH_LOCAL_VOLUME_NAME = 'joyentunsupported/image-with-local-volume';
+
+var ALICE_USER;
+
+test('setup', function (tt) {
+    tt.test('DockerEnv: alice init', function (t) {
+        cli.init(t, function onCliInit(err, env) {
+            t.ifErr(err, 'Docker environment initialization should not err');
+            if (env) {
+                ALICE_USER = env.user;
+            }
+        });
+    });
+
+    // Ensure the busybox image is around.
+    tt.test('pull ' + IMAGE_WITH_LOCAL_VOLUME_NAME + ' image', function (t) {
+        cli.pull(t, {
+            image: IMAGE_WITH_LOCAL_VOLUME_NAME
+        });
+    });
+});
+
+test('cleanup leftover resources from previous tests run', function (tt) {
+    tt.test('deleting all volumes for test user', function (t) {
+        volumesCli.deleteAllVolumes(ALICE_USER,
+            function allVolumesDeleted(err) {
+                t.ifErr(err, 'deleting all volumes should not error');
+
+                t.end();
+            });
+    });
+});
+
+test('mounting NFS shared volume at same mountpoint as local volume should '
+    + 'override it', function (tt) {
+    var containerName;
+    var volumeName;
+
+    tt.test('creating a NFS shared volume should succeed', function (t) {
+            volumeName =
+                common.makeResourceName(NFS_SHARED_VOLUME_NAMES_PREFIX);
+            volumesCli.createVolume({
+                user: ALICE_USER,
+                args: '--name ' + volumeName + ' --driver '
+                    + NFS_SHARED_VOLUMES_DRIVER_NAME
+            }, function onVolumeCreated(err, stdout, stderr) {
+                t.ifErr(err,
+                    'volume should have been created successfully');
+                t.equal(stdout, volumeName + '\n',
+                    'output should be newly created volume\'s name');
+
+                t.end();
+            });
+    });
+
+    tt.test('mounting a NFS shared volume from a container should succeed',
+        function (t) {
+            containerName =
+                common.makeResourceName(MOUNTING_CONTAINER_NAMES_PREFIX);
+            cli.run(t, {
+                args: '--name ' + containerName + ' -v ' + volumeName
+                    + ':/data/some-volume ' + IMAGE_WITH_LOCAL_VOLUME_NAME
+                    + ' /bin/sh -c '
+                    + '"touch /data/some-volume/foo.txt && ls '
+                    + '/data/some-volume"'
+            }, function onContainerRun(err, output) {
+                t.ifErr(err, 'Mounting a valid volume should not error');
+                t.equal(output.stdout, 'foo.txt\n', 'Output should equal '
+                    + 'file name from volume, not from image');
+                t.end();
+            });
+        });
+
+        tt.test('deleting first mounting container should succeed',
+            function (t) {
+                cli.rm(t, {args: containerName},
+                function onContainerDeleted(err, stdout, stderr) {
+                    t.ifErr(err,
+                        'deleting container mounting NFS shared volume '
+                            + 'should succeed');
+                    t.end();
+                });
+        });
+
+        tt.test('deleting shared volume should succeed', function (t) {
+            volumesCli.rmVolume({
+                user: ALICE_USER,
+                args: volumeName
+            }, function onVolumeDeleted(err, stdout, stderr) {
+                var dockerVolumeOutput = stdout;
+
+                t.ifErr(err,
+                    'Removing an existing shared volume should not error');
+                t.equal(dockerVolumeOutput, volumeName + '\n',
+                    'Output should be shared volume\'s name');
+                t.end();
+            });
+        });
+});
diff --git a/test/integration/cli-nfs-shared-volumes-mounting-mode.test.js b/test/integration/cli-nfs-shared-volumes-mounting-mode.test.js
new file mode 100644
index 0000000..d8a706e
--- /dev/null
+++ b/test/integration/cli-nfs-shared-volumes-mounting-mode.test.js
@@ -0,0 +1,281 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+/*
+ * This test makes sure that all volume mounting modes supported by docker's
+ * engine' are supported by Triton, and that a volume mounted with each valid
+ * mode behaves as expected (e.g, that a volume mounted read-only cannot be
+ * written to by its mounting container). Supported mounting modes are:
+ *
+ * - "ro", for read-only. - "rw", for read-write.
+ *
+ * The default mounting mode is equivalent to "rw".
+ */
+
+var assert = require('assert-plus');
+
+var cli = require('../lib/cli');
+var common = require('../lib/common');
+var log = require('../lib/log');
+var mod_testVolumes = require('../lib/volumes');
+var volumesCli = require('../lib/volumes-cli');
+
+var dockerVersion = common.parseDockerVersion(process.env.DOCKER_CLI_VERSION);
+if (dockerVersion.major < 1 || dockerVersion.minor < 9) {
+    console.log('Skipping volume tests: volumes are not supported in Docker '
+        + 'versions < 1.9');
+    process.exit(0);
+}
+
+var createTestVolume = mod_testVolumes.createTestVolume;
+var test = mod_testVolumes.testIfEnabled;
+var VOLAPI_CLIENT = mod_testVolumes.getVolapiClient();
+
+var NFS_SHARED_VOLUMES_DRIVER_NAME =
+    mod_testVolumes.getNfsSharedVolumesDriverName();
+var NFS_SHARED_VOLUME_NAMES_PREFIX =
+    mod_testVolumes.getNfsSharedVolumesNamePrefix();
+
+var MOUNT_MODE_SERVER_SIDE_VALIDATION = true;
+if (dockerVersion.major === 1 && dockerVersion.minor <= 9) {
+    // With docker version 1.9.x and older, validation for mounting modes (or
+    // flags) such as 'ro', etc. is done client side, so there's no need to run
+    // some tests that exercise the mode validation code in that case.
+    MOUNT_MODE_SERVER_SIDE_VALIDATION = false;
+}
+
+var MOUNTING_CONTAINER_NAMES_PREFIX = 'test-nfs-mounting-modes-container';
+
+var ALICE_USER;
+
+test('setup', function (tt) {
+    tt.test('DockerEnv: alice init', function (t) {
+        cli.init(t, function onCliInit(err, env) {
+            t.ifErr(err, 'Docker environment initialization should not err');
+            if (env) {
+                ALICE_USER = env.user;
+            }
+        });
+    });
+
+    // Ensure the busybox image is around.
+    tt.test('pull busybox image', function (t) {
+        cli.pull(t, {
+            image: 'busybox:latest'
+        });
+    });
+});
+
+test('docker volumes mounting modes', function (tt) {
+    var testVolumeName =
+        common.makeResourceName(NFS_SHARED_VOLUME_NAMES_PREFIX);
+    var readOnlyThatWritesContainerName, readOnlyThatReadsContainerName;
+    var defaultModeContainerName;
+    var readWriteModeContainerName;
+
+    tt.test('creating volume with name ' + testVolumeName + ' should succeed',
+        function (t) {
+            volumesCli.createTestVolume(ALICE_USER, {
+                name: testVolumeName
+            }, function volumeCreated(err, stdout, stderr) {
+                if (mod_testVolumes.nfsSharedVolumesSupported()) {
+                    t.ifErr(err,
+                        'volume should have been created successfully');
+                    t.equal(stdout, testVolumeName + '\n',
+                        'output is newly created volume\'s name');
+                } else {
+                    t.notEqual(stderr.indexOf('Volumes are not supported'),
+                        -1);
+                }
+
+                t.end();
+            });
+        }
+    );
+
+    /* Testing invalid modes */
+    if (MOUNT_MODE_SERVER_SIDE_VALIDATION) {
+        tt.test('mounting volume with empty mode should fail', function (t) {
+            var invalidModeContainerName =
+                common.makeResourceName(MOUNTING_CONTAINER_NAMES_PREFIX);
+            cli.run(t, {
+                args: '--name ' + invalidModeContainerName + ' -v '
+                    + testVolumeName + ':/data: busybox:latest /bin/sh '
+                    + '-c "echo empty-mode > /data/foo.txt && cat '
+                    + '/data/foo.txt"',
+                expectedErr: new RegExp(testVolumeName + ':/data:: an empty '
+                    + 'flag is not a valid flag')
+        }, function onContainerRun(err, output) {
+                t.end();
+            });
+        });
+
+        tt.test('mounting volume with invalid mode should fail', function (t) {
+            var invalidModeContainerName =
+                common.makeResourceName(MOUNTING_CONTAINER_NAMES_PREFIX);
+            cli.run(t, {
+                args: '--name ' + invalidModeContainerName + ' -v '
+                    + testVolumeName + ':/data:invalid-mode busybox:latest '
+                    + '/bin/sh -c "echo invalid-mode > /data/foo.txt && cat '
+                    + '/data/foo.txt"',
+                expectedErr: new RegExp(testVolumeName + ':/data:invalid-mode: '
+                    + '"invalid-mode" is not a valid flag')
+        }, function onContainerRun(err, output) {
+                t.end();
+            });
+        });
+    }
+
+    /*
+     * Testing default mode.
+     */
+    tt.test('writing to volume with default mode should succeed', function (t) {
+        defaultModeContainerName =
+            common.makeResourceName(MOUNTING_CONTAINER_NAMES_PREFIX);
+        cli.run(t, {
+            args: '--name ' + defaultModeContainerName + ' -v '
+                + testVolumeName + ':/data busybox:latest /bin/sh -c "echo '
+            + 'default-mode > /data/foo.txt && cat /data/foo.txt"'
+        }, function onContainerRun(err, output) {
+            var expectedOutput = 'default-mode\n';
+
+            t.ifErr(err,
+                'creating file in default mode mount should not error');
+            t.equal(output.stdout, expectedOutput,
+                'Output should be ' + expectedOutput);
+            t.end();
+        });
+    });
+
+    /*
+     * Testing "rw" mode.
+     */
+    tt.test('writing to volume with "rw" mode should succeed', function (t) {
+        readWriteModeContainerName =
+            common.makeResourceName(MOUNTING_CONTAINER_NAMES_PREFIX);
+        cli.run(t, {
+            args: '--name ' + readWriteModeContainerName + ' -v '
+                + testVolumeName + ':/data:rw busybox:latest /bin/sh -c "echo '
+                + 'rw-mode > /data/foo.txt && cat /data/foo.txt"'
+    }, function onContainerRun(err, output) {
+            var expectedOutput = 'rw-mode\n';
+
+            t.ifErr(err,
+                'creating file in "rw" mode mount should not error');
+            t.equal(output.stdout, expectedOutput,
+                'Output should be ' + expectedOutput);
+            t.end();
+        });
+    });
+
+    /*
+     * Testing "ro" mode.
+     */
+    tt.test('writing to volume with "ro" mode should fail', function (t) {
+        var expectedErrMsg =
+                '/bin/sh: can\'t create /data/foo.txt: Read-only file system\n';
+
+        readOnlyThatWritesContainerName =
+            common.makeResourceName(MOUNTING_CONTAINER_NAMES_PREFIX);
+
+        cli.run(t, {
+            args: '--name ' + readOnlyThatWritesContainerName + ' -v '
+                + testVolumeName + ':/data:ro busybox:latest /bin/sh -c "echo '
+                + 'ro-mode > /data/foo.txt && cat /data/foo.txt"',
+            expectRuntimeError: true
+        }, function onContainerRun(err, output) {
+            t.equal(output.stderr, expectedErrMsg, 'Error message should be: '
+                + expectedErrMsg);
+            t.end();
+        });
+    });
+
+    tt.test('reading from volume with "ro" mode should succeed', function (t) {
+        // We expect the output from the last write operaiton that succeeded.
+        var expectedOutput = 'rw-mode\n';
+
+        readOnlyThatReadsContainerName =
+            common.makeResourceName(MOUNTING_CONTAINER_NAMES_PREFIX);
+
+        cli.run(t, {
+            args: '--name ' + readOnlyThatReadsContainerName + ' -v '
+                + testVolumeName + ':/data:ro busybox:latest /bin/sh -c "cat '
+                + '/data/foo.txt"'
+        }, function onContainerRun(err, output) {
+            t.equal(output.stdout, expectedOutput, 'Output should be: '
+                + expectedOutput + ' but was: ' + output.stdout);
+            t.end();
+        });
+    });
+
+    tt.test('deleting container with name ' + defaultModeContainerName
+        + ' should succeed', function (t) {
+        cli.rm(t, {args: defaultModeContainerName},
+            function onContainerDeleted(err, stdout, stderr) {
+                t.ifErr(err,
+                    'deleting container mounting NFS shared volume '
+                        + 'should succeed');
+                t.end();
+            });
+    });
+
+    /* Cleaning up resources created for this tests suite */
+
+    tt.test('deleting container with name ' + readWriteModeContainerName
+        + ' should succeed', function (t) {
+        cli.rm(t, {args: readWriteModeContainerName},
+            function onContainerDeleted(err, stdout, stderr) {
+                t.ifErr(err,
+                    'deleting container mounting NFS shared volume '
+                        + 'should succeed');
+                t.end();
+            });
+    });
+
+    tt.test('deleting container with name ' + readOnlyThatWritesContainerName
+        + ' should succeed', function (t) {
+        cli.rm(t, {args: '-f ' + readOnlyThatWritesContainerName},
+            function onContainerDeleted(err, stdout, stderr) {
+                t.ifErr(err,
+                    'deleting container mounting NFS shared volume '
+                        + 'should succeed');
+                t.end();
+            });
+    });
+
+    tt.test('deleting container with name ' + readOnlyThatReadsContainerName
+        + ' should succeed', function (t) {
+        cli.rm(t, {args: '-f ' + readOnlyThatReadsContainerName},
+            function onContainerDeleted(err, stdout, stderr) {
+                t.ifErr(err,
+                    'deleting container mounting NFS shared volume '
+                        + 'should succeed');
+                t.end();
+            });
+    });
+
+    tt.test('removing volume with name ' + testVolumeName + ' should succeed',
+        function (t) {
+            volumesCli.rmVolume({
+                user: ALICE_USER,
+                args: testVolumeName
+            }, function onVolumeDeleted(err, stdout, stderr) {
+                    var dockerVolumeOutput = stdout;
+
+                    t.ifErr(err,
+                        'Removing an existing shared volume should not '
+                            + 'error');
+                    t.equal(dockerVolumeOutput, testVolumeName + '\n',
+                        'Output should be shared volume\'s name');
+
+                    t.end();
+                });
+        });
+});
diff --git a/test/integration/cli-nfs-shared-volumes-networks.test.js b/test/integration/cli-nfs-shared-volumes-networks.test.js
new file mode 100644
index 0000000..0617e00
--- /dev/null
+++ b/test/integration/cli-nfs-shared-volumes-networks.test.js
@@ -0,0 +1,389 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+var assert = require('assert-plus');
+var bunyan = require('bunyan');
+var jsprim = require('jsprim');
+var vasync = require('vasync');
+
+var cli = require('../lib/cli');
+var common = require('../lib/common');
+var configLoader = require('../../lib/config-loader');
+var helpers = require('./helpers');
+var testVolumes = require('../lib/volumes');
+var ufds = require('../../lib/ufds');
+var volumesCli = require('../lib/volumes-cli');
+
+if (!testVolumes.dockerClientSupportsVolumes(process.env.DOCKER_CLI_VERSION)) {
+    console.log('Skipping volume tests: volumes are not supported in Docker '
+        + 'versions < 1.9');
+    process.exit(0);
+}
+
+var test = testVolumes.testIfEnabled;
+
+var NFS_SHARED_VOLUME_NAMES_PREFIX =
+    testVolumes.getNfsSharedVolumesNamePrefix();
+
+var ALICE_USER;
+var DEFAULT_FABRIC_NETWORK_UUID;
+
+var LOG = bunyan.createLogger({
+    name: 'test-volumes-networks',
+    level: 'error'
+});
+
+var CONFIG = configLoader.loadConfigSync({log: LOG});
+
+var DC = CONFIG.datacenterName;
+
+var NAPI;
+var UFDS;
+var VMAPI;
+var VOLAPI;
+
+test('setup', function (tt) {
+    tt.test('DockerEnv: alice init', function (t) {
+        cli.init(t, function onCliInit(err, env) {
+            t.ifErr(err, 'Docker environment initialization should not err');
+            if (env) {
+                ALICE_USER = env.user;
+            }
+        });
+    });
+
+    tt.test('vmapi client init', function (t) {
+        helpers.createVmapiClient(function (err, client) {
+            t.ifErr(err, 'vmapi client');
+            VMAPI = client;
+            t.end();
+        });
+    });
+
+    tt.test('napi client init', function (t) {
+        helpers.createNapiClient(function (err, client) {
+            t.ifErr(err, 'napi client');
+            NAPI = client;
+            t.end();
+        });
+    });
+
+    tt.test('volapi client init', function (t) {
+        helpers.createVolapiClient(function (err, client) {
+            t.ifErr(err, 'volapi client');
+            VOLAPI = client;
+            t.end();
+        });
+    });
+
+    tt.test('ufds client init', function (t) {
+        var ufdsOptions = jsprim.deepCopy(CONFIG.ufds);
+        ufdsOptions.log = LOG;
+
+        ufds.createUfdsClient(ufdsOptions, function (err, ufdsClient) {
+            UFDS = ufdsClient;
+
+            t.ifErr(err, 'creating UFDS client should succeed');
+            t.end();
+        });
+    });
+
+    tt.test('retrieving default fabric network should be successful',
+        function (t) {
+        UFDS.getDcLocalConfig(ALICE_USER.account.uuid, DC,
+            function (getDcConfigErr, conf) {
+                if (getDcConfigErr || !conf || !conf.defaultnetwork) {
+                    t.ok(false, 'Networks: could not get default network, err: '
+                        + getDcConfigErr + ', config: ' + conf);
+                } else {
+                    DEFAULT_FABRIC_NETWORK_UUID = conf.defaultnetwork;
+                    t.ok(true, 'Got default network');
+                }
+
+                t.end();
+            });
+    });
+
+    tt.test('pull busybox image', function (t) {
+        cli.pull(t, {
+            image: 'busybox:latest'
+        });
+    });
+});
+
+test('cleanup leftover resources from previous tests run', function (tt) {
+    tt.test('deleting all volumes for test user', function (t) {
+        volumesCli.deleteAllVolumes(ALICE_USER,
+            function allVolumesDeleted(err) {
+                t.ifErr(err, 'deleting all volumes should not error');
+
+                t.end();
+            });
+    });
+});
+
+test('docker volume create uses default fabric network by default',
+    function (tt) {
+    var volumeName;
+    var storageVmUuid;
+    var storageVmNics;
+
+    tt.test('creating volume with default network should succeed',
+        function (t) {
+        volumeName =
+            common.makeResourceName(NFS_SHARED_VOLUME_NAMES_PREFIX);
+
+        vasync.pipeline({funcs: [
+            function createVolume(_, next) {
+                volumesCli.createVolume({
+                    user: ALICE_USER,
+                    args: '--name ' + volumeName
+                }, function onVolumeCreated(err, stdout, stderr) {
+                    t.ifErr(err,
+                        'volume with name ' + volumeName + ' should have '
+                            + 'been created successfully');
+                    t.equal(stdout, volumeName + '\n',
+                        'output is newly created volume\'s name ('
+                            + volumeName + ')');
+
+                    next(err);
+                });
+            },
+            function getVolumeStorageVm(_, next) {
+                VOLAPI.listVolumes({
+                    owner_uuid: ALICE_USER.account.uuid,
+                    name: volumeName
+                }, function onListVolumes(listVolumesErr, volumes) {
+                    t.ifErr(listVolumesErr, 'listing volumes should not error');
+                    t.ok(volumes,
+                        'found volumes matching name of newly created volume');
+                    t.equal(volumes.length, 1,
+                        'found only one volume matching name of newly created '
+                            + 'volume');
+
+                    if (volumes) {
+                        storageVmUuid = volumes[0].vm_uuid;
+                    }
+
+                    next(listVolumesErr);
+                });
+            },
+            function getStorageVmNetwork(_, next) {
+                if (!storageVmUuid) {
+                    next();
+                    return;
+                }
+
+                assert.string(storageVmUuid, 'storageVmUuid');
+
+                VMAPI.getVm({
+                    uuid: storageVmUuid
+                }, function onGetStorageVm(getVmErr, storageVm) {
+                    var vmUsesDefaultFabricNetwork = false;
+
+                    if (storageVm) {
+                        storageVmNics = storageVm.nics;
+                    }
+
+                    if (storageVmNics) {
+                        storageVmNics.forEach(function isDefaultFabricNet(nic) {
+                            if (nic.network_uuid ===
+                                DEFAULT_FABRIC_NETWORK_UUID) {
+                                vmUsesDefaultFabricNetwork = true;
+                            }
+                        });
+                    }
+
+                    t.ok(vmUsesDefaultFabricNetwork,
+                        'storage VM should use default fabric network');
+                    next(getVmErr);
+                });
+            }
+        ]}, function allDone(err) {
+            volumesCli.rmVolume({
+                user: ALICE_USER,
+                args: volumeName
+            }, function onVolumeDeleted(delVolumeErr, stdout, stderr) {
+                var dockerVolumeOutput = stdout;
+
+                t.ifErr(delVolumeErr,
+                    'Removing an existing shared volume should not error');
+                t.equal(dockerVolumeOutput, volumeName + '\n',
+                    'Output should be shared volume\'s name');
+                t.end();
+            });
+        });
+    });
+});
+
+test('docker volume create using non-default network uses non-default network',
+    function (tt) {
+    var NEW_FABRIC_NETWORK_NAME =
+        common.makeResourceName('sdc-docker-test-volumes');
+    var newFabricNetwork;
+    var FABRIC_VLAN_ID = 2;
+    var fabricNetworkParams = {
+        name: NEW_FABRIC_NETWORK_NAME,
+        subnet: '192.168.42.0/24',
+        provision_start_ip: '192.168.42.1',
+        provision_end_ip: '192.168.42.254',
+        gateway: '192.168.42.1',
+        resolvers: '8.8.8.8,8.8.4.4',
+        mtu: 1400,
+        /*
+         * This avoids creating a NAT zone for the newly created network, and
+         * makes cleaning up resources tied to this network easier.
+         */
+        internet_nat: false
+    };
+    var volumeName;
+    var storageVmUuid;
+    var storageVmNics;
+
+    tt.test('creating new fabric network should succeed', function (t) {
+        NAPI.createFabricNetwork(ALICE_USER.account.uuid, FABRIC_VLAN_ID,
+            fabricNetworkParams,
+            function onFabricNetworkCreated(creationErr, fabricNetwork) {
+                t.ifErr(creationErr,
+                    'creating new fabric network should succeed');
+                newFabricNetwork = fabricNetwork;
+
+                t.end();
+            });
+    });
+
+    tt.test('creating volume with non-default network should succeed',
+        function (t) {
+        volumeName =
+            common.makeResourceName(NFS_SHARED_VOLUME_NAMES_PREFIX);
+
+        vasync.pipeline({funcs: [
+            function createVolume(_, next) {
+                volumesCli.createVolume({
+                    user: ALICE_USER,
+                    args: '--name ' + volumeName + ' --opt network='
+                        + NEW_FABRIC_NETWORK_NAME
+                }, function onVolumeCreated(err, stdout, stderr) {
+                    t.ifErr(err,
+                        'volume with name ' + volumeName + ' should have '
+                            + 'been created successfully');
+                    t.equal(stdout, volumeName + '\n',
+                        'output is newly created volume\'s name ('
+                            + volumeName + ')');
+
+                    next(err);
+                });
+            },
+            function getVolumeStorageVm(_, next) {
+                VOLAPI.listVolumes({
+                    owner_uuid: ALICE_USER.account.uuid,
+                    name: volumeName
+                }, function onListVolumes(listVolumesErr, volumes) {
+                    t.ifErr(listVolumesErr, 'listing volumes should not error');
+                    t.ok(volumes,
+                        'found volumes matching name of newly created volume');
+                    t.equal(volumes.length, 1,
+                        'found only one volume matching name of newly created '
+                            + 'volume');
+
+                    if (volumes) {
+                        storageVmUuid = volumes[0].vm_uuid;
+                    }
+
+                    next(listVolumesErr);
+                });
+            },
+            function getStorageVmNetwork(_, next) {
+                if (!storageVmUuid) {
+                    next();
+                    return;
+                }
+
+                assert.string(storageVmUuid, 'storageVmUuid');
+
+                VMAPI.getVm({
+                    uuid: storageVmUuid
+                }, function onGetStorageVm(getVmErr, storageVm) {
+                    var vmUsesNewFabricNetwork = false;
+                    if (storageVm) {
+                        storageVmNics = storageVm.nics;
+                    }
+
+                    if (storageVmNics) {
+                        storageVmNics.forEach(function isDefaultFabricNet(nic) {
+                            if (nic.network_uuid ===
+                                newFabricNetwork.uuid) {
+                                vmUsesNewFabricNetwork = true;
+                            }
+                        });
+                    }
+
+                    t.ok(vmUsesNewFabricNetwork,
+                        'storage VM should use new fabric network');
+                    next(getVmErr);
+                });
+            }
+        ]}, function allDone(err) {
+            volumesCli.rmVolume({
+                user: ALICE_USER,
+                args: volumeName
+            }, function onVolumeDeleted(delVolumeErr, stdout, stderr) {
+                var dockerVolumeOutput = stdout;
+
+                t.ifErr(delVolumeErr,
+                    'Removing an existing shared volume should not error');
+                t.equal(dockerVolumeOutput, volumeName + '\n',
+                    'Output should be shared volume\'s name');
+                t.end();
+            });
+        });
+    });
+
+    tt.test('deleting new fabric network should succeed', function (t) {
+        NAPI.deleteFabricNetwork(ALICE_USER.account.uuid, FABRIC_VLAN_ID,
+            newFabricNetwork.uuid, fabricNetworkParams,
+            function onFabricNetworkDeleted(fabricNetworkDelErr) {
+                t.ifErr(fabricNetworkDelErr,
+                    'deleting new fabric network should succeed');
+                t.end();
+            });
+    });
+
+});
+
+test('docker volume create using non-existent network should fail',
+    function (tt) {
+        var volumeName;
+
+        tt.test('volume creation should fail', function (t) {
+            volumeName =
+                common.makeResourceName(NFS_SHARED_VOLUME_NAMES_PREFIX);
+
+            volumesCli.createVolume({
+                user: ALICE_USER,
+                args: '--name ' + volumeName
+                    + ' --opt network=non-existent-network'
+            }, function onVolumeCreated(err, stdout, stderr) {
+                t.ok(err, 'volume with name ' + volumeName + ' should not have '
+                    + 'been created successfully');
+
+                t.end();
+            });
+        });
+    });
+
+test('teardown', function (tt) {
+    tt.test('closing UFDS client connection', function (t) {
+        UFDS.close(function onClientClosed(clientCloseErr) {
+            t.ifErr(clientCloseErr, 'Closing UFDS client should not error');
+            t.end();
+        });
+    });
+});
diff --git a/test/integration/cli-nfs-shared-volumes.test.js b/test/integration/cli-nfs-shared-volumes.test.js
new file mode 100644
index 0000000..488cf80
--- /dev/null
+++ b/test/integration/cli-nfs-shared-volumes.test.js
@@ -0,0 +1,514 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+/*
+ * Integration tests for `docker volume` using the driver that implements NFS
+ * shared volumes (currently named 'tritonnfs').
+ */
+
+var assert = require('assert-plus');
+var vasync = require('vasync');
+
+var cli = require('../lib/cli');
+var volumesCli = require('../lib/volumes-cli');
+var common = require('../lib/common');
+var testVolumes = require('../lib/volumes');
+
+if (!testVolumes.dockerClientSupportsVolumes(process.env.DOCKER_CLI_VERSION)) {
+    console.log('Skipping volume tests: volumes are not supported in Docker '
+        + 'versions < 1.9');
+    process.exit(0);
+}
+
+var test = testVolumes.testIfEnabled;
+
+var NFS_SHARED_VOLUME_NAMES_PREFIX =
+    testVolumes.getNfsSharedVolumesNamePrefix();
+var NFS_SHARED_VOLUMES_DRIVER_NAME =
+    testVolumes.getNfsSharedVolumesDriverName();
+
+var MOUNTING_CONTAINER_NAMES_PREFIX = 'test-nfs-mounting-container';
+
+var ALICE_USER;
+
+test('setup', function (tt) {
+    tt.test('DockerEnv: alice init', function (t) {
+        cli.init(t, function onCliInit(err, env) {
+            t.ifErr(err, 'Docker environment initialization should not err');
+            if (env) {
+                ALICE_USER = env.user;
+            }
+        });
+    });
+
+    // Ensure the busybox image is around.
+    tt.test('pull busybox image', function (t) {
+        cli.pull(t, {
+            image: 'busybox:latest'
+        });
+    });
+});
+
+test('cleanup leftover resources from previous tests run', function (tt) {
+    tt.test('deleting all volumes for test user', function (t) {
+        volumesCli.deleteAllVolumes(ALICE_USER,
+            function allVolumesDeleted(err) {
+                t.ifErr(err, 'deleting all volumes should not error');
+
+                t.end();
+            });
+    });
+});
+
+test('docker volume with default driver', function (tt) {
+    var volumeName;
+
+    tt.test('creating volume with no driver should succeed', function (t) {
+        volumeName =
+            common.makeResourceName(NFS_SHARED_VOLUME_NAMES_PREFIX);
+
+        vasync.pipeline({funcs: [
+            function createVolume(_, next) {
+                volumesCli.createVolume({
+                    user: ALICE_USER,
+                    args: '--name ' + volumeName
+                }, function onVolumeCreated(err, stdout, stderr) {
+                    t.ifErr(err,
+                        'volume should have been created successfully');
+                    t.equal(stdout, volumeName + '\n',
+                        'output is newly created volume\'s name');
+
+                    next(err);
+                });
+            },
+            function inspectVolume(_, next) {
+                volumesCli.inspectVolume({
+                    user: ALICE_USER,
+                    args: volumeName
+                }, function onInspect(err, stdout, stderr) {
+                    var inspectParsedOutput;
+
+                    t.ifErr(err, 'inspect should succeed');
+                    try {
+                        inspectParsedOutput = JSON.parse(stdout);
+                    } catch (inspectParseErr) {
+                    }
+
+                    t.equal(inspectParsedOutput[0].Driver,
+                        NFS_SHARED_VOLUMES_DRIVER_NAME,
+                            'volume driver should be '
+                                + NFS_SHARED_VOLUMES_DRIVER_NAME);
+
+                    next();
+                });
+            },
+            function _deleteVolume(_, next) {
+                volumesCli.rmVolume({
+                    user: ALICE_USER,
+                    args: volumeName
+                }, function onVolumeDeleted(err, stdout, stderr) {
+                    var dockerVolumeOutput = stdout;
+
+                    t.ifErr(err,
+                        'Removing an existing shared volume should not error');
+                    t.equal(dockerVolumeOutput, volumeName + '\n',
+                        'Output should be shared volume\'s name');
+                    next();
+                });
+            }
+        ]}, function allDone(err) {
+            t.end();
+        });
+    });
+});
+
+test('docker volume with default name', function (tt) {
+    var volumeName;
+
+    tt.test('creating volume without specifying a name should succeed and '
+        + 'generate a new name', function (t) {
+        vasync.pipeline({funcs: [
+            function _createVolume(_, next) {
+                volumesCli.createVolume({
+                    user: ALICE_USER,
+                    args: '--driver ' + NFS_SHARED_VOLUMES_DRIVER_NAME
+                }, function onVolumeCreated(err, stdout, stderr) {
+                    var stdoutLines;
+
+                    t.ifErr(err,
+                        'volume should have been created successfully');
+
+                    stdoutLines = stdout.split('\n');
+                    t.equal(stdoutLines.length, 2,
+                        'output should be two lines');
+
+                    volumeName = stdoutLines[0];
+                    t.ok(testVolumes.validGeneratedVolumeName(volumeName),
+                        'newly created volume\'s name "' + volumeName
+                            + '" should match automatically generated '
+                            + 'volume name pattern');
+
+                    next();
+                });
+            },
+            function _deleteVolume(_, next) {
+                volumesCli.rmVolume({
+                    user: ALICE_USER,
+                    args: volumeName
+                },
+                function onVolumeDeleted(err, stdout, stderr) {
+                    var dockerVolumeOutput = stdout;
+
+                    t.ifErr(err,
+                        'Removing an existing shared volume should not error');
+                    t.equal(dockerVolumeOutput, volumeName + '\n',
+                        'Output should be shared volume\'s name');
+                    next();
+                });
+            }
+        ]}, function allDone(err) {
+            t.end();
+        });
+    });
+});
+
+test('docker NFS shared volume simple creation', function (tt) {
+    var containerName;
+    var volumeName;
+
+    tt.test('creating a NFS shared volume should succeed', function (t) {
+            volumeName =
+                common.makeResourceName(NFS_SHARED_VOLUME_NAMES_PREFIX);
+            volumesCli.createVolume({
+                user: ALICE_USER,
+                args: '--name ' + volumeName + ' --driver '
+                    + NFS_SHARED_VOLUMES_DRIVER_NAME
+            }, function onVolumeCreated(err, stdout, stderr) {
+                t.ifErr(err,
+                    'volume should have been created successfully');
+                t.equal(stdout, volumeName + '\n',
+                    'output should be newly created volume\'s name');
+
+                t.end();
+            });
+    });
+
+    tt.test('listing volumes should output newly created volume', function (t) {
+        volumesCli.listVolumes({
+            user: ALICE_USER
+        }, function onVolumesListed(err, stdout, stderr) {
+            var outputLines;
+            var foundNewlyCreatedVolume = false;
+
+            t.ifErr(err, 'listing volumes should not error');
+            outputLines = stdout.trim().split(/\n/);
+            // Remove header from docker volume ls' output.
+            outputLines = outputLines.slice(1);
+            t.ok(outputLines.length > 0,
+                'volumes list should not be empty');
+
+            outputLines.forEach(function checkVolumeLsOutputLine(line) {
+                var driverAndName = line.trim().split(/\s+/);
+                t.equal(driverAndName[0], NFS_SHARED_VOLUMES_DRIVER_NAME,
+                    'driver should be ' + NFS_SHARED_VOLUMES_DRIVER_NAME);
+                if (driverAndName[1] === volumeName) {
+                    foundNewlyCreatedVolume = true;
+                }
+            });
+
+            t.ok(foundNewlyCreatedVolume, 'newly created volume should be '
+                + 'present in volume ls output');
+
+            t.end();
+        });
+    });
+
+    tt.test('mounting a NFS shared volume from a container should succeed',
+        function (t) {
+            containerName =
+                common.makeResourceName(MOUNTING_CONTAINER_NAMES_PREFIX);
+            cli.run(t, {
+                args: '--name ' + containerName + ' -v ' + volumeName
+                    + ':/data busybox:latest /bin/sh -c '
+                    + '"touch /data/foo.txt && ls /data"'
+            }, function onContainerRun(err, output) {
+                t.ifErr(err, 'Mounting a valid volume should not error');
+                t.equal(output.stdout, 'foo.txt\n', 'Output should equal '
+                    + 'newly created container\'s name');
+                t.end();
+            });
+        });
+
+        tt.test('deleting first mounting container should succeed',
+            function (t) {
+                cli.rm(t, {args: containerName},
+                function onContainerDeleted(err, stdout, stderr) {
+                    t.ifErr(err,
+                        'deleting container mounting NFS shared volume '
+                            + 'should succeed');
+                    t.end();
+                });
+        });
+
+        tt.test('file created by a container should be visible from another '
+            + 'container', function (t) {
+                containerName =
+                    common.makeResourceName(MOUNTING_CONTAINER_NAMES_PREFIX);
+                cli.run(t, {
+                    args: '--name ' + containerName + ' -v ' + volumeName
+                        + ':/data busybox:latest ls /data/foo.txt'
+                }, function onContainerRun(err, output) {
+                    t.ifErr(err, 'Mounting a valid volume should not error');
+                    t.equal(output.stdout, '/data/foo.txt\n',
+                        'Output should equal name of file created in shared '
+                            + 'volume');
+                    t.end();
+                });
+        });
+
+        tt.test('deleting second mounting container should succeed',
+            function (t) {
+                cli.rm(t, {args: containerName},
+                function onContainerDeleted(err, stdout, stderr) {
+                    t.ifErr(err,
+                        'deleting container mounting NFS shared volume '
+                            + 'should succeed');
+                    t.end();
+                });
+        });
+
+        tt.test('deleting shared volume should succeed', function (t) {
+            volumesCli.rmVolume({
+                user: ALICE_USER,
+                args: volumeName
+            }, function onVolumeDeleted(err, stdout, stderr) {
+                var dockerVolumeOutput = stdout;
+
+                t.ifErr(err,
+                    'Removing an existing shared volume should not error');
+                t.equal(dockerVolumeOutput, volumeName + '\n',
+                    'Output should be shared volume\'s name');
+                t.end();
+            });
+        });
+});
+
+test('mounting more than one NFS shared volume', function (tt) {
+    var firstVolumeName;
+    var secondVolumeName;
+    var containerName;
+
+    tt.test('creating first NFS shared volume should succeed', function (t) {
+            firstVolumeName =
+                common.makeResourceName(NFS_SHARED_VOLUME_NAMES_PREFIX);
+            volumesCli.createVolume({
+                user: ALICE_USER,
+                args: '--name ' + firstVolumeName + ' --driver '
+                    + NFS_SHARED_VOLUMES_DRIVER_NAME
+            }, function onVolumeCreated(err, stdout, stderr) {
+                t.ifErr(err,
+                    'volume should have been created successfully');
+                t.equal(stdout, firstVolumeName + '\n',
+                    'output should be newly created volume\'s name');
+
+                t.end();
+            });
+    });
+
+    tt.test('creating second NFS shared volume should succeed', function (t) {
+            secondVolumeName =
+                common.makeResourceName(NFS_SHARED_VOLUME_NAMES_PREFIX);
+            volumesCli.createVolume({
+                user: ALICE_USER,
+                args: '--name ' + secondVolumeName + ' --driver '
+                    + NFS_SHARED_VOLUMES_DRIVER_NAME
+            }, function onVolumeCreated(err, stdout, stderr) {
+                t.ifErr(err,
+                    'volume should have been created successfully');
+                t.equal(stdout, secondVolumeName + '\n',
+                    'output should be newly created volume\'s name');
+
+                t.end();
+            });
+    });
+
+    tt.test('mounting both NFS shared volumes from a container should succeed',
+        function (t) {
+            containerName =
+                common.makeResourceName(MOUNTING_CONTAINER_NAMES_PREFIX);
+            cli.run(t, {
+                args: '--name ' + containerName + ' '
+                + '-v ' + firstVolumeName + ':/data-first '
+                + '-v ' + secondVolumeName + ':/data-second '
+                + 'busybox:latest /bin/sh -c "'
+                + 'touch /data-first/foo.txt && '
+                + 'touch /data-second/bar.txt && ls /data*"'
+            }, function onContainerRun(err, output) {
+                var expectedOutput =
+                    '/data-first:\nfoo.txt\n\n/data-second:\nbar.txt\n';
+                t.ifErr(err, 'Mounting both volumes should not error');
+                t.equal(output.stdout, expectedOutput,
+                    'Output should list files from both volumes');
+                t.end();
+            });
+        });
+
+    tt.test('deleting mounting container should succeed', function (t) {
+            cli.rm(t, {args: containerName},
+            function onContainerDeleted(err, stdout, stderr) {
+                t.ifErr(err,
+                    'deleting container mounting NFS shared volume '
+                        + 'should succeed');
+                t.end();
+            });
+    });
+
+    tt.test('deleting first shared volume should succeed', function (t) {
+        volumesCli.rmVolume({
+            user: ALICE_USER,
+            args: firstVolumeName
+        }, function onVolumeDeleted(err, stdout, stderr) {
+                var dockerVolumeOutput = stdout;
+
+                t.ifErr(err,
+                    'Removing first shared volume should not error');
+                t.equal(dockerVolumeOutput, firstVolumeName + '\n',
+                    'Output should be first shared volume\'s name');
+                t.end();
+            });
+    });
+
+    tt.test('deleting second shared volume should succeed', function (t) {
+        volumesCli.rmVolume({
+            user: ALICE_USER,
+            args: secondVolumeName
+        }, function onVolumeDeleted(err, stdout, stderr) {
+                var dockerVolumeOutput = stdout;
+
+                t.ifErr(err,
+                    'Removing second shared volume should not error');
+                t.equal(dockerVolumeOutput, secondVolumeName + '\n',
+                    'Output should be secondVolumeName shared volume\'s name');
+                t.end();
+            });
+    });
+});
+
+test('docker run mounting non-existent volume', function (tt) {
+    var nonExistingVolumeName = 'non-existent-volume';
+    var containerName =
+                common.makeResourceName(MOUNTING_CONTAINER_NAMES_PREFIX);
+
+    tt.test('mounting a non-existent NFS shared volume should succeed',
+        function (t) {
+            cli.run(t, {
+                args: '--name ' + containerName
+                    + ' -v ' + nonExistingVolumeName + ':/data busybox:latest'
+                    + ' /bin/sh -c "touch /data/foo.txt && ls /data"'
+            }, function onContainerRun(err, output) {
+                t.ifErr(err, 'Mounting a non-existent volume should not '
+                    + 'error');
+                t.equal(output.stdout, 'foo.txt\n', 'Output should include '
+                    + 'newly created file\'s name');
+
+                t.end();
+            });
+        });
+
+    tt.test('listing volumes should output newly created volume', function (t) {
+        volumesCli.listVolumes({
+            user: ALICE_USER
+        }, function onVolumesListed(err, stdout, stderr) {
+            var outputLines;
+            var foundNewlyCreatedVolume = false;
+
+            t.ifErr(err, 'listing volumes should not error');
+            outputLines = stdout.trim().split(/\n/);
+            // Remove header from docker volume ls' output.
+            outputLines = outputLines.slice(1);
+            t.ok(outputLines.length > 0,
+                'volumes list should not be empty');
+
+            outputLines.forEach(function checkVolumeLsOutputLine(line) {
+                var driverAndName = line.trim().split(/\s+/);
+                t.equal(driverAndName[0], NFS_SHARED_VOLUMES_DRIVER_NAME,
+                    'driver should be ' + NFS_SHARED_VOLUMES_DRIVER_NAME);
+                if (driverAndName[1] === nonExistingVolumeName) {
+                    foundNewlyCreatedVolume = true;
+                }
+            });
+
+            t.ok(foundNewlyCreatedVolume, 'newly created volume should be '
+                + 'present in volume ls output');
+
+            t.end();
+        });
+    });
+
+    tt.test('deleting mounting container should succeed', function (t) {
+            cli.rm(t, {args: containerName},
+            function onContainerDeleted(err, stdout, stderr) {
+                t.ifErr(err,
+                    'deleting container mounting NFS shared volume '
+                        + 'should succeed');
+                t.end();
+            });
+    });
+
+    tt.test('deleting shared volume should succeed', function (t) {
+        volumesCli.rmVolume({
+            user: ALICE_USER,
+            args: nonExistingVolumeName
+        }, function onVolumeDeleted(err, stdout, stderr) {
+                var dockerVolumeOutput = stdout;
+
+                t.ifErr(err,
+                    'Removing shared volume should not error');
+                t.equal(dockerVolumeOutput, nonExistingVolumeName + '\n',
+                    'Output should be shared volume\'s name');
+                t.end();
+            });
+    });
+});
+
+test('list docker volumes', function (tt) {
+
+    tt.test('should not output deleted volumes', function (t) {
+        volumesCli.listVolumes({
+            user: ALICE_USER
+        }, function onVolumesListed(err, stdout, stderr) {
+            var outputLines;
+            var foundDeletedVolume = false;
+
+            t.ifErr(err, 'listing volumes should not error');
+            outputLines = stdout.trim().split(/\n/);
+            // Remove header from docker volume ls' output.
+            outputLines = outputLines.slice(1);
+
+            outputLines.forEach(function checkVolumeLsOutputLine(line) {
+                var driverAndName = line.trim().split(/\s+/);
+                var volumeDriver = driverAndName[0];
+                var volumeName = driverAndName[1];
+
+                t.equal(volumeDriver, NFS_SHARED_VOLUMES_DRIVER_NAME,
+                    'driver should be ' + NFS_SHARED_VOLUMES_DRIVER_NAME);
+                if (volumeName.match(NFS_SHARED_VOLUME_NAMES_PREFIX)) {
+                    foundDeletedVolume = true;
+                }
+            });
+
+            t.equal(foundDeletedVolume, false,
+                'volumes created and deleted by this tests suite should '
+                    + 'not be listed in volume ls output');
+
+            t.end();
+        });
+    });
+});
diff --git a/test/integration/helpers.js b/test/integration/helpers.js
index ff7faaa..d3d2463 100644
--- a/test/integration/helpers.js
+++ b/test/integration/helpers.js
@@ -39,7 +39,8 @@ var CONFIG = {
     papi_url: process.env.PAPI_URL,
     sapi_url: process.env.SAPI_URL,
     vmapi_url: process.env.VMAPI_URL,
-    napi_url: process.env.NAPI_URL
+    napi_url: process.env.NAPI_URL,
+    volapi_url: process.env.VOLAPI_URL
 };
 var p = console.error;
 var UA = 'sdcdockertest';
@@ -1259,6 +1260,25 @@ function createNapiClient(callback) {
     });
 }
 
+/**
+ * Get a simple restify JSON client to VOLAPI.
+ */
+function createVolapiClient(callback) {
+    assert.func(callback, 'callback');
+
+    createClientOpts('volapi', function (err, opts) {
+        if (err) {
+            return callback(err);
+        }
+
+        opts.version = '^1';
+        opts.userAgent = 'sdc-docker-integration-tests';
+
+        callback(null, new sdcClients.VOLAPI(opts));
+        return;
+    });
+}
+
 /**
  * Test the given Docker 'info' API response.
  */
@@ -1528,9 +1548,10 @@ function createDockerContainer(opts, callback) {
     assert.optionalBool(opts.start, 'opts.start');
     assert.object(opts.test, 'opts.test');
     assert.object(opts.vmapiClient, 'opts.vmapiClient');
+    assert.optionalBool(opts.wait, 'opts.wait');
     assert.func(callback, 'callback');
 
-    var imageName = opts.imageName || 'nginx';
+    var imageName = opts.imageName || 'nginx:latest';
 
     var payload = {
         'Hostname': '',
@@ -1603,8 +1624,8 @@ function createDockerContainer(opts, callback) {
 
     vasync.waterfall([
         function (next) {
-            // There is a dependency here, in order to create a container,
-            // the image must first be downloaded.
+            // There is a dependency here, in order to create a container, its
+            // image must first be downloaded.
             ensureImage({
                 name: imageName,
                 user: dockerClient.user
@@ -1640,6 +1661,30 @@ function createDockerContainer(opts, callback) {
                 next(err);
             }
         },
+        function attachToContainer(next) {
+            if (!opts.start || !opts.wait) {
+                next();
+                return;
+            }
+
+            dockerClient.post('/containers/' + response.id + '/attach',
+                function onAttach(err, res, req, body) {
+                    t.error(err);
+                    next(err);
+                });
+        },
+        function waitForContainer(next) {
+            if (!opts.start || !opts.wait) {
+                next();
+                return;
+            }
+
+            dockerClient.post('/containers/' + response.id + '/wait',
+                function onWait(err, res, req, body) {
+                    t.error(err);
+                    next(err);
+                });
+        },
         function (next) {
             // Attempt to get container json (i.e. docker inspect).
             dockerClient.get(
@@ -1669,7 +1714,6 @@ function createDockerContainer(opts, callback) {
     });
 }
 
-
 function listContainers(opts, callback) {
     assert.object(opts, 'opts');
     assert.func(callback, 'callback');
@@ -2053,6 +2097,7 @@ module.exports = {
     createPapiClient: createPapiClient,
     createVmapiClient: createVmapiClient,
     createNapiClient: createNapiClient,
+    createVolapiClient: createVolapiClient,
     dockerIdToUuid: sdcCommon.dockerIdToUuid,
     ensureImage: ensureImage,
     initDockerEnv: initDockerEnv,
diff --git a/test/lib/cli.js b/test/lib/cli.js
index dce0d87..7ad831a 100644
--- a/test/lib/cli.js
+++ b/test/lib/cli.js
@@ -411,7 +411,7 @@ function cliRun(t, opts, callback) {
             common.expCliErr(t, stderr, opts.expectedErr, callback);
             return;
 
-        } else {
+        } else if (!opts.expectRuntimeError) {
             t.ifErr(err, 'docker run');
             // Docker run may need to download the image, which produces
             // stderr - only allow for that case:
@@ -546,7 +546,7 @@ function cliStart(t, opts, callback) {
     ALICE.docker('start ' + opts.args, function (err, stdout, stderr) {
         t.ifErr(err, 'docker start ' + opts.args);
         t.equal(stderr, '', 'stderr');
-        callback(err);
+        callback(err, stdout, stderr);
     });
 }
 
@@ -586,7 +586,6 @@ function cliCommit(t, opts, callback) {
     });
 }
 
-
 /**
  * `docker attach <id>`
  *
diff --git a/test/lib/common.js b/test/lib/common.js
index 0c347f4..c6f2e69 100644
--- a/test/lib/common.js
+++ b/test/lib/common.js
@@ -281,7 +281,7 @@ function partialExp(t, opts, obj) {
 /*
  * Make a prefixed, randomized name for a test container.
  */
-function makeContainerName(prefix) {
+function makeResourceName(prefix) {
     return prefix + '-' + libuuid.create().split('-')[0];
 }
 
@@ -446,10 +446,10 @@ module.exports = {
     expApiErr: expApiErr,
     expCliErr: expCliErr,
     ifErr: ifErr,
-    makeContainerName: makeContainerName,
-    makeImageName: makeImageName,
+    makeResourceName: makeResourceName,
     objCopy: objCopy,
     parseDockerVersion: parseDockerVersion,
     parseOutputUsingHeader: parseOutputUsingHeader,
-    partialExp: partialExp
+    partialExp: partialExp,
+    parseDockerVersion: parseDockerVersion
 };
diff --git a/test/lib/volumes-api.js b/test/lib/volumes-api.js
new file mode 100644
index 0000000..2f578c8
--- /dev/null
+++ b/test/lib/volumes-api.js
@@ -0,0 +1,88 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2016, Joyent, Inc.
+ */
+
+var assert = require('assert-plus');
+var vasync = require('vasync');
+
+var common = require('./common');
+var constants = require('../../lib/constants');
+var testVolumes = require('./volumes');
+
+/*
+ * Creates a docker volume with the following parameters:
+ *
+ * - opts: an object with properties that represents the following options:
+ *   - opts.dockerClient: an object representing the docker API client that will
+ *     be used to perform all operations.
+ *
+ *   - opts.name: the name of the volume to create.
+ *
+ * - callback: a function that will be called when the volume is created. That
+ *   function will be called with the following parameters:
+ *
+ *     - err: an object representing an error, if an error occured.
+ *
+ *     - volume: an object representing the volume that was just created. That
+ *       object is of the same form than the object returned by the docker
+ *       volume inspect command.
+ */
+function createDockerVolume(opts, callback) {
+    assert.object(opts, 'opts');
+    assert.object(opts.dockerClient, 'opts.dockerClient');
+    assert.optionalString(opts.name, 'opts.name');
+    assert.func(callback, 'callback');
+
+    assert.ok(testVolumes.nfsSharedVolumesSupported());
+
+    var dockerClient = opts.dockerClient;
+    var dockerApiVersion = opts.apiVersion || ('v' + constants.API_VERSION);
+
+    vasync.waterfall([
+        function createVolume(next) {
+            var volumeNamesPrefix =
+                testVolumes.getNfsSharedVolumesNamePrefix();
+            var volumeName = opts.name;
+            var volumeType = testVolumes.getNfsSharedVolumesDriverName();
+
+            if (!volumeName) {
+                volumeName = common.makeResourceName(volumeNamesPrefix);
+            }
+
+            var payload =  {
+                Name: volumeName,
+                Driver: volumeType,
+                DriverOpts: {},
+                Labels: {}
+            };
+
+            dockerClient.post('/' + dockerApiVersion + '/volumes/create',
+                payload,
+                function onVolumeCreated(err, res, req, body) {
+                    next(err, body);
+                });
+        },
+        function getVolumeInfo(volumeCreationResponse, next) {
+            assert.object(volumeCreationResponse, 'volumeCreationResponse');
+            assert.string(volumeCreationResponse.Name,
+                'volumeCreationResponse.Name');
+
+            var volumeName = volumeCreationResponse.Name;
+
+            dockerClient.get('/' + dockerApiVersion + '/volumes/' + volumeName,
+                function onVolumeInspect(err, res, req, body) {
+                    next(err, body);
+                });
+        }
+    ], callback);
+}
+
+module.exports = {
+    createDockerVolume: createDockerVolume
+};
\ No newline at end of file
diff --git a/test/lib/volumes-cli.js b/test/lib/volumes-cli.js
new file mode 100644
index 0000000..2a99a8c
--- /dev/null
+++ b/test/lib/volumes-cli.js
@@ -0,0 +1,216 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2016, Joyent, Inc.
+ */
+
+var assert = require('assert-plus');
+var vasync = require('vasync');
+
+/**
+ * `docker volume create <opts.args>`
+ */
+function cliCreateVolume(opts, callback) {
+    assert.object(opts, 'opts');
+    assert.object(opts.user, 'opts.user');
+    assert.optionalObject(opts.t, 'opts.t');
+    assert.optionalString(opts.args, 'opts.args');
+
+    var t = opts.t;
+    var command = [
+        'volume create'
+    ];
+    var user = opts.user;
+
+    if (opts.args) {
+        command.push(opts.args);
+    }
+
+    user.docker(command.join(' '), function (err, stdout, stderr) {
+        if (t) {
+            t.ifErr(err, 'docker volume create ' + opts.args);
+        }
+
+        callback(err, stdout, stderr);
+    });
+}
+
+/**
+ * `docker volume rm <opts.args>`
+ */
+function cliDeleteVolume(opts, callback) {
+    assert.object(opts, 'opts');
+    assert.object(opts.user, 'opts.user');
+    assert.optionalObject(opts.t, 'opts.t');
+    assert.string(opts.args, 'opts.args');
+
+    var t = opts.t;
+    var user = opts.user;
+
+    user.docker('volume rm ' + opts.args, function (err, stdout, stderr) {
+        if (t) {
+            t.ifErr(err, 'docker volume rm ' + opts.args);
+        }
+
+        callback(err, stdout, stderr);
+    });
+}
+
+/**
+ * `docker volume ls <opts.args>`
+ */
+function cliListVolumes(opts, callback) {
+    assert.object(opts, 'opts');
+    assert.object(opts.user, 'opts.user');
+    assert.optionalObject(opts.t, 'opts.t');
+    assert.optionalString(opts.args, 'opts.args');
+
+    var t = opts.t;
+    var listVolumesCommand = 'volume ls';
+    var user = opts.user;
+
+    if (opts.args) {
+        listVolumesCommand += ' ' + opts.args;
+    }
+
+    user.docker(listVolumesCommand, function (err, stdout, stderr) {
+        if (t) {
+            t.ifErr(err, 'docker volume ls ' + opts.args);
+        }
+
+        callback(err, stdout, stderr);
+    });
+}
+
+/*
+ * `docker volume inspect <opts.args>`
+ */
+function cliInspectVolume(opts, callback) {
+    assert.object(opts, 'opts');
+    assert.object(opts.user, 'opts.user');
+    assert.optionalObject(opts.t, 'opts.t');
+    assert.string(opts.args, 'opts.args');
+
+    var t = opts.t;
+    var user = opts.user;
+
+    user.docker('volume inspect ' + opts.args, function (err, stdout, stderr) {
+        if (t) {
+            t.ifErr(err, 'docker volume inspect ' + opts.args);
+        }
+
+        callback(err, stdout, stderr);
+    });
+}
+
+/*
+ * Creates a test volume using the docker command "docker volume create". It
+ * passes any parameter key/value present in the object "params" as arguments to
+ * "docker volume create" as following:
+ *
+ * docker volume create --name someName --opt argName1=argValue1
+ *      --opt argName2=argValue2
+ *
+ * The key/value for the key "name" in the "params" object is treated
+ * differently from other parameters, as it generates a "--name someName"
+ * command line parameter, instead of "--opt name=someName".
+ *
+ * The function callback is called with an error object, the output written on
+ * stdout and the output written on stderr.
+ */
+function createTestVolume(user, params, callback) {
+    assert.object(user, 'user');
+    assert.object(params, 'params');
+    assert.func(callback, 'callback');
+
+    var cmdLineArgs = [];
+    var paramName;
+
+    for (paramName in params) {
+        if (paramName === 'name') {
+            cmdLineArgs.push('--name ' + params[paramName]);
+        } else {
+            cmdLineArgs.push('--opt ' + paramName + '=' + params[paramName]);
+        }
+    }
+
+    cliCreateVolume({
+        user: user,
+        args: cmdLineArgs.join(' ')
+    }, callback);
+}
+
+function cliDeleteVolumes(user, volumeNames, callback) {
+    assert.object(user, 'user');
+    assert.arrayOfString(volumeNames, 'volumeNames');
+    assert.func(callback, 'callback');
+
+    vasync.forEachParallel({
+        func: function _deleteVolume(volumeName, done) {
+            cliDeleteVolume({
+                user: user,
+                args: volumeName
+            }, done);
+        },
+        inputs: volumeNames
+    }, callback);
+}
+
+/*
+ * Deletes all volumes that are in the state 'ready', and calls the function
+ * `callback` when done. 'callback' is passed an error object as its first
+ * argument if an error occured.
+ */
+function cliDeleteAllVolumes(user, callback) {
+    assert.object(user, 'user');
+    assert.func(callback, 'callback');
+
+    var leftoverVolumeNames = [];
+
+    vasync.pipeline({funcs: [
+        function listLeftoverVolumes(ctx, next) {
+            cliListVolumes({
+                user: user
+            }, function onVolumesListed(listVolumesErr, stdout, stderr) {
+                var outputLines;
+                var err;
+
+                if (!listVolumesErr) {
+                    outputLines = stdout.trim().split(/\n/);
+                    // Remove header from docker volume ls' output.
+                    outputLines = outputLines.slice(1);
+
+                    outputLines.forEach(function addLeftoverVolume(line) {
+                        var driverAndName = line.trim().split(/\s+/);
+                        var volumeName = driverAndName[1];
+
+                        leftoverVolumeNames.push(volumeName);
+                    });
+                } else {
+                    err = listVolumesErr;
+                }
+
+                next(err);
+            });
+        },
+        function deleteVolumesFound(ctx, next) {
+            cliDeleteVolumes(user, leftoverVolumeNames, next);
+        }
+    ]}, function cleanupDone(err) {
+        callback(err);
+    });
+}
+
+module.exports = {
+    createVolume: cliCreateVolume,
+    rmVolume: cliDeleteVolume,
+    listVolumes: cliListVolumes,
+    inspectVolume: cliInspectVolume,
+    createTestVolume: createTestVolume,
+    deleteAllVolumes: cliDeleteAllVolumes,
+    deleteVolumes: cliDeleteVolumes
+};
\ No newline at end of file
diff --git a/test/lib/volumes.js b/test/lib/volumes.js
new file mode 100644
index 0000000..0e5b643
--- /dev/null
+++ b/test/lib/volumes.js
@@ -0,0 +1,153 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+var assert = require('assert-plus');
+var jsprim = require('jsprim');
+var sdcClients = require('sdc-clients');
+var test = require('tape');
+var vasync = require('vasync');
+
+var cli = require('./cli');
+var common = require('./common');
+var configLoader = require('../../lib/config-loader');
+var log = require('./log');
+
+var CONFIG = configLoader.loadConfigSync({log: log});
+
+var NFS_SHARED_VOLUMES_SUPPORTED = false;
+if (CONFIG.experimental_docker_nfs_shared_volumes === true) {
+    NFS_SHARED_VOLUMES_SUPPORTED = true;
+}
+
+/*
+ * Returns true if the instance of sdc-docker against which these tests are run
+ * supports NFS shared volumes, false otherwise.
+ */
+function nfsSharedVolumesSupported() {
+    // override config value with environment variable since runtest(s) will
+    // have looked this up from the *running* docker instance instead of
+    // trusting the current config file which may be wrong if docker wasn't
+    // restarted.
+    if (process.env.hasOwnProperty('NFS_VOLUMES_ENABLED')) {
+        if (process.env.NFS_VOLUMES_ENABLED === 'true') {
+            NFS_SHARED_VOLUMES_SUPPORTED = true;
+        } else {
+            NFS_SHARED_VOLUMES_SUPPORTED = false;
+        }
+    }
+
+    return NFS_SHARED_VOLUMES_SUPPORTED;
+}
+
+/*
+ * Returns a string representing the driver name used by NFS shared volumes.
+ */
+function getNfsSharedVolumesDriverName() {
+    return 'tritonnfs';
+}
+
+/*
+ * Returns a string representing the prefix that can (and should) be used when
+ * creating any NFS shared volume as part of the tests suite.
+ */
+function getNfsSharedVolumesNamePrefix() {
+    return 'test-nfs-shared-volume';
+}
+
+/*
+ * Returns true if the version of the Docker client used by the current
+ * integration test supports volumes, false otherwise.
+ */
+function dockerClientSupportsVolumes(dockerVersionString) {
+    assert.string(dockerVersionString, 'dockerVersionString');
+
+    var dockerVersion = common.parseDockerVersion(dockerVersionString);
+
+    if (dockerVersion.major < 1 || dockerVersion.minor < 9) {
+        return false;
+    }
+
+    return true;
+}
+
+/*
+ * Returns true if the string "volumeName" represents a valid
+ * automatically-generated volume name, false otherwise.
+ */
+function validGeneratedVolumeName(volumeName) {
+    assert.string(volumeName, 'volumeName');
+
+    var GENERATED_VOLUME_NAME_REGEXP = /^[\w0-9]{64}$/;
+
+    return GENERATED_VOLUME_NAME_REGEXP.test(volumeName);
+}
+
+/*
+ * Returns true if the error object "err" and the optional stderr output
+ * "stderr" represent an error that means that NFS shared volumes are not
+ * supported by the instance of the sdc-docker service from which this error was
+ * received.
+ */
+function errorMeansNFSSharedVolumeSupportDisabled(err, errMsg) {
+    assert.optionalObject(err, 'err');
+    assert.string(errMsg, 'errMsg');
+
+    var expectedErrMsg = 'Volumes are not supported';
+
+    if (err && (errMsg === undefined
+        || errMsg.indexOf(expectedErrMsg) !== -1)) {
+        return true;
+    }
+
+    return false;
+}
+
+var VOLAPI_CLIENT;
+
+function getVolapiClient() {
+    var volapiConfig;
+
+    if (VOLAPI_CLIENT === undefined) {
+        volapiConfig = jsprim.deepCopy(CONFIG.volapi);
+
+        volapiConfig.version = '^1';
+        volapiConfig.userAgent = 'sdc-docker-tests';
+
+        VOLAPI_CLIENT = new sdcClients.VOLAPI(volapiConfig);
+    }
+
+    return VOLAPI_CLIENT;
+}
+
+function testIfNfsVolumesEnabled(testName, testFunc) {
+    assert.string(testName, 'testName');
+    assert.func(testFunc, 'testFunc');
+
+    test(testName, function (t) {
+        if (!nfsSharedVolumesSupported()) {
+            t.ok(true, 'NFS volumes disabled - skipping test');
+            t.end();
+            return;
+        }
+        testFunc(t);
+    });
+}
+
+module.exports = {
+    dockerClientSupportsVolumes: dockerClientSupportsVolumes,
+    errorMeansNFSSharedVolumeSupportDisabled:
+        errorMeansNFSSharedVolumeSupportDisabled,
+    getNfsSharedVolumesNamePrefix: getNfsSharedVolumesNamePrefix,
+    getNfsSharedVolumesDriverName: getNfsSharedVolumesDriverName,
+    getVolapiClient: getVolapiClient,
+    nfsSharedVolumesSupported: nfsSharedVolumesSupported,
+    testIfEnabled: testIfNfsVolumesEnabled,
+    validGeneratedVolumeName: validGeneratedVolumeName
+};
diff --git a/test/runtest.common b/test/runtest.common
index 545d2fa..6e543b4 100644
--- a/test/runtest.common
+++ b/test/runtest.common
@@ -47,6 +47,8 @@ export SAPI_URL="http://$(vmadm lookup -j alias=sapi0 | json 0.nics \
     | json -c 'this.nic_tag==="admin"' 0.ip)"
 export NAPI_URL="http://$(vmadm lookup -j alias=napi0 | json 0.nics \
     | json -c 'this.nic_tag==="admin"' 0.ip)"
+export VOLAPI_URL="http://$(vmadm lookup -j alias=volapi0 | json 0.nics \
+    | json -c 'this.nic_tag==="admin"' 0.ip)"
 
 # Docker CLI versions for testing:
 # 1. 'DOCKER_AVAILABLE_CLI_VERSIONS' is the set of 'docker' CLI versions we
@@ -77,6 +79,11 @@ EOF
     exit 2
 fi
 
+# Determine whether NFS Volumes (RFD 26) are supported in this setup
+NFS_VOLUMES_ENABLED=$(curl -sS ${DOCKER_ADMIN_URL}/admin/config \
+    | json -H experimental_docker_nfs_shared_volumes)
+[[ ${NFS_VOLUMES_ENABLED} == "true" ]] || NFS_VOLUMES_ENABLED="false"
+export NFS_VOLUMES_ENABLED
 
 # Prerequisite: ensure there is a package matching the current sdc-docker
 # 'packagePrefix'.
@@ -123,11 +130,13 @@ fi
 
 
 echo "# Test config:"
-echo "#  DOCKER_UUID=${DOCKER_UUID}"
-echo "#  DOCKER_URL=${DOCKER_URL}"
+echo "#  DOCKER_ADMIN_URL=${DOCKER_ADMIN_URL}"
 echo "#  DOCKER_AVAILABLE_CLI_VERSIONS=${DOCKER_AVAILABLE_CLI_VERSIONS}"
 echo "#  DOCKER_CLI_VERSIONS=${DOCKER_CLI_VERSIONS}"
+echo "#  DOCKER_URL=${DOCKER_URL}"
+echo "#  DOCKER_UUID=${DOCKER_UUID}"
 echo "#  FWAPI_URL=${FWAPI_URL}"
+echo "#  NFS_VOLUMES_ENABLED=${NFS_VOLUMES_ENABLED}"
 echo "#  PAPI_URL=${PAPI_URL}"
-echo "#  VMAPI_URL=${VMAPI_URL}"
 echo "#  SAPI_URL=${SAPI_URL}"
+echo "#  VMAPI_URL=${VMAPI_URL}"
-- 
2.21.0

