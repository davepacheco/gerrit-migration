commit 18f73900ffea1a8e72a62ee8069e6815eeb65941 (refs/changes/55/5355/1)
Author: Mike Gerdts <mike.gerdts@joyent.com>
Date:   2019-01-11T16:57:44+00:00 (9 months ago)
    
    OS-7497 bhyve disks need uuids

diff --git a/src/Makefile b/src/Makefile
index 852e7bf8..af71175b 100644
--- a/src/Makefile
+++ b/src/Makefile
@@ -111,6 +111,7 @@ JS_CHECK_TARGETS=\
 	vm/tests/test-create-filesystems.js \
 	vm/tests/test-create.js \
 	vm/tests/test-defaults.js \
+	vm/tests/test-disk-uuid.js \
 	vm/tests/test-docker.js \
 	vm/tests/test-firewall.js \
 	vm/tests/test-fswatcher.js \
diff --git a/src/vm/man/vmadm.1m.md b/src/vm/man/vmadm.1m.md
index ecac3e4d..9d5aad10 100644
--- a/src/vm/man/vmadm.1m.md
+++ b/src/vm/man/vmadm.1m.md
@@ -1019,6 +1019,17 @@ tab-complete UUIDs rather than having to type them out for every command.
 
         NOTE: SDC does not support any pool name other than the default 'zones'.
 
+    disks.*.uuid:
+
+        A UUID that may be used to uniquely identify this disk.
+
+        type: uuid
+        vmtype: bhyve
+        listable: yes (see above)
+        create: yes
+        update: yes
+        default: Assigned while adding the disk or at next `vmadm start`.
+
     disk_driver:
 
         This specifies the default values for disks.*.model for disks attached
diff --git a/src/vm/node_modules/VM.js b/src/vm/node_modules/VM.js
index a9f863ee..e2596b23 100644
--- a/src/vm/node_modules/VM.js
+++ b/src/vm/node_modules/VM.js
@@ -59,7 +59,7 @@
  * INFO_TYPES - list of supported types for the info command
  * SYSRQ_TYPES - list of supported requests for sysrq
  *
- * IMPORTANT: Per OS-2427, this file is for the exlusive use of vmadmd and
+ * IMPORTANT: Per OS-2427, this file is for the exclusive use of vmadmd and
  *            vmadm. If you are using this and you are not one of those two,
  *            please switch to calling vmadm instead.
  *
@@ -1452,7 +1452,7 @@ function setQuota(opts, callback)
  *   - Set quota on zone root dataset, based on whether flexible_disk_size is
  *     set. If flexible_disk_size is set, quota becomes the root dataset's
  *     refquota + sum(auto_refres - volsize) + flexible_disk_size. Otherwise,
- *     quota becomes the root dataset's refquota + sum(auto_frefres).
+ *     quota becomes the root dataset's refquota + sum(auto_refres).
  */
 
 function setQuotaBhyve(opts, callback) {
@@ -2440,7 +2440,7 @@ function createVolume(volume, opts, callback)
                 var args;
 
                 // This volume is from a template/dataset/image so we create it
-                // as a clone of a the @final snapshot on the original.  We
+                // as a clone of the @final snapshot on the original.  We
                 // already set 'snapshot' to the correct location above.
                 args = ['clone', '-F'];
                 if (volume.hasOwnProperty('compression')) {
@@ -4428,7 +4428,7 @@ exports.waitForZoneState = function (payload, state, options, callback)
             cb();
         }, function (_, cb) {
             if (!vs) {
-                // alreday in the correct state; we're done
+                // already in the correct state; we're done
                 cb();
                 return;
             }
@@ -4657,7 +4657,7 @@ function fixPayloadMemory(payload, vmobj, log)
     }
 
     // now that we've possibly adjusted target values, lower/raise values to
-    // satisify max/min.
+    // satisfy max/min.
 
     min_overhead = BRAND_OPTIONS[brand].features.min_memory_overhead;
     if (min_overhead) {
@@ -6043,6 +6043,11 @@ function buildDiskZonecfg(vmobj, payload)
                 + disk.pci_slot + '")\n';
         }
 
+        if (disk.hasOwnProperty('uuid')) {
+            zcfg = zcfg + 'add property (name=uuid, value="'
+                + disk.uuid + '")\n';
+        }
+
         zcfg = zcfg + 'end\n';
     }
 
@@ -6440,7 +6445,7 @@ function buildFilesystemZonecfg(vmobj, payload, options)
  *   -t-             n      y       n
  *   ---             n      n       n
  *
- * There are 6 cases here we deail with:
+ * There are 6 cases here we deal with:
  *
  *   - tty and json-file
  *       - write to the log in the GZ, zlog-mode=gt-
@@ -6947,7 +6952,7 @@ function buildZonecfgUpdate(vmobj, payload, log)
     }
 
     // We only get here with a 'datasets' member on payload if we're doing a
-    // recive. So in that case we always want to add to zonecfg input.
+    // receive. So in that case we always want to add to zonecfg input.
     if (payload.hasOwnProperty('datasets')) {
         zcfg = zcfg + buildDatasetZonecfg(vmobj, payload);
     }
@@ -9458,13 +9463,22 @@ function normalizeNics(payload, vmobj)
     }
 }
 
+/*
+ * Called during create and update to fix up various disk properties for the
+ * disks that are being added.  If conflicts are found, an Error will be raised.
+ */
+function normalizeDisks(payload, vmobj, log) {
+    normalizeDiskSlots(payload, vmobj, log);
+    normalizeDiskUuids(payload, vmobj, log);
+}
+
 /*
  * Ensure all disks being added have pci_slot set. This is called during create
  * and update.  During create, vmobj is null.
  *
  * This may throw an Error that was originally thrown by assignBhyvePCIslots.
  */
-function normalizeDisks(payload, vmobj, log)
+function normalizeDiskSlots(payload, vmobj, log)
 {
     var brand = (vmobj && vmobj.brand) ? vmobj.brand : payload.brand;
     if (brand !== 'bhyve' || !payload.hasOwnProperty('add_disks')) {
@@ -9481,7 +9495,7 @@ function normalizeDisks(payload, vmobj, log)
     // created instance may lack static assignments, but those will get fixed
     // the next time that VM.start() is called. legacy_compat is used in this
     // pass so that the right slots are set aside for use by the bhyve brand
-    // boot hook. See block comment above assignBhyvePCISlots().
+    // boot hook. See block comment above assignBhyvePCIslots().
     if (vmobj) {
         opts.disks = vmobj.disks;
         opts.legacy_compat = true;
@@ -9490,17 +9504,61 @@ function normalizeDisks(payload, vmobj, log)
 
     // Now see what needs to be fixed in the add_disks array.
     log.debug({add_disks: payload.add_disks},
-        'normalizeDisks start payload.add_disks');
+        'normalizeDiskSlots start payload.add_disks');
     opts.disks = payload.add_disks;
     opts.legacy_compat = false;
     var assignments = assignBhyvePCIslots(opts);
     assignments.forEach(function _eachAssignment(assignment) {
-        log.debug({assignment: assignment}, 'normalizeDisks updating payload');
+        log.debug({assignment: assignment},
+            'normalizeDiskSlots updating payload');
         assignment.disk.pci_slot = assignment.pci_slot;
     });
 
     log.debug({add_disks: payload.add_disks},
-        'normalizeDisks end payload.add_disks');
+        'normalizeDiskSlots end payload.add_disks');
+}
+
+/*
+ * Ensures each bhyve disk has a unique UUID.  Assigns UUIDs to
+ * payload.add_disks.* if they are missing and raises an Error if a duplicate is
+ * found.
+ */
+function normalizeDiskUuids(payload, vmobj, log)
+{
+    var uuids = [];
+    var disk, i;
+
+    if (!payload.hasOwnProperty('add_disks')) {
+        return;
+    }
+    var brand = (vmobj && vmobj.brand) ? vmobj.brand : payload.brand;
+    if (brand !== 'bhyve' || !payload.hasOwnProperty('add_disks')) {
+        return;
+    }
+
+    // Find uuids in use by existing disks
+    if (vmobj && vmobj.hasOwnProperty('disks')) {
+        uuids = vmobj.disks.filter(function diskFilter(_disk) {
+            return _disk.hasOwnProperty('uuid');
+        }).map(function diskToUuid(_disk) {
+            return _disk.uuid;
+        });
+    }
+
+    // Ensure presence and uniqueness of new disk uuids.
+    for (i = 0; i < payload.add_disks.length; i++) {
+        disk = payload.add_disks[i];
+        if (disk.hasOwnProperty('uuid')) {
+            if (uuids.indexOf(disk.uuid) !== -1) {
+                throw new Error('duplicate disk uuid ' + disk.uuid);
+            }
+        } else {
+            disk.uuid = libuuid.create();
+            assert(uuids.indexOf(disk.uuid) === -1,
+                'automatically assigned uuid must be unique');
+            uuids.push(disk.uuid);
+        }
+    }
 }
 
 /*
@@ -9854,7 +9912,7 @@ function normalizePayload(payload, vmobj, log, callback)
         // this will ensure we've got a MAC, etc.
         normalizeNics(payload, vmobj);
 
-        // ensure that pci slots are assigned for bhyve disks
+        // ensure that optional properties (pci_slot, uuid) are set on disks
         try {
             normalizeDisks(payload, vmobj, log);
         }
@@ -10221,7 +10279,7 @@ function receiveVM(json, log, callback)
     payload.transition =
         {'transition': 'receiving', 'target': 'stopped', 'timeout': 86400};
 
-    // We delete tags and metadata here becasue this exists in the root
+    // We delete tags and metadata here because this exists in the root
     // dataset which we will be copying, so it would be duplicated here.
     delete payload.customer_metadata;
     delete payload.internal_metadata;
@@ -10920,7 +10978,7 @@ exports.reprovision = function (uuid, payload, options, callback)
                 }
 
                 if (fds.stderr.match(/dataset does not exist/)) {
-                    // we'll use a different one. (falls throught to next func)
+                    // we'll use a different one. (falls through to next func)
                     cb();
                 } else {
                     cb(err);
@@ -12112,7 +12170,7 @@ function deleteZone(uuid, log, callback)
             // delete the incoming payload if it exists
             fs.unlink('/etc/zones/' + vmobj.uuid + '-receiving.json',
                 function (e) {
-                    // we can't do anyhing if this fails other than log
+                    // we can't do anything if this fails other than log
                     if (e && e.code !== 'ENOENT') {
                         log.warn(e, 'Failed to delete ' + vmobj.uuid
                             + '-receiving.json (' + e.code + '): ' + e.message);
@@ -12757,7 +12815,7 @@ function startVM(vmobj, extra, log, callback)
                 && nic.gateway == defaultgw) {
 
                 /*
-                 * XXX this exists here for backward compatibilty.  New VMs
+                 * XXX this exists here for backward compatibility.  New VMs
                  *     and old VMs that are upgraded should not use
                  *     default_gateway.  When we've implemented autoupgrade
                  *     this block (and all reference to default_gateway)
@@ -13709,7 +13767,7 @@ exports.create_snapshot = function (uuid, snapname, options, callback)
                     log.error({err: e}, 'unable to mount snapshot: '
                         + e.message);
                 }
-                // not fatal becase snapshot was already created.
+                // not fatal because snapshot was already created.
                 cb();
             });
         }, function (cb) {
@@ -14078,11 +14136,11 @@ function setDockerRestartOpts(uuid, options, callback) {
  * The allocation algorithm varies on whether opts.legacy_compat is true. The
  * legacy compat mode is used by VM.start() to persist disks that were allocated
  * prior to the static PCI slot assignment feature's availability. The legacy
- * algorithm here matches the algorithm used by the the bhyve brand boot hook
+ * algorithm here matches the algorithm used by the bhyve brand boot hook
  * (/usr/lib/brand/bhyve/boot) when disks don't have pci_slot specified. Because
  * bhyve instances start at global zone boot by `zoneadm boot` issued by
  * svc:/system/zones:default, an instance that was provisioned prior to the
- * introduction of static slot assignments will not get static assignements
+ * introduction of static slot assignments will not get static assignments
  * until such a time as it is stopped then started.
  *
  * The modern allocation scheme puts the boot disk at 0:4:0 and data disks at
@@ -14093,8 +14151,8 @@ function setDockerRestartOpts(uuid, options, callback) {
  * of the first data device would render any remaining data devices unusable.
  *
  * In the case where new devices are added to an instance that has a disk at
- * 0:5:0, VM.update() will use legacy compatibilty while figuring out which
- * slots are currently in use and will not use legacy compatibilty while
+ * 0:5:0, VM.update() will use legacy compatibility while figuring out which
+ * slots are currently in use and will not use legacy compatibility while
  * assigning new disks.  This will make it so that the original boot disk will
  * stay at 0:4:0, the original data disk will stay at 0:5:0, and the new data
  * disk will be found at 0:4:1.  VM.update() can only add disks while the
@@ -14260,7 +14318,7 @@ function assignBhyvePCIslots(opts)
 }
 
 /*
- * Updates disks.*.pci_slot for each disk that has no static assignement.
+ * Updates disks.*.pci_slot for each disk that has no static assignment.
  * Expected to be called during VM.start(). If this called during a provisioning
  * boot, it is expected that normalizeDisks() will have already been called
  * during VM.create().
@@ -14298,6 +14356,36 @@ function updateBhyvePCIslots(opts, cb)
     VM.update(vmobj.uuid, {update_disks: updates}, {log: log}, cb);
 }
 
+function updateBhyveDiskUuids(opts, cb)
+{
+    assert.object(opts);
+    assert.object(opts.vmobj);
+    assert.object(opts.log);
+
+    var vmobj = opts.vmobj;
+    var log = opts.log;
+
+    if (vmobj.brand !== 'bhyve') {
+        cb();
+        return;
+    }
+
+    var ud = [];
+    var disk, i;
+    for (i = 0; i < vmobj.disks.length; i++) {
+        disk = vmobj.disks[i];
+        if (!disk.hasOwnProperty('uuid')) {
+            ud.push({path: disk.path, uuid: libuuid.create()});
+        }
+    }
+    if (ud.length === 0) {
+        cb();
+        return;
+    }
+    log.debug({update_disks: ud}, 'updateBhyveDiskUuids assigning uuids');
+    VM.update(vmobj.uuid, {update_disks: ud}, {log: log}, cb);
+}
+
 exports.start = function (uuid, extra, options, callback)
 {
     var load_fields;
@@ -14504,6 +14592,8 @@ exports.start = function (uuid, extra, options, callback)
         }, function _persistBhyveDevs(cb) {
             updateBhyvePCIslots({vmobj: vmobj, log: log, legacy_compat: true},
                 cb);
+        }, function _persistBhyveDiskUuids(cb) {
+            updateBhyveDiskUuids({vmobj: vmobj, log: log}, cb);
         }, function _start(cb) {
             var err;
             var vm_type = BRAND_OPTIONS[vmobj.brand].features.type;
@@ -14713,7 +14803,7 @@ function resizeDisks(vmobj, updates, log, callback)
          *
          * Gets the size of the refreservation change that will happen as a
          * result of resizing a volume. This is used to determine how much the
-         * quota and/or refereservation on the zone's dataset (zonepath dataset)
+         * quota and/or refreservation on the zone's dataset (zonepath dataset)
          * will need to change to allow the resize.
          *
          * opts.volname The name of the zfs volume that is being resized.
@@ -16252,7 +16342,7 @@ exports.update = function (uuid, payload, options, callback)
                 re = /^(set_|remove_)?(customer_metadata|internal_metadata|tags|routes)$/;
                 if (key.match(re)) {
                     /*
-                     * Metadata blocking logic is handleded in updateRoutes and
+                     * Metadata blocking logic is handled in updateRoutes and
                      * updateMetadata - so just short-circuit here.
                      */
                     return;
@@ -17440,7 +17530,7 @@ exports.stop = function (uuid, options, callback)
     async.series([
         function (cb) {
             /*
-             * Create the vminfod event stream here that wil be used below to
+             * Create the vminfod event stream here that will be used below to
              * block on the VM transitioning into the "stopped" state.  Also,
              * use the "ready" event to get the vmobj for the VM uuid given
              */
diff --git a/src/vm/node_modules/proptable.js b/src/vm/node_modules/proptable.js
index b4d191d1..18d6bedf 100644
--- a/src/vm/node_modules/proptable.js
+++ b/src/vm/node_modules/proptable.js
@@ -697,6 +697,15 @@ exports.properties = {
             fields: ['volsize'],
             types: ['volume']
         }
+    }, 'disks.*.uuid': {
+        payload: {
+            allowed: {
+                'bhyve': ['add', 'update', 'receive']
+            },
+            type: 'uuid'
+        },
+        updatable: true,
+        zonexml: 'zone.device.net-attr.uuid'
     }, 'disks.*.zfs_filesystem': {
         payload: {
             allowed: {
diff --git a/src/vm/tests/test-disk-uuid.js b/src/vm/tests/test-disk-uuid.js
new file mode 100644
index 00000000..d82996ae
--- /dev/null
+++ b/src/vm/tests/test-disk-uuid.js
@@ -0,0 +1,279 @@
+/*
+ * CDDL HEADER START
+ *
+ * The contents of this file are subject to the terms of the
+ * Common Development and Distribution License, Version 1.0 only
+ * (the "License").  You may not use this file except in compliance
+ * with the License.
+ *
+ * You can obtain a copy of the license at http://smartos.org/CDDL
+ *
+ * See the License for the specific language governing permissions
+ * and limitations under the License.
+ *
+ * When distributing Covered Code, include this CDDL HEADER in each
+ * file.
+ *
+ * If applicable, add the following below this CDDL HEADER, with the
+ * fields enclosed by brackets "[]" replaced with your own identifying
+ * information: Portions Copyright [yyyy] [name of copyright owner]
+ *
+ * CDDL HEADER END
+ *
+ * Copyright (c) 2019, Joyent, Inc.
+ *
+ */
+
+var assert = require('/usr/node/node_modules/assert-plus');
+var common = require('./common');
+var jsprim = require('/usr/vm/node_modules/jsprim');
+var properties = require('/usr/vm/node_modules/props');
+var sprintf = require('/usr/node/node_modules/sprintf').sprintf;
+var vasync = require('/usr/vm/node_modules/vasync');
+var VM = require('/usr/vm/node_modules/VM');
+var vminfod = require('/usr/vm/node_modules/vminfod/client');
+var vmtest = require('../common/vmtest.js');
+var zonecfg = require('/usr/vm/node_modules/zonecfg');
+
+// this puts test stuff in global, so we need to tell jsl about that:
+/* jsl:import ../node_modules/nodeunit-plus/index.js */
+require('nodeunit-plus');
+
+VM.loglevel = 'DEBUG';
+
+var log = createLogger();
+
+/*
+ * nodeunit-plus executes the callback specified by before() before each test
+ * is run and executes the callback specified by after() after each test is run.
+ * These callbacks ensure that vmobj is initialized to undefined prior to each
+ * test and that any VM that was created by the test is deleted after the test
+ * completes.
+ *
+ * Tests that create a VM should be setting vmobj so that the after() hook can
+ * clean up the VM when the test finishes or gives up.  If a test uses vmobj
+ * then deletes the VM on its own, it should set vmobj to undefined.
+ */
+var vmobj;
+
+before(function _before(cb) {
+    assert.func(cb, 'cb');
+    vmobj = undefined;
+    cb();
+});
+
+after(function _after(cb) {
+    assert.func(cb, 'cb');
+    if (!vmobj) {
+        cb();
+        return;
+    }
+    VM.delete(vmobj.uuid, {}, function _delete_cb(err) {
+        if (err) {
+            console.log(sprintf('Could not delete vm %s: %s', vmobj.uuid,
+                err.message));
+        }
+        vmobj = undefined;
+        cb();
+    });
+});
+
+function createVM(t, payload, next)
+{
+    VM.create(payload, function _create_cb(err, obj) {
+        if (err) {
+            t.ok(false, 'error creating VM: ' + err);
+        } else {
+            t.ok(true, 'VM created with uuid ' + obj.uuid);
+        }
+        vmobj = obj;
+        next(err, t);
+    });
+}
+
+function startVM(t, next)
+{
+    VM.start(vmobj.uuid, {}, function _start_cb(err) {
+        if (err) {
+            t.ok(false, 'error starting VM: ' + err);
+            next(err);
+            return;
+        }
+        VM.waitForZoneState(vmobj, 'running', function waitRunning(_err) {
+            common.ifError(t, _err, 'zone start');
+            next(_err, t);
+        });
+    });
+}
+
+function stopVM(t, next)
+{
+    VM.stop(vmobj.uuid, {force: true}, function _stop_cb(err) {
+        if (err) {
+            t.ok(false, 'error stoping VM: ' + err);
+            next(err);
+            return;
+        }
+        VM.waitForZoneState(vmobj, 'installed', function waitInstalled(_err) {
+            common.ifError(t, _err, 'zone stop');
+            next(_err, t);
+        });
+    });
+}
+
+function loadVM(t, next)
+{
+    VM.load(vmobj.uuid, function _load_cb(err, obj) {
+        if (err) {
+            t.ok(false, 'error loading VM: ' + err);
+        } else {
+            t.ok(true, sprintf('VM loaded uuid %s state %s zone_state %s',
+                obj.uuid, obj.state, obj.zone_state));
+            vmobj = obj;
+        }
+        next(err, t);
+    });
+}
+
+function checkRunning(t, next)
+{
+    t.equal(vmobj.state, 'running', 'VM is running');
+    next(null, t);
+}
+
+function checkStopped(t, next)
+{
+    t.equal(vmobj.state, 'stopped', 'VM is stopped');
+    next(null, t);
+}
+
+function verifyDisksHaveUuids(t, next)
+{
+    var i, disk;
+    for (i = 0; i < vmobj.disks.length; i++) {
+        disk = vmobj.disks[i];
+        if (!disk.hasOwnProperty('uuid')) {
+            t.ok(false, 'disk missing uuid: ' + JSON.stringify(disk));
+        } else {
+            t.equal(disk.uuid.length, 36, 'disk has valid uuid: ' + disk.uuid);
+        }
+    }
+    next(null, t);
+}
+
+function verifyDisksHaveNoUuids(t, next)
+{
+    var i, disk;
+    for (i = 0; i < vmobj.disks.length; i++) {
+        disk = vmobj.disks[i];
+        t.equal(disk.uuid, undefined,
+            'disk ' + disk.path + ' should not have uuid');
+    }
+    next(null, t);
+}
+
+function removeDiskUuids(t, next)
+{
+    var vs = new vminfod.VminfodEventStream({
+        name: 'test-disk-uuid',
+        log: log
+    });
+    var cancelFn;
+    var zcfg = '';
+    var changes = [];
+    var i;
+
+    assert.notEqual(vmobj.disks.length, 0, 'must have disks');
+    for (i = 0; i < vmobj.disks.length; i++) {
+        var disk = vmobj.disks[i];
+        assert.uuid(disk.uuid, 'disk.uuid for ' + disk.path);
+        zcfg += sprintf('select device match=%s;'
+            + 'remove property (name=uuid,value="%s"); end;\n',
+            disk.path, disk.uuid);
+        changes.push({
+            path: ['disks', null, 'uuid'],
+            action: 'removed',
+            oldValue: disk.uuid
+        });
+    }
+
+    vs.once('ready', function () {
+        vasync.parallel({funcs: [
+            function _watcher(cb) {
+                var obj = {
+                    uuid: vmobj.uuid
+                };
+                var _opts = {
+                    timeout: 5000,
+                    catchErrors: true,
+                    teardown: true
+                };
+                cancelFn = vs.watchForChanges(obj, changes, _opts, cb);
+            },
+            function _zonecfg(cb) {
+                zonecfg(vmobj.uuid, [], {log: log, stdin: zcfg},
+                    function (err, fds) {
+
+                    common.ifError(t, err, 'remmove disk uuids');
+                    if (err) {
+                        cancelFn();
+                        cb(err);
+                        return;
+                    }
+                    cb();
+                });
+            }
+        ]}, function _done(err, results) {
+            vs.stop();
+            next(null, t);
+        });
+    });
+}
+
+var base_payload = {
+    alias: 'test-disk-uuid-' + process.pid,
+    brand: 'bhyve',
+    do_not_inventory: true,
+    autoboot: false,
+    ram: 256,
+    vcpus: 1,
+    disks: [
+        {
+            image_uuid: vmtest.CURRENT_BHYVE_CENTOS_UUID,
+            boot: true,
+            model: 'virtio'
+        },
+        {
+            size: 100,
+            model: 'virtio'
+        }
+    ]
+};
+
+test('new disks get uuids', function test1(t) {
+    var payload = jsprim.deepCopy(base_payload);
+
+    vasync.waterfall([
+        function _create(next) {
+            createVM(t, payload, next);
+        },
+        loadVM,
+        checkStopped,
+        verifyDisksHaveUuids,
+        startVM,
+        loadVM,
+        checkRunning,
+        verifyDisksHaveUuids,
+        stopVM,
+        loadVM,
+        checkStopped,
+        removeDiskUuids,
+        loadVM,
+        verifyDisksHaveNoUuids,
+        startVM,
+        loadVM,
+        verifyDisksHaveUuids
+        ], function _done() {
+            t.end();
+        });
+});
