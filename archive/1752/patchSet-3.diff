commit 7f69cb5b50997545e46bf2c147a114bd89382bfa (refs/changes/52/1752/3)
Author: Jason King <jason.king@joyent.com>
Date:   2017-04-13T10:05:25-05:00 (2 years, 6 months ago)
    
    TOOLS-1653 triton development guide should describe where to find ./tools/setup-remote-build-zone.sh
    TOOLS-1747 Triton development guide shouldn't use sdc-* command in examples

diff --git a/docs/developer-guide/building.md b/docs/developer-guide/building.md
index cc2cf04..7036caf 100644
--- a/docs/developer-guide/building.md
+++ b/docs/developer-guide/building.md
@@ -1,18 +1,18 @@
-# Building SDC
+# Building Triton
 
 ## Prerequisites
 
  * we assume you have npm/node installed on your workstation
  * we assume you have git installed on your workstation
  * we assume you have json (npm install -g json) installed on your workstation
- * we assume you understand the basics of SDC, if not please start with [the
-   SmartDataCenter README](https://github.com/joyent/sdc#readme)
+ * we assume you understand the basics of Triton, if not please start with [the
+   Triton DataCenter README](https://github.com/joyent/triton#readme)
  * we assume you have your SSH keys loaded in your ssh-agent when connecting
    to build zones via SSH.
 
 ## Decisions to Make
 
-To build SDC components you to decide first make a couple choices:
+To build Triton components you to decide first make a couple choices:
 
  * which components are you going to build?
  * where are you going to build components?
@@ -21,31 +21,42 @@ To build SDC components you to decide first make a couple choices:
 
 If you are building any of the following components:
 
+ * cns
  * manta-manatee
  * sdc-manatee
  * electric-moray
  * platform
 
-you will need a sdc-multiarch 13.3.1 build zone. For any other components you
-will need an sdc-smartos 1.6.3. If you want to build *all* components, you'll
-need both.
+you will need an sdc-multiarch 13.3.1 build zone. For the following components,
+you will need an sdc-minimal-multiarch-lts 15.4.1 zone:
 
-### Where to build
+ * cloudapi
+ * cmon
+ * docker
+ * imgapi
+ * nfsserver
+ * papi
+ * vmapi
+ * volapi
+
+For any other components you will need an sdc-smartos 1.6.3 zone. If you want
+to build *all* components, you'll need all three.  The targets.json file
+(created by targets.json.in) in MG contain the definitive image UUIDs.
+The ./tools/target-\*.sh scripts in MG (see "Clone MG (mountain-gorilla)") use
+this file to output the list of components for each respective version.
 
-If you have an account in the Joyent Public Cloud you can build all components
-(except "platform", see "Building the Platform") required to create a working
-SDC headnode in the JPC and have the outputs pushed to Joyent's Manta. This is
-the easiest method for building, but will create several zones (one for each
-zone image built) and store results in Manta, both of which will have billing
-consequences.
+### Where to build
 
-If instead you would like to build components in a local SDC install or in a
-downloaded CoaL image, you will have some additional setup to do before you can
-build. In this case, see the section "Setting up an SDC for builds".
+Currently, the easiest method for building Triton is to do so on a local Triton
+DataCenter or CoaL install.  Easily building the images in the Triton Public
+Cloud (TPC) depends on the work described in [RFD
+46](https://github.com/joyent/rfd/blob/mastert/rfc/0046/README.md) to be
+complete.
 
-NOTE: Even if you do not use JPC and output your builds to Manta, you will still
-need an account in JPC in order to do a build. This is because dependent
-components will still need to be downloaded from Manta.
+NOTE: Even if you do not use TPC and output your builds to Manta, you will still
+need a Manta account (either local install or TPC) in order to do a build.
+The build process will store built components into Manta and will utilize Manta
+to retrieve dependent components during the build process.
 
 ## Setting up your workspace for driving builds
 
@@ -77,7 +88,7 @@ export PATH=${PATH}:$(pwd)/node_modules/smartdc/bin
 ### Setup your environment variables
 
 This is one of the most critical steps. The environment variables define which
-SDC/manta target/credentials will be used for the rest of setup. You must set
+Triton/manta target/credentials will be used for the rest of setup. You must set
 at least:
 
 ```
@@ -89,40 +100,97 @@ export SDC_KEY_ID=<KEY>
 export SDC_URL=https://us-east-1.api.joyentcloud.com
 ```
 
-where <USER> is the name of the JPC/SDC user you want to build with, and <KEY>
-is the SSH fingerprint of the SSH key that you've added for your user.
+where <USER> is the name of the TPC/Triton user you want to build with, and
+<KEY> is the SSH fingerprint of the SSH key that you've added for your user.
 
-If you're using CoaL and using the default self-signed certificates for cloudapi
-you will also want to:
+The most convenient method of setting the SDC\_\* variables is to use a
+triton profile (note: you will still need to set the MANTA\_\* variables).
+If you do not already have a profile created for the Triton instance you wish
+to use, run the following:
 
 ```
-export SDC_TESTING=1
+$ triton profile create
+A profile name. A short string to identify this profile to the `triton` command.
+name: us-east-1
+
+
+The CloudAPI endpoint URL.
+url: https://us-east-1.api.joyent.com
+
+
+Your account login name.
+account: jsmith
+
+
+The fingerprint of the SSH key you want to use to authenticate with CloudAPI.
+Specify the fingerprint or the index of one of the found keys in the list
+below. If the key you want to use is not listed, make sure it is either saved
+in your SSH keys directory (~/.ssh) or loaded into your SSH agent.
+
+1. Fingerprint "e0:c8:63:2c:43:82:3e:97:c0:4d:97:0a:8e:d7:16:21" (2048-bit RSA)
+   - in agent: /Users/JohnSmith/.ssh/id_rsa
+   - in homedir (locked): $HOME/.ssh/id_rsa (comment "my ssh key")
+
+2. Fingerprint "32:2e:55:31:91:1d:d8:ba:39:21:2f:45:2d:67:7c:e5" (256-bit ECDSA)
+   - in agent: /Users/JohnSmith/.ssh/id_ecdsa
+   - in homedir (locked): $HOME/.ssh/id_ecdsa (comment "my ssh key")
+
+keyId: 1
+Using key 1: e0:c8:63:2c:43:82:3e:97:c0:4d:97:0a:8e:d7:16:21
+
+Saved profile "us-east-1".
+
+
+# Docker setup
+
+This section will setup authentication to Triton DataCenter's Docker endpoint
+using your account and key information specified above. This is only required
+if you intend to use `docker` with this profile.
+
+WARNING: Docker uses authentication via client TLS certificates that do not
+support encrypted (passphrase protected) keys or SSH agents. If you continue,
+this profile setup will attempt to write a copy of your SSH private key
+formatted as an unencrypted TLS certificate in "~/.triton/docker" for use by
+the Docker client.
+
+Continue? [y/n] n
+Skipping Docker setup (you can run "triton profile docker-setup" later).
+$
+```
+
+Since a number of the mountain gorilla components still use the older sdc\*
+commands which require the SDC\_\* environment variables, run the following
+command (substituting the appropriate triton profile name) to set all the needed
+values:
+
+```
+eval "$(triton env us-east-1)"
 ```
 
-otherwise you'll get errors like:
+If you're using CoaL and using the default self-signed certificates for cloudapi
+you will also want to:
 
 ```
-sdc-listpackages: error: undefined
+export TRITON_TLS_INSECURE=1
 ```
 
-It's possible to use different <USER> values for MANTA_USER and SDC_ACCOUNT if
-you've pointed these at different SDC standups. In that case zones will be
-created using the SDC_ACCOUNT credentials and any files pulled from / pushed to
-Manta will be done using the MANTA_USER's credentials.
+otherwise you may get certificate related errors.
 
-If you're *not* using JPC here, you'll want to change the SDC_URL and MANTA_URL
-above to match your local cloudapi and manta respectively.
+It's possible to use different <USER> values for MANTA\_USER and SDC\_ACCOUNT
+if you've pointed these at different Triton standups. In that case, zones will
+be created using the credentials from the active triton profile and any files
+pulled from / pushed to Manta will be done using the MANTA\_USER's credentials.
 
-NOTE: if your SDC is not yet setup, you need to set SDC_URL *after* setting up
-cloudapi in the next section.
+Be sure to validate that the SDC\_URL and MANTA\_URL values match your local
+cloudapi and manta respectively.
 
+NOTE: if your Triton install is not yet setup, you need to set SDC\_URL *after*
+setting up cloudapi in the next section.
 
-## Setting up an SDC for builds
 
-NOTE: skip this section (move on to "Setting up the build environment") if
-you're going to build in the JPC.
+## Setting up Triton for builds
 
-This section assumes that you have a local SDC/CoaL setup and have access to
+This section assumes that you have a local Triton/CoaL setup and have access to
 the global zone.
 
 ### Setting up cloudapi
@@ -161,7 +229,7 @@ Firewall rules can also be setup after the build zone(s) are created, but before
 the first build.
 
 Record the external IP address for imgapi. You'll need this later to set
-SDC_IMGAPI_URL.
+SDC\_IMGAPI\_URL.
 
 For your convienence, here are commands for the previous steps if you are
 running in COAL:
@@ -178,9 +246,9 @@ $ sdc-vmapi /vms/$(vmadm lookup alias=~imgapi) | json -H nics | \
 10.88.88.3
 ```
 
-If you're using your own SDC and do not have imgapi connected to a Manta (eg.
-you're using CoaL) you'll also need to run the following from the GZ of the
-headnode:
+If you're using your own Triton install and do not have imgapi connected to a
+Manta (eg.  you're using CoaL) you'll also need to run the following from the
+GZ of the headnode:
 
 ```
 echo '{"metadata": {"IMGAPI_ALLOW_LOCAL_CREATE_IMAGE_FROM_VM": true}}' \
@@ -196,7 +264,23 @@ images you need are:
 
  * fd2cc906-8938-11e3-beab-4359c665ac99 / sdc-smartos 1.6.3
  * b4bdc598-8939-11e3-bea4-8341f6861379 / sdc-multiarch 13.3.1
+ * 18b094b0-eb01-11e5-80c1-175dac7ddf02 / sdc-minimal-multiarch-lts 15.4.1
+
+### Adding build packages
+
+The build zone needs a package sufficiently large enough to allow the builds
+to complete.  If using a CoaL install, the easiest way is to import the
+sample packages and use the sample-4G package.  To import the sample packages,
+run the following command from the headnode:
 
+```
+sdcadm post-setup dev-sample-data
+```
+
+Optionally, if present, the sdc\_4096 package will also work fine, however
+the Triton user doing the build must be given access to the package before
+it can be used to create an instance.  Consult the Triton documentation for
+information on how to do this.
 
 ## Setting up the build environment(s)
 
@@ -206,10 +290,10 @@ the creation of the required zones.
 
 ### Common steps to creating any build zone
 
-Before you continue, ensure that whatever user you're going to use (whether your
-personal account in JPC or 'admin' or other user in your local SDC/CoaL) has
-your SSH keys added to it. This is important as these instructions will have
-you running sdc-* commands and manta commands which will need these credentials.
+Before you continue, ensure that whatever user you're going to use (whether
+'admin' or other user in your local Triton/CoaL) has your SSH keys added to it.
+This is important as these instructions will have you running triton commands
+and manta commands which will need these credentials.
 
 If you don't do this you'll see errors like:
 
@@ -217,64 +301,33 @@ If you don't do this you'll see errors like:
 sdc-listpackages: error (InvalidCredentials): Invalid key d5:19:78:bb:d8:f5:ba:cd:6b:40:96:3f:5a:23:59:a9
 ```
 
-### Find the package uuid for your build package
-
-Whether you're building in JPC or a local SDC you need to find the UUID of the
-package you're going to use to build. To do this (assuming you've setup all the
-variables listed in the previous section correctly) you can run:
-
-```
-sdc-listpackages | json -c "this.name == 'g3-standard-2-smartos'" 0.id
-```
-
-replacing 'g3-standard-2-smartos' with the name of your package if you're not
-using JPC. For CoaL you can use package 'sdc_2048' if you haven't changed the
-default packages. The output of this command will be a UUID which you should
-substitute in commands below. In my case the value was
-'486bb054-6a97-4ba3-97b7-2413d5f8e849' so substitute your own value where you
-see that.  If your SDC_ACCOUNT isn't an administrator, you may not be able
-to find the `sdc_2048` package.  If you are using COAL this is because the
-package's owner_uuid is admin.  To make images public for you ruser to see, run
-this from the global zone:
-
-```
-sdc-papi /packages | json -Ha uuid | while read l; do \
-    echo '{ "owner_uuids": null }' | sdc-papi /packages/$l -X PUT -d@-; done
-```
-
-Note that you probably do *not* want to do this for a public SDC.  You are
-better off creating a new, public package.
-
 ### Creating a sdc-smartos 1.6.3 build zone
 
-To create a sdc-smartos 1.6.3 zone you'll want to run:
+To create a sdc-smartos 1.6.3 zone you'll want to run (using the sample-4G
+package, replace with your own value if using a different package):
 
 ```
-sdc-createmachine \
-    --dataset fd2cc906-8938-11e3-beab-4359c665ac99 \
-    --package 486bb054-6a97-4ba3-97b7-2413d5f8e849 \
-    --name "build-1.6.3"
+triton instance create \
+    --name "build-1.6.3" \
+    -w sdc-smartos@1.6.3 sample-4G
 ```
 
-changing "486bb054-6a97-4ba3-97b7-2413d5f8e849" to match the UUID you got in
-the previous step. The output should be a JSON object. The only field from
-that which you need to keep track of right now is the 'id' field. This is the
-UUID of the new build VM.
-
-You can run:
+This will also wait until the instance has finished provisioning before
+returning.  Alternatively, you can run the following command to check on
+the instance status:
 
 ```
-sdc-getmachine 721182fa-d4f1-61f6-8fae-9875512356e2 | json state
+triton instance get build-1.6.3 | json state
 ```
 
-substituting your own UUID for '721182fa-d4f1-61f6-8fae-9875512356e2' until the
-result is 'running'. Once the VM goes running, you can find its IP using:
+substituting the instance name given during instance creation.  Once the
+instance is running, you can find it's IP using:
 
 ```
-sdc-getmachine 721182fa-d4f1-61f6-8fae-9875512356e2 | json ips
+triton ip build-1.6.3
 ```
 
-(again substituting your own UUID for '721182fa-d4f1-61f6-8fae-9875512356e2').
+(substituting the name you gave the instance during creation).
 
 At this point you'll want to take the IP address which is public (in the case
 there are more than one) and fill that in as <BUILD_ZONE_IP> in the section
@@ -285,21 +338,29 @@ there are more than one) and fill that in as <BUILD_ZONE_IP> in the section
 
 To create a sdc-multiarch 13.3.1 build zone, you should follow the steps for
 creating a sdc-smartos 1.6.3 build zone with the exception that instead of
-dataset "fd2cc906-8938-11e3-beab-4359c665ac99" you should use dataset
-"b4bdc598-8939-11e3-bea4-8341f6861379" and you'll want to use a different name.
-For example:
+sdc-smartos@1.6.3 you should use "sdc-multiarch@13.3.1" as well as a different
+name.  For example:
 
 ```
-sdc-createmachine \
-  --dataset b4bdc598-8939-11e3-bea4-8341f6861379 \
-  --package 486bb054-6a97-4ba3-97b7-2413d5f8e849 \
-  --name "build-13.3.1"
+triton instance create \
+  --name "build-13.3.1" \
+  -w sdc-multiarch@13.3.1 sample-4G
+```
+
+### Creating an sdc-minimal-multiarch-lts 15.4.1 build zone
+
+Similarly, use 'sdc-minimal-multiarch-lts@15.4.1' and a new name.  For example:
+
+```
+triton instance create \
+  --name "build-15.4.1" \
+  -w sdc-minimal-multiarch-lts@15.4.1 sample-4G
 ```
 
 ### Preparing build zone(s) for builds
 
-For each build zone (1.6.3 or 13.3.1) you want to follow the same set of
-instructions. First you want to do:
+For each build zone you want to follow the same set of instructions. First
+from your local MG repo you want to do:
 
 ```
 ./tools/setup-remote-build-zone.sh root@<BUILD_ZONE_IP>
@@ -310,7 +371,7 @@ and generally gets it ready for you to login and start some builds.
 
 ### Cloning MG in your build zone
 
-For each build zone (1.6.3 or 13.3.1) you want to clone MG before you start
+For each build zone you want to clone MG before you start
 building. So SSH to the build zone, then run:
 
 ```
@@ -320,13 +381,13 @@ git clone git@github.com:joyent/mountain-gorilla.git MG && cd MG
 
 ### Add additional environment variables
 
-If you're building in JPC, you can skip this step. If you're building in a local
+If you're building in TPC, you can skip this step. If you're building in a local
 SDC/CoaL setup, you'll probably also need to also set the following at this
 point:
 
 ```
 export SDC_LOCAL_BUILD=1
-export SDC_IMAGE_PACKAGE=sdc_2048
+export SDC_IMAGE_PACKAGE=sample-4G
 export SDC_TESTING=1
 export SDC_IMGAPI_URL=https://10.88.0.15
 ```
@@ -335,13 +396,14 @@ You'll also need to ensure these variables are set at the time of each build.
 
 where:
 
- * SDC_LOCAL_BUILD tells MG that you don't want the build creating zones in JPC
-   or pushing files to JPC's Manta as part of the build process.
- * SDC_IMAGE_PACKAGE is the name of the package you want to use for the build
-   zones. CoaL ships with an sdc_2048 package which should work.
- * SDC_TESTING allows the node-smartdc tools to work even when you've got a
+ * SDC\_LOCAL\_BUILD tells MG that you don't want the build creating zones
+   in JPC or pushing files to JPC's Manta as part of the build process.
+ * SDC\_IMAGE\_PACKAGE is the name of the package you want to use for the build
+   zones. CoaL ships with an sdc\_4096 package which should work, or the
+   sample-4G package if imported will also work.
+ * SDC\_TESTING allows the node-smartdc tools to work even when you've got a
    self-signed SSL certificate.
- * SDC_IMGAPI_URL should be set to https://<IP> where <IP> is the external IP
+ * SDC\_IMGAPI\_URL should be set to https://<IP> where <IP> is the external IP
    you added to imgapi (remembering to add the firewall rules if you have not
    already)
 
@@ -351,39 +413,59 @@ The following commands should be run in the MG directory in the appropriate
 build zone for the target you're building. They should also be run with all the
 environment variables described earlier set.
 
-### Option 1: build a single target, taking dependencies from joyager
+By default, the build process will attempt to fetch the dependencies from
+/${MANTA\_USER}/stor/builds.  The `-d` and `-D` options can be used to specify
+an alternate Manta account and location (respectively).
+
+### Option 1: build a single target, taking dependencies from Joyent\_Dev
 
 Ensure you've set the appropriate environment variables, especially:
 
- * SDC_LOCAL_BUILD if you're building against your own SDC/CoaL
- * SDC_URL set to the proper cloudapi
+ * SDC\_LOCAL\_BUILD if you're building against your own SDC/CoaL
+ * SDC\_URL set to the proper cloudapi
 
 Then to build, run the following in your MG directory in your build zone:
 
 ```
-TARG=<build>; ./configure -t ${TARG} -d joyager -D /stor/builds \
+TARG=<build>; ./configure -t ${TARG} -d Joyent_Dev -D /stor/builds \
     -O /stor/whatever/builds && make ${TARG}_upload_manta
 ```
 
 if we use 'assets' for the build for example:
 
 ```
-TARG=assets; ./configure -t ${TARG} -d joyager -D /stor/builds \
+TARG=assets; ./configure -t ${TARG} -d Joyent_Dev -D /stor/builds \
     -O /stor/whatever/builds && make ${TARG}_upload_manta
 ```
 
 which will:
 
- * download dependencies from /joyager/stor/builds
+ * download dependencies from /Joyent\_Dev/stor/builds
  * create a tarball of the assets bits + dependencies
- * create a SmartOS VM in JPC (using cloudapi)
- * install the tarball of bits into the JPC VM
+ * create a SmartOS VM in the Triton install indicated in SDC\_URL (using
+   cloudapi)
+ * install the tarball of bits into the VM
  * create an image from the VM, sending to Manta
  * download the image from Manta modify the manifest
- * push the build back to manta in ${MANTA_USER}/stor/whatever/builds/assets
+ * push the build back to manta in ${MANTA\_USER}/stor/whatever/builds/assets
+
+Note: most packages include a required platform version for building.  This
+is used so that images are built on the oldest supported (at build time)
+platform.  The `-P` flag can be added to the `./configure` command to bypass
+this requirement.
+
+It is also possible to build a component on an image other than the one
+indicated in the MG ./targets.json file (such as for testing).  To do so,
+add `<TARGET>_IMAGE_UUID=<alternate image UUID>` as a parameter to make.
+For example (including ignoring the platform version check):
 
+```
+TARG=<something>; ./configure -t ${TARG} -d Joyent_Dev -D /stor/builds \
+    -O /stor/whatever/builds -P && make ${TARG}_upload_manta \
+    ${TARG}_IMAGE_UUID=ede31770-e19c-11e5-bb6e-3b7de3cca9ce
+```
 
-### Option 2: build a single target, taking dependencies from joyager but not uploading results
+### Option 2: build a single target, taking dependencies from Joyent\_Dev but not uploading results
 
 To *not* upload results to Manta, follow the same procedure as in "Option 1" but
 change the make target from:
@@ -400,7 +482,6 @@ make ${TARG}
 
 The result will then be in the bits/ directory instead of going to Manta.
 
-
 ### Option 3: build all targets from scratch
 
 If you want to ensure you've built every bit that you're using, you'll want to
@@ -429,6 +510,12 @@ Once this is complete, we can run the same command just with:
 ./tools/targets-13.3.1.sh
 ```
 
+and then
+
+```
+./tools/targets-15.4.1.sh
+```
+
 instead of:
 
 ```
