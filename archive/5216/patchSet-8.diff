commit ce9d9d7bfcf32f4af51e89536859b61747fbfbc7 (refs/changes/16/5216/8)
Author: Patrick Mooney <pmooney@pfmooney.com>
Date:   2019-02-05T00:41:02+00:00 (8 months ago)
    
    OS-7438 refactor bhyve EPT to use generic page tables
    OS-7437 want generic indexed page table system

diff --git a/usr/src/uts/i86pc/Makefile.files b/usr/src/uts/i86pc/Makefile.files
index db2472d773..98c5616f1e 100644
--- a/usr/src/uts/i86pc/Makefile.files
+++ b/usr/src/uts/i86pc/Makefile.files
@@ -23,7 +23,7 @@
 # Copyright (c) 1992, 2010, Oracle and/or its affiliates. All rights reserved.
 #
 # Copyright (c) 2010, Intel Corporation.
-# Copyright 2018 Joyent, Inc.
+# Copyright 2019 Joyent, Inc.
 #
 #	This Makefile defines file modules in the directory uts/i86pc
 #	and its children. These are the source files which are i86pc
@@ -275,8 +275,10 @@ VMM_OBJS += vmm.o \
 	vmcb.o \
 	svm_support.o \
 	amdv.o \
+	gipt.o \
 	vmm_sol_vm.o \
 	vmm_sol_glue.o \
+	vmm_sol_ept.o \
 	vmm_zsd.o
 
 VIONA_OBJS += viona.o
diff --git a/usr/src/uts/i86pc/io/vmm/vm/vm_glue.h b/usr/src/uts/i86pc/io/vmm/vm/vm_glue.h
index c5620a91f6..7ac745f509 100644
--- a/usr/src/uts/i86pc/io/vmm/vm/vm_glue.h
+++ b/usr/src/uts/i86pc/io/vmm/vm/vm_glue.h
@@ -10,7 +10,7 @@
  */
 
 /*
- * Copyright 2018 Joyent, Inc.
+ * Copyright 2019 Joyent, Inc.
  */
 
 #ifndef	_VM_GLUE_
@@ -24,6 +24,7 @@ struct vmspace;
 struct vm_map;
 struct pmap;
 struct vm_object;
+struct vmm_pt_ops;
 
 struct vm_map {
 	struct vmspace *vmm_space;
@@ -36,7 +37,8 @@ struct pmap {
 
 	/* Implementation private */
 	enum pmap_type	pm_type;
-	void		*pm_map;
+	struct vmm_pt_ops *pm_ops;
+	void		*pm_impl;
 };
 
 struct vmspace {
@@ -81,4 +83,16 @@ void *vmspace_find_kva(struct vmspace *, uintptr_t, size_t);
 void vmm_arena_init(void);
 void vmm_arena_fini(void);
 
+struct vmm_pt_ops {
+	void * (*vpo_init)(uint64_t *);
+	void (*vpo_free)(void *);
+	uint64_t (*vpo_wired_cnt)(void *);
+	int (*vpo_is_wired)(void *, uint64_t, uint_t *);
+	int (*vpo_map)(void *, uint64_t, pfn_t, uint_t, uint_t, uint8_t);
+	uint64_t (*vpo_unmap)(void *, uint64_t, uint64_t);
+};
+
+extern struct vmm_pt_ops ept_ops;
+
+
 #endif /* _VM_GLUE_ */
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_sol_ept.c b/usr/src/uts/i86pc/io/vmm/vmm_sol_ept.c
new file mode 100644
index 0000000000..ed3d1a8e12
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/vmm_sol_ept.c
@@ -0,0 +1,268 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2019 Joyent, Inc.
+ */
+
+#include <sys/types.h>
+#include <sys/param.h>
+#include <sys/kmem.h>
+#include <sys/machsystm.h>
+
+#include <sys/gipt.h>
+#include <vm/vm_glue.h>
+
+
+struct ept_map {
+	gipt_map_t	em_gipt;
+	uint64_t	em_wired_page_count;
+};
+typedef struct ept_map ept_map_t;
+
+#define	EPT_LOCK(m)	(&(m)->em_gipt.giptm_lock)
+
+#define	EPT_MAX_LEVELS	4
+
+CTASSERT(EPT_MAX_LEVELS <= GIPT_MAX_LEVELS);
+
+#define	EPT_R		(0x1 << 0)
+#define	EPT_W		(0x1 << 1)
+#define	EPT_X		(0x1 << 2)
+#define	EPT_RWX		(EPT_R | EPT_W | EPT_X)
+#define	EPT_LGPG	(0x1 << 7)
+
+#define	EPT_PA_MASK	(0x000ffffffffff000ull)
+
+CTASSERT(EPT_R == PROT_READ);
+CTASSERT(EPT_W == PROT_WRITE);
+CTASSERT(EPT_X == PROT_EXEC);
+
+
+#define	EPT_PAT(attr)	(((attr) & 0x7) << 3)
+#define	EPT_PADDR(addr)	((addr) & EPT_PA_MASK)
+
+#define	EPT_IS_ABSENT(pte)	(((pte) & EPT_RWX) == 0)
+#define	EPT_PTE_PFN(pte)	mmu_btop(EPT_PADDR(pte))
+#define	EPT_PTE_PROT(pte)	((pte) & EPT_RWX)
+#define	EPT_MAPS_PAGE(pte, lvl)	\
+	(EPT_PTE_PROT(pte) != 0 && (((pte) & EPT_LGPG) != 0 || (lvl) == 0))
+
+/*
+ * Only assign EPT_LGPG for levels higher than 0.  Although this bit is defined
+ * as being ignored at level 0, some versions of VMWare fail to honor this and
+ * report such a PTE as an EPT mis-configuration.
+ */
+#define	EPT_PTE_ASSIGN_PAGE(lvl, pfn, prot, attr)	\
+	(EPT_PADDR(pfn_to_pa(pfn)) |			\
+	(((lvl) != 0) ? EPT_LGPG : 0) |			\
+	EPT_PAT(attr) | ((prot) & EPT_RWX))
+#define	EPT_PTE_ASSIGN_TABLE(pfn)	(EPT_PADDR(pfn_to_pa(pfn)) | EPT_RWX)
+
+
+static gipt_pte_type_t
+ept_pte_type(uint64_t pte, uint_t level)
+{
+	if (EPT_IS_ABSENT(pte)) {
+		return (PTET_EMPTY);
+	} else if (EPT_MAPS_PAGE(pte, level)) {
+		return (PTET_PAGE);
+	} else {
+		return (PTET_TABLE);
+	}
+}
+
+static uint64_t
+ept_pte_map(uint64_t pfn)
+{
+	return (EPT_PTE_ASSIGN_TABLE(pfn));
+}
+
+static void *
+ept_create(uintptr_t *pml4_kaddr)
+{
+	ept_map_t *emap;
+	gipt_map_t *map;
+	gipt_t *root;
+	struct gipt_cbs cbs = {
+		.giptc_pte_type = ept_pte_type,
+		.giptc_pte_map = ept_pte_map,
+	};
+
+	emap = kmem_zalloc(sizeof (*emap), KM_SLEEP);
+	map = &emap->em_gipt;
+	root = gipt_alloc();
+	root->gipt_level = EPT_MAX_LEVELS - 1;
+	gipt_map_init(map, EPT_MAX_LEVELS, GIPT_HASH_SIZE_DEFAULT, &cbs, root);
+
+	*pml4_kaddr = (uintptr_t)root->gipt_kva;
+	return (emap);
+}
+
+static void
+ept_destroy(void *arg)
+{
+	ept_map_t *emap = arg;
+
+	if (emap != NULL) {
+		gipt_map_t *map = &emap->em_gipt;
+
+		gipt_map_fini(map);
+		kmem_free(emap, sizeof (*emap));
+	}
+}
+
+static uint64_t
+ept_wired_count(void *arg)
+{
+	ept_map_t *emap = arg;
+	uint64_t res;
+
+	mutex_enter(EPT_LOCK(emap));
+	res = emap->em_wired_page_count;
+	mutex_exit(EPT_LOCK(emap));
+
+	return (res);
+}
+
+static int
+ept_is_wired(void *arg, uint64_t va, uint_t *protp)
+{
+	ept_map_t *emap = arg;
+	gipt_t *pt;
+	int rv = -1;
+
+	mutex_enter(EPT_LOCK(emap));
+	pt = gipt_map_lookup_deepest(&emap->em_gipt, va);
+	if (pt != NULL) {
+		const uint64_t pte = GIPT_VA2PTE(pt, va);
+
+		if (EPT_MAPS_PAGE(pte, pt->gipt_level)) {
+			*protp = EPT_PTE_PROT(pte);
+			rv = 0;
+		}
+	}
+	mutex_exit(EPT_LOCK(emap));
+
+	return (rv);
+}
+
+static int
+ept_map(void *arg, uint64_t va, pfn_t pfn, uint_t lvl, uint_t prot,
+    uint8_t attr)
+{
+	ept_map_t *emap = arg;
+	gipt_map_t *map = &emap->em_gipt;
+	gipt_t *pt;
+	uint64_t *ptep, pte;
+
+	ASSERT((prot & EPT_RWX) != 0 && (prot & ~EPT_RWX) == 0);
+	ASSERT3U(lvl, <, EPT_MAX_LEVELS);
+
+	mutex_enter(EPT_LOCK(emap));
+	pt = gipt_map_lookup(map, va, lvl);
+	if (pt == NULL) {
+		/*
+		 * A table at the appropriate VA/level that would house this
+		 * mapping does not currently exist.  Try to walk down to that
+		 * point, creating any necessary parent(s).
+		 */
+		pt = gipt_map_create_parents(map, va, lvl);
+
+		/*
+		 * There was a large page mapping in the way of creating the
+		 * necessary parent table(s).
+		 */
+		if (pt == NULL) {
+			panic("unexpected large page @ %08lx", va);
+		}
+	}
+	ptep = GIPT_VA2PTEP(pt, va);
+
+	pte = *ptep;
+	if (!EPT_IS_ABSENT(pte)) {
+		if (!EPT_MAPS_PAGE(pte, lvl)) {
+			panic("unexpected PT link @ %08lx in %p", va, pt);
+		} else {
+			panic("unexpected page mapped @ %08lx in %p", va, pt);
+		}
+	}
+
+	pte = EPT_PTE_ASSIGN_PAGE(lvl, pfn, prot, attr);
+	*ptep = pte;
+	pt->gipt_valid_cnt++;
+	emap->em_wired_page_count += gipt_level_count[lvl];
+
+	mutex_exit(EPT_LOCK(emap));
+	return (0);
+}
+
+static uint64_t
+ept_unmap(void *arg, uint64_t va, uint64_t end_va)
+{
+	ept_map_t *emap = arg;
+	gipt_map_t *map = &emap->em_gipt;
+	gipt_t *pt;
+	uint64_t cur_va = va;
+	uint64_t unmapped = 0;
+
+	mutex_enter(EPT_LOCK(emap));
+
+	pt = gipt_map_lookup_deepest(map, cur_va);
+	if (pt == NULL) {
+		mutex_exit(EPT_LOCK(emap));
+		return (0);
+	}
+	if (!EPT_MAPS_PAGE(GIPT_VA2PTE(pt, cur_va), pt->gipt_level)) {
+		cur_va = gipt_map_next_page(map, cur_va, end_va, &pt);
+		if (cur_va == 0) {
+			mutex_exit(EPT_LOCK(emap));
+			return (0);
+		}
+	}
+
+	while (cur_va < end_va) {
+		uint64_t *ptep = GIPT_VA2PTEP(pt, cur_va);
+		const uint_t lvl = pt->gipt_level;
+
+		ASSERT(EPT_MAPS_PAGE(*ptep, lvl));
+		*ptep = 0;
+		pt->gipt_valid_cnt--;
+		unmapped += gipt_level_count[pt->gipt_level];
+
+		gipt_t *next_pt = pt;
+		uint64_t next_va;
+		next_va = gipt_map_next_page(map, cur_va, end_va, &next_pt);
+
+		if (pt->gipt_valid_cnt == 0) {
+			gipt_map_clean_parents(map, pt);
+		}
+		if (next_va == 0) {
+			break;
+		}
+		pt = next_pt;
+		cur_va = next_va;
+	}
+	emap->em_wired_page_count -= unmapped;
+
+	mutex_exit(EPT_LOCK(emap));
+
+	return (unmapped);
+}
+
+struct vmm_pt_ops ept_ops = {
+	.vpo_init	= ept_create,
+	.vpo_free	= ept_destroy,
+	.vpo_wired_cnt	= ept_wired_count,
+	.vpo_is_wired	= ept_is_wired,
+	.vpo_map	= ept_map,
+	.vpo_unmap	= ept_unmap,
+};
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c b/usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c
index 58c10d9da0..ab8e1e5162 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c
+++ b/usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c
@@ -10,7 +10,7 @@
  */
 
 /*
- * Copyright 2018 Joyent, Inc.
+ * Copyright 2019 Joyent, Inc.
  */
 
 #include <sys/param.h>
@@ -38,73 +38,6 @@
 #define	VMMAP_TO_VMSPACE(vmmap)	((struct vmspace *)		\
 	((caddr_t)(vmmap) - offsetof(struct vmspace, vm_map)))
 
-/* Similar to htable, but without the bells and whistles */
-struct eptable {
-	struct eptable	*ept_next;
-	uintptr_t	ept_vaddr;
-	pfn_t		ept_pfn;
-	int16_t		ept_level;
-	int16_t		ept_valid_cnt;
-	uint32_t	_ept_pad2;
-	struct eptable	*ept_prev;
-	struct eptable	*ept_parent;
-	void		*ept_kva;
-};
-typedef struct eptable eptable_t;
-
-struct eptable_map {
-	kmutex_t		em_lock;
-	eptable_t		*em_root;
-	eptable_t		**em_hash;
-	size_t			em_table_cnt;
-	size_t			em_wired;
-
-	/* Protected by eptable_map_lock */
-	struct eptable_map	*em_next;
-	struct eptable_map	*em_prev;
-};
-typedef struct eptable_map eptable_map_t;
-
-#define	EPTABLE_HASH(va, lvl, sz)				\
-	((((va) >> LEVEL_SHIFT(1)) + ((va) >> 28) + (lvl))	\
-	& ((sz) - 1))
-
-#define	EPTABLE_VA2IDX(tbl, va)					\
-	(((va) - (tbl)->ept_vaddr) >>				\
-	LEVEL_SHIFT((tbl)->ept_level))
-
-#define	EPTABLE_IDX2VA(tbl, idx)				\
-	((tbl)->ept_vaddr +					\
-	((idx) << LEVEL_SHIFT((tbl)->ept_level)))
-
-#define	EPT_R		(0x1 << 0)
-#define	EPT_W		(0x1 << 1)
-#define	EPT_X		(0x1 << 2)
-#define	EPT_RWX		(EPT_R|EPT_W|EPT_X)
-#define	EPT_LGPG	(0x1 << 7)
-
-#define	EPT_PAT(attr)	(((attr) & 0x7) << 3)
-
-#define	EPT_PADDR	(0x000ffffffffff000ull)
-
-#define	EPT_IS_ABSENT(pte)	(((pte) & EPT_RWX) == 0)
-#define	EPT_PTE_PFN(pte)	mmu_btop((pte) & EPT_PADDR)
-#define	EPT_PTE_PROT(pte)	((pte) & EPT_RWX)
-#define	EPT_MAPS_PAGE(lvl, pte)				\
-	((lvl) == 0 || ((pte) & EPT_LGPG))
-
-#define	EPT_PTE_ASSIGN_TABLE(pfn)			\
-	((pfn_to_pa(pfn) & EPT_PADDR) | EPT_RWX)
-
-/*
- * We only assign EPT_LGPG for levels higher than 0: although this bit is
- * defined as being ignored at level 0, some versions of VMWare fail to honor
- * this and report such a PTE as an EPT mis-configuration.
- */
-#define	EPT_PTE_ASSIGN_PAGE(lvl, pfn, prot, attr)	\
-	((pfn_to_pa(pfn) & EPT_PADDR) |			\
-	(((lvl) != 0) ? EPT_LGPG : 0) |			\
-	EPT_PAT(attr) | ((prot) & EPT_RWX))
 
 struct vmspace_mapping {
 	list_node_t	vmsm_node;
@@ -123,26 +56,10 @@ typedef struct vmspace_mapping vmspace_mapping_t;
 
 /* Private glue interfaces */
 static void pmap_free(pmap_t);
-static eptable_t *eptable_alloc(void);
-static void eptable_free(eptable_t *);
-static void eptable_init(eptable_map_t *);
-static void eptable_fini(eptable_map_t *);
-static eptable_t *eptable_hash_lookup(eptable_map_t *, uintptr_t, level_t);
-static void eptable_hash_insert(eptable_map_t *, eptable_t *);
-static void eptable_hash_remove(eptable_map_t *, eptable_t *);
-static eptable_t *eptable_walk(eptable_map_t *, uintptr_t, level_t, uint_t *,
-    boolean_t);
-static pfn_t eptable_mapin(eptable_map_t *, uintptr_t, pfn_t, uint_t, uint_t,
-    vm_memattr_t);
-static void eptable_mapout(eptable_map_t *, uintptr_t);
-static int eptable_find(eptable_map_t *, uintptr_t, pfn_t *, uint_t *);
 static vmspace_mapping_t *vm_mapping_find(struct vmspace *, uintptr_t, size_t,
     boolean_t);
 static void vm_mapping_remove(struct vmspace *, vmspace_mapping_t *);
 
-static kmutex_t eptable_map_lock;
-static struct eptable_map *eptable_map_head = NULL;
-
 static vmem_t *vmm_alloc_arena = NULL;
 
 static void *
@@ -249,110 +166,29 @@ vmspace_find_kva(struct vmspace *vms, uintptr_t addr, size_t size)
 	return (result);
 }
 
-static int
-vmspace_pmap_wire(struct vmspace *vms, uintptr_t addr, pfn_t pfn, uint_t lvl,
-    uint_t prot, vm_memattr_t attr)
-{
-	enum pmap_type type = vms->vms_pmap.pm_type;
-
-	ASSERT(MUTEX_HELD(&vms->vms_lock));
-
-	switch (type) {
-	case PT_EPT: {
-		eptable_map_t *map = (eptable_map_t *)vms->vms_pmap.pm_map;
-
-		(void) eptable_mapin(map, addr, pfn, lvl, prot, attr);
-
-		vms->vms_pmap.pm_eptgen++;
-		return (0);
-	}
-	case PT_RVI:
-		/* RVI support not yet implemented */
-	default:
-		panic("unsupported pmap type: %x", type);
-		/* NOTREACHED */
-		break;
-	}
-	return (0);
-}
-
 static int
 vmspace_pmap_iswired(struct vmspace *vms, uintptr_t addr, uint_t *prot)
 {
-	enum pmap_type type = vms->vms_pmap.pm_type;
+	pmap_t pmap = &vms->vms_pmap;
+	int rv;
 
 	ASSERT(MUTEX_HELD(&vms->vms_lock));
 
-	switch (type) {
-	case PT_EPT: {
-		eptable_map_t *map = (eptable_map_t *)vms->vms_pmap.pm_map;
-		pfn_t pfn;
-
-		return (eptable_find(map, addr, &pfn, prot));
-	}
-	case PT_RVI:
-		/* RVI support not yet implemented */
-	default:
-		panic("unsupported pmap type: %x", type);
-		/* NOTREACHED */
-		break;
-	}
-	return (-1);
-}
-
-static int
-vmspace_pmap_unmap(struct vmspace *vms, uintptr_t addr, size_t size)
-{
-	enum pmap_type type = vms->vms_pmap.pm_type;
-
-	ASSERT(MUTEX_HELD(&vms->vms_lock));
-
-	switch (type) {
-	case PT_EPT: {
-		eptable_map_t *map = (eptable_map_t *)vms->vms_pmap.pm_map;
-		uintptr_t maddr = (uintptr_t)addr;
-		const ulong_t npages = btop(size);
-		ulong_t idx;
-
-		/* XXXJOY: punt on large pages for now */
-		for (idx = 0; idx < npages; idx++, maddr += PAGESIZE) {
-			eptable_mapout(map, maddr);
-		}
-		vms->vms_pmap.pm_eptgen++;
-		return (0);
-	}
-		break;
-	case PT_RVI:
-		/* RVI support not yet implemented */
-	default:
-		panic("unsupported pmap type: %x", type);
-		/* NOTREACHED */
-		break;
-	}
-	return (0);
+	rv = pmap->pm_ops->vpo_is_wired(pmap->pm_impl, addr, prot);
+	return (rv);
 }
 
 static void
 pmap_free(pmap_t pmap)
 {
-	switch (pmap->pm_type) {
-	case PT_EPT: {
-		eptable_map_t *map = (eptable_map_t *)pmap->pm_map;
+	void *pmi = pmap->pm_impl;
+	struct vmm_pt_ops *ops = pmap->pm_ops;
 
-		pmap->pm_pml4 = NULL;
-		pmap->pm_map = NULL;
+	pmap->pm_pml4 = NULL;
+	pmap->pm_impl = NULL;
+	pmap->pm_ops = NULL;
 
-		eptable_fini(map);
-		kmem_free(map, sizeof (*map));
-		return;
-	}
-	case PT_RVI:
-		/* RVI support not yet implemented */
-	default:
-		panic("unsupported pmap type: %x", pmap->pm_type);
-		/* NOTREACHED */
-		break;
-	}
+	ops->vpo_free(pmi);
 }
 
 int
@@ -362,18 +198,18 @@ pmap_pinit_type(pmap_t pmap, enum pmap_type type, int flags)
 	pmap->pm_type = type;
 	switch (type) {
 	case PT_EPT: {
-		eptable_map_t *map;
+		struct vmm_pt_ops *ops = &ept_ops;
+		void *pml4, *pmi;
 
-		map = kmem_zalloc(sizeof (*map), KM_SLEEP);
-		eptable_init(map);
+		pmi = ops->vpo_init((uintptr_t *)&pml4);
 
-		pmap->pm_map = map;
-		pmap->pm_pml4 = map->em_root->ept_kva;
+		pmap->pm_ops = ops;
+		pmap->pm_impl = pmi;
+		pmap->pm_pml4 = pml4;
 		return (1);
 	}
 	case PT_RVI:
 		/* RVI support not yet implemented */
-		return (0);
 	default:
 		panic("unsupported pmap type: %x", type);
 		/* NOTREACHED */
@@ -387,20 +223,11 @@ pmap_pinit_type(pmap_t pmap, enum pmap_type type, int flags)
 long
 pmap_wired_count(pmap_t pmap)
 {
-	enum pmap_type type = pmap->pm_type;
-	long val = 0L;
+	long val;
+
+	val = pmap->pm_ops->vpo_wired_cnt(pmap->pm_impl);
+	VERIFY3S(val, >=, 0);
 
-	switch (type) {
-	case PT_EPT:
-		val = ((eptable_map_t *)pmap->pm_map)->em_wired;
-		break;
-	case PT_RVI:
-		/* RVI support not yet implemented */
-	default:
-		panic("unsupported pmap type: %x", type);
-		/* NOTREACHED */
-		break;
-	}
 	return (val);
 }
 
@@ -412,361 +239,6 @@ pmap_emulate_accessed_dirty(pmap_t pmap, vm_offset_t va, int ftype)
 }
 
 
-static eptable_t *
-eptable_alloc(void)
-{
-	eptable_t *ept;
-	caddr_t page;
-
-	ept = kmem_zalloc(sizeof (*ept), KM_SLEEP);
-	page = kmem_zalloc(PAGESIZE, KM_SLEEP);
-	ept->ept_kva = page;
-	ept->ept_pfn = hat_getpfnum(kas.a_hat, page);
-
-	return (ept);
-}
-
-static void
-eptable_free(eptable_t *ept)
-{
-	void *page = ept->ept_kva;
-
-	ASSERT(ept->ept_pfn != PFN_INVALID);
-	ASSERT(ept->ept_kva != NULL);
-
-	ept->ept_pfn = PFN_INVALID;
-	ept->ept_kva = NULL;
-
-	kmem_free(page, PAGESIZE);
-	kmem_free(ept, sizeof (*ept));
-}
-
-static void
-eptable_init(eptable_map_t *map)
-{
-	eptable_t *root;
-
-	VERIFY0(mmu.hash_cnt & (mmu.hash_cnt - 1));
-
-	map->em_table_cnt = mmu.hash_cnt;
-	map->em_hash = kmem_zalloc(sizeof (eptable_t *) * map->em_table_cnt,
-	    KM_SLEEP);
-
-	root = eptable_alloc();
-	root->ept_level = mmu.max_level;
-	map->em_root = root;
-
-	/* Insert into global tracking list of eptable maps */
-	mutex_enter(&eptable_map_lock);
-	map->em_next = eptable_map_head;
-	map->em_prev = NULL;
-	if (eptable_map_head != NULL) {
-		eptable_map_head->em_prev = map;
-	}
-	eptable_map_head = map;
-	mutex_exit(&eptable_map_lock);
-}
-
-static void
-eptable_fini(eptable_map_t *map)
-{
-	const uint_t cnt = map->em_table_cnt;
-
-	/* Remove from global tracking list of eptable maps */
-	mutex_enter(&eptable_map_lock);
-	if (map->em_next != NULL) {
-		map->em_next->em_prev = map->em_prev;
-	}
-	if (map->em_prev != NULL) {
-		map->em_prev->em_next = map->em_next;
-	} else {
-		eptable_map_head = map->em_next;
-	}
-	mutex_exit(&eptable_map_lock);
-
-	mutex_enter(&map->em_lock);
-	/* XXJOY: Should we expect to need this clean-up? */
-	for (uint_t i = 0; i < cnt; i++) {
-		eptable_t *ept = map->em_hash[i];
-
-		while (ept != NULL) {
-			eptable_t *next = ept->ept_next;
-
-			eptable_hash_remove(map, ept);
-			eptable_free(ept);
-			ept = next;
-		}
-	}
-
-	kmem_free(map->em_hash, sizeof (eptable_t *) * cnt);
-	eptable_free(map->em_root);
-
-	mutex_exit(&map->em_lock);
-	mutex_destroy(&map->em_lock);
-}
-
-static eptable_t *
-eptable_hash_lookup(eptable_map_t *map, uintptr_t va, level_t lvl)
-{
-	const uint_t hash = EPTABLE_HASH(va, lvl, map->em_table_cnt);
-	eptable_t *ept;
-
-	ASSERT(MUTEX_HELD(&map->em_lock));
-
-	for (ept = map->em_hash[hash]; ept != NULL; ept = ept->ept_next) {
-		if (ept->ept_vaddr == va && ept->ept_level == lvl)
-			break;
-	}
-	return (ept);
-}
-
-static void
-eptable_hash_insert(eptable_map_t *map, eptable_t *ept)
-{
-	const uintptr_t va = ept->ept_vaddr;
-	const uint_t lvl = ept->ept_level;
-	const uint_t hash = EPTABLE_HASH(va, lvl, map->em_table_cnt);
-
-	ASSERT(MUTEX_HELD(&map->em_lock));
-	ASSERT(eptable_hash_lookup(map, va, lvl) == NULL);
-
-	ept->ept_prev = NULL;
-	if (map->em_hash[hash] == NULL) {
-		ept->ept_next = NULL;
-	} else {
-		eptable_t *chain = map->em_hash[hash];
-
-		ept->ept_next = chain;
-		chain->ept_prev = ept;
-	}
-	map->em_hash[hash] = ept;
-}
-
-static void
-eptable_hash_remove(eptable_map_t *map, eptable_t *ept)
-{
-	const uintptr_t va = ept->ept_vaddr;
-	const uint_t lvl = ept->ept_level;
-	const uint_t hash = EPTABLE_HASH(va, lvl, map->em_table_cnt);
-
-	ASSERT(MUTEX_HELD(&map->em_lock));
-
-	if (ept->ept_prev == NULL) {
-		ASSERT(map->em_hash[hash] == ept);
-
-		map->em_hash[hash] = ept->ept_next;
-	} else {
-		ept->ept_prev->ept_next = ept->ept_next;
-	}
-	if (ept->ept_next != NULL) {
-		ept->ept_next->ept_prev = ept->ept_prev;
-	}
-	ept->ept_next = NULL;
-	ept->ept_prev = NULL;
-}
-
-static eptable_t *
-eptable_walk(eptable_map_t *map, uintptr_t va, level_t tgtlvl, uint_t *idxp,
-    boolean_t do_create)
-{
-	eptable_t *ept = map->em_root;
-	level_t lvl = ept->ept_level;
-	uint_t idx = UINT_MAX;
-
-	ASSERT(MUTEX_HELD(&map->em_lock));
-
-	while (lvl >= tgtlvl) {
-		x86pte_t *ptes, entry;
-		const uintptr_t masked_va = va & LEVEL_MASK((uint_t)lvl);
-		eptable_t *newept = NULL;
-
-		idx = EPTABLE_VA2IDX(ept, va);
-		if (lvl == tgtlvl || lvl == 0) {
-			break;
-		}
-
-		ptes = (x86pte_t *)ept->ept_kva;
-		entry = ptes[idx];
-		if (EPT_IS_ABSENT(entry)) {
-			if (!do_create) {
-				break;
-			}
-
-			newept = eptable_alloc();
-			newept->ept_level = lvl - 1;
-			newept->ept_vaddr = masked_va;
-			newept->ept_parent = ept;
-
-			eptable_hash_insert(map, newept);
-			entry = EPT_PTE_ASSIGN_TABLE(newept->ept_pfn);
-			ptes[idx] = entry;
-			ept->ept_valid_cnt++;
-		} else if (!EPT_MAPS_PAGE(lvl, entry)) {
-			/* Do lookup in next level of page table */
-			newept = eptable_hash_lookup(map, masked_va, lvl - 1);
-
-			VERIFY(newept);
-			VERIFY3P(pfn_to_pa(newept->ept_pfn), ==,
-			    (entry & EPT_PADDR));
-		} else {
-			/*
-			 * There is a (large) page mapped here.  Since support
-			 * for non-PAGESIZE pages is not yet present, this is a
-			 * surprise.
-			 */
-			panic("unexpected large page in pte %p", &ptes[idx]);
-		}
-		ept = newept;
-		lvl--;
-	}
-
-	VERIFY(lvl >= 0 && idx != UINT_MAX);
-	*idxp = idx;
-	return (ept);
-}
-
-static pfn_t
-eptable_mapin(eptable_map_t *map, uintptr_t va, pfn_t pfn, uint_t lvl,
-    uint_t prot, vm_memattr_t attr)
-{
-	uint_t idx;
-	eptable_t *ept;
-	x86pte_t *ptes, entry;
-	const size_t pgsize = (size_t)LEVEL_SIZE(lvl);
-	pfn_t oldpfn = PFN_INVALID;
-
-	CTASSERT(EPT_R == PROT_READ);
-	CTASSERT(EPT_W == PROT_WRITE);
-	CTASSERT(EPT_X == PROT_EXEC);
-	ASSERT((prot & EPT_RWX) != 0 && (prot & ~EPT_RWX) == 0);
-
-	/* XXXJOY: punt on large pages for now */
-	VERIFY(lvl == 0);
-
-	mutex_enter(&map->em_lock);
-	ept = eptable_walk(map, va, (level_t)lvl, &idx, B_TRUE);
-	ptes = (x86pte_t *)ept->ept_kva;
-	entry = ptes[idx];
-
-	if (!EPT_IS_ABSENT(entry)) {
-		if (!EPT_MAPS_PAGE(lvl, entry)) {
-			panic("unexpected PT link %lx in %p[%d]",
-			    entry, ept, idx);
-		}
-
-		/*
-		 * XXXJOY: Just clean the entry for now. Assume(!) that
-		 * invalidation is going to occur anyways.
-		 */
-		oldpfn = EPT_PTE_PFN(ptes[idx]);
-		ept->ept_valid_cnt--;
-		ptes[idx] = (x86pte_t)0;
-		map->em_wired -= (pgsize >> PAGESHIFT);
-	}
-
-	entry = EPT_PTE_ASSIGN_PAGE(lvl, pfn, prot, attr);
-	ptes[idx] = entry;
-	ept->ept_valid_cnt++;
-	map->em_wired += (pgsize >> PAGESHIFT);
-	mutex_exit(&map->em_lock);
-
-	return (oldpfn);
-}
-
-static void
-eptable_mapout(eptable_map_t *map, uintptr_t va)
-{
-	eptable_t *ept;
-	uint_t idx;
-	x86pte_t *ptes, entry;
-
-	mutex_enter(&map->em_lock);
-	/* Find the lowest level entry at this VA */
-	ept = eptable_walk(map, va, -1, &idx, B_FALSE);
-
-	ptes = (x86pte_t *)ept->ept_kva;
-	entry = ptes[idx];
-
-	if (EPT_IS_ABSENT(entry)) {
-		/*
-		 * There is nothing here to free up.  If this was a sparsely
-		 * wired mapping, the absence is no concern.
-		 */
-		mutex_exit(&map->em_lock);
-		return;
-	} else {
-		const size_t pagesize = LEVEL_SIZE((uint_t)ept->ept_level);
-
-		if (!EPT_MAPS_PAGE(ept->ept_level, entry)) {
-			panic("unexpected PT link %lx in %p[%d]",
-			    entry, ept, idx);
-		}
-
-		/*
-		 * XXXJOY: Just clean the entry for now. Assume(!) that
-		 * invalidation is going to occur anyways.
-		 */
-		ept->ept_valid_cnt--;
-		ptes[idx] = (x86pte_t)0;
-		map->em_wired -= (pagesize >> PAGESHIFT);
-	}
-
-	while (ept->ept_valid_cnt == 0 && ept->ept_parent != NULL) {
-		eptable_t *next = ept->ept_parent;
-
-		idx = EPTABLE_VA2IDX(next, va);
-		ptes = (x86pte_t *)next->ept_kva;
-
-		entry = ptes[idx];
-		ASSERT(!EPT_MAPS_PAGE(next->ept_level, entry));
-		ASSERT(EPT_PTE_PFN(entry) == ept->ept_pfn);
-
-		ptes[idx] = (x86pte_t)0;
-		next->ept_valid_cnt--;
-		eptable_hash_remove(map, ept);
-		ept->ept_parent = NULL;
-		eptable_free(ept);
-
-		ept = next;
-	}
-	mutex_exit(&map->em_lock);
-}
-
-static int
-eptable_find(eptable_map_t *map, uintptr_t va, pfn_t *pfn, uint_t *prot)
-{
-	eptable_t *ept;
-	uint_t idx;
-	x86pte_t *ptes, entry;
-	int err = -1;
-
-	mutex_enter(&map->em_lock);
-	/* Find the lowest level entry at this VA */
-	ept = eptable_walk(map, va, -1, &idx, B_FALSE);
-
-	/* XXXJOY: Until large pages are supported, this check is easy */
-	if (ept->ept_level != 0) {
-		mutex_exit(&map->em_lock);
-		return (-1);
-	}
-
-	ptes = (x86pte_t *)ept->ept_kva;
-	entry = ptes[idx];
-
-	if (!EPT_IS_ABSENT(entry)) {
-		if (!EPT_MAPS_PAGE(ept->ept_level, entry)) {
-			panic("unexpected PT link %lx in %p[%d]",
-			    entry, ept, idx);
-		}
-
-		*pfn = EPT_PTE_PFN(entry);
-		*prot = EPT_PTE_PROT(entry);
-		err = 0;
-	}
-
-	mutex_exit(&map->em_lock);
-	return (err);
-}
 
 struct sglist_ent {
 	vm_paddr_t	sge_pa;
@@ -1169,6 +641,8 @@ int
 vm_fault(vm_map_t map, vm_offset_t off, vm_prot_t type, int flag)
 {
 	struct vmspace *vms = VMMAP_TO_VMSPACE(map);
+	pmap_t pmap = &vms->vms_pmap;
+	void *pmi = pmap->pm_impl;
 	const uintptr_t addr = off;
 	vmspace_mapping_t *vmsm;
 	struct vm_object *vmo;
@@ -1181,7 +655,7 @@ vm_fault(vm_map_t map, vm_offset_t off, vm_prot_t type, int flag)
 		int err = 0;
 
 		/*
-		 * It is possible that multiple will vCPUs race to fault-in a
+		 * It is possible that multiple vCPUs will race to fault-in a
 		 * given address.  In such cases, the race loser(s) will
 		 * encounter the already-mapped page, needing to do nothing
 		 * more than consider it a success.
@@ -1211,11 +685,12 @@ vm_fault(vm_map_t map, vm_offset_t off, vm_prot_t type, int flag)
 	VERIFY(pfn != PFN_INVALID);
 
 	/*
-	 * If pmap failure is to be handled, the previously
-	 * acquired page locks would need to be released.
+	 * If pmap failure is to be handled, the previously acquired page locks
+	 * would need to be released.
 	 */
-	VERIFY0(vmspace_pmap_wire(vms, map_addr, pfn, map_lvl, prot,
+	VERIFY0(pmap->pm_ops->vpo_map(pmi, map_addr, pfn, map_lvl, prot,
 	    vmo->vmo_attr));
+	pmap->pm_eptgen++;
 
 	mutex_exit(&vms->vms_lock);
 	return (0);
@@ -1331,10 +806,11 @@ int
 vm_map_remove(vm_map_t map, vm_offset_t start, vm_offset_t end)
 {
 	struct vmspace *vms = VMMAP_TO_VMSPACE(map);
+	pmap_t pmap = &vms->vms_pmap;
+	void *pmi = pmap->pm_impl;
 	const uintptr_t addr = start;
 	const size_t size = (size_t)(end - start);
 	vmspace_mapping_t *vmsm;
-	objtype_t type;
 
 	ASSERT(start < end);
 
@@ -1348,17 +824,8 @@ vm_map_remove(vm_map_t map, vm_offset_t start, vm_offset_t end)
 		return (ENOENT);
 	}
 
-	type = vmsm->vmsm_object->vmo_type;
-	switch (type) {
-	case OBJT_DEFAULT:
-	case OBJT_SG:
-		VERIFY0(vmspace_pmap_unmap(vms, addr, size));
-		break;
-	default:
-		panic("unsupported object type: %x", type);
-		/* NOTREACHED */
-		break;
-	}
+	(void) pmap->pm_ops->vpo_unmap(pmi, addr, end);
+	pmap->pm_eptgen++;
 
 	vm_mapping_remove(vms, vmsm);
 	vms->vms_map_changing = B_FALSE;
@@ -1370,6 +837,8 @@ int
 vm_map_wire(vm_map_t map, vm_offset_t start, vm_offset_t end, int flags)
 {
 	struct vmspace *vms = VMMAP_TO_VMSPACE(map);
+	pmap_t pmap = &vms->vms_pmap;
+	void *pmi = pmap->pm_impl;
 	const uintptr_t addr = start;
 	const size_t size = end - start;
 	vmspace_mapping_t *vmsm;
@@ -1397,8 +866,10 @@ vm_map_wire(vm_map_t map, vm_offset_t start, vm_offset_t end, int flags)
 		map_addr = P2ALIGN(pos, pg_size);
 		VERIFY(pfn != PFN_INVALID);
 
-		VERIFY0(vmspace_pmap_wire(vms, map_addr, pfn, map_lvl, prot,
-		    vmo->vmo_attr));
+		VERIFY0(pmap->pm_ops->vpo_map(pmi, map_addr, pfn, map_lvl,
+		    prot, vmo->vmo_attr));
+		vms->vms_pmap.pm_eptgen++;
+
 		pos += pg_size;
 	}
 
diff --git a/usr/src/uts/i86pc/os/gipt.c b/usr/src/uts/i86pc/os/gipt.c
new file mode 100644
index 0000000000..f8be828acc
--- /dev/null
+++ b/usr/src/uts/i86pc/os/gipt.c
@@ -0,0 +1,568 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2019 Joyent, Inc.
+ */
+
+#include <sys/gipt.h>
+#include <sys/malloc.h>
+#include <sys/kmem.h>
+#include <sys/sysmacros.h>
+#include <sys/sunddi.h>
+#include <sys/panic.h>
+#include <vm/hat.h>
+#include <vm/as.h>
+
+/*
+ * Generic Indexed Page Table
+ *
+ * There are several applications, such as hardware virtualization or IOMMU
+ * control, which require construction of a page table tree to represent a
+ * virtual address space.  Many features of the existing htable system would be
+ * convenient for this, but its tight coupling to the VM system make it
+ * undesirable for independent consumers.  The GIPT interface exists to provide
+ * page table allocation and indexing on top of which a table hierarchy
+ * (EPT, VT-d, etc) can be built by upstack logic.
+ *
+ * Types:
+ *
+ * gipt_t - Represents a single page table with a physical backing page and
+ *     associated metadata.
+ * gipt_map_t - The workhorse of this facility, it contains an hash table to
+ *     index all of the gipt_t entries which make up the page table tree.
+ * struct gipt_cbs - Callbacks used by the gipt_map_t:
+ *     gipt_pte_type_cb_t - Given a PTE, emit the type (empty/page/table)
+ *     gipt_pte_map_cb_t - Given a PFN, emit a (child) table mapping
+ */
+
+/*
+ * For now, the level shifts are hard-coded to match with standard 4-level
+ * 64-bit paging structures.
+ */
+
+#define	GIPT_HASH(map, va, lvl)			\
+	((((va) >> 12) + ((va) >> 28) + (lvl)) & ((map)->giptm_table_cnt - 1))
+
+const uint_t gipt_level_shift[GIPT_MAX_LEVELS+1] = {
+	12,	/* 4K */
+	21,	/* 2M */
+	30,	/* 1G */
+	39,	/* 512G */
+	48	/* MAX */
+};
+const uint64_t gipt_level_mask[GIPT_MAX_LEVELS+1] = {
+	0xfffffffffffff000ull,	/* 4K */
+	0xffffffffffe00000ull,	/* 2M */
+	0xffffffffc0000000ull,	/* 1G */
+	0xffffff8000000000ull,	/* 512G */
+	0xffff000000000000ull	/* MAX */
+};
+const uint64_t gipt_level_size[GIPT_MAX_LEVELS+1] = {
+	0x0000000000001000ull,	/* 4K */
+	0x0000000000200000ull,	/* 2M */
+	0x0000000040000000ull,	/* 1G */
+	0x0000008000000000ull,	/* 512G */
+	0x0001000000000000ull	/* MAX */
+};
+const uint64_t gipt_level_count[GIPT_MAX_LEVELS+1] = {
+	0x0000000000000001ull,	/* 4K */
+	0x0000000000000200ull,	/* 2M */
+	0x0000000000040000ull,	/* 1G */
+	0x0000000008000000ull,	/* 512G */
+	0x0000001000000000ull	/* MAX */
+};
+
+/*
+ * Allocate a gipt_t structure with corresponding page of memory to hold the
+ * PTEs which it contains.
+ */
+gipt_t *
+gipt_alloc(void)
+{
+	gipt_t *pt;
+	void *page;
+
+	pt = kmem_zalloc(sizeof (*pt), KM_SLEEP);
+	page = kmem_zalloc(PAGESIZE, KM_SLEEP);
+	pt->gipt_kva = page;
+	pt->gipt_pfn = hat_getpfnum(kas.a_hat, page);
+
+	return (pt);
+}
+
+/*
+ * Free a gipt_t structure along with its page of PTE storage.
+ */
+void
+gipt_free(gipt_t *pt)
+{
+	void *page = pt->gipt_kva;
+
+	ASSERT(pt->gipt_pfn != PFN_INVALID);
+	ASSERT(pt->gipt_kva != NULL);
+
+	pt->gipt_pfn = PFN_INVALID;
+	pt->gipt_kva = NULL;
+
+	kmem_free(page, PAGESIZE);
+	kmem_free(pt, sizeof (*pt));
+}
+
+/*
+ * Initialize a gipt_map_t with a max level (must be >= 1) and allocating its
+ * hash table based on a provided size (must be a power of 2).
+ */
+void
+gipt_map_init(gipt_map_t *map, uint_t levels, uint_t hash_table_size,
+    const struct gipt_cbs *cbs, gipt_t *root)
+{
+	VERIFY(map->giptm_root == NULL);
+	VERIFY(map->giptm_hash == NULL);
+	VERIFY3U(levels, >, 0);
+	VERIFY3U(levels, <=, GIPT_MAX_LEVELS);
+	VERIFY(ISP2(hash_table_size));
+	VERIFY(root != NULL);
+
+	mutex_init(&map->giptm_lock, NULL, MUTEX_DEFAULT, NULL);
+	map->giptm_table_cnt = hash_table_size;
+	bcopy(cbs, &map->giptm_cbs, sizeof (*cbs));
+	map->giptm_hash = kmem_alloc(sizeof (list_t) * map->giptm_table_cnt,
+	    KM_SLEEP);
+	for (uint_t i = 0; i < hash_table_size; i++) {
+		list_create(&map->giptm_hash[i], sizeof (gipt_t),
+		    offsetof(gipt_t, gipt_node));
+	}
+	map->giptm_levels = levels;
+
+	/*
+	 * Insert the table root into the hash.  It will be held in existence
+	 * with an extra "valid" reference.  This will prevent its clean-up
+	 * during gipt_map_clean_parents() calls, even if it has no children.
+	 */
+	mutex_enter(&map->giptm_lock);
+	gipt_map_insert(map, root);
+	map->giptm_root = root;
+	root->gipt_valid_cnt++;
+	mutex_exit(&map->giptm_lock);
+}
+
+/*
+ * Clean up a gipt_map_t by removing any lingering gipt_t entries referenced by
+ * it, and freeing its hash table.
+ */
+void
+gipt_map_fini(gipt_map_t *map)
+{
+	const uint_t cnt = map->giptm_table_cnt;
+	const size_t sz = sizeof (list_t) * cnt;
+
+	mutex_enter(&map->giptm_lock);
+	/* Clean up any lingering tables */
+	for (uint_t i = 0; i < cnt; i++) {
+		list_t *list = &map->giptm_hash[i];
+		gipt_t *pt;
+
+		while ((pt = list_remove_head(list)) != NULL) {
+			gipt_free(pt);
+		}
+		ASSERT(list_is_empty(list));
+	}
+
+	kmem_free(map->giptm_hash, sz);
+	map->giptm_hash = NULL;
+	map->giptm_root = NULL;
+	map->giptm_levels = 0;
+	mutex_exit(&map->giptm_lock);
+	mutex_destroy(&map->giptm_lock);
+}
+
+/*
+ * Look in the map for a gipt_t containing a given VA which is located at a
+ * specified level.
+ */
+gipt_t *
+gipt_map_lookup(gipt_map_t *map, uint64_t va, uint_t lvl)
+{
+	gipt_t *pt;
+
+	ASSERT(MUTEX_HELD(&map->giptm_lock));
+	ASSERT3U(lvl, <=, GIPT_MAX_LEVELS);
+
+	/*
+	 * Lookup gipt_t at the VA aligned to the next level up.  For example,
+	 * level 0 corresponds to a page table containing 512 PTEs which cover
+	 * 4k each, spanning a total 2MB. As such, the base VA of that table
+	 * must be aligned to the same 2MB.
+	 */
+	const uint64_t masked_va = va & gipt_level_mask[lvl + 1];
+	const uint_t hash = GIPT_HASH(map, masked_va, lvl);
+
+	/* Only the root is expected to be at the top level. */
+	if (lvl == (map->giptm_levels - 1) && map->giptm_root != NULL) {
+		pt = map->giptm_root;
+
+		ASSERT3U(pt->gipt_level, ==, lvl);
+
+		/*
+		 * It may be so that the VA in question is not covered by the
+		 * range of the table root.
+		 */
+		if (pt->gipt_vaddr != masked_va) {
+			return (NULL);
+		}
+
+		return (pt);
+	}
+
+	list_t *list = &map->giptm_hash[hash];
+	for (pt = list_head(list); pt != NULL; pt = list_next(list, pt)) {
+		if (pt->gipt_vaddr == masked_va && pt->gipt_level == lvl)
+			break;
+	}
+	return (pt);
+}
+
+/*
+ * Look in the map for the deepest (lowest level) gipt_t which contains a given
+ * VA.  This could still fail if the VA is outside the range of the table root.
+ */
+gipt_t *
+gipt_map_lookup_deepest(gipt_map_t *map, uint64_t va)
+{
+	gipt_t *pt = NULL;
+	uint_t lvl;
+
+	ASSERT(MUTEX_HELD(&map->giptm_lock));
+
+	for (lvl = 0; lvl < map->giptm_levels; lvl++) {
+		pt = gipt_map_lookup(map, va, lvl);
+		if (pt != NULL) {
+			break;
+		}
+	}
+	return (pt);
+}
+
+/*
+ * Given a VA inside a gipt_t, calculate (based on the level of that PT) the VA
+ * corresponding to the next entry in the table.  It returns 0 if that VA would
+ * fall beyond the bounds of the table.
+ */
+static __inline__ uint64_t
+gipt_next_va(gipt_t *pt, uint64_t va)
+{
+	const uint_t lvl = pt->gipt_level;
+	const uint64_t masked = va & gipt_level_mask[lvl];
+	const uint64_t max = pt->gipt_vaddr + gipt_level_size[lvl+1];
+	const uint64_t next = masked + gipt_level_size[lvl];
+
+	ASSERT3U(masked, >=, pt->gipt_vaddr);
+	ASSERT3U(masked, <, max);
+
+	/*
+	 * If the "next" VA would be outside this table, including cases where
+	 * it overflowed, indicate an error result.
+	 */
+	if (next >= max || next <= masked) {
+		return (0);
+	}
+	return (next);
+}
+
+/*
+ * For a given VA, find the next VA which corresponds to a valid page mapping.
+ * The gipt_t containing that VA will be indicated via 'ptp'.  (The gipt_t of
+ * the starting VA can be passed in via 'ptp' for a minor optimization).  If
+ * there is no valid mapping higher than 'va' but contained within 'max_va',
+ * then this will indicate failure with 0 returned.
+ */
+uint64_t
+gipt_map_next_page(gipt_map_t *map, uint64_t va, uint64_t max_va, gipt_t **ptp)
+{
+	gipt_t *pt = *ptp;
+	uint64_t cur_va = va;
+	gipt_pte_type_cb_t pte_type = map->giptm_cbs.giptc_pte_type;
+
+	ASSERT(MUTEX_HELD(&map->giptm_lock));
+	ASSERT3U(max_va, !=, 0);
+	ASSERT3U(ptp, !=, NULL);
+
+	/*
+	 * If a starting table is not provided, search the map for the deepest
+	 * table which contains the VA.  If for some reason that VA is beyond
+	 * coverage of the map root, indicate failure.
+	 */
+	if (pt == NULL) {
+		pt = gipt_map_lookup_deepest(map, cur_va);
+		if (pt == NULL) {
+			goto fail;
+		}
+	}
+
+	/*
+	 * From the starting table (at whatever level that may reside), walk
+	 * forward through the PTEs looking for a valid page mapping.
+	 */
+	while (cur_va < max_va) {
+		const uint64_t next_va = gipt_next_va(pt, cur_va);
+		if (next_va == 0) {
+			/*
+			 * The end of this table has been reached.  Ascend one
+			 * level to continue the walk if possible.  If already
+			 * at the root, the end of the table means failure.
+			 */
+			if (pt->gipt_level >= map->giptm_levels) {
+				goto fail;
+			}
+			pt = gipt_map_lookup(map, cur_va, pt->gipt_level + 1);
+			if (pt == NULL) {
+				goto fail;
+			}
+			continue;
+		} else if (next_va >= max_va) {
+			/*
+			 * Terminate the walk with a failure if the VA
+			 * corresponding to the next PTE is beyond the max.
+			 */
+			goto fail;
+		}
+		cur_va = next_va;
+
+		const uint64_t pte = GIPT_VA2PTE(pt, cur_va);
+		const gipt_pte_type_t ptet = pte_type(pte, pt->gipt_level);
+		if (ptet == PTET_EMPTY) {
+			continue;
+		} else if (ptet == PTET_PAGE) {
+			/* A valid page mapping: success. */
+			*ptp = pt;
+			return (cur_va);
+		} else if (ptet == PTET_TABLE) {
+			/*
+			 * A child page table is present at this PTE.  Look it
+			 * up from the map.
+			 */
+			ASSERT3U(pt->gipt_level, >, 0);
+			pt = gipt_map_lookup(map, cur_va, pt->gipt_level - 1);
+			ASSERT3P(pt, !=, NULL);
+			break;
+		} else {
+			panic("unexpected PTE type %x @ va %p", ptet, cur_va);
+			/* NOTREACHED */
+		}
+	}
+
+	/*
+	 * By this point, the above loop has located a table structure to
+	 * descend into in order to find the next page.
+	 */
+	while (cur_va < max_va) {
+		const uint64_t pte = GIPT_VA2PTE(pt, cur_va);
+		const gipt_pte_type_t ptet = pte_type(pte, pt->gipt_level);
+
+		if (ptet == PTET_EMPTY) {
+			const uint64_t next_va = gipt_next_va(pt, cur_va);
+			if (next_va == 0 || next_va >= max_va) {
+				goto fail;
+			}
+			cur_va = next_va;
+			continue;
+		} else if (ptet == PTET_PAGE) {
+			/* A valid page mapping: success. */
+			*ptp = pt;
+			return (cur_va);
+		} else if (ptet == PTET_TABLE) {
+			/*
+			 * A child page table is present at this PTE.  Look it
+			 * up from the map.
+			 */
+			ASSERT3U(pt->gipt_level, >, 0);
+			pt = gipt_map_lookup(map, cur_va, pt->gipt_level - 1);
+			ASSERT3P(pt, !=, NULL);
+		} else {
+			panic("unexpected PTE type %x @ va %p", ptet, cur_va);
+			/* NOTREACHED */
+		}
+	}
+
+fail:
+	*ptp = NULL;
+	return (0);
+}
+
+/*
+ * Insert a gipt_t into the map based on its VA and level.  It is up to the
+ * caller to ensure that a duplicate entry does not already exist in the map.
+ */
+void
+gipt_map_insert(gipt_map_t *map, gipt_t *pt)
+{
+	const uint_t hash = GIPT_HASH(map, pt->gipt_vaddr, pt->gipt_level);
+
+	ASSERT(MUTEX_HELD(&map->giptm_lock));
+	ASSERT(gipt_map_lookup(map, pt->gipt_vaddr, pt->gipt_level) == NULL);
+	VERIFY3U(pt->gipt_level, <, map->giptm_levels);
+
+	list_insert_head(&map->giptm_hash[hash], pt);
+}
+
+/*
+ * Remove a gipt_t from the map.
+ */
+void
+gipt_map_remove(gipt_map_t *map, gipt_t *pt)
+{
+	const uint_t hash = GIPT_HASH(map, pt->gipt_vaddr, pt->gipt_level);
+
+	ASSERT(MUTEX_HELD(&map->giptm_lock));
+
+	list_remove(&map->giptm_hash[hash], pt);
+}
+
+/*
+ * Given a VA, create any missing gipt_t entries from the specified level all
+ * the way up to (but not including) the root.  This is done from lowest level
+ * to highest, and stops when an existing table covering that VA is found.
+ * References to any created gipt_t tables, plus the final "found" gipt_t are
+ * stored in 'pts'.  The number of gipt_t pointers stored to 'pts' serves as
+ * the return value (1 <= val <= root level).  It is up to the caller to
+ * populate linking PTEs to the newly created empty tables.
+ */
+static uint_t
+gipt_map_ensure_chain(gipt_map_t *map, uint64_t va, uint_t lvl, gipt_t **pts)
+{
+	const uint_t root_lvl = map->giptm_root->gipt_level;
+	uint_t clvl = lvl, count = 0;
+	gipt_t *child_pt = NULL;
+
+	ASSERT(MUTEX_HELD(&map->giptm_lock));
+	ASSERT3U(lvl, <, root_lvl);
+	ASSERT3P(map->giptm_root, !=, NULL);
+
+	do {
+		const uint64_t pva = (va & gipt_level_mask[clvl + 1]);
+		gipt_t *pt;
+
+		pt = gipt_map_lookup(map, pva, clvl);
+		if (pt != NULL) {
+			ASSERT3U(pva, ==, pt->gipt_vaddr);
+
+			if (child_pt != NULL) {
+				child_pt->gipt_parent = pt;
+			}
+			pts[count++] = pt;
+			return (count);
+		}
+
+		pt = gipt_alloc();
+		pt->gipt_vaddr = pva;
+		pt->gipt_level = clvl;
+		if (child_pt != NULL) {
+			child_pt->gipt_parent = pt;
+		}
+
+		gipt_map_insert(map, pt);
+		child_pt = pt;
+		pts[count++] = pt;
+		clvl++;
+	} while (clvl <= root_lvl);
+
+	return (count);
+}
+
+/*
+ * Ensure that a page table covering a VA at a specified level exists.  This
+ * will create any necessary tables chaining up to the root as well.
+ */
+gipt_t *
+gipt_map_create_parents(gipt_map_t *map, uint64_t va, uint_t lvl)
+{
+	gipt_t *pt, *pts[GIPT_MAX_LEVELS] = { 0 };
+	gipt_pte_type_cb_t pte_type = map->giptm_cbs.giptc_pte_type;
+	gipt_pte_map_cb_t pte_map = map->giptm_cbs.giptc_pte_map;
+	uint64_t *ptep, pte;
+	uint_t i, count;
+
+	ASSERT(MUTEX_HELD(&map->giptm_lock));
+
+	count = gipt_map_ensure_chain(map, va, lvl, pts);
+	if (count == 1) {
+		/* Table already exists in the hierarchy */
+		return (pts[0]);
+	}
+	ASSERT3U(count, >, 1);
+
+	/* Make sure there is not already a large page mapping at the top */
+	pt = pts[count - 1];
+	if (pte_type(GIPT_VA2PTE(pt, va), pt->gipt_level) == PTET_PAGE) {
+		const uint_t end = count - 1;
+
+		/*
+		 * Nuke those gipt_t entries which were optimistically created
+		 * for what was found to be a conflicted mapping.
+		 */
+		for (i = 0; i < end; i++) {
+			gipt_map_remove(map, pts[i]);
+			gipt_free(pts[i]);
+		}
+		return (NULL);
+	}
+
+	/* Initialize the appropriate tables from bottom to top */
+	for (i = 1; i < count; i++) {
+		pt = pts[i];
+		ptep = GIPT_VA2PTEP(pt, va);
+
+		/*
+		 * Since gipt_map_ensure_chain() creates missing tables until
+		 * it find a valid one, and that existing table has been
+		 * checked for the existence of a large page, nothing should
+		 * occupy this PTE.
+		 */
+		ASSERT3U(pte_type(*ptep, pt->level), ==, PTET_EMPTY);
+
+		*ptep = pte_map(pts[i - 1]->gipt_pfn);
+		pt->gipt_valid_cnt++;
+	}
+
+	return (pts[0]);
+}
+
+/*
+ * If a page table is empty, free it from the map, as well as any parent tables
+ * that would subsequently become empty as part of the clean-up.  As noted in
+ * gipt_map_init(), the table root is a special case and will remain in the
+ * map, even when empty.
+ */
+void
+gipt_map_clean_parents(gipt_map_t *map, gipt_t *pt)
+{
+	ASSERT(MUTEX_HELD(&map->giptm_lock));
+
+	while (pt->gipt_valid_cnt == 0) {
+		gipt_t *parent = pt->gipt_parent;
+		uint64_t *ptep = GIPT_VA2PTEP(parent, pt->gipt_vaddr);
+
+		ASSERT(!EPT_MAPS_PAGE(*ptep, parent->gipt_level));
+		ASSERT3U(EPT_PTE_PFN(*ptep), ==, pt->gipt_pfn);
+
+		/*
+		 * For now, it is assumed that all gipt consumers consider PTE
+		 * zeroing as an adequate action for table unmap.
+		 */
+		*ptep = 0;
+
+		parent->gipt_valid_cnt--;
+		gipt_map_remove(map, pt);
+		gipt_free(pt);
+		pt = parent;
+	}
+}
diff --git a/usr/src/uts/i86pc/sys/gipt.h b/usr/src/uts/i86pc/sys/gipt.h
new file mode 100644
index 0000000000..aba41ae282
--- /dev/null
+++ b/usr/src/uts/i86pc/sys/gipt.h
@@ -0,0 +1,92 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2019 Joyent, Inc.
+ */
+
+#ifndef _GIPT_H_
+#define	_GIPT_H_
+
+#include <sys/types.h>
+#include <sys/mutex.h>
+#include <sys/param.h>
+#include <sys/list.h>
+
+struct gipt {
+	list_node_t	gipt_node;
+	uint64_t	gipt_vaddr;
+	uint64_t	gipt_pfn;
+	uint16_t	gipt_level;
+	uint16_t	gipt_valid_cnt;
+	uint32_t	_gipt_pad;
+	struct gipt	*gipt_parent;
+	uint64_t	*gipt_kva;
+	uint64_t	_gipt_pad2;
+};
+typedef struct gipt gipt_t;
+
+typedef enum {
+	PTET_EMPTY	= 0,
+	PTET_PAGE	= 1,
+	PTET_TABLE	= 2,
+} gipt_pte_type_t;
+
+/* Given a PTE and its level, determine the type of that PTE */
+typedef gipt_pte_type_t (*gipt_pte_type_cb_t)(uint64_t, uint_t);
+/* Given the PFN of a child table, emit a PFN that references it */
+typedef uint64_t (*gipt_pte_map_cb_t)(uint64_t);
+
+struct gipt_cbs {
+	gipt_pte_type_cb_t	giptc_pte_type;
+	gipt_pte_map_cb_t	giptc_pte_map;
+};
+
+struct gipt_map {
+	kmutex_t	giptm_lock;
+	gipt_t		*giptm_root;
+	list_t		*giptm_hash;
+	struct gipt_cbs	giptm_cbs;
+	size_t		giptm_table_cnt;
+	uint_t		giptm_levels;
+};
+typedef struct gipt_map gipt_map_t;
+
+#define	GIPT_HASH_SIZE_DEFAULT	0x2000
+#define	GIPT_MAX_LEVELS	4
+
+#define	GIPT_VA2IDX(pt, va)			\
+	(((va) - (pt)->gipt_vaddr) >>		\
+	gipt_level_shift[(pt)->gipt_level])
+
+#define	GIPT_VA2PTE(pt, va)	((pt)->gipt_kva[GIPT_VA2IDX(pt, va)])
+#define	GIPT_VA2PTEP(pt, va)	(&(pt)->gipt_kva[GIPT_VA2IDX(pt, va)])
+
+extern const uint_t gipt_level_shift[GIPT_MAX_LEVELS+1];
+extern const uint64_t gipt_level_mask[GIPT_MAX_LEVELS+1];
+extern const uint64_t gipt_level_size[GIPT_MAX_LEVELS+1];
+extern const uint64_t gipt_level_count[GIPT_MAX_LEVELS+1];
+
+extern gipt_t *gipt_alloc(void);
+extern void gipt_free(gipt_t *);
+extern void gipt_map_init(gipt_map_t *, uint_t, uint_t,
+    const struct gipt_cbs *, gipt_t *);
+extern void gipt_map_fini(gipt_map_t *);
+extern gipt_t *gipt_map_lookup(gipt_map_t *, uint64_t, uint_t);
+extern gipt_t *gipt_map_lookup_deepest(gipt_map_t *, uint64_t);
+extern uint64_t gipt_map_next_page(gipt_map_t *, uint64_t, uint64_t,
+    gipt_t **);
+extern void gipt_map_insert(gipt_map_t *, gipt_t *);
+extern void gipt_map_remove(gipt_map_t *, gipt_t *);
+extern gipt_t *gipt_map_create_parents(gipt_map_t *, uint64_t, uint_t);
+extern void gipt_map_clean_parents(gipt_map_t *, gipt_t *);
+
+#endif /* _GIPT_H_ */
