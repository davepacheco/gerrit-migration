From 3f1733dab1e4444c15b6671f6e98c6f8f617798f Mon Sep 17 00:00:00 2001
From: Patrick Mooney <pmooney@pfmooney.com>
Date: Wed, 29 Aug 2018 19:24:26 +0000
Subject: [PATCH] OS-7438 refactor bhyve EPT to use generic page tables OS-7437
 want generic indexed page table system

---
 usr/src/uts/i86pc/Makefile.files       |   2 +
 usr/src/uts/i86pc/io/vmm/vm/vm_glue.h  |  16 +-
 usr/src/uts/i86pc/io/vmm/vmm_sol_ept.c | 451 +++++++++++++++++++
 usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c  | 599 ++-----------------------
 usr/src/uts/i86pc/os/gipt.c            | 340 ++++++++++++++
 usr/src/uts/i86pc/sys/gipt.h           |  72 +++
 6 files changed, 914 insertions(+), 566 deletions(-)
 create mode 100644 usr/src/uts/i86pc/io/vmm/vmm_sol_ept.c
 create mode 100644 usr/src/uts/i86pc/os/gipt.c
 create mode 100644 usr/src/uts/i86pc/sys/gipt.h

diff --git a/usr/src/uts/i86pc/Makefile.files b/usr/src/uts/i86pc/Makefile.files
index addafb123a..a3ffeed502 100644
--- a/usr/src/uts/i86pc/Makefile.files
+++ b/usr/src/uts/i86pc/Makefile.files
@@ -273,8 +273,10 @@ VMM_OBJS += vmm.o \
 	vmcb.o \
 	svm_support.o \
 	amdv.o \
+	gipt.o \
 	vmm_sol_vm.o \
 	vmm_sol_glue.o \
+	vmm_sol_ept.o \
 	vmm_zsd.o
 
 VIONA_OBJS += viona.o
diff --git a/usr/src/uts/i86pc/io/vmm/vm/vm_glue.h b/usr/src/uts/i86pc/io/vmm/vm/vm_glue.h
index c5620a91f6..434ee5bd4c 100644
--- a/usr/src/uts/i86pc/io/vmm/vm/vm_glue.h
+++ b/usr/src/uts/i86pc/io/vmm/vm/vm_glue.h
@@ -24,6 +24,7 @@ struct vmspace;
 struct vm_map;
 struct pmap;
 struct vm_object;
+struct vmm_pt_ops;
 
 struct vm_map {
 	struct vmspace *vmm_space;
@@ -36,7 +37,8 @@ struct pmap {
 
 	/* Implementation private */
 	enum pmap_type	pm_type;
-	void		*pm_map;
+	struct vmm_pt_ops *pm_ops;
+	void		*pm_impl;
 };
 
 struct vmspace {
@@ -81,4 +83,16 @@ void *vmspace_find_kva(struct vmspace *, uintptr_t, size_t);
 void vmm_arena_init(void);
 void vmm_arena_fini(void);
 
+struct vmm_pt_ops {
+	void * (*vpo_init)(uint64_t *);
+	void (*vpo_free)(void *);
+	uint64_t (*vpo_wired_cnt)(void *);
+	int (*vpo_is_wired)(void *, uint64_t, uint_t *);
+	int (*vpo_map)(void *, uint64_t, pfn_t, uint_t, uint_t, uint8_t);
+	uint64_t (*vpo_unmap)(void *, uint64_t, uint64_t);
+};
+
+extern struct vmm_pt_ops ept_ops;
+
+
 #endif /* _VM_GLUE_ */
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_sol_ept.c b/usr/src/uts/i86pc/io/vmm/vmm_sol_ept.c
new file mode 100644
index 0000000000..f194b85387
--- /dev/null
+++ b/usr/src/uts/i86pc/io/vmm/vmm_sol_ept.c
@@ -0,0 +1,451 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2018 Joyent, Inc.
+ */
+
+#include <sys/types.h>
+#include <sys/param.h>
+#include <sys/kmem.h>
+#include <sys/machsystm.h>
+
+#include <sys/gipt.h>
+#include <vm/vm_glue.h>
+
+
+struct ept_map {
+	gipt_map_t	em_gipt;
+	uint64_t	em_wired_page_count;
+};
+typedef struct ept_map ept_map_t;
+
+#define	EPT_LOCK(m)	(&(m)->em_gipt.giptm_lock)
+
+#define	EPT_MAX_LEVELS	4
+
+CTASSERT(EPT_MAX_LEVELS <= GIPT_MAX_LEVELS);
+
+#define	EPT_R		(0x1 << 0)
+#define	EPT_W		(0x1 << 1)
+#define	EPT_X		(0x1 << 2)
+#define	EPT_RWX		(EPT_R|EPT_W|EPT_X)
+#define	EPT_LGPG	(0x1 << 7)
+
+#define	EPT_PA_MASK	(0x000ffffffffff000ull)
+
+CTASSERT(EPT_R == PROT_READ);
+CTASSERT(EPT_W == PROT_WRITE);
+CTASSERT(EPT_X == PROT_EXEC);
+
+
+#define	EPT_PAT(attr)	(((attr) & 0x7) << 3)
+#define	EPT_PADDR(addr)	((addr) & EPT_PA_MASK)
+
+#define	EPT_IS_ABSENT(pte)	(((pte) & EPT_RWX) == 0)
+#define	EPT_PTE_PFN(pte)	mmu_btop(EPT_PADDR(pte))
+#define	EPT_PTE_PROT(pte)	((pte) & EPT_RWX)
+#define	EPT_MAPS_PAGE(pte, lvl)	\
+	(EPT_PTE_PROT(pte) != 0 && (((pte) & EPT_LGPG) != 0 || (lvl) == 0))
+
+/*
+ * Only assign EPT_LGPG for levels higher than 0.  Although this bit is defined
+ * as being ignored at level 0, some versions of VMWare fail to honor this and
+ * report such a PTE as an EPT mis-configuration.
+ */
+#define	EPT_PTE_ASSIGN_PAGE(lvl, pfn, prot, attr)	\
+	(EPT_PADDR(pfn_to_pa(pfn)) |			\
+	(((lvl) != 0) ? EPT_LGPG : 0) |			\
+	EPT_PAT(attr) | ((prot) & EPT_RWX))
+#define	EPT_PTE_ASSIGN_TABLE(pfn)	(EPT_PADDR(pfn_to_pa(pfn)) | EPT_RWX)
+
+
+/*
+ * Ensure that a page table covering a VA at a specified level exists.  This
+ * will create any necessary tables chaining up to the root as well.
+ */
+static gipt_t *
+ept_create_parents(ept_map_t *emap, uint64_t va, uint_t lvl)
+{
+	gipt_map_t *map = &emap->em_gipt;
+	gipt_t *pt, *pts[EPT_MAX_LEVELS] = { 0 };
+	uint64_t *ptep;
+	uint_t i, count;
+
+	count = gipt_map_ensure_chain(map, va, lvl, pts);
+	if (count == 1) {
+		/* Table already exists in the hierarchy */
+		return (pts[0]);
+	}
+	ASSERT3U(count, >, 1);
+
+	/* Make sure there is not already a large page mapping at the top */
+	pt = pts[count - 1];
+	ptep = GIPT_VA2PTEP(pt, va);
+	if (EPT_MAPS_PAGE(*ptep, pt->gipt_level)) {
+		const uint_t end = count - 1;
+
+		/*
+		 * Nuke those gipt_t entries which were optimistically created
+		 * for what was found to be a conflicted mapping.
+		 */
+		for (i = 0; i < end; i++) {
+			gipt_map_remove(map, pts[i]);
+			gipt_free(pts[i]);
+		}
+		return (NULL);
+	}
+
+	/* Initialize the appropriate tables from bottom to top */
+	for (i = 1; i < count; i++ ) {
+		pt = pts[i];
+		ptep = GIPT_VA2PTEP(pt, va);
+
+		/*
+		 * Since gipt_map_ensure_chain() creates missing tables until
+		 * it find a valid one, and that existing table has been
+		 * checked for the existence of a large page, nothing should
+		 * occupy this PTE.
+		 */
+		ASSERT(EPT_IS_ABSENT(*ptep));
+
+		*ptep = EPT_PTE_ASSIGN_TABLE(pts[i-1]->gipt_pfn);
+		pt->gipt_valid_cnt++;
+	}
+
+	return (pts[0]);
+}
+
+/*
+ * If a page table is empty, free it as well as any parent tables that would
+ * subsequently become empty as part of the clean-up.
+ */
+static void
+ept_clean_parents(ept_map_t *emap, gipt_t *pt)
+{
+	gipt_map_t *map = &emap->em_gipt;
+
+	ASSERT(MUTEX_HELD(EPT_LOCK(emap)));
+
+	while (pt->gipt_valid_cnt == 0) {
+		gipt_t *parent = pt->gipt_parent;
+		uint64_t *ptep = GIPT_VA2PTEP(parent, pt->gipt_vaddr);
+
+		ASSERT(!EPT_MAPS_PAGE(*ptep, parent->gipt_level));
+		ASSERT3U(EPT_PTE_PFN(*ptep), ==, pt->gipt_pfn);
+
+		*ptep = 0;
+		parent->gipt_valid_cnt--;
+		gipt_map_remove(map, pt);
+		gipt_free(pt);
+		pt = parent;
+	}
+}
+
+/*
+ * For a given VA, find the next VA which corresponds to a valid page mapping.
+ * The gipt_t containing that VA will be indicated via 'ptp'.  (The gipt_t of
+ * the starting VA can be passed in via 'ptp' for a minor optimization).  If
+ * there is no valid mapping higher than 'va' but contained within 'max_va',
+ * then this will indicate failure with 0 returned.
+ */
+static uint64_t
+ept_next_page(ept_map_t *emap, uint64_t va, uint64_t max_va, gipt_t **ptp)
+{
+	gipt_map_t *map = &emap->em_gipt;
+	gipt_t *pt = *ptp;
+	uint64_t cur_va = va;
+
+	ASSERT3U(max_va, !=, 0);
+	ASSERT3U(ptp, !=, NULL);
+
+	/*
+	 * If a starting table is not provided, search the map for the deepest
+	 * table which contains the VA.  If for some reason that VA is beyond
+	 * coverage of the map root, indicate failure.
+	 */
+	if (pt == NULL) {
+		pt = gipt_map_lookup_deepest(map, cur_va);
+		if (pt == NULL) {
+			goto fail;
+		}
+	}
+
+	/*
+	 * From the starting table (at whatever level that may reside), walk
+	 * forward through the PTEs looking for a valid page mapping.
+	 */
+	while (cur_va < max_va) {
+		const uint64_t next_va = gipt_next_va(pt, cur_va);
+		if (next_va == 0) {
+			/*
+			 * The end of this table has been reached.  Ascend one
+			 * level to continue the walk if possible.  If already
+			 * at the root, the end of the table means failure.
+			 */
+			if (pt->gipt_level >= map->giptm_levels) {
+				goto fail;
+			}
+			pt = gipt_map_lookup(map, cur_va, pt->gipt_level + 1);
+			if (pt == NULL) {
+				goto fail;
+			}
+			continue;
+		} else if (next_va >= max_va) {
+			/*
+			 * Terminate the walk with a failure if the VA
+			 * corresponding to the next PTE is beyond the max.
+			 */
+			goto fail;
+		}
+		cur_va = next_va;
+
+		const uint64_t pte = GIPT_VA2PTE(pt, cur_va);
+		if (EPT_IS_ABSENT(pte)) {
+			continue;
+		} else if (EPT_MAPS_PAGE(pte, pt->gipt_level)) {
+			/* A valid page mapping: success. */
+			*ptp = pt;
+			return (cur_va);
+		} else {
+			/*
+			 * A child page table is present at this PTE.  Look it
+			 * up from the map.
+			 */
+			ASSERT3U(pt->gipt_level, >, 0);
+			pt = gipt_map_lookup(map, cur_va, pt->gipt_level - 1);
+			ASSERT3P(pt, !=, NULL);
+			break;
+		}
+	}
+
+	/*
+	 * By this point, the above loop has located a table structure to
+	 * descend into in order to find the next page.
+	 */
+	while (cur_va < max_va) {
+		const uint64_t pte = GIPT_VA2PTE(pt, cur_va);
+
+		if (EPT_IS_ABSENT(pte)) {
+			const uint64_t next_va = gipt_next_va(pt, cur_va);
+			if (next_va == 0 || next_va >= max_va) {
+				goto fail;
+			}
+			cur_va = next_va;
+			continue;
+		} else if (EPT_MAPS_PAGE(pte, pt->gipt_level)) {
+			/* A valid page mapping: success. */
+			*ptp = pt;
+			return (cur_va);
+		} else {
+			/*
+			 * A child page table is present at this PTE.  Look it
+			 * up from the map.
+			 */
+			ASSERT3U(pt->gipt_level, >, 0);
+			pt = gipt_map_lookup(map, cur_va, pt->gipt_level - 1);
+			ASSERT3P(pt, !=, NULL);
+			break;
+		}
+	};
+
+fail:
+	*ptp = NULL;
+	return (0);
+}
+
+
+static void *
+ept_create(uintptr_t *pml4_kaddr)
+{
+	ept_map_t *emap;
+	gipt_map_t *map;
+	gipt_t *root;
+
+	emap = kmem_zalloc(sizeof (*emap), KM_SLEEP);
+	map = &emap->em_gipt;
+	gipt_map_init(map, EPT_MAX_LEVELS, GIPT_HASH_SIZE_DEFAULT);
+
+	root = gipt_alloc();
+	root->gipt_level = EPT_MAX_LEVELS - 1;
+
+	mutex_enter(EPT_LOCK(emap));
+	gipt_map_insert(map, root);
+	map->giptm_root = root;
+	mutex_exit(EPT_LOCK(emap));
+
+	/*
+	 * Hold the root table in existence with an extra "valid" reference.
+	 * This will prevent 'ept_clean_parents()' from removing it when its
+	 * last child is cleaned up.
+	 */
+	root->gipt_valid_cnt = 1;
+
+	*pml4_kaddr = (uintptr_t)root->gipt_kva;
+	return (emap);
+}
+
+static void
+ept_destroy(void *arg)
+{
+	ept_map_t *emap = arg;
+	gipt_map_t *map = &emap->em_gipt;
+
+	gipt_map_fini(map);
+	kmem_free(emap, sizeof (*emap));
+}
+
+static uint64_t
+ept_wired_count(void *arg)
+{
+	ept_map_t *emap = arg;
+	uint64_t res;
+
+	mutex_enter(EPT_LOCK(emap));
+	res = emap->em_wired_page_count;
+	mutex_exit(EPT_LOCK(emap));
+
+	return (res);
+}
+
+static int
+ept_is_wired(void *arg, uint64_t va, uint_t *protp)
+{
+	ept_map_t *emap = arg;
+	gipt_t *pt;
+	int rv = -1;
+
+	mutex_enter(EPT_LOCK(emap));
+	pt = gipt_map_lookup_deepest(&emap->em_gipt, va);
+	if (pt != NULL) {
+		const uint64_t pte = GIPT_VA2PTE(pt, va);
+
+		if (!EPT_IS_ABSENT(pte) &&
+		    EPT_MAPS_PAGE(pte, pt->gipt_level)) {
+			*protp = EPT_PTE_PROT(pte);
+			rv = 0;
+		}
+	}
+	mutex_exit(EPT_LOCK(emap));
+
+	return (rv);
+}
+
+static int
+ept_map(void *arg, uint64_t va, pfn_t pfn, uint_t lvl, uint_t prot,
+    uint8_t attr)
+{
+	ept_map_t *emap = arg;
+	gipt_t *pt;
+	uint64_t *ptep, pte;
+
+	ASSERT((prot & EPT_RWX) != 0 && (prot & ~EPT_RWX) == 0);
+	ASSERT3U(lvl, <, EPT_MAX_LEVELS);
+
+	mutex_enter(EPT_LOCK(emap));
+	pt = gipt_map_lookup(&emap->em_gipt, va, lvl);
+	if (pt == NULL) {
+		/*
+		 * A table at the appropriate VA/level that would house this
+		 * mapping does not currently exist.  Try to walk down to that
+		 * point, creating any necessary parent(s).
+		 */
+		pt = ept_create_parents(emap, va, lvl);
+
+		/*
+		 * There was a large page mapping in the way of creating the
+		 * necessary parent table(s).
+		 */
+		if (pt == NULL) {
+			panic("unexpected large page @ %08lx", va);
+		}
+	}
+	ptep = GIPT_VA2PTEP(pt, va);
+
+	pte = *ptep;
+	if (!EPT_IS_ABSENT(pte)) {
+		if (!EPT_MAPS_PAGE(pte, lvl)) {
+			panic("unexpected PT link @ %08lx", va);
+		} else {
+			panic("unexpected page mapped @ %08lx", va);
+		}
+	}
+
+	pte = EPT_PTE_ASSIGN_PAGE(lvl, pfn, prot, attr);
+	*ptep = pte;
+	pt->gipt_valid_cnt++;
+	emap->em_wired_page_count += gipt_level_count[lvl];
+
+	mutex_exit(EPT_LOCK(emap));
+	return (0);
+}
+
+static uint64_t
+ept_unmap(void *arg, uint64_t va, uint64_t end_va)
+{
+	ept_map_t *emap = arg;
+	gipt_map_t *map = &emap->em_gipt;
+	gipt_t *pt;
+	uint64_t cur_va = va;
+	uint64_t unmapped = 0;
+
+	mutex_enter(EPT_LOCK(emap));
+
+	pt = gipt_map_lookup_deepest(map, cur_va);
+	if (pt == NULL) {
+		mutex_exit(EPT_LOCK(emap));
+		return (0);
+	}
+	if (!EPT_MAPS_PAGE(GIPT_VA2PTE(pt, cur_va), pt->gipt_level)) {
+		cur_va = ept_next_page(emap, cur_va, end_va, &pt);
+		if (cur_va == 0) {
+			mutex_exit(EPT_LOCK(emap));
+			return (0);
+		}
+	}
+
+	while (cur_va < end_va) {
+		uint64_t *ptep = GIPT_VA2PTEP(pt, cur_va);
+		const uint_t lvl = pt->gipt_level;
+
+		ASSERT(EPT_MAPS_PAGE(*ptep, lvl));
+		*ptep = 0;
+		pt->gipt_valid_cnt--;
+		unmapped += gipt_level_count[pt->gipt_level];
+
+		gipt_t *next_pt = pt;
+		uint64_t next_va;
+		next_va = ept_next_page(emap, cur_va, end_va, &next_pt);
+
+		if (pt->gipt_valid_cnt == 0) {
+			ept_clean_parents(emap, pt);
+		}
+		if (next_va == 0) {
+			break;
+		}
+		pt = next_pt;
+		cur_va = next_va;
+	}
+	emap->em_wired_page_count -= unmapped;
+
+	mutex_exit(EPT_LOCK(emap));
+
+	return (unmapped);
+}
+
+struct vmm_pt_ops ept_ops = {
+	.vpo_init	= ept_create,
+	.vpo_free	= ept_destroy,
+	.vpo_wired_cnt	= ept_wired_count,
+	.vpo_is_wired	= ept_is_wired,
+	.vpo_map	= ept_map,
+	.vpo_unmap	= ept_unmap,
+};
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c b/usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c
index 58c10d9da0..d3845ce16f 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c
+++ b/usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c
@@ -38,73 +38,6 @@
 #define	VMMAP_TO_VMSPACE(vmmap)	((struct vmspace *)		\
 	((caddr_t)(vmmap) - offsetof(struct vmspace, vm_map)))
 
-/* Similar to htable, but without the bells and whistles */
-struct eptable {
-	struct eptable	*ept_next;
-	uintptr_t	ept_vaddr;
-	pfn_t		ept_pfn;
-	int16_t		ept_level;
-	int16_t		ept_valid_cnt;
-	uint32_t	_ept_pad2;
-	struct eptable	*ept_prev;
-	struct eptable	*ept_parent;
-	void		*ept_kva;
-};
-typedef struct eptable eptable_t;
-
-struct eptable_map {
-	kmutex_t		em_lock;
-	eptable_t		*em_root;
-	eptable_t		**em_hash;
-	size_t			em_table_cnt;
-	size_t			em_wired;
-
-	/* Protected by eptable_map_lock */
-	struct eptable_map	*em_next;
-	struct eptable_map	*em_prev;
-};
-typedef struct eptable_map eptable_map_t;
-
-#define	EPTABLE_HASH(va, lvl, sz)				\
-	((((va) >> LEVEL_SHIFT(1)) + ((va) >> 28) + (lvl))	\
-	& ((sz) - 1))
-
-#define	EPTABLE_VA2IDX(tbl, va)					\
-	(((va) - (tbl)->ept_vaddr) >>				\
-	LEVEL_SHIFT((tbl)->ept_level))
-
-#define	EPTABLE_IDX2VA(tbl, idx)				\
-	((tbl)->ept_vaddr +					\
-	((idx) << LEVEL_SHIFT((tbl)->ept_level)))
-
-#define	EPT_R		(0x1 << 0)
-#define	EPT_W		(0x1 << 1)
-#define	EPT_X		(0x1 << 2)
-#define	EPT_RWX		(EPT_R|EPT_W|EPT_X)
-#define	EPT_LGPG	(0x1 << 7)
-
-#define	EPT_PAT(attr)	(((attr) & 0x7) << 3)
-
-#define	EPT_PADDR	(0x000ffffffffff000ull)
-
-#define	EPT_IS_ABSENT(pte)	(((pte) & EPT_RWX) == 0)
-#define	EPT_PTE_PFN(pte)	mmu_btop((pte) & EPT_PADDR)
-#define	EPT_PTE_PROT(pte)	((pte) & EPT_RWX)
-#define	EPT_MAPS_PAGE(lvl, pte)				\
-	((lvl) == 0 || ((pte) & EPT_LGPG))
-
-#define	EPT_PTE_ASSIGN_TABLE(pfn)			\
-	((pfn_to_pa(pfn) & EPT_PADDR) | EPT_RWX)
-
-/*
- * We only assign EPT_LGPG for levels higher than 0: although this bit is
- * defined as being ignored at level 0, some versions of VMWare fail to honor
- * this and report such a PTE as an EPT mis-configuration.
- */
-#define	EPT_PTE_ASSIGN_PAGE(lvl, pfn, prot, attr)	\
-	((pfn_to_pa(pfn) & EPT_PADDR) |			\
-	(((lvl) != 0) ? EPT_LGPG : 0) |			\
-	EPT_PAT(attr) | ((prot) & EPT_RWX))
 
 struct vmspace_mapping {
 	list_node_t	vmsm_node;
@@ -123,26 +56,10 @@ typedef struct vmspace_mapping vmspace_mapping_t;
 
 /* Private glue interfaces */
 static void pmap_free(pmap_t);
-static eptable_t *eptable_alloc(void);
-static void eptable_free(eptable_t *);
-static void eptable_init(eptable_map_t *);
-static void eptable_fini(eptable_map_t *);
-static eptable_t *eptable_hash_lookup(eptable_map_t *, uintptr_t, level_t);
-static void eptable_hash_insert(eptable_map_t *, eptable_t *);
-static void eptable_hash_remove(eptable_map_t *, eptable_t *);
-static eptable_t *eptable_walk(eptable_map_t *, uintptr_t, level_t, uint_t *,
-    boolean_t);
-static pfn_t eptable_mapin(eptable_map_t *, uintptr_t, pfn_t, uint_t, uint_t,
-    vm_memattr_t);
-static void eptable_mapout(eptable_map_t *, uintptr_t);
-static int eptable_find(eptable_map_t *, uintptr_t, pfn_t *, uint_t *);
 static vmspace_mapping_t *vm_mapping_find(struct vmspace *, uintptr_t, size_t,
     boolean_t);
 static void vm_mapping_remove(struct vmspace *, vmspace_mapping_t *);
 
-static kmutex_t eptable_map_lock;
-static struct eptable_map *eptable_map_head = NULL;
-
 static vmem_t *vmm_alloc_arena = NULL;
 
 static void *
@@ -249,110 +166,29 @@ vmspace_find_kva(struct vmspace *vms, uintptr_t addr, size_t size)
 	return (result);
 }
 
-static int
-vmspace_pmap_wire(struct vmspace *vms, uintptr_t addr, pfn_t pfn, uint_t lvl,
-    uint_t prot, vm_memattr_t attr)
-{
-	enum pmap_type type = vms->vms_pmap.pm_type;
-
-	ASSERT(MUTEX_HELD(&vms->vms_lock));
-
-	switch (type) {
-	case PT_EPT: {
-		eptable_map_t *map = (eptable_map_t *)vms->vms_pmap.pm_map;
-
-		(void) eptable_mapin(map, addr, pfn, lvl, prot, attr);
-
-		vms->vms_pmap.pm_eptgen++;
-		return (0);
-	}
-	case PT_RVI:
-		/* RVI support not yet implemented */
-	default:
-		panic("unsupported pmap type: %x", type);
-		/* NOTREACHED */
-		break;
-	}
-	return (0);
-}
-
 static int
 vmspace_pmap_iswired(struct vmspace *vms, uintptr_t addr, uint_t *prot)
 {
-	enum pmap_type type = vms->vms_pmap.pm_type;
+	pmap_t pmap = &vms->vms_pmap;
+	int rv;
 
 	ASSERT(MUTEX_HELD(&vms->vms_lock));
 
-	switch (type) {
-	case PT_EPT: {
-		eptable_map_t *map = (eptable_map_t *)vms->vms_pmap.pm_map;
-		pfn_t pfn;
-
-		return (eptable_find(map, addr, &pfn, prot));
-	}
-	case PT_RVI:
-		/* RVI support not yet implemented */
-	default:
-		panic("unsupported pmap type: %x", type);
-		/* NOTREACHED */
-		break;
-	}
-	return (-1);
-}
-
-static int
-vmspace_pmap_unmap(struct vmspace *vms, uintptr_t addr, size_t size)
-{
-	enum pmap_type type = vms->vms_pmap.pm_type;
-
-	ASSERT(MUTEX_HELD(&vms->vms_lock));
-
-	switch (type) {
-	case PT_EPT: {
-		eptable_map_t *map = (eptable_map_t *)vms->vms_pmap.pm_map;
-		uintptr_t maddr = (uintptr_t)addr;
-		const ulong_t npages = btop(size);
-		ulong_t idx;
-
-		/* XXXJOY: punt on large pages for now */
-		for (idx = 0; idx < npages; idx++, maddr += PAGESIZE) {
-			eptable_mapout(map, maddr);
-		}
-		vms->vms_pmap.pm_eptgen++;
-		return (0);
-	}
-		break;
-	case PT_RVI:
-		/* RVI support not yet implemented */
-	default:
-		panic("unsupported pmap type: %x", type);
-		/* NOTREACHED */
-		break;
-	}
-	return (0);
+	rv = pmap->pm_ops->vpo_is_wired(pmap->pm_impl, addr, prot);
+	return (rv);
 }
 
 static void
 pmap_free(pmap_t pmap)
 {
-	switch (pmap->pm_type) {
-	case PT_EPT: {
-		eptable_map_t *map = (eptable_map_t *)pmap->pm_map;
+	void *pmi = pmap->pm_impl;
+	struct vmm_pt_ops *ops = pmap->pm_ops;
 
-		pmap->pm_pml4 = NULL;
-		pmap->pm_map = NULL;
+	pmap->pm_pml4 = NULL;
+	pmap->pm_impl = NULL;
+	pmap->pm_ops = NULL;
 
-		eptable_fini(map);
-		kmem_free(map, sizeof (*map));
-		return;
-	}
-	case PT_RVI:
-		/* RVI support not yet implemented */
-	default:
-		panic("unsupported pmap type: %x", pmap->pm_type);
-		/* NOTREACHED */
-		break;
-	}
+	ops->vpo_free(pmi);
 }
 
 int
@@ -362,18 +198,18 @@ pmap_pinit_type(pmap_t pmap, enum pmap_type type, int flags)
 	pmap->pm_type = type;
 	switch (type) {
 	case PT_EPT: {
-		eptable_map_t *map;
+		struct vmm_pt_ops *ops = &ept_ops;
+		void *pml4, *pmi;
 
-		map = kmem_zalloc(sizeof (*map), KM_SLEEP);
-		eptable_init(map);
+		pmi = ops->vpo_init((uintptr_t *)&pml4);
 
-		pmap->pm_map = map;
-		pmap->pm_pml4 = map->em_root->ept_kva;
+		pmap->pm_ops = ops;
+		pmap->pm_impl = pmi;
+		pmap->pm_pml4 = pml4;
 		return (1);
 	}
 	case PT_RVI:
 		/* RVI support not yet implemented */
-		return (0);
 	default:
 		panic("unsupported pmap type: %x", type);
 		/* NOTREACHED */
@@ -387,20 +223,9 @@ pmap_pinit_type(pmap_t pmap, enum pmap_type type, int flags)
 long
 pmap_wired_count(pmap_t pmap)
 {
-	enum pmap_type type = pmap->pm_type;
-	long val = 0L;
+	long val;
 
-	switch (type) {
-	case PT_EPT:
-		val = ((eptable_map_t *)pmap->pm_map)->em_wired;
-		break;
-	case PT_RVI:
-		/* RVI support not yet implemented */
-	default:
-		panic("unsupported pmap type: %x", type);
-		/* NOTREACHED */
-		break;
-	}
+	val = pmap->pm_ops->vpo_wired_cnt(pmap->pm_impl);
 	return (val);
 }
 
@@ -412,361 +237,6 @@ pmap_emulate_accessed_dirty(pmap_t pmap, vm_offset_t va, int ftype)
 }
 
 
-static eptable_t *
-eptable_alloc(void)
-{
-	eptable_t *ept;
-	caddr_t page;
-
-	ept = kmem_zalloc(sizeof (*ept), KM_SLEEP);
-	page = kmem_zalloc(PAGESIZE, KM_SLEEP);
-	ept->ept_kva = page;
-	ept->ept_pfn = hat_getpfnum(kas.a_hat, page);
-
-	return (ept);
-}
-
-static void
-eptable_free(eptable_t *ept)
-{
-	void *page = ept->ept_kva;
-
-	ASSERT(ept->ept_pfn != PFN_INVALID);
-	ASSERT(ept->ept_kva != NULL);
-
-	ept->ept_pfn = PFN_INVALID;
-	ept->ept_kva = NULL;
-
-	kmem_free(page, PAGESIZE);
-	kmem_free(ept, sizeof (*ept));
-}
-
-static void
-eptable_init(eptable_map_t *map)
-{
-	eptable_t *root;
-
-	VERIFY0(mmu.hash_cnt & (mmu.hash_cnt - 1));
-
-	map->em_table_cnt = mmu.hash_cnt;
-	map->em_hash = kmem_zalloc(sizeof (eptable_t *) * map->em_table_cnt,
-	    KM_SLEEP);
-
-	root = eptable_alloc();
-	root->ept_level = mmu.max_level;
-	map->em_root = root;
-
-	/* Insert into global tracking list of eptable maps */
-	mutex_enter(&eptable_map_lock);
-	map->em_next = eptable_map_head;
-	map->em_prev = NULL;
-	if (eptable_map_head != NULL) {
-		eptable_map_head->em_prev = map;
-	}
-	eptable_map_head = map;
-	mutex_exit(&eptable_map_lock);
-}
-
-static void
-eptable_fini(eptable_map_t *map)
-{
-	const uint_t cnt = map->em_table_cnt;
-
-	/* Remove from global tracking list of eptable maps */
-	mutex_enter(&eptable_map_lock);
-	if (map->em_next != NULL) {
-		map->em_next->em_prev = map->em_prev;
-	}
-	if (map->em_prev != NULL) {
-		map->em_prev->em_next = map->em_next;
-	} else {
-		eptable_map_head = map->em_next;
-	}
-	mutex_exit(&eptable_map_lock);
-
-	mutex_enter(&map->em_lock);
-	/* XXJOY: Should we expect to need this clean-up? */
-	for (uint_t i = 0; i < cnt; i++) {
-		eptable_t *ept = map->em_hash[i];
-
-		while (ept != NULL) {
-			eptable_t *next = ept->ept_next;
-
-			eptable_hash_remove(map, ept);
-			eptable_free(ept);
-			ept = next;
-		}
-	}
-
-	kmem_free(map->em_hash, sizeof (eptable_t *) * cnt);
-	eptable_free(map->em_root);
-
-	mutex_exit(&map->em_lock);
-	mutex_destroy(&map->em_lock);
-}
-
-static eptable_t *
-eptable_hash_lookup(eptable_map_t *map, uintptr_t va, level_t lvl)
-{
-	const uint_t hash = EPTABLE_HASH(va, lvl, map->em_table_cnt);
-	eptable_t *ept;
-
-	ASSERT(MUTEX_HELD(&map->em_lock));
-
-	for (ept = map->em_hash[hash]; ept != NULL; ept = ept->ept_next) {
-		if (ept->ept_vaddr == va && ept->ept_level == lvl)
-			break;
-	}
-	return (ept);
-}
-
-static void
-eptable_hash_insert(eptable_map_t *map, eptable_t *ept)
-{
-	const uintptr_t va = ept->ept_vaddr;
-	const uint_t lvl = ept->ept_level;
-	const uint_t hash = EPTABLE_HASH(va, lvl, map->em_table_cnt);
-
-	ASSERT(MUTEX_HELD(&map->em_lock));
-	ASSERT(eptable_hash_lookup(map, va, lvl) == NULL);
-
-	ept->ept_prev = NULL;
-	if (map->em_hash[hash] == NULL) {
-		ept->ept_next = NULL;
-	} else {
-		eptable_t *chain = map->em_hash[hash];
-
-		ept->ept_next = chain;
-		chain->ept_prev = ept;
-	}
-	map->em_hash[hash] = ept;
-}
-
-static void
-eptable_hash_remove(eptable_map_t *map, eptable_t *ept)
-{
-	const uintptr_t va = ept->ept_vaddr;
-	const uint_t lvl = ept->ept_level;
-	const uint_t hash = EPTABLE_HASH(va, lvl, map->em_table_cnt);
-
-	ASSERT(MUTEX_HELD(&map->em_lock));
-
-	if (ept->ept_prev == NULL) {
-		ASSERT(map->em_hash[hash] == ept);
-
-		map->em_hash[hash] = ept->ept_next;
-	} else {
-		ept->ept_prev->ept_next = ept->ept_next;
-	}
-	if (ept->ept_next != NULL) {
-		ept->ept_next->ept_prev = ept->ept_prev;
-	}
-	ept->ept_next = NULL;
-	ept->ept_prev = NULL;
-}
-
-static eptable_t *
-eptable_walk(eptable_map_t *map, uintptr_t va, level_t tgtlvl, uint_t *idxp,
-    boolean_t do_create)
-{
-	eptable_t *ept = map->em_root;
-	level_t lvl = ept->ept_level;
-	uint_t idx = UINT_MAX;
-
-	ASSERT(MUTEX_HELD(&map->em_lock));
-
-	while (lvl >= tgtlvl) {
-		x86pte_t *ptes, entry;
-		const uintptr_t masked_va = va & LEVEL_MASK((uint_t)lvl);
-		eptable_t *newept = NULL;
-
-		idx = EPTABLE_VA2IDX(ept, va);
-		if (lvl == tgtlvl || lvl == 0) {
-			break;
-		}
-
-		ptes = (x86pte_t *)ept->ept_kva;
-		entry = ptes[idx];
-		if (EPT_IS_ABSENT(entry)) {
-			if (!do_create) {
-				break;
-			}
-
-			newept = eptable_alloc();
-			newept->ept_level = lvl - 1;
-			newept->ept_vaddr = masked_va;
-			newept->ept_parent = ept;
-
-			eptable_hash_insert(map, newept);
-			entry = EPT_PTE_ASSIGN_TABLE(newept->ept_pfn);
-			ptes[idx] = entry;
-			ept->ept_valid_cnt++;
-		} else if (!EPT_MAPS_PAGE(lvl, entry)) {
-			/* Do lookup in next level of page table */
-			newept = eptable_hash_lookup(map, masked_va, lvl - 1);
-
-			VERIFY(newept);
-			VERIFY3P(pfn_to_pa(newept->ept_pfn), ==,
-			    (entry & EPT_PADDR));
-		} else {
-			/*
-			 * There is a (large) page mapped here.  Since support
-			 * for non-PAGESIZE pages is not yet present, this is a
-			 * surprise.
-			 */
-			panic("unexpected large page in pte %p", &ptes[idx]);
-		}
-		ept = newept;
-		lvl--;
-	}
-
-	VERIFY(lvl >= 0 && idx != UINT_MAX);
-	*idxp = idx;
-	return (ept);
-}
-
-static pfn_t
-eptable_mapin(eptable_map_t *map, uintptr_t va, pfn_t pfn, uint_t lvl,
-    uint_t prot, vm_memattr_t attr)
-{
-	uint_t idx;
-	eptable_t *ept;
-	x86pte_t *ptes, entry;
-	const size_t pgsize = (size_t)LEVEL_SIZE(lvl);
-	pfn_t oldpfn = PFN_INVALID;
-
-	CTASSERT(EPT_R == PROT_READ);
-	CTASSERT(EPT_W == PROT_WRITE);
-	CTASSERT(EPT_X == PROT_EXEC);
-	ASSERT((prot & EPT_RWX) != 0 && (prot & ~EPT_RWX) == 0);
-
-	/* XXXJOY: punt on large pages for now */
-	VERIFY(lvl == 0);
-
-	mutex_enter(&map->em_lock);
-	ept = eptable_walk(map, va, (level_t)lvl, &idx, B_TRUE);
-	ptes = (x86pte_t *)ept->ept_kva;
-	entry = ptes[idx];
-
-	if (!EPT_IS_ABSENT(entry)) {
-		if (!EPT_MAPS_PAGE(lvl, entry)) {
-			panic("unexpected PT link %lx in %p[%d]",
-			    entry, ept, idx);
-		}
-
-		/*
-		 * XXXJOY: Just clean the entry for now. Assume(!) that
-		 * invalidation is going to occur anyways.
-		 */
-		oldpfn = EPT_PTE_PFN(ptes[idx]);
-		ept->ept_valid_cnt--;
-		ptes[idx] = (x86pte_t)0;
-		map->em_wired -= (pgsize >> PAGESHIFT);
-	}
-
-	entry = EPT_PTE_ASSIGN_PAGE(lvl, pfn, prot, attr);
-	ptes[idx] = entry;
-	ept->ept_valid_cnt++;
-	map->em_wired += (pgsize >> PAGESHIFT);
-	mutex_exit(&map->em_lock);
-
-	return (oldpfn);
-}
-
-static void
-eptable_mapout(eptable_map_t *map, uintptr_t va)
-{
-	eptable_t *ept;
-	uint_t idx;
-	x86pte_t *ptes, entry;
-
-	mutex_enter(&map->em_lock);
-	/* Find the lowest level entry at this VA */
-	ept = eptable_walk(map, va, -1, &idx, B_FALSE);
-
-	ptes = (x86pte_t *)ept->ept_kva;
-	entry = ptes[idx];
-
-	if (EPT_IS_ABSENT(entry)) {
-		/*
-		 * There is nothing here to free up.  If this was a sparsely
-		 * wired mapping, the absence is no concern.
-		 */
-		mutex_exit(&map->em_lock);
-		return;
-	} else {
-		const size_t pagesize = LEVEL_SIZE((uint_t)ept->ept_level);
-
-		if (!EPT_MAPS_PAGE(ept->ept_level, entry)) {
-			panic("unexpected PT link %lx in %p[%d]",
-			    entry, ept, idx);
-		}
-
-		/*
-		 * XXXJOY: Just clean the entry for now. Assume(!) that
-		 * invalidation is going to occur anyways.
-		 */
-		ept->ept_valid_cnt--;
-		ptes[idx] = (x86pte_t)0;
-		map->em_wired -= (pagesize >> PAGESHIFT);
-	}
-
-	while (ept->ept_valid_cnt == 0 && ept->ept_parent != NULL) {
-		eptable_t *next = ept->ept_parent;
-
-		idx = EPTABLE_VA2IDX(next, va);
-		ptes = (x86pte_t *)next->ept_kva;
-
-		entry = ptes[idx];
-		ASSERT(!EPT_MAPS_PAGE(next->ept_level, entry));
-		ASSERT(EPT_PTE_PFN(entry) == ept->ept_pfn);
-
-		ptes[idx] = (x86pte_t)0;
-		next->ept_valid_cnt--;
-		eptable_hash_remove(map, ept);
-		ept->ept_parent = NULL;
-		eptable_free(ept);
-
-		ept = next;
-	}
-	mutex_exit(&map->em_lock);
-}
-
-static int
-eptable_find(eptable_map_t *map, uintptr_t va, pfn_t *pfn, uint_t *prot)
-{
-	eptable_t *ept;
-	uint_t idx;
-	x86pte_t *ptes, entry;
-	int err = -1;
-
-	mutex_enter(&map->em_lock);
-	/* Find the lowest level entry at this VA */
-	ept = eptable_walk(map, va, -1, &idx, B_FALSE);
-
-	/* XXXJOY: Until large pages are supported, this check is easy */
-	if (ept->ept_level != 0) {
-		mutex_exit(&map->em_lock);
-		return (-1);
-	}
-
-	ptes = (x86pte_t *)ept->ept_kva;
-	entry = ptes[idx];
-
-	if (!EPT_IS_ABSENT(entry)) {
-		if (!EPT_MAPS_PAGE(ept->ept_level, entry)) {
-			panic("unexpected PT link %lx in %p[%d]",
-			    entry, ept, idx);
-		}
-
-		*pfn = EPT_PTE_PFN(entry);
-		*prot = EPT_PTE_PROT(entry);
-		err = 0;
-	}
-
-	mutex_exit(&map->em_lock);
-	return (err);
-}
 
 struct sglist_ent {
 	vm_paddr_t	sge_pa;
@@ -1169,6 +639,8 @@ int
 vm_fault(vm_map_t map, vm_offset_t off, vm_prot_t type, int flag)
 {
 	struct vmspace *vms = VMMAP_TO_VMSPACE(map);
+	pmap_t pmap = &vms->vms_pmap;
+	void *pmi = pmap->pm_impl;
 	const uintptr_t addr = off;
 	vmspace_mapping_t *vmsm;
 	struct vm_object *vmo;
@@ -1211,11 +683,12 @@ vm_fault(vm_map_t map, vm_offset_t off, vm_prot_t type, int flag)
 	VERIFY(pfn != PFN_INVALID);
 
 	/*
-	 * If pmap failure is to be handled, the previously
-	 * acquired page locks would need to be released.
+	 * If pmap failure is to be handled, the previously acquired page locks
+	 * would need to be released.
 	 */
-	VERIFY0(vmspace_pmap_wire(vms, map_addr, pfn, map_lvl, prot,
+	VERIFY0(pmap->pm_ops->vpo_map(pmi, map_addr, pfn, map_lvl, prot,
 	    vmo->vmo_attr));
+	pmap->pm_eptgen++;
 
 	mutex_exit(&vms->vms_lock);
 	return (0);
@@ -1331,10 +804,11 @@ int
 vm_map_remove(vm_map_t map, vm_offset_t start, vm_offset_t end)
 {
 	struct vmspace *vms = VMMAP_TO_VMSPACE(map);
+	pmap_t pmap = &vms->vms_pmap;
+	void *pmi = pmap->pm_impl;
 	const uintptr_t addr = start;
 	const size_t size = (size_t)(end - start);
 	vmspace_mapping_t *vmsm;
-	objtype_t type;
 
 	ASSERT(start < end);
 
@@ -1348,17 +822,8 @@ vm_map_remove(vm_map_t map, vm_offset_t start, vm_offset_t end)
 		return (ENOENT);
 	}
 
-	type = vmsm->vmsm_object->vmo_type;
-	switch (type) {
-	case OBJT_DEFAULT:
-	case OBJT_SG:
-		VERIFY0(vmspace_pmap_unmap(vms, addr, size));
-		break;
-	default:
-		panic("unsupported object type: %x", type);
-		/* NOTREACHED */
-		break;
-	}
+	(void) pmap->pm_ops->vpo_unmap(pmi, addr, end);
+	pmap->pm_eptgen++;
 
 	vm_mapping_remove(vms, vmsm);
 	vms->vms_map_changing = B_FALSE;
@@ -1370,6 +835,8 @@ int
 vm_map_wire(vm_map_t map, vm_offset_t start, vm_offset_t end, int flags)
 {
 	struct vmspace *vms = VMMAP_TO_VMSPACE(map);
+	pmap_t pmap = &vms->vms_pmap;
+	void *pmi = pmap->pm_impl;
 	const uintptr_t addr = start;
 	const size_t size = end - start;
 	vmspace_mapping_t *vmsm;
@@ -1397,8 +864,10 @@ vm_map_wire(vm_map_t map, vm_offset_t start, vm_offset_t end, int flags)
 		map_addr = P2ALIGN(pos, pg_size);
 		VERIFY(pfn != PFN_INVALID);
 
-		VERIFY0(vmspace_pmap_wire(vms, map_addr, pfn, map_lvl, prot,
-		    vmo->vmo_attr));
+		VERIFY0(pmap->pm_ops->vpo_map(pmi, map_addr, pfn, map_lvl,
+		    prot, vmo->vmo_attr));
+		vms->vms_pmap.pm_eptgen++;
+
 		pos += pg_size;
 	}
 
diff --git a/usr/src/uts/i86pc/os/gipt.c b/usr/src/uts/i86pc/os/gipt.c
new file mode 100644
index 0000000000..38c5f20717
--- /dev/null
+++ b/usr/src/uts/i86pc/os/gipt.c
@@ -0,0 +1,340 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2018 Joyent, Inc.
+ */
+
+#include <sys/gipt.h>
+#include <sys/malloc.h>
+#include <sys/kmem.h>
+#include <vm/hat.h>
+#include <vm/as.h>
+
+/* Similar to htable, but without the bells and whistles */
+
+/*
+ * For now, the level shifts are hard-coded to match with standard 4-level
+ * 64-bit paging structures.
+ */
+
+#define	GIPT_HASH(map, va, lvl)			\
+	((((va) >> 12) + ((va) >> 28) + (lvl)) & ((map)->giptm_table_cnt - 1))
+
+const uint_t gipt_level_shift[GIPT_MAX_LEVELS+1] = {
+	12,	/* 4K */
+	21,	/* 2M */
+	30,	/* 1G */
+	39,	/* 512G */
+	48	/* MAX */
+};
+const uint64_t gipt_level_mask[GIPT_MAX_LEVELS+1] = {
+	0xfffffffffffff000ull,	/* 4K */
+	0xffffffffffe00000ull,	/* 2M */
+	0xffffffffc0000000ull,	/* 1G */
+	0xffffff8000000000ull,	/* 512G */
+	0xffff000000000000ull	/* MAX */
+};
+const uint64_t gipt_level_size[GIPT_MAX_LEVELS+1] = {
+	0x0000000000001000ull,	/* 4K */
+	0x0000000000200000ull,	/* 2M */
+	0x0000000040000000ull,	/* 1G */
+	0x0000008000000000ull,	/* 512G */
+	0x0001000000000000ull	/* MAX */
+};
+const uint64_t gipt_level_count[GIPT_MAX_LEVELS+1] = {
+	0x0000000000000001ull,	/* 4K */
+	0x0000000000000200ull,	/* 2M */
+	0x0000000000040000ull,	/* 1G */
+	0x0000000008000000ull,	/* 512G */
+	0x0000001000000000ull	/* MAX */
+};
+
+/*
+ * Allocate a gipt_t structure with corresponding page of memory to hold the
+ * PTEs which it contains.
+ */
+gipt_t *
+gipt_alloc(void)
+{
+	gipt_t *pt;
+	void *page;
+
+	pt = kmem_zalloc(sizeof (*pt), KM_SLEEP);
+	page = kmem_zalloc(PAGESIZE, KM_SLEEP);
+	pt->gipt_kva = page;
+	pt->gipt_pfn = hat_getpfnum(kas.a_hat, page);
+
+	return (pt);
+}
+
+/*
+ * Free a gipt_t structure along with its page of PTE storage.
+ */
+void
+gipt_free(gipt_t *pt)
+{
+	void *page = pt->gipt_kva;
+
+	ASSERT(pt->gipt_pfn != PFN_INVALID);
+	ASSERT(pt->gipt_kva != NULL);
+
+	pt->gipt_pfn = PFN_INVALID;
+	pt->gipt_kva = NULL;
+
+	kmem_free(page, PAGESIZE);
+	kmem_free(pt, sizeof (*pt));
+}
+
+/*
+ * Given a VA inside a gipt_t, calculate (based on the level of that PT) the VA
+ * corresponding to the next entry in the table.  It returns 0 if that VA would
+ * fall beyond the bounds of the table.
+ */
+uint64_t
+gipt_next_va(gipt_t *pt, uint64_t va)
+{
+	const uint_t lvl = pt->gipt_level;
+	const uint64_t masked = va & gipt_level_mask[lvl];
+	const uint64_t max = pt->gipt_vaddr + gipt_level_size[lvl+1];
+	const uint64_t next = masked + gipt_level_size[lvl];
+
+	ASSERT3U(masked, >=, pt->gipt_vaddr);
+	ASSERT3U(masked, <, max);
+
+	/*
+	 * If the "next" VA would be outside this table, including cases where
+	 * it overflowed, indicate an error result.
+	 */
+	if (next >= max || next <= masked) {
+		return (0);
+	}
+	return (next);
+}
+
+/*
+ * Initialize a gipt_map_t with a max level (must be >= 1) and allocating its
+ * hash table based on a provided size (must be a power of 2).
+ */
+void
+gipt_map_init(gipt_map_t *map, uint_t levels, uint_t hash_table_size)
+{
+	VERIFY(map->giptm_root == NULL);
+	VERIFY(map->giptm_hash == NULL);
+	VERIFY3U(levels, >, 0);
+	/* hash size must be power of 2 */
+	VERIFY0(hash_table_size & (hash_table_size - 1));
+
+	mutex_init(&map->giptm_lock, NULL, MUTEX_DEFAULT, NULL);
+	map->giptm_table_cnt = hash_table_size;
+	map->giptm_hash = kmem_zalloc(sizeof (gipt_t *) * map->giptm_table_cnt,
+	    KM_SLEEP);
+	map->giptm_levels = levels;
+}
+
+/*
+ * Clean up a gipt_map_t by removing any linger gipt_t entries referenced by
+ * it, and freeing its hash table.
+ */
+void
+gipt_map_fini(gipt_map_t *map)
+{
+	const uint_t cnt = map->giptm_table_cnt;
+	const size_t sz = sizeof (gipt_t *) * cnt;
+
+	mutex_enter(&map->giptm_lock);
+	/* Clean up any lingering tables */
+	for (uint_t i = 0; i < cnt; i++) {
+		gipt_t *pt = map->giptm_hash[i];
+
+		while (pt != NULL) {
+			gipt_t *next = pt->gipt_next;
+
+			gipt_map_remove(map, pt);
+			gipt_free(pt);
+			pt = next;
+		}
+	}
+
+	kmem_free(map->giptm_hash, sz);
+	map->giptm_hash = NULL;
+	map->giptm_root = NULL;
+	map->giptm_levels = 0;
+	mutex_exit(&map->giptm_lock);
+	mutex_destroy(&map->giptm_lock);
+}
+
+/*
+ * Look in the map for a gipt_t containing a given VA which is located at a
+ * specified level.
+ */
+gipt_t *
+gipt_map_lookup(gipt_map_t *map, uint64_t va, uint_t lvl)
+{
+	gipt_t *pt;
+
+	ASSERT(MUTEX_HELD(&map->giptm_lock));
+	ASSERT3U(lvl, <=, GIPT_MAX_LEVELS);
+
+	/*
+	 * Lookup gipt_t at the VA aligned to the next level up.  For example,
+	 * level 0 corresponds to a page table containing 512 PTEs which cover
+	 * 4k each, spanning a total 2MB. As such, the base VA of that table
+	 * must be aligned to the same 2MB.
+	 */
+	const uint64_t masked_va = va & gipt_level_mask[lvl + 1];
+	const uint_t hash = GIPT_HASH(map, masked_va, lvl);
+
+	/* Only the root is expected to be at the top level. */
+	if (lvl == (map->giptm_levels - 1) && map->giptm_root != NULL) {
+		pt = map->giptm_root;
+
+		ASSERT3U(pt->gipt_level, ==, lvl);
+
+		/*
+		 * It may be so that the VA in question is not covered by the
+		 * range of the table root.
+		 */
+		if (pt->gipt_vaddr != masked_va) {
+			return (NULL);
+		}
+
+		return (pt);
+	}
+
+	for (pt = map->giptm_hash[hash]; pt != NULL; pt = pt->gipt_next) {
+		if (pt->gipt_vaddr == masked_va && pt->gipt_level == lvl)
+			break;
+	}
+	return (pt);
+}
+
+/*
+ * Look in the map for the deepest (lowest level) gipt_t which contains a given
+ * VA.  This could still fail if the VA is outside the range of the root entry.
+ */
+gipt_t *
+gipt_map_lookup_deepest(gipt_map_t *map, uint64_t va)
+{
+	gipt_t *pt = NULL;
+	uint_t lvl;
+
+	ASSERT(MUTEX_HELD(&map->giptm_lock));
+
+	for (lvl = 0; lvl < map->giptm_levels; lvl++) {
+		pt = gipt_map_lookup(map, va, lvl);
+		if (pt != NULL) {
+			break;
+		}
+	}
+	return (pt);
+}
+
+/*
+ * Insert a gipt_t into the map based on its VA and level.  It is up to the
+ * caller to ensure that a duplicate entry does not already exist in the map.
+ */
+void
+gipt_map_insert(gipt_map_t *map, gipt_t *pt)
+{
+	const uint_t hash = GIPT_HASH(map, pt->gipt_vaddr, pt->gipt_level);
+
+	ASSERT(MUTEX_HELD(&map->giptm_lock));
+	ASSERT(gipt_map_lookup(map, pt->gipt_vaddr, pt->gipt_level) == NULL);
+	VERIFY3U(pt->gipt_level, <, map->giptm_levels);
+
+	pt->gipt_prev = NULL;
+	if (map->giptm_hash[hash] == NULL) {
+		pt->gipt_next = NULL;
+	} else {
+		gipt_t *chain = map->giptm_hash[hash];
+
+		pt->gipt_next = chain;
+		chain->gipt_prev = pt;
+	}
+	map->giptm_hash[hash] = pt;
+}
+
+/*
+ * Remove a gipt_t from the map.
+ */
+void
+gipt_map_remove(gipt_map_t *map, gipt_t *pt)
+{
+	const uint_t hash = GIPT_HASH(map, pt->gipt_vaddr, pt->gipt_level);
+	gipt_t *prev = pt->gipt_prev;
+	gipt_t *next = pt->gipt_next;
+
+	ASSERT(MUTEX_HELD(&map->giptm_lock));
+
+	if (prev == NULL) {
+		ASSERT(map->giptm_hash[hash] == pt);
+
+		map->giptm_hash[hash] = next;
+	} else {
+		prev->gipt_next = next;
+	}
+	if (next != NULL) {
+		next->gipt_prev = prev;
+	}
+	pt->gipt_next = NULL;
+	pt->gipt_prev = NULL;
+}
+
+/*
+ * Given a VA, create any missing gipt_t entries from the specified level all
+ * the way up to (but not including) the root.  This is done from lowest level
+ * to highest, and stops when an existing table covering that VA is found.
+ * References to any created gipt_t tables, plus the final "found" gipt_t are
+ * stored in 'pts'.  The number of gipt_t pointers stored to 'pts' serves as
+ * the return value (1 <= val <= root level).
+ */
+uint_t
+gipt_map_ensure_chain(gipt_map_t *map, uint64_t va, uint_t lvl, gipt_t **pts)
+{
+	const uint_t root_lvl = map->giptm_root->gipt_level;
+	uint_t clvl = lvl, count = 0;
+	gipt_t *child_pt = NULL;
+
+	ASSERT(MUTEX_HELD(&map->giptm_lock));
+	ASSERT3U(lvl, <, root_lvl);
+	ASSERT3P(map->giptm_root, !=, NULL);
+
+	do {
+		const uint64_t pva = (va & gipt_level_mask[clvl + 1]);
+		gipt_t *pt;
+
+		pt = gipt_map_lookup(map, pva, clvl);
+		if (pt != NULL) {
+			ASSERT3U(pva, ==, pt->gipt_vaddr);
+
+			if (child_pt != NULL) {
+				child_pt->gipt_parent = pt;
+			}
+			pts[count++] = pt;
+			return (count);
+		}
+
+		pt = gipt_alloc();
+		pt->gipt_vaddr = pva;
+		pt->gipt_level = clvl;
+		if (child_pt != NULL) {
+			child_pt->gipt_parent = pt;
+		}
+
+		gipt_map_insert(map, pt);
+		child_pt = pt;
+		pts[count++] = pt;
+		clvl++;
+	} while (clvl <= root_lvl);
+
+	return (count);
+}
diff --git a/usr/src/uts/i86pc/sys/gipt.h b/usr/src/uts/i86pc/sys/gipt.h
new file mode 100644
index 0000000000..b70c47d135
--- /dev/null
+++ b/usr/src/uts/i86pc/sys/gipt.h
@@ -0,0 +1,72 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2018 Joyent, Inc.
+ */
+
+#ifndef _GIPT_H_
+#define	_GIPT_H_
+
+#include <sys/types.h>
+#include <sys/mutex.h>
+#include <sys/param.h>
+
+struct gipt {
+	struct gipt	*gipt_next;
+	uint64_t	gipt_vaddr;
+	uint64_t	gipt_pfn;
+	uint16_t	gipt_level;
+	uint16_t	gipt_valid_cnt;
+	uint32_t	_gipt_pad;
+	struct gipt	*gipt_prev;
+	struct gipt	*gipt_parent;
+	uint64_t	*gipt_kva;
+	uint64_t	_gipt_pad2;
+};
+typedef struct gipt gipt_t;
+
+struct gipt_map {
+	kmutex_t	giptm_lock;
+	gipt_t		*giptm_root;
+	gipt_t		**giptm_hash;
+	size_t		giptm_table_cnt;
+	uint_t		giptm_levels;
+};
+typedef struct gipt_map gipt_map_t;
+
+#define	GIPT_HASH_SIZE_DEFAULT	0x2000
+#define	GIPT_MAX_LEVELS	4
+
+#define	GIPT_VA2IDX(pt, va)			\
+	(((va) - (pt)->gipt_vaddr) >>		\
+	gipt_level_shift[(pt)->gipt_level])
+
+#define	GIPT_VA2PTE(pt, va)	((pt)->gipt_kva[GIPT_VA2IDX(pt, va)])
+#define	GIPT_VA2PTEP(pt, va)	(&(pt)->gipt_kva[GIPT_VA2IDX(pt, va)])
+
+extern const uint_t gipt_level_shift[GIPT_MAX_LEVELS+1];
+extern const uint64_t gipt_level_mask[GIPT_MAX_LEVELS+1];
+extern const uint64_t gipt_level_size[GIPT_MAX_LEVELS+1];
+extern const uint64_t gipt_level_count[GIPT_MAX_LEVELS+1];
+
+extern gipt_t *gipt_alloc(void);
+extern void gipt_free(gipt_t *);
+extern uint64_t gipt_next_va(gipt_t *, uint64_t);
+extern void gipt_map_init(gipt_map_t *, uint_t, uint_t);
+extern void gipt_map_fini(gipt_map_t *);
+extern gipt_t *gipt_map_lookup(gipt_map_t *, uint64_t, uint_t);
+extern gipt_t *gipt_map_lookup_deepest(gipt_map_t *, uint64_t);
+extern void gipt_map_insert(gipt_map_t *, gipt_t *);
+extern void gipt_map_remove(gipt_map_t *, gipt_t *);
+extern uint_t gipt_map_ensure_chain(gipt_map_t *, uint64_t, uint_t, gipt_t **);
+
+#endif /* _GIPT_H_ */
-- 
2.21.0

