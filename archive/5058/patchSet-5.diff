commit bb3b296c61682fd8f239efa525e33c03fb47f955 (refs/changes/58/5058/5)
Author: Mike Gerdts <mike.gerdts@joyent.com>
Date:   2018-11-21T20:24:40+00:00 (11 months ago)
    
    OS-7353 bhyve support for persistent pci slots for disks
    OS-7004 support adding new disks to bhyve VMs via 'vmadm update'

diff --git a/src/Makefile b/src/Makefile
index f402ccd5..9266ed8f 100644
--- a/src/Makefile
+++ b/src/Makefile
@@ -104,6 +104,8 @@ JS_CHECK_TARGETS=\
 	vm/common/nictag.js \
 	vm/tests/common.js \
 	vm/tests/test-alias.js \
+	vm/tests/test-bhyve-pci.js \
+	vm/tests/test-bhyve-pci_slot.js \
 	vm/tests/test-cleanup-on-failure.js \
 	vm/tests/test-create-filesystems.js \
 	vm/tests/test-create.js \
diff --git a/src/vm/man/vmadm.1m.md b/src/vm/man/vmadm.1m.md
index 03ac8d95..f9d289d4 100644
--- a/src/vm/man/vmadm.1m.md
+++ b/src/vm/man/vmadm.1m.md
@@ -896,6 +896,44 @@ tab-complete UUIDs rather than having to type them out for every command.
         update: yes (special, see description in 'update' section above)
         default: no
 
+    disks.*.pci_slot:
+
+        Specifies the virtual PCI slot that this disk will occupy. Bhyve places
+        each disk into a PCI slot that is identified by the PCI bus, device, and
+        function (BDF). The slot may be specified as <bus>:<device>:<function>
+        ("0:4:0"), <device>:<function> ("4:0") or <device> ("4"). If bus or
+        function is not specified, 0 is used.
+
+        Per the PCI specification legal values for bus, device and function are:
+
+          bus: 0 - 255, inclusive
+          device: 0 - 31, inclusive
+          function: 0 - 7, inclusive
+
+        All functions on devices 0, 6, 30, and 31 on bus 0 are reserved.  For
+        maximum compatibility with boot ROMs and guest operating systems, the
+        disk with boot=true should exist on bus 0 device 3, 4, or 5. If any
+        function other than zero (e.g. 0:5:1) is used, function zero on the same
+        device (e.g. 0:5:0) must also be used for the guest OS to recognize the
+        disk in the non-zero slot.
+
+        If pci_slot is not specified, disks will be assigned to available slots
+        in the 0:4:0 - 0:4:7 range. Disks with media=cdrom will be assigned to
+        0:3:0 - 0:3:7.
+
+        The format used by pci_slot is slightly different than that reported by
+        the Linux `lspci` utility that may be used in guests. The format used by
+        `lspci` is <bus>:<device>.<function> with each number is represented in
+        hexadecimal. Also notice the mixture of `:` and `.` separators by
+        `lspci`.
+
+        type: string (<bus>:<device>:<function>, <device>:function, or <device>)
+        vmtype: bhyve
+        listable: yes
+        create: yes
+        update: yes (special, see description in 'update' section above)
+        default: no
+
     disks.*.refreservation:
 
         Specifies a refreservation for this disk. This property controls the
@@ -1807,14 +1845,14 @@ tab-complete UUIDs rather than having to type them out for every command.
 
     routes:
 
-	This is a key-value object that maps destinations to gateways. These
-	will be set as static routes in the VM. The destinations can be either
-	IPs or subnets in CIDR form. The gateways can either be IP addresses,
-	or can be of the form "nics[0]" or "macs[aa:bb:cc:12:34:56]". Using
-	nics[] or macs[] specifies a link-local route. When using nics[] the IP
-	of the numbered nic in that VM's nics array (the first nic is 0) is
-	used. When using macs[] the IP of the nic with the matching mac address
-	in that VM's nic array is used. As an example:
+        This is a key-value object that maps destinations to gateways. These
+        will be set as static routes in the VM. The destinations can be either
+        IPs or subnets in CIDR form. The gateways can either be IP addresses,
+        or can be of the form "nics[0]" or "macs[aa:bb:cc:12:34:56]". Using
+        nics[] or macs[] specifies a link-local route. When using nics[] the IP
+        of the numbered nic in that VM's nics array (the first nic is 0) is
+        used. When using macs[] the IP of the nic with the matching mac address
+        in that VM's nic array is used. As an example:
 
             {
                 "10.2.2.0/24": "10.2.1.1",
@@ -1822,10 +1860,10 @@ tab-complete UUIDs rather than having to type them out for every command.
                 "10.4.0.1": "macs[aa:bb:cc:12:34:56]"
             }
 
-	This sets three static routes: to the 10.2.2.0/24 subnet with a gateway
-	of 10.2.1.1, a link-local route to the host 10.3.0.1 over the VM's
-	second nic, and a link-local route to the host 10.4.0.1 over the VM's
-	nic with the corresponding mac address.
+        This sets three static routes: to the 10.2.2.0/24 subnet with a gateway
+        of 10.2.1.1, a link-local route to the host 10.3.0.1 over the VM's
+        second nic, and a link-local route to the host 10.4.0.1 over the VM's
+        nic with the corresponding mac address.
 
         type: object
         vmtype: OS
diff --git a/src/vm/node_modules/VM.js b/src/vm/node_modules/VM.js
index 710ce962..782a445e 100644
--- a/src/vm/node_modules/VM.js
+++ b/src/vm/node_modules/VM.js
@@ -1300,13 +1300,10 @@ exports.validate = function (brand, action, payload, options, callback)
                 if (!dev.hasOwnProperty('pci_slot')) {
                     errors.missing_properties.push(prop_prefix + '.pci_slot');
                 } else {
-                    var a = dev.pci_slot.split(':');
-                    if (a.length < 1 || a.length > 3
-                        || isNaN(a[0]) || a[0] < 0 || a[0] > 255
-                        || (a.length > 1
-                        && (isNaN(a[1]) || a[1] < 0 || a[1] > 31))
-                        || (a.length > 2
-                        && (isNaN(a[2]) || a[2] < 0 || a[2] > 7))) {
+                    try {
+                        parsePCIslot(dev.pci_slot);
+                    }
+                    catch (e) {
                         errors.bad_values.push(prop_prefix + '.pci_slot');
                     }
                 }
@@ -5843,6 +5840,11 @@ function buildDiskZonecfg(vmobj, payload)
                 + disk.image_name + '")\n';
         }
 
+        if (disk.hasOwnProperty('pci_slot')) {
+            zcfg = zcfg + 'add property (name=pci-slot, value="'
+                + disk.pci_slot + '")\n';
+        }
+
         zcfg = zcfg + 'end\n';
     }
 
@@ -9235,6 +9237,53 @@ function normalizeNics(payload, vmobj)
     }
 }
 
+/*
+ * Ensure all disks being added have pci_slot set. This is called during create
+ * and update.  During create, vmobj is null.
+ *
+ * This may throw an Error that was originally thrown by assignBhyvePCIslots.
+ */
+function normalizeDisks(payload, vmobj, log)
+{
+    var brand = (vmobj && vmobj.brand) ? vmobj.brand : payload.brand;
+    if (brand !== 'bhyve' || !payload.hasOwnProperty('add_disks')) {
+        return;
+    }
+    var opts = {
+        brand: brand,
+        slots: {},
+        log: log
+    };
+
+    // First gather up the list of slots already in use by the VM. We won't
+    // make an attempt to fix them because that would get messy. A previously
+    // created instance may lack static assignments, but those will get fixed
+    // the next time that VM.start() is called. legacy_compat is used in this
+    // pass so that the right slots are set aside for use by the bhyve brand
+    // boot hook. See block comment above assignBhyvePCISlots().
+    if (vmobj) {
+        opts.disks = vmobj.disks;
+        opts.legacy_compat = true;
+        assignBhyvePCIslots(opts);
+    }
+
+    // Now see what needs to be fixed in the add_disks array.
+    log.debug({add_disks: payload.add_disks},
+        'normalizeDisks start payload.add_disks');
+    opts.disks = payload.add_disks;
+    opts.legacy_compat = false;
+    var assignments = assignBhyvePCIslots(opts);
+    var i;
+    for (i = 0; i < assignments.length; i += 1) {
+        var assignment = assignments[i];
+        log.debug({assignment: assignment}, 'normalizeDisks updating payload');
+        assignment.disk.pci_slot = assignment.pci_slot;
+    }
+
+    log.debug({add_disks: payload.add_disks},
+        'normalizeDisks end payload.add_disks');
+}
+
 /*
  * This is called for both create and update, everything here should be safe for
  * both. The vmobj will be set if it's an update.
@@ -9586,6 +9635,15 @@ function normalizePayload(payload, vmobj, log, callback)
         // this will ensure we've got a MAC, etc.
         normalizeNics(payload, vmobj);
 
+        // ensure that pci slots are assigned for bhyve disks
+        try {
+            normalizeDisks(payload, vmobj, log);
+        }
+        catch (err) {
+            callback(err);
+            return;
+        }
+
         vasync.pipeline({
             arg: payload,
             funcs: [
@@ -13737,6 +13795,240 @@ function setDockerRestartOpts(uuid, options, callback) {
     });
 }
 
+/*
+ * Returns an array of {disk, pci_slot} objects contain the recommended
+ * PCI slot assignments. Each returned disk is a member of the opts.disks array.
+ *
+ * The allocation algorithm varies on whether opts.legacy_compat is true. The
+ * legacy compat mode is used by VM.start() to persist disks that were allocated
+ * prior to the static PCI slot assignment feature's availability. The legacy
+ * algorithm here matches the algorithm used by the the bhyve brand boot hook
+ * (/usr/lib/brand/bhyve/boot) when disks don't have pci_slot specified. Because
+ * bhyve instances start at global zone boot by `zoneadm boot` issued by
+ * svc:/system/zones:default, an instance that was provisioned prior to the
+ * introduction of static slot assignments will not get static assignements
+ * until such a time as it is stopped then started.
+ *
+ * The modern allocation scheme puts the boot disk at 0:4:0 and data disks at
+ * 0:4:1 through 0:4:7.  We no longer strive to put disks at 0:5:* so that any
+ * data disk can be removed.  If function 0 of a device (i.e. 0:5:0) does not
+ * exist, guest operating systems may not see other functions of that same
+ * device (e.g. 0:5:1 ... 0:5:7).  This behavior would make it so that removal
+ * of the first data device would render any remaining data devices unusable.
+ *
+ * In the case where new devices are added to an instance that has a disk at
+ * 0:5:0, VM.update() will use legacy compatibilty while figuring out which
+ * slots are currently in use and will not use legacy compatibilty while
+ * assigning new disks.  This will make it so that the original boot disk will
+ * stay at 0:4:0, the original data disk will stay at 0:5:0, and the new data
+ * disk will be found at 0:4:1.  VM.update() can only add disks while the
+ * instance is stopped.  The subsequent VM.start() will assign persistent IDs to
+ * the boot and data disks.
+ *
+ * If a caller needs to figure out slot assignments from multiple sources (i.e.
+ * vmobj and payload), the same opts.slots should be passed for multiple calls.
+ * The first time it should be {}.  The obvious use case here is VM.update()
+ * where the first pass will also set opts.legacy_compat as described above.
+ *
+ * In the event of an error, an Error is thrown.
+ *
+ * Terminology note: A slot is a bus:device:function (bdf) triple.
+ */
+function assignBhyvePCIslots(opts)
+{
+    // See usr/src/lib/brand/bhyve/zone/boot.c in illumos-joyent.
+    var BHYVE_PCI_SLOT_HOSTBRIDGE = 0;
+    var BHYVE_PCI_SLOT_CD = 3;
+    var BHYVE_PCI_SLOT_BOOT_DISK = 4;
+    var BHYVE_PCI_SLOT_OTHER_DISKS = 5;
+    var BHYVE_PCI_SLOT_NICS = 6;
+    var BHYVE_PCI_SLOT_FBUF = 30;
+    var BHYVE_PCI_SLOT_LPC = 31;
+    var brand = opts.brand;
+    var disks = opts.disks;
+    var log = opts.log;
+    var unassigned = [];
+    var slots = opts.slots || {};
+    var i, disk;
+    var legacy_compat = !!opts.legacy_compat;
+    var boot_disk;
+
+    log.debug({disks: disks, slots: slots}, 'assignBhyvePCIslots starting');
+
+    assert.equal(brand, 'bhyve', 'brand must be bhyve');
+    assert.arrayOfObject(disks, 'disks must be as in vmobj or payload');
+    assert.object(log, 'log');
+
+    /*
+     * A few devices on bus 0 are reserved and/or dynamically assigned by
+     * /usr/lib/brand/bhyve/boot.  Stay clear of all functions on those devices.
+     */
+    function reserved_dev(_pcidev) {
+        return ([
+            BHYVE_PCI_SLOT_HOSTBRIDGE,
+            BHYVE_PCI_SLOT_NICS,
+            BHYVE_PCI_SLOT_FBUF,
+            BHYVE_PCI_SLOT_LPC
+        ].indexOf(_pcidev) !== -1);
+    }
+
+    /* Determine which PCI device a disk should be associated with. */
+    function get_default_dev(_disk) {
+        if (_disk.media === 'cdrom') {
+            return BHYVE_PCI_SLOT_CD;
+        }
+        if ([undefined, 'disk'].indexOf(_disk.media) != -1) {
+            if (_disk.boot || !legacy_compat) {
+                return BHYVE_PCI_SLOT_BOOT_DISK;
+            }
+            return BHYVE_PCI_SLOT_OTHER_DISKS;
+        }
+        return undefined;
+    }
+
+    /*
+     * A helper to give useful information about a disk while its object is
+     * still rather sparse.
+     */
+    function disk_desc(_disk) {
+        if (_disk.path) {
+            return 'path=' + _disk.path;
+        }
+        if (_disk.pci_slot) {
+            return 'pci_slot=' + _disk.pci_slot;
+        }
+        return sprintf('"%s"', JSON.stringify(_disk));
+    }
+
+    /*
+     * Determine which functions are in use. Use the `unassigned` array to keep
+     * track of each disk that lacks `pci_slot`.
+     *
+     * Throws an Error if an invalid static assignment is found.
+     */
+    for (i = 0; i < disks.length; i += 1) {
+        disk = disks[i];
+        if (disk.boot) {
+            if (boot_disk) {
+                throw new Error(sprintf('multiple boot disks: "%s" and "%s"',
+                    disk_desc(boot_disk), disk_desc(disk)));
+            }
+            boot_disk = disk.boot;
+        }
+        if (disk.hasOwnProperty('pci_slot')) {
+            var bdf = parsePCIslot(disk.pci_slot);
+            if (bdf.bus === 0) {
+                // Do not allow use of any function on a reserved device.
+                if (reserved_dev(bdf.dev)) {
+                    throw new Error('pci_slot "' + disk.pci_slot
+                        + '" invalid: PCI device ' + bdf.dev + ' is reserved');
+                }
+                // Watch out for multiple occupants
+                if (!slots.hasOwnProperty(bdf.dev)) {
+                    slots[bdf.dev] = {};
+                } else if (slots[bdf.dev].hasOwnProperty(bdf.fn)) {
+                    throw new Error('VM has multiple disks in pci_slot "'
+                        + disk.pci_slot + '": "' + slots[bdf.dev][bdf.fn].path
+                        + '" and "' + disk.path);
+                }
+                // Seems to be a valid assignment so keep track of it.
+                slots[bdf.dev][bdf.fn] = disk;
+                log.debug({disk: disk}, '0:' + bdf.dev + ':' + bdf.fn
+                    + ' already occupied');
+            }
+        } else {
+            // Be sure the boot device gets function 0 if available.
+            if (disk.boot) {
+                unassigned.splice(0, 0, disk);
+            } else {
+                unassigned.push(disk);
+            }
+        }
+    }
+
+    /*
+     * Create the update list. The lowest unused function number is used on each
+     * PCI device.
+     */
+    var assignments = [];
+    log.debug({slots: slots}, 'assignBhyvePCIslots slots before assignments');
+    for (i = 0; i < unassigned.length; i += 1) {
+        disk = unassigned[i];
+        var pcidev = get_default_dev(disk);
+        var fn = 0;
+
+        if (pcidev === undefined) {
+            throw new Error('VM has no PCI bus or slot assigned for '
+                + disk.media
+                + ' devices; automatic pci_slot assignment not possible');
+        }
+        if (!slots.hasOwnProperty(pcidev)) {
+            slots[pcidev] = {};
+        }
+        while (slots[pcidev].hasOwnProperty(fn)) {
+            log.debug({disk: slots[pcidev][fn]}, '0:' + pcidev + ':' + fn
+                + ' already assigned');
+            fn += 1;
+        }
+        if (fn > 7) {
+            throw new Error('VM has more than 8 ' + disk.media
+                + ' devices on PCI device ' + pcidev);
+        }
+        slots[pcidev][fn] = disk;
+        var pci_slot = '0:' + pcidev + ':' + fn;
+        var assignment = {disk: disk, pci_slot: pci_slot};
+        log.debug({assignment: assignment}, pci_slot + ' now assigned');
+        assignments.push(assignment);
+    }
+    log.debug({assignments: assignments}, 'assignBhyvePCIslots done');
+
+    return assignments;
+}
+
+/*
+ * Updates disks.*.pci_slot for each disk that has no static assignement.
+ * Expected to be called during VM.start(). If this called during a provisioning
+ * boot, it is expected that normalizeDisks() will have already been called
+ * during VM.create().
+ */
+function updateBhyvePCIslots(opts, cb)
+{
+    var vmobj = opts.vmobj;
+    var log = opts.log;
+    if (vmobj.brand !== 'bhyve') {
+        cb();
+        return;
+    }
+
+    try {
+        var assignments = assignBhyvePCIslots({
+            log: log,
+            brand: vmobj.brand,
+            disks: vmobj.disks,
+            legacy_compat: opts.legacy_compat
+        });
+    } catch (err) {
+        cb(err);
+        return;
+    }
+
+    if (assignments.length === 0) {
+        cb();
+        return;
+    }
+
+    var updates = [];
+    var i;
+    for (i = 0; i < assignments.length; i += 1) {
+        updates.push({
+            path: assignments[i].disk.path,
+            pci_slot: assignments[i].pci_slot
+        });
+    }
+    log.debug({updates: updates}, 'updateBhyvePCIslots assigning PCI slots');
+    VM.update(vmobj.uuid, {update_disks: updates}, {log: log}, cb);
+}
+
 exports.start = function (uuid, extra, options, callback)
 {
     var load_fields;
@@ -13834,7 +14126,7 @@ exports.start = function (uuid, extra, options, callback)
     log.info('Starting VM ' + uuid);
 
     async.series([
-        function (cb) {
+        function _loadVM(cb) {
             /*
              * If we're being called by something that just loaded the object,
              * we can use that instead of loading again ourselves.
@@ -13874,7 +14166,7 @@ exports.start = function (uuid, extra, options, callback)
                 vmobj = obj;
                 cb();
             });
-        }, function (cb) {
+        }, function _validateNicTags(cb) {
             validateNicTags(vmobj.nics, log, function (e) {
                 if (e) {
                     cb(e);
@@ -13882,7 +14174,7 @@ exports.start = function (uuid, extra, options, callback)
                 }
                 cb();
             });
-        }, function (cb) {
+        }, function _getDockerDeps(cb) {
             var im;
             var to_download = [];
 
@@ -13919,7 +14211,7 @@ exports.start = function (uuid, extra, options, callback)
                 cb();
                 return;
             }
-        }, function (cb) {
+        }, function _restartDocker(cb) {
             if (!vmobj.docker || !vmobj.zonepath) {
                 cb();
                 return;
@@ -13940,7 +14232,10 @@ exports.start = function (uuid, extra, options, callback)
                     value: 0
                 }, cb);
             }
-        }, function (cb) {
+        }, function _persistBhyveDevs(cb) {
+            updateBhyvePCIslots({vmobj: vmobj, log: log, legacy_compat: true},
+                cb);
+        }, function _start(cb) {
             var err;
             var vm_type = BRAND_OPTIONS[vmobj.brand].features.type;
 
@@ -15928,7 +16223,13 @@ exports.update = function (uuid, payload, options, callback)
                 // use in createVolume()
                 if (vmobj.hasOwnProperty('disks')) {
                     for (n in vmobj.disks) {
-                        matches = vmobj.disks[n].path.match(/^.*-disk(\d+)$/);
+                        if (vmobj.brand === 'bhyve') {
+                            matches =
+                                vmobj.disks[n].path.match(/^.*\/disk(\d+)$/);
+                        } else {
+                            matches =
+                                vmobj.disks[n] .path.match(/^.*-disk(\d+)$/);
+                        }
                         if (matches) {
                             used_disk_indexes.push(Number(matches[1]));
                         }
@@ -17715,3 +18016,39 @@ function zoneConfigLockpath(uuid) {
 
     return sprintf('/var/run/vm.%s.config.lockfile', uuid);
 }
+
+/*
+ * Returns object with {bus, dev, fn} or an Error. slotstr is of the form
+ * bus:dev:fn, dev:fn, or dev.  If bus and/or fn are not specified, they will
+ * be returned as 0.
+ *
+ * Throws an Error if slotstr is bogus.
+ */
+function parsePCIslot(slotstr) {
+    var bdf = slotstr.split(':');
+    var bus, dev, fn;
+
+    if (bdf.length === 1) {
+        bus = 0;
+        dev = parseInt(bdf[0], 10);
+        fn = 0;
+    } else if (bdf.length === 2) {
+        bus = 0;
+        dev = parseInt(bdf[0], 10);
+        fn = parseInt(bdf[1], 10);
+    } else if (bdf.length === 3) {
+        bus = parseInt(bdf[0], 10);
+        dev = parseInt(bdf[1], 10);
+        fn = parseInt(bdf[2], 10);
+    } else {
+        throw new Error(sprintf('pci_slot "%s" is not valid', slotstr));
+    }
+
+    if (Number.isNaN(bus) || bus < 0 || bus > 255
+        || Number.isNaN(dev) || dev < 0 || dev > 31
+        || Number.isNaN(fn) || fn < 0 || fn > 7) {
+        throw new Error(sprintf('pci_slot "%s" is not valid', slotstr));
+    }
+
+    return {bus: bus, dev: dev, fn: fn};
+}
diff --git a/src/vm/node_modules/proptable.js b/src/vm/node_modules/proptable.js
index d0865f2a..4b9786a0 100644
--- a/src/vm/node_modules/proptable.js
+++ b/src/vm/node_modules/proptable.js
@@ -655,6 +655,15 @@ exports.properties = {
             type: 'string'
         },
         zonexml: 'zone.device.match'
+    }, 'disks.*.pci_slot': {
+        payload: {
+            allowed: {
+                'bhyve': ['add', 'update']
+            },
+            type: 'string'
+        },
+        updatable: true,
+        zonexml: 'zone.device.net-attr.pci-slot'
     }, 'disks.*.refreservation': {
         payload: {
             allowed: {
diff --git a/src/vm/tests/test-bhyve-pci.js b/src/vm/tests/test-bhyve-pci.js
index 6f9aa445..50481bcf 100644
--- a/src/vm/tests/test-bhyve-pci.js
+++ b/src/vm/tests/test-bhyve-pci.js
@@ -39,11 +39,11 @@ require('nodeunit-plus');
 VM.loglevel = 'DEBUG';
 
 var payload = {
-    alias: 'test-create-bhyve-' + process.pid,
+    alias: 'test-bhyve-pci-' + process.pid,
     autoboot: false,
     brand: 'bhyve',
-    ram: "1024",
-    vcpus: "2",
+    ram: 1024,
+    vcpus: 2,
     do_not_inventory: true,
     nics: [
         {
@@ -56,7 +56,7 @@ var payload = {
         {
             image_uuid: vmtest.CURRENT_BHYVE_CENTOS_UUID,
             boot: true,
-            model: "virtio"
+            model: 'virtio'
         }
     ]
 };
@@ -91,92 +91,92 @@ function validate_missing_property(t, prop, devs) {
     });
 }
 
-test('test validate with bad pci_device path', function(t) {
+test('test validate with bad pci_device path', function (t) {
     validate_bad_value(t, 'path', [
         {
-            path: "/notdevices/pci@0,0/",
-            pci_slot: "5:0:0"
+            path: '/notdevices/pci@0,0/',
+            pci_slot: '5:0:0'
         }
     ]);
 });
 
-test('test validate with bad pci_slot', function(t) {
+test('test validate with bad pci_slot', function (t) {
     validate_bad_value(t, 'pci_slot', [
         {
-            path: "/devices/pci@0,0/pci8086,6f0a@3,2/pci15d9,1528@9,1",
-            pci_slot: "foo"
+            path: '/devices/pci@0,0/pci8086,6f0a@3,2/pci15d9,1528@9,1',
+            pci_slot: 'foo'
         }
     ]);
 });
 
-test('test validate with bad pci_slot (2)', function(t) {
+test('test validate with bad pci_slot (2)', function (t) {
     validate_bad_value(t, 'pci_slot', [
         {
-            path: "/devices/pci@0,0/pci8086,6f0a@3,2/pci15d9,1528@9,1",
-            pci_slot: "4:0:0:0"
+            path: '/devices/pci@0,0/pci8086,6f0a@3,2/pci15d9,1528@9,1',
+            pci_slot: '4:0:0:0'
         }
     ]);
 });
 
-test('test validate with bad pci_slot (3)', function(t) {
+test('test validate with bad pci_slot (3)', function (t) {
     validate_bad_value(t, 'pci_slot', [
         {
-            path: "/devices/pci@0,0/pci8086,6f0a@3,2/pci15d9,1528@9,1",
-            pci_slot: "256:0:0"
+            path: '/devices/pci@0,0/pci8086,6f0a@3,2/pci15d9,1528@9,1',
+            pci_slot: '256:0:0'
         }
     ]);
 });
 
-test('test validate with bad pci_slot (4)', function(t) {
+test('test validate with bad pci_slot (4)', function (t) {
     validate_bad_value(t, 'pci_slot', [
         {
-            path: "/devices/pci@0,0/pci8086,6f0a@3,2/pci15d9,1528@9,1",
-            pci_slot: "-1:0:0"
+            path: '/devices/pci@0,0/pci8086,6f0a@3,2/pci15d9,1528@9,1',
+            pci_slot: '-1:0:0'
         }
     ]);
 });
 
 
-test('test validate with bad pci_slot (4)', function(t) {
+test('test validate with bad pci_slot (4)', function (t) {
     validate_bad_value(t, 'pci_slot', [
         {
-            path: "/devices/pci@0,0/pci8086,6f0a@3,2/pci15d9,1528@9,1",
-            pci_slot: "4:32:0"
+            path: '/devices/pci@0,0/pci8086,6f0a@3,2/pci15d9,1528@9,1',
+            pci_slot: '4:32:0'
         }
     ]);
 });
 
-test('test validate with bad pci_slot (5)', function(t) {
+test('test validate with bad pci_slot (5)', function (t) {
     validate_bad_value(t, 'pci_slot', [
         {
-            path: "/devices/pci@0,0/pci8086,6f0a@3,2/pci15d9,1528@9,1",
-            pci_slot: "4:0:8"
+            path: '/devices/pci@0,0/pci8086,6f0a@3,2/pci15d9,1528@9,1',
+            pci_slot: '4:0:8'
         }
     ]);
 });
 
-test('test validate with bad model)', function(t) {
+test('test validate with bad model)', function (t) {
     validate_bad_value(t, 'model', [
         {
-            path: "/devices/pci@0,0/pci8086,6f0a@3,2/pci15d9,1528@9,1",
-            pci_slot: "4:0:0",
-            model: "passover"
+            path: '/devices/pci@0,0/pci8086,6f0a@3,2/pci15d9,1528@9,1',
+            pci_slot: '4:0:0',
+            model: 'passover'
         }
     ]);
 });
 
-test('test validate with missing path', function(t) {
+test('test validate with missing path', function (t) {
     validate_missing_property(t, 'path', [
         {
-            pci_slot: "4:0:0"
+            pci_slot: '4:0:0'
         }
     ]);
 });
 
-test('test validate with missing pci_slot', function(t) {
+test('test validate with missing pci_slot', function (t) {
     validate_missing_property(t, 'pci_slot', [
         {
-            path: "/devices/pci@0,0/pci8086,6f0a@3,2/pci15d9,1528@9,1",
+            path: '/devices/pci@0,0/pci8086,6f0a@3,2/pci15d9,1528@9,1'
         }
     ]);
 });
diff --git a/src/vm/tests/test-bhyve-pci_slot.js b/src/vm/tests/test-bhyve-pci_slot.js
new file mode 100644
index 00000000..d51a28c2
--- /dev/null
+++ b/src/vm/tests/test-bhyve-pci_slot.js
@@ -0,0 +1,831 @@
+/*
+ * CDDL HEADER START
+ *
+ * The contents of this file are subject to the terms of the
+ * Common Development and Distribution License, Version 1.0 only
+ * (the "License").  You may not use this file except in compliance
+ * with the License.
+ *
+ * You can obtain a copy of the license at http://smartos.org/CDDL
+ *
+ * See the License for the specific language governing permissions
+ * and limitations under the License.
+ *
+ * When distributing Covered Code, include this CDDL HEADER in each
+ * file.
+ *
+ * If applicable, add the following below this CDDL HEADER, with the
+ * fields enclosed by brackets "[]" replaced with your own identifying
+ * information: Portions Copyright [yyyy] [name of copyright owner]
+ *
+ * CDDL HEADER END
+ *
+ * Copyright (c) 2018, Joyent, Inc.
+ *
+ */
+
+var assert = require('/usr/node/node_modules/assert-plus');
+var bunyan = require('/usr/vm/node_modules/bunyan');
+var common = require('./common.js');
+var execFile = require('child_process').execFile;
+var fs = require('fs');
+var sprintf = require('/usr/node/node_modules/sprintf').sprintf;
+var vasync = require('/usr/vm/node_modules/vasync');
+var VM = require('/usr/vm/node_modules/VM');
+var vminfod = require('/usr/vm/node_modules/vminfod/client');
+var vmtest = require('../common/vmtest.js');
+var zonecfg = require('/usr/vm/node_modules/zonecfg');
+
+var log = bunyan.createLogger({
+    level: 'debug',
+    name: 'test-bhyve-pci_slot',
+    streams: [ { stream: process.stderr, level: 'error' } ],
+    serializers: bunyan.stdSerializers
+});
+
+// this puts test stuff in global, so we need to tell jsl about that:
+/* jsl:import ../node_modules/nodeunit-plus/index.js */
+require('nodeunit-plus');
+
+VM.loglevel = 'DEBUG';
+
+/*
+ * This is the main driver for the happy path tests.  It does the following:
+ *
+ *  - creates a VM using the specified payload
+ *  - optionally alters it with opts.damage (and verifies it is damaged)
+ *  - optionally updates it with an update payload
+ *  - optionally sets an operator script to collect PCI device info in guest
+ *  - if damaged or collecting guest info, it is booted
+ *  - PCI slot assignments are compared against expected
+ *  - if collecting guest info, that is verified against config in host
+ *  - if booted, it is stopped
+ *  - VM deletion is handled by after(), which is called after t.end().
+ *
+ * opts.t           test object
+ * opts.payload     VM.create payload. Most likely should have autoboot set to
+ *                  false.
+ * opts.update      Optional VM.update payload
+ * opts.damage      Optional dict with fn and check_disks keys. If present,
+ *                  `fn(opts, next)` will be called between create and update
+ *                  phases.  opts.vmobj will be automatically filled. This can
+ *                  be used to call zonecfg to alter the configuration in ways
+ *                  that VM.js will no longer do - such as to remove pci_slot
+ *                  from existing disks. If this is set, the VM will be started
+ *                  so that VM.start() may repair it.
+ * opts.disks       List of disks to compare to vmobj to ensure assignments are
+ *                  correct. Each disk.path is used to match the disk to the
+ *                  same in vmobj.disks. Set disk.mumble to undefined if wanting
+ *                  to ensure that vmobj does not have mumble set on a
+ *                  particular disk.
+ * opts.expect_bsf  Optional list of 'b:s:f' (bus slot function) elements
+ *                  (decimal) that should match up with virtio-blk devices the
+ *                  guest sees. If this is present, the VM will be booted and
+ *                  the root disk image must support bash scripts and lspci.
+ */
+function testCreateAndCheckDisks(opts) {
+    var t = opts.t;
+    var payload = opts.payload;
+    var expect_bsf = opts.expect_bsf;
+    var disks = opts.disks;
+    var update = opts.update;
+    var damage = opts.damage;
+    var need_start = (opts.hasOwnProperty('expect_bsf')
+        || opts.hasOwnProperty('damage'));
+    var lspci_md = 'lspci-output';
+    var guest_lspci = '#! /bin/bash\n'
+        + 'lspci -n -d 1af4:1001 | mdata-put ' + lspci_md + '\n'
+        + 'poweroff\n';
+
+    vasync.waterfall([
+        function _create(next) {
+            VM.create(payload, function _create_cb(err, obj) {
+                if (err) {
+                    t.ok(false, 'error creating VM: ' + err);
+                } else {
+                    t.ok(true, 'VM created with uuid ' + obj.uuid);
+                }
+                vmobj = obj;
+                next(err);
+            });
+        },
+        function _damage(next) {
+            if (!damage) {
+                t.ok(true, 'Skipping damage - nothing to do');
+                next();
+                return;
+            }
+            assert.func(damage.fn, 'damage.fn must be a function');
+            assert.arrayOfObject(damage.check_disks,
+                'damage.check_disks must be an array of disks');
+
+            t.ok(true, 'damaging zone');
+            vasync.waterfall([
+                function _do_damage(dnext) {
+                    damage.opts.vmobj = vmobj;
+                    damage.fn(damage.opts, dnext);
+                },
+                function _load_damage(dnext) {
+                    VM.load(vmobj.uuid, function _load_damage_cb(err, obj) {
+                        if (err) {
+                            t.ok(false, 'error creating VM: ' + err);
+                        } else {
+                            t.ok(true, 'damaged VM loaded');
+                            vmobj = obj;
+                        }
+                        dnext(err);
+                    });
+                },
+                function _check_damage(dnext) {
+                    t.ok(true, 'Checking damage');
+                    var err = checkDisks({
+                        t: t,
+                        haves: vmobj.disks,
+                        wants: damage.check_disks,
+                        uuid: vmobj.uuid
+                    });
+                    dnext(err);
+                }
+            ],
+            function _damage_done(err) {
+                if (err) {
+                    t.ok(false, 'damage failed; aborting test');
+                } else {
+                    t.ok(true, 'damage done and verified');
+                }
+                next(err);
+            });
+        },
+        function _update(next) {
+            if (!update) {
+                t.ok(true, 'Skipping update - nothing to do');
+                next();
+                return;
+            }
+            VM.update(vmobj.uuid, update, function _update_cb(err) {
+                if (err) {
+                    t.ok(false, 'error updating VM: ' + err);
+                } else {
+                    t.ok(true, 'VM updated');
+                }
+                next(err);
+            });
+        },
+        function _add_operator_script(next) {
+            if (!expect_bsf) {
+                t.ok(true, 'Skipping guest PCI slot check');
+                next();
+                return;
+            }
+            var _payload = {
+                set_internal_metadata: {
+                    'operator-script': guest_lspci
+                }
+            };
+            VM.update(vmobj.uuid, _payload, function _update_cb(err) {
+                if (err) {
+                    t.ok(false, 'error updating VM with operator script: '
+                        + err);
+                } else {
+                    t.ok(true, 'VM updated with operator script');
+                }
+                next(err);
+            });
+        },
+        function _start(next) {
+            if (!need_start) {
+                t.ok(true, 'Skipping guest start');
+                next();
+                return;
+            }
+            VM.start(vmobj.uuid, {}, function _start_cb(err) {
+                if (err) {
+                    t.ok(false, 'error starting VM: ' + err);
+                } else {
+                    t.ok(true, 'VM started');
+                }
+                next(err);
+            });
+        },
+        function _wait_operator_script(next) {
+            if (!expect_bsf) {
+                next();
+                return;
+            }
+            VM.waitForZoneState(payload, 'installed', function (err) {
+                t.ok(!err, 'zone stopped after running operator script');
+                next(err);
+            });
+        },
+        function _load(next) {
+            VM.load(vmobj.uuid, function _load_cb(err, obj) {
+                if (err) {
+                    t.ok(false, 'error creating VM: ' + err);
+                } else {
+                    t.ok(true, 'VM loaded uuid ' + obj.uuid);
+                    vmobj = obj;
+                }
+                next(err);
+            });
+        },
+        function _check(next) {
+            next(checkDisks({
+                t: t,
+                haves: vmobj.disks,
+                wants: disks,
+                uuid: vmobj.uuid
+            }));
+        },
+        function _check_lspci_md(next) {
+            if (!expect_bsf) {
+                next();
+                return;
+            }
+            var cm = vmobj.customer_metadata;
+            if (!cm.hasOwnProperty(lspci_md)) {
+                t.ok(false, 'customer_metadata["' + lspci_md
+                    + '"] not found');
+                next();
+                return;
+            }
+            var guest_bsf = [];
+            var lines = cm[lspci_md].trim().split('\n');
+            lines.forEach(function _gather_guest_pci(line) {
+                /* lspci reports bb:ss.f, with each number in hex */
+                var bsf = line.split(' ')[0].replace('.', ':').split(':');
+                guest_bsf.push(sprintf('%d:%d:%d', parseInt(bsf[0], 16),
+                    parseInt(bsf[1], 16), parseInt(bsf[2], 16)));
+            });
+            t.equal(guest_bsf.sort().join(' '), expect_bsf.sort().join(' '),
+                'PCI slots occupied: ' + expect_bsf.sort().join(' '));
+            next();
+        }
+    ], function _done(err) {
+        t.end(err);
+    });
+}
+
+function dup(thing) {
+    return JSON.parse(JSON.stringify(thing));
+}
+
+/*
+ * Runs zonecfg and waits for vminfod to pick up the changes. If you are having
+ * troubles figuring out what should go into the changes array, set log.level to
+ * debug and observe the bunyan logs that appear in test output.  There are
+ * hints, but not answers, in that output.
+ */
+function zonecfgSync(uuid, args, opts, changes, cb)
+{
+    var vs = new vminfod.VminfodEventStream({
+        name: sprintf('test-bhyve-pci_slot (%s)', uuid),
+        log: log
+    });
+    var cancelFn;
+
+    vs.once('ready', function () {
+        vasync.parallel({funcs: [
+            function _watcher(cb2) {
+                var obj = {
+                    uuid: uuid
+                };
+                var _opts = {
+                    timeout: 5000,
+                    catchErrors: true,
+                    teardown: true
+                };
+                cancelFn = vs.watchForChanges(obj, changes, _opts, cb2);
+            },
+            function _zonecfg(cb2) {
+                zonecfg(uuid, args, opts, function (err, fds) {
+                    if (err) {
+                        cancelFn();
+                        cb2(err);
+                        return;
+                    }
+                    cb2();
+                });
+            }
+        ]}, function _done(err, results) {
+            cb(err);
+        });
+    });
+}
+
+function checkDisks(opts) {
+    var t = opts.t;
+    var haves = opts.haves;
+    var uuid = opts.uuid;
+    var wants = opts.wants;
+    var errors = 0;
+
+    wants.forEach(function _check_disk(want) {
+        var found = false;
+        var path = sprintf(want.path, uuid);
+
+        t.ok(true, 'Checking disk ' + path);
+
+        haves.forEach(function _select_disk(have) {
+            if (path !== have.path) {
+                return;
+            }
+            found = true;
+            Object.keys(want).forEach(function _check_prop(prop) {
+                if (prop === 'path') {
+                    return;
+                }
+                t.equal(have[prop], want[prop], 'matching prop: ' + prop
+                    + '=' + have[prop]);
+                if (have[prop] !== want[prop]) {
+                    errors++;
+                }
+            });
+        });
+        t.ok(found, 'disk ' + path + ' found');
+        if (!found) {
+            errors++;
+        }
+    });
+    if (errors !== 0) {
+        return new Error('checkDisks encountered ' + errors + 'error(s)');
+    }
+    return null;
+}
+
+/*
+ * VM.create() is expected to fail with an Error message that starts with
+ * opts.expect.
+ */
+function testFailCreate(opts) {
+    assert.object(opts.t, 't must be test object');
+    assert.object(opts.payload, 'payload must be an object');
+    assert.string(opts.expect, 'expect must be a string');
+    assert.notEqual(opts.expect.length, 0, 'expect must not be empty');
+
+    var t = opts.t;
+    var payload = opts.payload;
+    var expect = opts.expect;
+
+    vasync.waterfall([
+        function _create(next) {
+            VM.create(payload, function _create_cb(err, obj) {
+                if (err) {
+                    t.equal(err.message.substring(0, expect.length), expect,
+                        'error detected');
+                } else {
+                    t.ok(false, 'No error detected');
+                }
+                // This is just a stub right now. We need to try a load to
+                // see if it was really created.
+                vmobj = obj;
+
+                next();
+            });
+        },
+        function _load(next) {
+            VM.load(vmobj.uuid, function _load_cb(err, obj) {
+                vmobj = obj;
+                t.ok(err, 'VM load should fail');
+                if (err) {
+                    err = next();
+                    return;
+                }
+                next(new Error('VM ' + vmobj.uuid
+                    + '  unexpectedly exists'));
+            });
+        }
+        ], function _done(err) {
+            t.end(err);
+        });
+}
+
+/*
+ * Tests that create a VM should be setting vmobj so that the after hook can
+ * clean up the VM when the test finishes or gives up.  If a test uses vmobj
+ * then deletes the VM on its own, it should set vmobj to undefined.
+ */
+var vmobj;
+
+before(function (cb) {
+    vmobj = undefined;
+    cb();
+});
+
+after(function (cb) {
+    if (vmobj) {
+        VM.delete(vmobj.uuid, {}, function _delete_cb(err) {
+            if (err) {
+                console.log(sprintf('Could not delete vm %s: %s', vmobj.uuid,
+                    err.message));
+            }
+            vmobj = undefined;
+            cb();
+        });
+    } else {
+        cb();
+    }
+});
+
+/*
+ * Common payload elements
+ */
+var image_uuid = vmtest.CURRENT_BHYVE_CENTOS_UUID;
+
+var base_payload = {
+    alias: 'test-bhyve-pci_slot-' + process.pid,
+    brand: 'bhyve',
+    do_not_inventory: true,
+    autoboot: false,
+    ram: 1024,
+    vcpus: 1,
+    disks: [
+        {
+            image_uuid: image_uuid,
+            boot: true,
+            model: 'virtio'
+        },
+        {
+            size: 512,
+            model: 'virtio'
+        }
+    ]
+};
+
+/*
+ * Tests, finally!
+ */
+
+/*
+ * VM.configure() should put the boot disk in 0:4:0 and the data disk in 0:4:1.
+ */
+test('Verify disk.*.pci_slot are populated by VM.configure',
+    function _verify_populate_on_boot(t) {
+        var payload = dup(base_payload);
+        var check_disks = [
+            {
+                path: '/dev/zvol/rdsk/zones/%s/disk0',
+                image_uuid: image_uuid,
+                boot: true,
+                model: 'virtio',
+                pci_slot: '0:4:0'
+            }, {
+                path: '/dev/zvol/rdsk/zones/%s/disk1',
+                image_uuid: undefined,
+                model: 'virtio',
+                pci_slot: '0:4:1'
+            }
+        ];
+
+        testCreateAndCheckDisks({
+            t: t,
+            payload: payload,
+            disks: check_disks,
+            expect_bsf: ['0:4:0', '0:4:1']
+        });
+    });
+
+test('Verify cdrom is in PCI slot 3:0',
+    function _verify_cdrom_on_boot(t) {
+        var payload = dup(base_payload);
+
+        payload.disks.pop();
+        payload.disks.push({
+            // cdrom media is not created. Choose a file that exists.
+            path: '/usr/share/bhyve/uefi-rom.bin',
+            model: 'ahci',
+            media: 'cdrom'
+        });
+
+        var check_disks = [
+            {
+                path: '/dev/zvol/rdsk/zones/%s/disk0',
+                image_uuid: image_uuid,
+                boot: true,
+                model: 'virtio',
+                pci_slot: '0:4:0'
+            }, {
+                path: '/usr/share/bhyve/uefi-rom.bin',
+                image_uuid: undefined,
+                model: 'ahci',
+                pci_slot: '0:3:0',
+                media: 'cdrom'
+            }
+        ];
+
+        testCreateAndCheckDisks({
+            t: t,
+            payload: payload,
+            disks: check_disks
+        });
+    });
+
+test('Verify 8 disks automatically assigned properly',
+    function _verify_8_disks(t) {
+        var payload = dup(base_payload);
+        var check_disks = [
+            {
+                path: '/dev/zvol/rdsk/zones/%s/disk0',
+                image_uuid: image_uuid,
+                boot: true,
+                model: 'virtio',
+                pci_slot: '0:4:0'
+            }
+        ];
+        var i;
+
+        payload.disks.pop();
+        for (i = 1; i < 8; i++) {
+            payload.disks.push({
+                size: 256,
+                model: 'virtio'
+            });
+            check_disks.push({
+                path: '/dev/zvol/rdsk/zones/%s/disk' + i,
+                pci_slot: '0:4:' + i
+            });
+        }
+
+        testCreateAndCheckDisks({
+            t: t,
+            payload: payload,
+            disks: check_disks,
+            expect_bsf: [
+                '0:4:0', '0:4:1', '0:4:2', '0:4:3',
+                '0:4:4', '0:4:5', '0:4:6', '0:4:7'
+            ]});
+    });
+
+test('Verify create time assignments are sticky',
+    function _verify_create_sticky_disks(t) {
+        var payload = dup(base_payload);
+        var check_disks = [
+            {
+                path: '/dev/zvol/rdsk/zones/%s/disk0',
+                image_uuid: image_uuid,
+                boot: true,
+                model: 'virtio',
+                pci_slot: '0:4:0'
+            }
+        ];
+        var i;
+
+        payload.disks.pop();
+        for (i = 1; i < 4; i++) {
+            payload.disks.push({
+                size: 256,
+                model: 'virtio',
+                pci_slot: '0:4:' + (i + 4)
+            });
+            check_disks.push({
+                path: '/dev/zvol/rdsk/zones/%s/disk' + i,
+                pci_slot: '0:4:' + (i + 4)
+            });
+        }
+
+        testCreateAndCheckDisks({
+            t: t,
+            payload: payload,
+            disks: check_disks,
+            expect_bsf: [ '0:4:0', '0:4:5', '0:4:6', '0:4:7' ]
+        });
+    });
+
+/*
+ * Verifies that "slot:fn" and "slot" are accepted and placed in the right
+ * places.
+ */
+test('Verify alternate slot schemes are allowed',
+    function _verify_partial_bsf_disks(t) {
+        var check_disks = [
+            {path: '/dev/zvol/rdsk/zones/%s/disk0', pci_slot: '0:4:0'},
+            {path: '/dev/zvol/rdsk/zones/%s/disk1', pci_slot: '4:1'},
+            {path: '/dev/zvol/rdsk/zones/%s/disk2', pci_slot: '5'}
+        ];
+        var payload = dup(base_payload);
+        payload.disks.pop();
+        payload.disks.push({size: 256, model: 'virtio', pci_slot: '4:1'});
+        payload.disks.push({size: 256, model: 'virtio', pci_slot: '5'});
+
+        testCreateAndCheckDisks({
+            t: t,
+            payload: payload,
+            disks: check_disks,
+            expect_bsf: [ '0:4:0', '0:4:1', '0:5:0' ]
+        });
+    });
+
+test('Verify holes are filled',
+    function _verify_holes_filled(t) {
+        var check_disks = [
+            {path: '/dev/zvol/rdsk/zones/%s/disk0', pci_slot: '0:4:0'},
+            {path: '/dev/zvol/rdsk/zones/%s/disk1', pci_slot: '0:4:2'},
+            {path: '/dev/zvol/rdsk/zones/%s/disk2', pci_slot: '0:4:1'}
+        ];
+        var payload = dup(base_payload);
+        payload.flexible_disk_size = 13 * 1024;
+        payload.disks.pop();
+        payload.disks.push({size: 256, model: 'virtio', pci_slot: '0:4:2'});
+
+        var update_payload = {
+            add_disks: [ {
+                size: 512,
+                model: 'virtio'
+            } ]
+        };
+
+        testCreateAndCheckDisks({
+            t: t,
+            payload: payload,
+            update: update_payload,
+            disks: check_disks,
+            expect_bsf: [ '0:4:0', '0:4:1', '0:4:2' ]
+        });
+    });
+
+/*
+ * When coming from an old PI, disks.*.pci_slot is probably not set.
+ * `vmadm start` should fix that, putting them into the legacy slots.
+ */
+test('Verify VM.start performs static assignment',
+    function _start_static_assignment(t) {
+        var disks = [ {
+            path: '/dev/zvol/rdsk/zones/%s/disk0',
+            legacy_pci_slot: '0:4:0',
+            new_pci_slot: '0:4:0'
+        }, {
+            path: '/dev/zvol/rdsk/zones/%s/disk1',
+            legacy_pci_slot: '0:5:0',
+            new_pci_slot: '0:4:1'
+        } ];
+        var check_disks = [];
+        var payload = dup(base_payload);
+
+        var damage_fn = function (opts, cb) {
+            var uuid = opts.vmobj.uuid;
+            var zcfg = '';
+            var changes = [];
+            var i;
+
+            for (i in disks) {
+                var disk = disks[i];
+                // We are testing legacy assignments.  After booting, the legacy
+                // slot assignment should be present.
+                check_disks.push({
+                    path: disk.path,
+                    pci_slot: disk.legacy_pci_slot
+                });
+                // In order to ensure legacy assignments, we have to remove the
+                // new assignments from the config.
+                zcfg += 'select device match=' + sprintf(disk.path, uuid)
+                    + ';\nremove property(name=pci-slot,value="'
+                    + disk.new_pci_slot + '");\nend;\n';
+                changes.push({
+                    path: [ 'disks', null, 'pci_slot' ],
+                    action: 'removed',
+                    oldValue: disk.new_pci_slot
+                });
+            }
+            zonecfgSync(opts.vmobj.uuid, [], {log: log, stdin: zcfg},
+                changes, function _zonecfg_cb(zcfg_err, fds) {
+                    if (zcfg_err) {
+                        t.ok(false, 'zonecfg failed: ' + zcfg_err);
+                    } else {
+                        t.ok(true, 'zone config succeeded');
+                    }
+                    cb(zcfg_err);
+                });
+        };
+
+        // After damaging the config, no disk should have a pci_slot set.
+        var damage_check = dup(check_disks);
+        damage_check.forEach(function (disk) {
+            disk.pci_slot = undefined;
+        });
+
+        testCreateAndCheckDisks({
+            t: t,
+            payload: payload,
+            damage: {
+                fn: damage_fn,
+                check_disks: damage_check,
+                opts: {}
+            },
+            disks: check_disks,
+            expect_bsf: [ '0:4:0', '0:5:0' ]
+        });
+    });
+
+/*
+ * If two disks try to share the same slot during create, the create should
+ * fail.
+ */
+test('Conflict during create',
+    function _conflict_during_create(t) {
+        var payload = dup(base_payload);
+        payload.disks[0].pci_slot = '0:4:0';
+        payload.disks[1].pci_slot = '0:4:0';
+
+        testFailCreate({
+            t: t,
+            payload: payload,
+            expect: 'VM has multiple disks in pci_slot "0:4:0"'
+        });
+    });
+
+/*
+ * Two boot disks are not supported.
+ */
+test('Multiple boot disks',
+    function _multiple_boot_disks(t) {
+        var payload = dup(base_payload);
+        payload.disks[1].boot = true;
+
+        testFailCreate({
+            t: t,
+            payload: payload,
+            expect: 'multiple boot disks:'
+        });
+    });
+
+/*
+ * If an update tries to put a disk into an occupied slot, it should fail.
+ */
+test('Conflict during update',
+    function _conflict_during_update(t) {
+        var payload = dup(base_payload);
+        payload.flexible_disk_size = 13 * 1024;
+
+        var disk0 = payload.disks[0];
+        var disk1 = payload.disks[1];
+        disk0.pci_slot = '0:4:0';
+        disk1.pci_slot = '0:4:0';
+
+        payload.disks = [ disk0 ];
+        var update = {add_disks: [ disk1 ]};
+        t.expect(3);
+
+        vasync.waterfall([
+            function _create(next) {
+                VM.create(payload, function _create_cb(err, obj) {
+                    if (err) {
+                        t.ok(false, 'VM create failed: ' + err);
+                        next(err);
+                        return;
+                    }
+                    t.ok(true, 'VM created with uuid ' + obj.uuid);
+                    vmobj = obj;
+                    next();
+                });
+            },
+            function _update(next) {
+                VM.update(vmobj.uuid, update, function _update_cb(err) {
+                    var expect = 'VM has multiple disks in pci_slot "0:4:0"';
+                    t.ok(err, 'update should not succeed');
+                    if (!err) {
+                        next(new Error('update unexpectedly succeeded'));
+                        return;
+                    }
+                    t.equal(err.message.substring(0, expect.length), expect,
+                        'conflict detected');
+                    if (err.message.substring(0, expect.length) !== expect) {
+                        next(err);
+                        return;
+                    }
+                    next();
+                });
+            }
+            ], function _done(err) {
+                t.end(err);
+            }
+        );
+    });
+
+/*
+ * Ensure that functions of the reseved devices are not valid for disks.
+ */
+[
+    {name: 'hostbridge', pcidev: 0},
+    {name: 'nics', pcidev: 6},
+    {name: 'fbuf', pcidev: 30},
+    {name: 'lpc', pcidev: 31}
+].forEach(function _squat(squatter) {
+    var fn = 0;
+    // We could check 0 - 7, but 0 and 1 should exercise all relevant paths.
+    while (fn < 2) {
+        var pcislot = sprintf('0:%d:%d', squatter.pcidev, fn);
+        var desc = sprintf('No squatters on %s: %s', pcislot, squatter.name);
+
+        test(desc, function _test_squatter(t) {
+            var payload = dup(base_payload);
+            payload.disks[0].pci_slot = pcislot;
+            testFailCreate({
+                t: t,
+                payload: payload,
+                expect: sprintf(
+                    'pci_slot "%s" invalid: PCI device %d is reserved',
+                    pcislot, squatter.pcidev)
+            });
+        });
+        fn++;
+    }
+});
