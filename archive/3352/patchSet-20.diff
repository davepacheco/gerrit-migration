commit 69c1dfc04920c27f2a971924d9b222cbf957347d (refs/changes/52/3352/20)
Author: Kody A Kantor <kody@kkantor.com>
Date:   2018-05-10T19:25:26+00:00 (1 year, 5 months ago)
    
    joyent/pgstatsmon#4 improve connection handling and backend discovery
    joyent/pgstatsmon#5 add and improve queries
    joyent/pgstatsmon#7 add 'release' and 'publish' targets
    joyent/pgstatsmon#8 create a Postgres user for pgstatsmon

diff --git a/.gitmodules b/.gitmodules
index 2f12284..6c9d358 100644
--- a/.gitmodules
+++ b/.gitmodules
@@ -1,3 +1,6 @@
 [submodule "deps/catest"]
 	path = deps/catest
 	url = https://github.com/joyent/catest.git
+[submodule "deps/manta-scripts"]
+	path = deps/manta-scripts
+	url = https://github.com/joyent/manta-scripts
diff --git a/CHANGES.md b/CHANGES.md
index 5be47d2..a6f9936 100644
--- a/CHANGES.md
+++ b/CHANGES.md
@@ -1,6 +1,10 @@
 # pgstatsmon Changelog
 
 ## Not yet released.
+* #8 create a Postgres user for pgstatsmon
+* #7 add 'release' and 'publish' targets
+* #5 add and improve queries
+* #4 improve connection handling and backend discovery
 * #3 need a test suite
 * #2 repo housekeeping
 * #1 support Prometheus-style metric collection
diff --git a/Makefile b/Makefile
index 9c1b8bd..f04b799 100644
--- a/Makefile
+++ b/Makefile
@@ -1,7 +1,7 @@
 #
 # Copyright (c) 2018, Joyent, Inc. All rights reserved.
 #
-# Makefile: top-level Makefile
+# Makefile: pgstatsmon - Postgres monitoring system
 #
 # This Makefile contains only repo-specific logic and uses included makefiles
 # to supply common targets (javascriptlint, jsstyle, restdown, etc.), which are
@@ -11,31 +11,138 @@
 #
 # Tools must be installed on the path
 #
-JSL		 = jsl
-JSSTYLE		 = jsstyle
-CATEST		 = deps/catest/catest
+
+JSL	= jsl
+JSSTYLE	= jsstyle
+CATEST	= deps/catest/catest
+
+#
+# Variables
+#
+
+NAME	:= pgstatsmon
+
+#
+# Prebuilt Node.js
+#
+
+NODE_PREBUILT_TAG	= gz
+NODE_PREBUILT_IMAGE	= 18b094b0-eb01-11e5-80c1-175dac7ddf02
+NODE_PREBUILT_VERSION	= v4.8.7
 
 #
 # Files
 #
+
 JS_FILES	:= $(shell find bin etc lib test -name '*.js')
-JSON_FILES      := package.json $(shell find etc test -name '*.json')
-JSL_FILES_NODE   = $(JS_FILES)
+JSON_FILES	:= package.json $(shell find etc test -name '*.json')
+JSL_FILES_NODE	 = $(JS_FILES)
 JSSTYLE_FILES	 = $(JS_FILES)
 JSL_CONF_NODE	 = jsl.node.conf
-CLEAN_FILES	+= node_modules
 
 #
 # Guard tests from mistakenly being run against production
 #
-GUARD			= test/.not_production
-DEFAULT_TEST_CONFIG	= test/etc/testconfig.json
-TEST_BACKEND_URL	:= $(shell json -f $(DEFAULT_TEST_CONFIG) dbs | json -a url)
 
+GUARD			 = test/.not_production
+DEFAULT_TEST_CONFIG	 = test/etc/testconfig.json
+TEST_BACKEND_URL	:= $(shell json -f $(DEFAULT_TEST_CONFIG) static.dbs | json -a ip)
+
+include ./tools/mk/Makefile.defs
+include ./tools/mk/Makefile.node_modules.defs
+include ./tools/mk/Makefile.node_prebuilt.defs
+
+#
+# Install macros and targets
+#
+
+PROTO			= proto
+PREFIX			= /opt/smartdc/$(NAME)
+LIB_FILES		= $(notdir $(wildcard lib/*.js))
+ETC_FILES		= $(notdir $(wildcard etc/*.json))
+RELEASE_TARBALL		= $(NAME)-pkg-$(STAMP).tar.bz2
+NODE_MODULE_INSTALL	= $(PREFIX)/node_modules/.ok
+
+SCRIPTS		= firstboot.sh \
+		  everyboot.sh \
+		  backup.sh \
+		  services.sh \
+		  util.sh
+SCRIPTS_DIR	= $(PREFIX)/scripts
+
+BOOT_SCRIPTS		= setup.sh configure.sh
+BOOT_SCRIPTS_DIR	= /opt/smartdc/boot
+
+NODE_BITS	= bin/node \
+		  lib/libgcc_s.so.1 \
+		  lib/libstdc++.so.6
+NODE_BITS_DIR	= $(PREFIX)/node
+
+SAPI_MANIFESTS		= pgstatsmon
+SAPI_MANIFESTS_DIRS	= $(SAPI_MANIFESTS:%=$(PREFIX)/sapi_manifests/%)
+
+SMF_MANIFEST		= pgstatsmon
+SMF_MANIFEST_DIR	= $(PREFIX)/smf/manifests
+
+INSTALL_FILES	= $(addprefix $(PROTO), \
+		  $(PREFIX)/bin/pgstatsmon.js \
+		  $(BOOT_SCRIPTS:%=$(BOOT_SCRIPTS_DIR)/%) \
+		  $(SCRIPTS:%=$(SCRIPTS_DIR)/%) \
+		  $(LIB_FILES:%=$(PREFIX)/lib/%) \
+		  $(ETC_FILES:%=$(PREFIX)/etc/%) \
+		  $(NODE_MODULE_INSTALL) \
+		  $(NODE_BITS:%=$(NODE_BITS_DIR)/%) \
+		  $(BOOT_SCRIPTS:%=$(BOOT_SCRIPTS_DIR)/%) \
+		  $(SAPI_MANIFESTS_DIRS:%=%/template) \
+		  $(SAPI_MANIFESTS_DIRS:%=%/manifest.json) \
+		  $(SMF_MANIFEST:%=$(SMF_MANIFEST_DIR)/%.xml) \
+		  )
 
-all:
-	npm install
+INSTALL_DIRS	= $(addprefix $(PROTO), \
+		  $(PREFIX)/bin \
+		  $(PREFIX)/lib \
+		  $(PREFIX)/etc \
+		  $(SCRIPTS_DIR) \
+		  $(NODE_BITS_DIR) \
+		  $(NODE_BITS_DIR)/bin \
+		  $(NODE_BITS_DIR)/lib \
+		  $(BOOT_SCRIPTS_DIR) \
+		  $(SMF_MANIFEST_DIR) \
+		  $(SAPI_MANIFESTS_DIRS) \
+		  )
 
+INSTALL_FILE = rm -f $@ && cp $< $@ && chmod 644 $@
+INSTALL_EXEC = rm -f $@ && cp $< $@ && chmod 755 $@
+
+#
+# build targets
+#
+
+.PHONY: all
+all: $(STAMP_NODE_PREBUILT) $(STAMP_NODE_MODULES)
+	$(NODE) --version
+
+.PHONY: install
+install: $(INSTALL_FILES)
+
+.PHONY: release
+release: install
+	@echo "==> Building $(RELEASE_TARBALL)"
+	cd $(PROTO) && gtar -jcf $(TOP)/$(RELEASE_TARBALL) \
+		--transform='s,^[^.],root/&,' \
+		--owner=0 --group=0 \
+		opt
+
+.PHONY: publish
+publish: release
+	@if [[ -z "$(BITS_DIR)" ]]; then \
+		echo "error: 'BITS_DIR' must be set for 'publish' target"; \
+		exit 1; \
+	fi
+	mkdir -p $(BITS_DIR)/$(NAME)
+	cp $(RELEASE_TARBALL) $(BITS_DIR)/$(NAME)/$(RELEASE_TARBALL)
+
+.PHONY: test
 test: $(GUARD) all
 	$(CATEST) test/*.tst.js
 
@@ -54,5 +161,54 @@ $(GUARD):
 	@echo
 	@exit 1
 
+# 'install' targets for each file, or set of files
+$(INSTALL_DIRS):
+	mkdir -p $@
+
+$(PROTO)$(PREFIX)/bin/%: bin/% | $(INSTALL_DIRS)
+	$(INSTALL_FILE)
+
+$(PROTO)$(PREFIX)/lib/%: lib/% | $(INSTALL_DIRS)
+	$(INSTALL_FILE)
+
+$(PROTO)$(PREFIX)/etc/%: etc/% | $(INSTALL_DIRS)
+	$(INSTALL_FILE)
+
+# copy node_modules into PROTO dir, and create touch file to signify 'done'
+$(PROTO)$(NODE_MODULE_INSTALL): $(STAMP_NODE_MODULES) | $(INSTALL_DIRS)
+	rm -rf $(@D)/
+	cp -rP node_modules/ $(@D)/
+	touch $@
+
+# copy the node binary into the PROTO dir
+$(PROTO)$(PREFIX)/node/bin/%: $(STAMP_NODE_PREBUILT) | $(INSTALL_DIRS)
+	rm -f $@ && cp $(NODE_INSTALL)/bin/$(@F) $@ && chmod 755 $@
+
+# copy node's linked libraries into the PROTO dir
+$(PROTO)$(PREFIX)/node/lib/%: $(STAMP_NODE_PREBUILT) | $(INSTALL_DIRS)
+	rm -f $@ && cp $(NODE_INSTALL)/lib/$(@F) $@ && chmod 755 $@
+
+# install the boot scripts
+$(PROTO)$(BOOT_SCRIPTS_DIR)/setup.sh: | $(INSTALL_DIRS)
+	rm -f $@ && ln -s ../$(NAME)/scripts/firstboot.sh $@
+
+$(PROTO)$(BOOT_SCRIPTS_DIR)/configure.sh: | $(INSTALL_DIRS)
+	rm -f $@ && ln -s ../$(NAME)/scripts/everyboot.sh $@
+
+$(PROTO)$(PREFIX)/scripts/%.sh: deps/manta-scripts/%.sh | $(INSTALL_DIRS)
+	$(INSTALL_EXEC)
+
+$(PROTO)$(PREFIX)/scripts/%.sh: boot/%.sh | $(INSTALL_DIRS)
+	$(INSTALL_EXEC)
+
+# install sapi manifests
+$(PROTO)$(PREFIX)/sapi_manifests/%: sapi_manifests/% | $(INSTALL_DIRS)
+	$(INSTALL_FILE)
+
+# install SMF manifests
+$(PROTO)$(PREFIX)/smf/manifests/%: smf/manifests/% | $(INSTALL_DIRS)
+	$(INSTALL_FILE)
 
-include ./Makefile.targ
+include ./tools/mk/Makefile.targ
+include ./tools/mk/Makefile.node_modules.targ
+include ./tools/mk/Makefile.node_prebuilt.targ
diff --git a/README.md b/README.md
index 09c4a52..7d324fa 100644
--- a/README.md
+++ b/README.md
@@ -1,6 +1,6 @@
 # pgstatsmon
 
-This is a *prototype* Node service to use the Postgres client interface to
+This is a Node.js service to use the Postgres client interface to
 periodically fetch stats from multiple Postgres instances and export them
 through a Prometheus server.
 
@@ -18,25 +18,41 @@ Then run the monitor with:
 It logs to stdout using bunyan.
 
 ## Example
+
+Using a configuration file for static backends:
 ```
 $ cat etc/myconfig.json
 {
     "interval": 10000,
-    "dbs": [ {
-        "name": "primary",
-        "url": "postgres://postgres@10.99.99.16:5432/moray"
-    } ],
+    "connections": {
+        "query_timeout": 1000,
+        "connect_timeout": 3000,
+        "connect_retries": 3
+    },
+    "backend_port": 5432,
+    "user": "pgstatsmon",
+    "database": "moray",
+    "static": {
+        "dbs": [{
+            "name": "primary",
+            "ip": "10.99.99.16"
+        }]
+    },
     "target": {
         "ip": "0.0.0.0",
-        "port": 9187,
-        "route": "/metrics"
+        "port": 8881,
+        "route": "/metrics",
+        "metadata": {
+            "datacenter": "my-dc"
+        }
     }
 }
+
 $ node ./bin/pgstatsmon.js etc/myconfig.json > pgstatsmon.log &
 
-... wait <interval> seconds ...
+... wait <interval> milliseconds ...
 
-$ curl http://localhost:9187/metrics
+$ curl -s http://localhost:8881/metrics
 ...
 # HELP pg_relation_size_toast_bytes bytes used by toast files
 # TYPE pg_relation_size_toast_bytes gauge
@@ -61,7 +77,57 @@ pg_stat_bgwriter_checkpoint_sync_time_ms{name="primary"} 19
 ...
 ```
 
+## VMAPI Discovery
+
+pgstatsmon can optionally be configured to use VMAPI for discovery of backend
+Postgres instances. This configuration will cause pgstatsmon to poll VMAPI at
+the given interval for information about running Postgres instances.
+
+The VMAPI discovery configuration takes a number of arguments:
+* 'url' - URL or IP address of the VMAPI server
+* 'pollInterval' - rate (in milliseconds) at which to poll VMAPI
+* 'tags' - an object describing which VMs to discover
+  * 'vm_tag_name' - name of the VM tag key for Postgres VMs
+  * 'vm_tag_value' - value of the VM tag for Postgres VMs
+  * 'nic_tag' - NIC tag of interface to use for connecting to Postgres
+* 'backend_port' - port number used to connect to Postgres instances
+* 'user' - pgstatsmon's Postgres user
+
+Example VMAPI configuration file:
+```
+$ cat etc/vmapiconfig.json
+{
+    "interval": 10000,
+    "connections": {
+        "query_timeout": 1000,
+        "connect_timeout": 3000,
+        "connect_retries": 3
+    },
+    "backend_port": 5432,
+    "user": "pgstatsmon",
+    "database": "moray",
+    "vmapi": {
+        "url": "http://vmapi.coal-1.example.com",
+        "pollInterval": 600000,
+        "tags": {
+            "vm_tag_name": "manta_role",
+            "vm_tag_value": "postgres",
+            "nic_tag": "manta"
+        }
+    },
+    "target": {
+        "ip": "0.0.0.0",
+        "port": 8881,
+        "route": "/metrics",
+        "metadata": {
+            "datacenter": "my-dc"
+        }
+    }
+}
+```
+
 ## Prometheus
+
 pgstatsmon makes metrics available in the Prometheus text format.  A user can
 issue `GET /metrics` to retrieve all of the metrics pgstatsmon collects from
 every Postgres instance being monitored.
@@ -94,18 +160,41 @@ to run the tests, your configuration file may look like this:
 ```
 {
     "interval": 2000,
-    "dbs": [ {
-        "name": "test",
-        "url": "postgres://pgstatsmon@localhost:5432/pgstatsmon"
-    } ],
+    "connections": {
+        "query_timeout": 1000,
+        "connect_timeout": 3000,
+        "connect_retries": 3,
+        "max_connections": 10
+    },
+    "user": "pgstatsmon",
+    "database": "pgstatsmon",
+    "backend_port": 5432,
+    "static": {
+        "dbs": [ {
+            "name": "test",
+            "ip": "127.0.0.1"
+        } ]
+    },
     "target": {
         "ip": "0.0.0.0",
-        "port": 9187,
-        "route": "/metrics"
+        "port": 8881,
+        "route": "/metrics",
+        "metadata": {
+            "datacenter": "testing-dc"
+        }
     }
 }
 ```
 
+## DTrace
+
+There are a number of DTrace probes built in to pgstatsmon.  The full
+listing of probes specific to pgstatsmon and their arguments can be found in
+the [lib/dtrace.js](./lib/dtrace.js) file.
+
+[node-artedi](https://github.com/joyent/node-artedi), which pgstatsmon uses to
+perform aggregation and serialize metrics, also exposes DTrace probes.
+
 ## License
 MPL-v2. See the LICENSE file.
 
diff --git a/bin/pgstatsmon.js b/bin/pgstatsmon.js
index 2bf181d..e91e81e 100644
--- a/bin/pgstatsmon.js
+++ b/bin/pgstatsmon.js
@@ -3,7 +3,7 @@
  * License, v. 2.0. If a copy of the MPL was not distributed with this
  * file, You can obtain one at http://mozilla.org/MPL/2.0/.
  *
- * Copyright (c) 2017, Joyent, Inc.
+ * Copyright (c) 2018, Joyent, Inc.
  */
 
 /*
@@ -47,8 +47,9 @@ function main()
 	});
 
 	log.info('config', config);
+
 	config['log'] = log;
-	pgstatsmon(config);
+	pgstatsmon(config).start();
 }
 
 main();
diff --git a/boot/everyboot.sh b/boot/everyboot.sh
new file mode 100644
index 0000000..3479cba
--- /dev/null
+++ b/boot/everyboot.sh
@@ -0,0 +1,18 @@
+#!/bin/bash
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2018, Joyent, Inc.
+#
+
+#
+# Runs on every boot of a newly reprovisioned "pgstatsmon" zone.
+#
+
+printf '==> everyboot @ %s\n' "$(date -u +%FT%TZ)"
+
+exit 0
diff --git a/boot/firstboot.sh b/boot/firstboot.sh
new file mode 100644
index 0000000..de42ae7
--- /dev/null
+++ b/boot/firstboot.sh
@@ -0,0 +1,82 @@
+#!/bin/bash
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2018, Joyent, Inc.
+#
+
+printf '==> firstboot @ %s\n' "$(date -u +%FT%TZ)"
+
+set -o xtrace
+
+NAME=pgstatsmon
+
+#
+# Runs on first boot of a newly reprovisioned "pgstatsmon" zone.
+#
+
+SVC_ROOT="/opt/smartdc/$NAME"
+
+#
+# Build PATH from this list of directories.  This PATH will be used both in the
+# execution of this script, as well as in the root user .bashrc file.
+#
+paths=(
+	"$SVC_ROOT/bin"
+	"$SVC_ROOT/node_modules/.bin"
+	"$SVC_ROOT/node/bin"
+	"/opt/local/bin"
+	"/opt/local/sbin"
+	"/usr/sbin"
+	"/usr/bin"
+	"/sbin"
+)
+
+
+PATH=
+for (( i = 0; i < ${#paths[@]}; i++ )); do
+	if (( i > 0 )); then
+		PATH+=':'
+	fi
+	PATH+="${paths[$i]}"
+done
+export PATH
+
+if ! source "$SVC_ROOT/scripts/util.sh" ||
+    ! source "$SVC_ROOT/scripts/services.sh"; then
+	exit 1
+fi
+
+manta_common_presetup
+
+manta_add_manifest_dir "/opt/smartdc/$NAME"
+
+manta_common_setup "$NAME"
+
+#
+# Replace the contents of PATH from the default root user .bashrc with one
+# more appropriate for this particular zone.
+#
+if ! /usr/bin/ed -s '/root/.bashrc'; then
+	fatal 'could not modify .bashrc'
+fi <<EDSCRIPT
+/export PATH/d
+a
+export PATH="$PATH"
+.
+w
+EDSCRIPT
+
+#
+# Import the pgstatsmon SMF service.  The manifest file creates the service
+# enabled by default.
+#
+if ! svccfg import "/opt/smartdc/$NAME/smf/manifests/$NAME.xml"; then
+	fatal 'could not import SMF service'
+fi
+
+manta_common_setup_end
diff --git a/deps/manta-scripts b/deps/manta-scripts
new file mode 160000
index 0000000..dc593cd
--- /dev/null
+++ b/deps/manta-scripts
@@ -0,0 +1 @@
+Subproject commit dc593cd90475e391c6fce874130f5d522dc0cc54
diff --git a/docs/overview.md b/docs/overview.md
new file mode 100644
index 0000000..fbedd9c
--- /dev/null
+++ b/docs/overview.md
@@ -0,0 +1,132 @@
+# pgstatsmon overview
+
+This is a description of the basic architecture of pgstatsmon.
+
+## Postgres discovery
+
+pgstatsmon supports two ways of discovering Postgres instances from which to
+retrieve metrics. pgstatsmon defaults to using the VMAPI-based discovery
+mechanism. If both VMAPI and static discovery parameters are provided, the
+static parameters are ignored.
+
+### VMAPI discovery
+
+If you have a Triton installation you can instruct pgstatsmon to poll VMAPI
+for information about deployed (and running) Postgres instances. pgstatsmon
+will poll VMAPI at a user-defined interval. VMAPI VM and NIC tags must be
+provided to help pgstatsmon find the proper instances.
+
+Only one VMAPI can currently be targeted. This means that if you have a three
+datacenter Triton deployment you will need to stand up three instances of
+pgstatsmon to collect metrics from the Postgres instances in the three
+datacenters.
+
+The VMAPI-based service discovery mechanism uses
+[node-vmapi-resolver](https://github.com/joyent/node-vmapi-resolver) to poll
+VMAPI.
+
+### Static discovery
+
+The other configuration option is to use a static list of Postgres backends.
+This is especially useful for development. You can hard-code a list of
+Postgres instances for pgstatsmon to collect metrics from. The tests use this
+discovery method.
+
+## Connection management
+
+pgstatsmon uses [node-cueball](https://github.com/joyent/node-cueball) to
+manage connection to Postgres. pgstatsmon creates a cueball connection pool
+with only one connection for each Postgres backend. Cueball will ensure that
+connections are destroyed when backends disappear temporarily, and that only one
+connection is maintained for each backend.
+
+If a backend is permanently removed, node-vmapi-resolver will emit a 'removed'
+event, which will cause pgstatsmon to remove all data and connections related
+to that instance.
+
+## Metric collection
+
+pgstatsmon retrieves metric data from backend Postgres instances by
+periodically polling at a user-defined interval. The polling interval isn't
+"smart," so manual intervention will be required to adjust the polling interval
+if pgstatsmon is overloaded or if pgstatsmon overloads Postgres.
+
+During each polling interval the following occurs:
+
+* Each Postgres backend is enqueued for metric collection
+  * Only a certain number of Postgres backends are actually polled at any given
+    time. The current (hard-coded) number of outstanding backends being polled
+    is ten.
+* A connection to each in-flight Postgres is claimed from Cueball
+  * If no connection exists, one will attempt to be created
+  * If a connection cannot be created, an error is noted in the logs, and
+    a failed connection counter is incremented
+* All queries are kicked off and will run asynchronously
+  * If a query times out, the backend connection is closed
+  * The execution time of each queries is tracked
+  * Any query error results in logging and incrementing of error metrics
+
+Metrics are maintained in memory using
+[node-artedi](https://github.com/joyent/node-artedi).
+
+## Metric retrieval
+
+pgstatsmon supports two metric exposition formats.
+
+### Prometheus
+
+As documented in the README, pgstatsmon exposes metrics in the Prometheus
+text format. In addition to the metrics collected from Postgres, pgstatsmon
+maintains counters for errors and histograms for query latency.
+
+### Log
+
+pgstatsmon also logs each metric that is observed. These are available at the
+'trace' log level and useful for debugging.
+
+## Initial setup
+
+When pgstatsmon first encounters a new backend it attempts to do a few things.
+
+- Connects to the database as the 'postgres' user
+- Check if the database is a synchronous or asynchronous peer. If it is,
+  pgstatsmon doesn't perform the rest of these steps
+- Creates a non-superuser (defined in the pgstatsmon configuration file) that
+  pgstatsmon will use on subsequent Postgres connection attempts
+- Creates two functions to allow pgstatsmon to glean information about Postgres
+  that is usually hidden from non-superusers
+  * get_stat_activity()
+    * Returns an unfiltered version of pg_stat_activity. When queried by a
+      non-superuser pg_stat_activity will hide queries from superusers (like
+      autovacuum operations).
+  * get_stat_replication()
+    * Returns an unfiltered version of pg_stat_replication. When queried by a
+      non-superuser pg_stat_replication will display very little information.
+      This function allows pgstatsmon to track things like WAL positions.
+
+If this initial setup operation fails for some reason, pgstatsmon will continue
+to attempt to run the setup on every metric collection 'tick' and skip metric
+collection for the backend needing to be set up.
+
+## Metrics collected
+
+pgstatsmon collects a lot of metrics from Postgres. The most up-to-date list of
+things that will be collected is in the lib/queries.js file. To aid those just
+wanting to see what pgstatsmon provides, here is a list that's easier to read.
+
+Each metric is broken down by an arbitrary number of metadata labels provided
+in the pgstatsmon configuration file (target.metadata). The default
+configuration files include a label for 'datacenter.' Other metrics have their
+own set of labels that will be added in addition to whatever the user provides
+in the configuration file. Usually this includes at least the name of the
+backend that the stat came from (e.g. 2.moray.us-east.joyent.us-12345).
+
+| Table | Values | Metadata | Notes |
+|-----------------------------------|
+|pg_stat_user_tables | Information about things done on a relation. This includes number of dead tuples, number of vacuum operations, table scan counts, and more | relname| |
+|pg_stat_replication | Absolute WAL positions in bytes | sync_state | Only works on PG 9.4+ |
+|pg_stat_activity    | Connection counts | datname, state | |
+|pg_stat_database    | Information about given databases. This includes transaction counts, tuple counts, the time spent reading from and writing to disk, and more | datname | |
+|pg_class            | Size of relations in bytes | relname | |
+|pg_class            | Distance to/from wraparound autovacuum | relname | |
+|pg_stat_bgwriter    | Information about the background writer process. This includes checkpoint stats and information about buffer activity | | | |
diff --git a/etc/config.json b/etc/config.json
deleted file mode 100644
index a9af07d..0000000
--- a/etc/config.json
+++ /dev/null
@@ -1,12 +0,0 @@
-{
-    "interval": 10000,
-    "dbs": [ {
-	"name": "primary",
-	"url": "postgres://postgres@10.99.99.16:5432/moray"
-    } ],
-    "target": {
-	"ip": "0.0.0.0",
-	"port": 9187,
-	"route": "/metrics"
-    }
-}
diff --git a/etc/static.json b/etc/static.json
new file mode 100644
index 0000000..3fb0397
--- /dev/null
+++ b/etc/static.json
@@ -0,0 +1,25 @@
+{
+    "interval": 10000,
+    "connections": {
+        "query_timeout": 1000,
+        "connect_timeout": 3000,
+        "connect_retries": 3
+    },
+    "user": "pgstatsmon",
+    "database": "moray",
+    "backend_port": 5432,
+    "static": {
+        "dbs": [{
+            "name": "primary",
+            "ip": "10.99.99.16"
+        }]
+    },
+    "target": {
+        "ip": "0.0.0.0",
+        "port": 8881,
+        "route": "/metrics",
+	"metadata": {
+		"datacenter": "my-coal"
+	}
+    }
+}
diff --git a/etc/vmapi.json b/etc/vmapi.json
new file mode 100644
index 0000000..16dee4b
--- /dev/null
+++ b/etc/vmapi.json
@@ -0,0 +1,28 @@
+{
+    "interval": 10000,
+    "connections": {
+        "query_timeout": 1000,
+        "connect_timeout": 3000,
+        "connect_retries": 3
+    },
+    "user": "pgstatsmon",
+    "database": "moray",
+    "backend_port": 5432,
+    "vmapi": {
+        "url": "http://vmapi.coal-1.example.com",
+        "pollInterval": 600000,
+        "tags": {
+            "vm_tag_name": "manta_role",
+            "vm_tag_value": "postgres",
+            "nic_tag_name": "manta"
+        }
+    },
+    "target": {
+        "ip": "0.0.0.0",
+        "port": 8881,
+        "route": "/metrics",
+	"metadata": {
+		"datacenter": "my-triton"
+	}
+    }
+}
diff --git a/lib/dbinit.js b/lib/dbinit.js
new file mode 100644
index 0000000..04d4d91
--- /dev/null
+++ b/lib/dbinit.js
@@ -0,0 +1,254 @@
+var mod_assert = require('assert-plus');
+var mod_bunyan = require('bunyan');
+var mod_pg = require('pg');
+var mod_vasync = require('vasync');
+var mod_util = require('util');
+var mod_verror = require('verror');
+
+var mod_pgclient = require('./pgclient');
+
+var ConnectTimeoutError = 'ConnectTimeoutError';
+var PostgresInRecoveryError = 'PostgresInRecoveryError';
+
+/*
+ * dbinit.js: set up an unprivileged Postgres user, and make sure it can get
+ * information from typically filtered tables.
+ *
+ * pgstatsmon shouldn't need to do most of the things a typical database user
+ * can do, like create other users, create databases, eat up a lot of network
+ * connections, etc. At the same time, pgstatsmon needs to know some things that
+ * normal users aren't supposed to know. Those are things like queries that
+ * are being run by other users (superusers), and information about downstream
+ * replicas (IP addresses, WAL positions, etc.).
+ *
+ * This file aims to create a Postgres role for pgstatsmon that gives us the
+ * two things mentioned above: restricted access to potentially dangerous
+ * actions, and unrestricted access to information about what the database is
+ * doing.
+ *
+ * To accomplish this, this file:
+ * - Connects to Postgres as the 'postgres' superuser
+ * - Creates a 'pgstatsmon' role with limited privileges
+ * - Creates a function in the given database ('moray' for Triton/Manta) to
+ *   access unfiltered pg_stat_activity information
+ * - Creates a function in the given database to access unfiltered
+ *   pg_stat_replication information
+ *
+ * The steps that create users or functions don't run if the target backend is
+ * identified as being a synchronous or asynchronous replica.
+ *
+ */
+
+/*
+ * connect to the database as a superuser
+ */
+function connect_to_database(args, callback) {
+	var client;
+	var superuser = 'postgres';
+	var query_timeout = args.conf.query_timeout;
+	var connect_timeout = args.conf.connect_timeout;
+
+	var create_client = mod_pgclient.pgCreate({
+		'queryTimeout': query_timeout,
+		'user': superuser,
+		'database': args.conf.targetdb,
+		'log': args.conf.log
+	});
+
+	var timer = setTimeout(function () {
+		client.removeAllListeners('connect');
+		if (client.connection &&
+		    client.connection.stream) {
+		    client.connection.stream.destroy();
+		}
+		args.client = null;
+		callback(new mod_verror.VError({
+			'name': ConnectTimeoutError
+		}));
+	}, connect_timeout);
+
+	client = create_client({
+		'name': 'testbackend',
+		'address': args.conf.hostname,
+		'port': args.conf.port
+	});
+
+	client.on('connect', function () {
+		clearTimeout(timer);
+		args.client = client;
+		callback();
+	});
+}
+
+/*
+ * If this isn't the Postgres primary then we can't do anything else since this
+ * is a read-only database. In this case, bail out early.
+ */
+function stop_if_standby(args, callback) {
+	var query = 'SELECT pg_is_in_recovery();';
+	var is_in_recovery = false;
+
+	var res;
+	res = args.client.query(query);
+	res.once('row', function (row) {
+		is_in_recovery = row.pg_is_in_recovery;
+	});
+
+	res.on('error', function (err) {
+		callback(err);
+	});
+
+	res.on('end', function () {
+		if (is_in_recovery) {
+			callback(new mod_verror.VError({
+				'name': PostgresInRecoveryError
+			}));
+			return;
+		}
+		callback();
+	});
+}
+
+/*
+ * create a restricted Postgres user for pgstatsmon
+ */
+function create_user(args, callback) {
+	var ALREADY_CREATED_ERR_CODE = '42710';
+
+	/* restrict the user as much as possible */
+	var options = ['NOSUPERUSER', 'NOCREATEDB', 'NOCREATEROLE', 'NOINHERIT',
+	    'NOREPLICATION', 'CONNECTION LIMIT 2', 'LOGIN'].join(' ');
+	var query = 'CREATE ROLE %s WITH %s;';
+	query = mod_util.format(query, args.conf.user, options);
+
+	run_rowless_query(query, args, function (err) {
+		if (err && err.code !== ALREADY_CREATED_ERR_CODE) {
+			/* this wasn't a 'user already created' error */
+			callback(err);
+			return;
+		}
+		callback();
+	});
+}
+
+/*
+ * Create functions for pgstatsmon to call to get unfiltered stats.
+ * Some pg_catalog relations allow non-superusers to SELECT on them, but
+ * hide some information. pg_stat_activity and pg_stat_replication are
+ * the two examples that are most relevant to metric collection.
+ *
+ * pgstatsmon will call the function which will execute the underlying
+ * query as the postgres superuser that created the function. The result
+ * is returned as a single string, which can be parsed as a table by
+ * issuing 'SELECT *' on the function. The resulting table should look
+ * identical to the unfiltered pg_catalog table.
+ *
+ * The 'SECURITY DEFINER' bit means that these functions are executed as if
+ * they are being run by the user that created it, _not_ the user that called
+ * it.
+ *
+ */
+
+/*
+ * create a function for pgstatsmon to view unfiltered pg_stat_activity stats
+ */
+function create_activity_function(args, callback) {
+	var query;
+	query = 'CREATE OR REPLACE FUNCTION public.get_stat_activity()'
+	    + ' RETURNS SETOF pg_stat_activity AS \'SELECT * FROM'
+	    + ' pg_catalog.pg_stat_activity;\' LANGUAGE SQL VOLATILE'
+	    + ' SECURITY DEFINER;';
+
+	run_rowless_query(query, args, callback);
+}
+
+/*
+ * create a function for pgstatsmon to view unfiltered pg_stat_replication stats
+ */
+function create_replication_function(args, callback) {
+	var query;
+	query = 'CREATE OR REPLACE FUNCTION public.get_stat_replication()'
+	    + ' RETURNS SETOF pg_stat_replication AS \'SELECT * FROM'
+	    + ' pg_catalog.pg_stat_replication;\' LANGUAGE SQL VOLATILE'
+	    + ' SECURITY DEFINER;';
+
+	run_rowless_query(query, args, callback);
+}
+
+/*
+ * wrapper function to run queries that don't return useful rows
+ */
+function run_rowless_query(query, args, callback) {
+	var log = args.conf.log;
+	var res;
+
+	log.info({
+		'query': query,
+		'backend': args.conf.hostname,
+		'database': args.conf.targetdb
+	}, 'executing query');
+	res = args.client.query(query);
+
+	res.on('row', function () {});
+	res.on('error', function (err) {
+		callback(err);
+	});
+	res.on('end', function () {
+		callback();
+	});
+}
+
+/*
+ * Caller provides the following arguments:
+ * - user, the name of the user/role to create in Postgres
+ * - hostname, the hostname of the Postgres instance on which to create the
+ *   role
+ * - port, the port number that the given Postgres instance is listening on
+ * - targetdb, the database that the user will live in
+ * - query_timeout, time in ms to wait before marking a query as failed
+ * - connect_timeout, time in ms to wait before failing a connection attempt
+ * - log, bunyan-style logger object
+ */
+function setup_monitoring_user(args, callback) {
+	mod_assert.object(args, 'args');
+	mod_assert.object(args.log, 'args.log');
+	mod_assert.string(args.user, 'args.user');
+	mod_assert.number(args.port, 'args.port');
+	mod_assert.string(args.hostname, 'args.hostname');
+	mod_assert.string(args.targetdb, 'args.targetdb');
+	mod_assert.number(args.query_timeout, 'args.query_timeout');
+	mod_assert.number(args.connect_timeout, 'args.connect_timeout');
+
+	var log = args.log;
+	var arg = {
+		'conf': args,
+		'client': null
+	};
+	mod_vasync.pipeline({
+		'funcs': [
+			connect_to_database,
+			stop_if_standby,
+			create_user,
+			create_activity_function,
+			create_replication_function
+		],
+		'arg': arg
+	}, function (err, results) {
+		/* ignore 'postgres in recovery' error */
+		if (err &&
+		    mod_verror.hasCauseWithName(err, PostgresInRecoveryError)) {
+			log.info({
+				'backend': args.hostname
+			}, 'PG in recovery, skipping initial setup');
+			err = null;
+		}
+		if (arg.client) {
+			arg.client.destroy();
+		}
+		callback(err);
+	});
+}
+
+module.exports = {
+	setup_monitoring_user: setup_monitoring_user
+};
diff --git a/lib/dtrace.js b/lib/dtrace.js
new file mode 100644
index 0000000..b6d346a
--- /dev/null
+++ b/lib/dtrace.js
@@ -0,0 +1,68 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ *
+ * Copyright (c) 2018, Joyent, Inc.
+ */
+
+/*
+ * dtrace.js: DTrace probe definitions.
+ */
+
+var mod_dtrace_provider = require('dtrace-provider');
+
+var PROBES = {
+	/* Postgres client probes */
+	/* sql, backend name */
+	'query-start': ['char *', 'char *'],
+
+	/* sql, row, backend name */
+	'query-row': ['char *', 'json', 'char *'],
+
+	/* sql, error message, backend name */
+	'query-error': ['char *', 'char *', 'char *'],
+
+	/* sql, backend name */
+	'query-timeout': ['char *', 'char *'],
+
+	/* sql, end data, backend name */
+	'query-done': ['char *', 'json', 'char *'],
+
+	/* pgstatsmon probes */
+	/* no arguments */
+	'tick-start': [],
+
+	/* no arguments */
+	'tick-done': [],
+
+	/* backend name */
+	'backend-start': ['char *'],
+
+	/* backend name */
+	'backend-done': ['char *'],
+
+	/* backend name, query name */
+	'backend-query-start': ['char *', 'char *'],
+
+	/* backend name, query name */
+	'backend-query-done': ['char *', 'char *']
+
+};
+var PROVIDER;
+
+module.exports = function exportStaticProvider() {
+	if (!PROVIDER) {
+		PROVIDER = mod_dtrace_provider.createDTraceProvider(
+		    'pgstatsmon');
+
+		Object.keys(PROBES).forEach(function (p) {
+			var args = PROBES[p].splice(0);
+			args.unshift(p);
+
+			PROVIDER.addProbe.apply(PROVIDER, args);
+		});
+		PROVIDER.enable();
+	}
+	return (PROVIDER);
+}();
diff --git a/lib/pgclient.js b/lib/pgclient.js
new file mode 100644
index 0000000..af6611e
--- /dev/null
+++ b/lib/pgclient.js
@@ -0,0 +1,239 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ *
+ * Copyright (c) 2018, Joyent, Inc.
+ */
+
+/*
+ * pgclient.js: a Postgres client wrapper intended to be used with node-cueball.
+ */
+
+var mod_cueball = require('cueball');
+var mod_dtrace = require('dtrace-provider');
+var mod_pg = require('pg');
+var mod_assertplus = require('assert-plus');
+var mod_util = require('util');
+var EventEmitter = require('events').EventEmitter;
+var WError = require('verror').WError;
+
+var dtrace = require('./dtrace');
+
+function QueryTimeoutError(cause, sql) {
+	WError.call(this, cause, 'query timeout: %s', sql);
+	this.name = 'QueryTimeoutError';
+}
+mod_util.inherits(QueryTimeoutError, WError);
+
+function pgCreate(opts) {
+	var queryTimeout = opts.queryTimeout;
+	var user = opts.user;
+	var database = opts.database;
+	var log = opts.log;
+
+	function _pgCreate(backend) {
+		mod_assertplus.object(backend, 'backend');
+		mod_assertplus.string(backend.name, 'backend.name');
+		mod_assertplus.string(backend.address, 'backend.address');
+		mod_assertplus.number(backend.port, 'backend.port');
+
+		/* construct the connection url */
+		var url = mod_util.format('postgres://%s@%s:%d/%s',
+		    user, backend.address, backend.port, database);
+
+		return (new PGClient({
+			'url': url,
+			'name': backend.name,
+			'queryTimeout': queryTimeout,
+			'user': user,
+			'database': database,
+			'log': log
+		}));
+	}
+	return (_pgCreate);
+}
+
+function PGClient(options) {
+	mod_assertplus.object(options, 'options');
+	mod_assertplus.string(options.url, 'options.url');
+	mod_assertplus.string(options.name, 'options.name');
+	mod_assertplus.string(options.user, 'options.user');
+	mod_assertplus.string(options.database, 'options.database');
+	mod_assertplus.object(options.log, 'options.log');
+	mod_assertplus.number(options.queryTimeout, 'options.queryTimeout');
+
+	var self = this;
+	this.client = new mod_pg.Client({
+		connectionString: options.url,
+		keepAlive: true
+	});
+	this.client.on('error', this._handleClientError.bind(this));
+
+	this.client.connect();
+	this.client.on('connect', function () {
+		self.log.info({
+			'backend': options.name
+		}, 'connected');
+		self.emit('connect');
+	});
+
+	this.url = options.url;
+	this.user = options.user;
+	this.name = options.name;
+
+	this.queryTimeout = options.queryTimeout;
+	this.client_had_err = null;
+	this.destroyed = false;
+
+	this.log = options.log.child({
+		component: 'PGClient'
+	}, true);
+
+	EventEmitter.call(this);
+}
+mod_util.inherits(PGClient, EventEmitter);
+
+PGClient.prototype.isDestroyed = function () {
+	return (this.destroyed);
+};
+
+/*
+ * The underlying Postgres will emit errors when it has a connection
+ * problem. This can fire multiple times: once when the connection goes
+ * away, and again if we try to make a query using this client. When
+ * this happens, we mark this client as having failed so that the pool
+ * will remove us once we're released.
+ */
+PGClient.prototype._handleClientError = function (err) {
+	this.log.error({
+		err: err
+	}, 'pg: client emitted an error');
+
+	this.client_had_err = err;
+};
+
+PGClient.prototype.query = function clientQuery(sql) {
+	mod_assertplus.string(sql, 'sql');
+
+	/* Clean up whitespace so queries are normalized to DTrace */
+	sql = sql.replace(/(\r\n|\n|\r)/gm, '').replace(/\s+/, ' ');
+
+	var log = this.log;
+	var req;
+	var res = new EventEmitter();
+	var self = this;
+	var timer;
+
+	var aborted = false;
+
+	function done(event, arg) {
+		if (aborted) {
+			return;
+		}
+
+		res.emit(event, arg);
+		clearTimeout(timer);
+	}
+
+	req = new mod_pg.Query(sql);
+
+	req.on('row', function onRow(row) {
+		dtrace['query-row'].fire(function () {
+			return ([sql, row, self.url]);
+		});
+
+		log.debug({
+			row: row
+		}, 'query: row');
+
+		if (aborted) {
+			return;
+		}
+
+		clearTimeout(timer);
+		res.emit('row', row);
+	});
+
+	req.on('end', function onQueryEnd(arg) {
+		dtrace['query-done'].fire(function () {
+			return ([sql, arg, self.url]);
+		});
+
+
+		log.debug({
+			res: arg
+		}, 'query: done');
+
+		done('end', arg);
+	});
+
+	req.on('error', function onQueryError(err) {
+		dtrace['query-error'].fire(function () {
+			return ([sql, err.toString(), self.url]);
+		});
+
+		log.debug({
+			err: err
+		}, 'query: failed');
+
+		/*
+		 * node-postgres's client.query() will fire "error"
+		 * synchronously, resulting in this handler firing in the same
+		 * tick as the client.query() call. Since the PGClient.query()
+		 * caller won't have had an opportunity to set up their own
+		 * "error" listener, we delay firing the event until the next
+		 * tick.
+		 */
+		setImmediate(done, 'error', err);
+	});
+
+	if (this.queryTimeout > 0) {
+		timer = setTimeout(function onRowTimeout() {
+			var err = new QueryTimeoutError(sql);
+			self.client_had_err = err;
+			dtrace['query-timeout'].fire(function () {
+				return ([sql, self.url]);
+			});
+
+			/*
+			 * We're timing out the query, but
+			 * the Postgres query is still running. It may
+			 * still return rows, return a SQL error, or end due
+			 * to connection problems. We don't emit anything
+			 * after this point, since we've already emitted an
+			 * "error" and will have replied to the client. We
+			 * do continue logging and firing DTrace probes for
+			 * anyone who's observing the process, though.
+			 */
+			aborted = true;
+
+			res.emit('error', err);
+		}, this.queryTimeout);
+	}
+
+	this.client.query(req);
+
+	dtrace['query-start'].fire(function () {
+		return ([sql, self.url]);
+	});
+
+	log.debug({
+		sql: sql
+	}, 'pg.query: started');
+
+	return (res);
+};
+
+PGClient.prototype.destroy = function closePGClient() {
+	var self = this;
+
+	this.destroyed = true;
+	self.client.end(function () {
+		self.emit('close');
+	});
+};
+
+module.exports = {
+	pgCreate: pgCreate
+};
diff --git a/lib/pgstatsmon.js b/lib/pgstatsmon.js
index b761778..2a4454b 100644
--- a/lib/pgstatsmon.js
+++ b/lib/pgstatsmon.js
@@ -13,12 +13,21 @@
 
 var mod_artedi = require('artedi');
 var mod_assertplus = require('assert-plus');
+var mod_backoff = require('backoff');
+var mod_cueball = require('cueball');
 var mod_jsprim = require('jsprim');
 var mod_pg = require('pg');
 var mod_restify = require('restify');
 var mod_util = require('util');
+var mod_url = require('url');
 var mod_vasync = require('vasync');
+var mod_verror = require('verror');
+var mod_vmapi_resolver = require('vmapi-resolver');
 
+var mod_pgclient = require('./pgclient');
+var mod_dbinit = require('./dbinit.js');
+
+var dtrace = require('./dtrace');
 var queries = require('./queries');
 
 /* Public interface */
@@ -53,27 +62,58 @@ module.exports = pgstatsmon;
  */
 function pgstatsmon(config)
 {
-	mod_assertplus.number(config['interval'], 'config.interval');
-	mod_assertplus.arrayOfObject(config['dbs'], 'config.dbs');
-	mod_assertplus.object(config['target'], 'config.target');
-	mod_assertplus.object(config['log'], 'config.log');
+	/* XXX use a JSON schema validator? */
+	mod_assertplus.object(config, 'config');
+	mod_assertplus.number(config.interval, 'config.interval');
+	mod_assertplus.object(config.connections, 'config.connections');
+	mod_assertplus.number(config.connections.query_timeout,
+	    'config.connections.query_timeout');
+	mod_assertplus.number(config.connections.connect_timeout,
+	    'config.connections.connect_timeout');
+	mod_assertplus.number(config.connections.connect_retries,
+	    'config.connections.connect_retries');
+	mod_assertplus.object(config.target, 'config.target');
+	mod_assertplus.object(config.log, 'config.log');
+
+	if (config.vmapi) {
+		/* default to using VMAPI to discover backends */
+		var vmapi = config.vmapi;
+		mod_assertplus.object(vmapi, 'config.vmapi');
+		mod_assertplus.string(vmapi.url, 'config.vmapi.url');
+		mod_assertplus.number(vmapi.pollInterval,
+		    'config.vmapi.pollInterval');
+		mod_assertplus.object(vmapi.tags, 'config.vmapi.tags');
+		mod_assertplus.string(vmapi.tags.vm_tag_name,
+		    'config.vmapi.tags.vm_tag_name');
+		mod_assertplus.string(vmapi.tags.vm_tag_value,
+		    'config.vmapi.tags.vm_tag_value');
+		mod_assertplus.string(vmapi.tags.nic_tag,
+		    'config.vmapi.tags.nic_tag');
+	} else {
+		/* use static backends if not using VMAPI */
+		var static_conf = config.static;
+		mod_assertplus.object(static_conf, 'config.static');
+		mod_assertplus.arrayOfObject(static_conf.dbs,
+		    'config.static.dbs');
+		static_conf.dbs.forEach(function (dbconf, pi) {
+			mod_assertplus.string(dbconf.name,
+			    'config.static.dbs[' + pi + '].name');
+			mod_assertplus.string(dbconf.ip,
+			    'config.static.dbs[' + pi + '].ip');
+		});
+	}
 
-	config['dbs'].forEach(function (dbconf, pi) {
-		mod_assertplus.string(dbconf['name'],
-		    'config.dbs[' + pi + '].name');
-		mod_assertplus.string(dbconf['url'],
-		    'config.dbs[' + pi + '].url');
-	});
+	var target = config.target;
+	mod_assertplus.string(target.ip, 'config.target.ip');
+	mod_assertplus.number(target.port, 'config.target.port');
+	mod_assertplus.string(target.route, 'config.target.route');
+	mod_assertplus.object(target.metadata, 'config.target.metadata');
 
-	var target = config['target'];
-	mod_assertplus.string(target['ip'], 'config.target.ip');
-	mod_assertplus.number(target['port'], 'config.target.port');
-	mod_assertplus.string(target['route'], 'config.target.route');
+	mod_assertplus.string(config.user, 'user');
+	mod_assertplus.string(config.database, 'database');
+	mod_assertplus.number(config.backend_port, 'config.backend_port');
 
-	mod_pg.defaults.parseInt8 = true; /* parse int8 into a numeric value */
-	var mon = new PgMon(config);
-	mon.start();
-	return (mon);
+	return (new PgMon(config));
 }
 
 /*
@@ -83,18 +123,56 @@ function pgstatsmon(config)
  */
 function PgMon(config)
 {
-	var log = config['log'];
+	mod_pg.defaults.parseInt8 = true; /* parse int8 into a numeric value */
 
 	/* Save log and configuration */
-	this.pm_log = log;
-	this.pm_dbs = mod_jsprim.deepCopy(config['dbs']);
-	this.pm_targetconf = mod_jsprim.deepCopy(config['target']);
-	this.pm_interval = config['interval'];
+	this.pm_log = config.log;
+	this.pm_targetconf = mod_jsprim.deepCopy(config.target);
+	this.pm_interval_rate = config.interval;
 	this.pm_targets = [];
 	this.pm_prometheus_target = null;
 
+	this.pm_query_timeout = config.connections.query_timeout;
+	this.pm_connect_timeout = config.connections.connect_timeout;
+	this.pm_connect_retries = config.connections.connect_retries;
+
+	if (config.vmapi) {
+		/* VMAPI Resolver configuration */
+		this.pm_vmapi = config.vmapi;
+
+		this.pm_vmapi.backend_port = config.backend_port;
+		this.pm_vmapi.log = this.pm_log.child({
+			'component': 'VMResolver'
+		});
+	} else {
+		/* list of static backends */
+		this.pm_static = config.static;
+
+		var port = config.backend_port;
+		var backends = [];
+		this.pm_static.dbs.forEach(function (db) {
+			backends.push({
+				'address': db.ip,
+				'port': port,
+				'name': db.name
+			});
+		});
+		this.pm_backend_list = backends;
+		this.pm_log.info(this.pm_backend_list, 'static backends');
+	}
+
+	this.pm_dbuser = config.user;
+	this.pm_database = config.database;
+
+	this.pm_client_constructor = mod_pgclient.pgCreate({
+		'queryTimeout': this.pm_query_timeout,
+		'user': this.pm_dbuser,
+		'database': this.pm_database,
+		'log': this.pm_log
+	});
+
 	/* interval returned from setInterval */
-	this.pm_intervalObj = -1;
+	this.pm_interval_object = null;
 
 	/* queries to run */
 	this.pm_queries = [];
@@ -102,9 +180,13 @@ function PgMon(config)
 	this.pm_state =	[];
 	/* last-seen datapoints for each instance */
 	this.pm_data = [];
+	/* datapoints from the previous poll - for debugging */
+	this.pm_old_data = [];
+	/* postgres instance data */
+	this.pm_pgs = [];
+	/* cueball connection pools for each instance */
+	this.pm_pools = [];
 
-	/* postgres client objects */
-	this.pm_pgs = new Array(this.pm_dbs.length);
 	this.initializeMetrics(queries.getQueries());
 }
 
@@ -114,8 +196,9 @@ function PgMon(config)
  */
 PgMon.prototype.createTarget = function (targetconf)
 {
-	this.pm_prom_target = new PrometheusTarget(targetconf, this.pm_log);
-	return (this.pm_prom_target);
+	this.pm_prometheus_target = new PrometheusTarget(
+	    targetconf, this.pm_log);
+	return (this.pm_prometheus_target);
 };
 
 /*
@@ -124,7 +207,7 @@ PgMon.prototype.createTarget = function (targetconf)
  */
 PgMon.prototype.getTarget = function ()
 {
-	return (this.pm_prom_target);
+	return (this.pm_prometheus_target);
 };
 
 /*
@@ -139,7 +222,6 @@ PgMon.prototype.getTarget = function ()
 PgMon.prototype.initializeMetrics = function (query_list)
 {
 	var mon = this;
-	var num_backends, num_queries;
 
 	/* make sure old targets (if any) are stopped before we drop them */
 	if (this.pm_targets) {
@@ -153,78 +235,376 @@ PgMon.prototype.initializeMetrics = function (query_list)
 	this.pm_queries = query_list.map(
 	    function (q) { return (new Query(q, mon.pm_log)); });
 
-	/* set up structures for tracking running queries and query results */
-	num_backends = this.pm_dbs.length;
-	num_queries = this.pm_queries.length;
+	/* Prometheus target */
+	this.pm_targets.push(mon.createTarget(this.pm_targetconf));
+
+	/* always add a "log" target */
+	this.pm_targets.push(new LogTarget(this.pm_log));
+};
+
+/*
+ * [private] Begin backend service discovery, and populate the requisite data
+ * structures for each backend.
+ */
+PgMon.prototype.connect = function ()
+{
+	var mon = this;
+	var log = mon.pm_log;
+	var resolver, pool;
+	var ind;
+
+	var delay = 1000; /* initial delay between reconnect tries */
+	var maxDelay = 5000; /* max delay between reconnect tries */
+	var maxTimeout = 10000; /* max connection attempt timeout */
+
+	if (mon.pm_vmapi) {
+		/* default to the VMAPI resolver */
+		mon.pm_resolver =
+			new mod_vmapi_resolver.VmapiResolver(mon.pm_vmapi);
+	} else {
+		/* use a cueball static IP resolver */
+		mon.pm_resolver = new mod_cueball.StaticIpResolver({
+			'backends': mon.pm_backend_list
+		});
+	}
+
+	/*
+	 * We'd like to maintain exactly one connection to each backend.
+	 * Cueball provides us with nice connection management, but we don't
+	 * need or want more than one connection per backend Postgres. We'll
+	 * create one pool per backend Postgres instance.
+	 */
+	mon.pm_resolver.on('added', function (key, backend) {
+		mod_assertplus.string(key, 'key');
+		mod_assertplus.object(backend, 'backend');
+		mod_assertplus.number(backend.port, 'backend.port');
+		mod_assertplus.string(backend.name, 'backend.name');
+		mod_assertplus.string(backend.address, 'backend.address');
+
+		log.info('backend discovered', backend);
+
+		/* start a staticIpResolver for each backend */
+		resolver = new mod_cueball.StaticIpResolver({
+			'backends': [ {
+				'address': backend.address,
+				'port': backend.port
+			} ]
+		});
+		resolver.start();
+
+		pool = new mod_cueball.ConnectionPool({
+			'constructor': mon.pm_client_constructor,
+			'domain': backend.name, /* not actually used */
+			'recovery': {
+				'default': {
+					'retries': mon.pm_connect_retries,
+					'timeout': mon.pm_connect_timeout,
+					'maxTimeout': maxTimeout,
+					'delay': delay,
+					'maxDelay': maxDelay
+				}
+			},
+			'spares': 1, /* need one spare to recover failed pool */
+			'maximum': 1, /* one connection per backend */
+			'log': log.child({'component': 'cueball'}),
+			'resolver': resolver
+		});
 
-	this.pm_data = new Array(num_backends);
-	this.pm_state = new Array(num_backends);
+		/* hold onto this pool and resolver */
+		ind = mon.pm_pools.push({
+			'key': key,
+			'pool': pool,
+			'resolver': resolver,
+			'backend': backend,
+			'needs_setup': true,
+			'setting_up': false
+		});
 
-	this.pm_dbs.forEach(function (backend, pi) {
-		mon.pm_data[pi] = new Array(num_queries);
-		mon.pm_state[pi] = new Array(num_queries);
+		/* make sure we have the data structures set up */
+		mon.add_connection_data({
+			'key': key,
+			'name': backend.name
+		});
 
-		mon.pm_queries.forEach(function (query, qi) {
-			mon.pm_data[pi][qi] = {};
-			mon.pm_state[pi][qi] = null;
+		setImmediate(function () {
+			mon.setup_backend(ind - 1, backend);
 		});
 	});
 
-	/* Prometheus target */
-	this.pm_targets.push(mon.createTarget(this.pm_targetconf));
+	/*
+	 * When a backend is removed:
+	 *  - Find the backend in the list of PG instances
+	 *  - Stop the cueball connection pool, which destroys all connections
+	 *  - Remove the data associated with the backend:
+	 *    - Remove entry from the PG instance list
+	 *    - Remove the data from the previous queries
+	 *    - Remove the state value
+	 */
+	mon.pm_resolver.on('removed', function (key) {
+		mod_assertplus.string(key, 'key');
+
+		mon.pm_pools.forEach(function (backend, pi) {
+			if (backend.key === key) {
+				/*
+				 * Stop the pool and it's staticIpResolver.
+				 * This also destroys all open connections.
+				 */
+				backend.pool.stop();
+
+				/* remove this connection's data */
+				mon.wait_and_remove(key);
+			}
+		});
 
-	/* always add a "log" target */
-	this.pm_targets.push(new LogTarget(this.pm_log));
+	});
+
+	mon.pm_resolver.start();
 };
 
-PgMon.prototype.start = function ()
+/*
+ * [private] Setup the backend. pgstatsmon needs a couple things before it can
+ * operate safely:
+ * - a non-superuser role with restricted permissions for connecting to the DB
+ * - functions to call to view normally restricted pg_catalog tables, like
+ *   pg_stat_replication and pg_stat_activity
+ *
+ * The function that this calls will attempt to connect as the 'postgres' user
+ * to do these things, and then disconnect. Nothing will be done if the backend
+ * is identified as being in recovery (a sync or async replica).
+ */
+PgMon.prototype.setup_backend = function setup_backend(pi, callback)
 {
 	var mon = this;
-	var log = this.pm_log;
-	var barrier = mod_vasync.barrier();
+	if (mon.pm_pools[pi].needs_setup === false ||
+	    mon.pm_pools[pi].setting_up) {
+		/*
+		 * This backend is either already set up, or currently getting
+		 * set up by another instance of this function.
+		 */
+		return;
+	}
+	mon.pm_pools[pi].setting_up = true; /* lock out other callers */
+
+	mod_dbinit.setup_monitoring_user({
+		'user': mon.pm_dbuser,
+		'targetdb': mon.pm_database,
+		'hostname': mon.pm_pools[pi].backend.address,
+		'port': mon.pm_pools[pi].backend.port,
+		'query_timeout': mon.pm_query_timeout,
+		'connect_timeout': mon.pm_connect_timeout,
+		'log': mon.pm_log.child({ 'component': 'backend_setup' })
+	}, function (err) {
+		mon.pm_pools[pi].setting_up = false;
+		if (err) {
+			mon.pm_pools[pi].needs_setup = true;
+			mon.pm_log.error({
+				'error': err,
+				'backend': mon.pm_pools[pi].name
+			}, 'error setting up backend');
+		} else {
+			mon.pm_pools[pi].needs_setup = false;
+		}
+	});
 
-	log.info('starting service');
+};
 
-	this.pm_dbs.forEach(function (pgconf, pi) {
-		var client;
+/*
+ * [private] Wait for queries to finish, then remove connection data. If the
+ * connection is being stubborn, kill the connection.
+ *
+ * 'key' is used to find the PG instance in the pm_pgs structure.
+ */
+PgMon.prototype.wait_and_remove = function (key)
+{
+	var mon = this;
+	var log = mon.pm_log;
 
-		barrier.start(pgconf['url']);
-		client = mon.pm_pgs[pi] = new mod_pg.Client(pgconf['url']);
-		client.connect(function (err) {
-			if (err) {
-				log.error(err, pgconf, 'failed to connect');
-				barrier.done(pgconf['url']);
-				return;
-			}
+	var num_backoffs = 2;
+	var pi = -1;
+
+	/* find the PG instance in question */
+	mon.pm_pgs.forEach(function (pg, ind) {
+		if (pg.key === key) {
+			pi = ind;
+		}
+	});
+
+	if (pi === -1) {
+		/* key not found, data must be deleted already */
+		return;
+	}
+
+	function is_running(_, cb) {
+		var running = false;
 
-			log.info(pgconf, 'connected');
-			barrier.done(pgconf['url']);
+		/*
+		 * Iterate through the query state array. If any query
+		 * has state (a timestamp), the query is still running,
+		 * meaning that this backend is busy.
+		 */
+		mon.pm_state[pi].forEach(function (st) {
+			if (st) {
+				running = true;
+			}
 		});
+
+		if (running) {
+			cb(new mod_verror.VError('query state still present'));
+		} else {
+			cb();
+		}
+	}
+
+	/*
+	 * Use a backoff scheme to wait for the backend's queries to drain. If
+	 * the queries don't drain after num_backoffs, forcibly remove the
+	 * connection and its data.
+	 */
+	var call = mod_backoff.call(is_running, null, function (err) {
+		if (err) {
+			log.warn({
+				'error': err,
+				'backend': mon.pm_pgs[pi].name
+			}, 'connection did not drain, forcing removal now');
+		} else {
+			log.info({
+				'backend': mon.pm_pgs[pi].name
+			}, 'connection drained, removing connection data');
+		}
+		mon.remove_connection_data(pi);
 	});
 
-	barrier.on('drain', function () {
-		log.info('all clients connected');
+	call.on('backoff', function (number, delay) {
+		log.info({
+			'backend': mon.pm_pgs[pi].name,
+			'number': number,
+			'delay': delay
+		}, 'backoff');
+	});
+
+	call.setStrategy(new mod_backoff.ExponentialStrategy({
+		'initialDelay': 1000
+	}));
+	call.failAfter(num_backoffs);
+	call.start();
+};
+
+/*
+ * [private] Remove data associated with a given Postgres instance.
+ *
+ * 'pi' is the index of the backend in the pm_pgs data structure.
+ *
+ */
+PgMon.prototype.remove_connection_data = function (pi)
+{
+	var mon = this;
+	var backend = mon.pm_pgs[pi].name;
+
+	mon.pm_pgs.splice(pi, 1);
+	mon.pm_data.splice(pi, 1);
+	mon.pm_state.splice(pi, 1);
+	mon.pm_pools.splice(pi, 1);
+	mon.pm_old_data.splice(pi, 1);
+
+	mon.pm_log.info({
+		'backend': backend
+	}, 'removed connection data');
+};
+
+/*
+ * [private] Add data associated with a given PG instance.
+ *
+ * This adds the backend to the list of discovered backends, and then reserves
+ * a slot for the backend in the data and state data structures.
+ */
+PgMon.prototype.add_connection_data = function (backend)
+{
+	var mon = this;
+	var num_queries = mon.pm_queries.length;
+	var data_array = new Array(num_queries);
+	var old_data_array = new Array(num_queries);
+	var state_array = new Array(num_queries);
+
+	/*
+	 * When a backend is added:
+	 *  - Add it to the list of PG instances
+	 *  - Add an object to keep track of the observed data points
+	 *  - Add a 'state' array to identify in-flight queries
+	 */
+	mon.pm_pgs.push({
+		'key': backend.key,
+		'name': backend.name,
+		'conn': null,
+		'handle': null
+	});
+
+	for (var i = 0; i < num_queries; i++) {
+		data_array[i] = {};
+		old_data_array[i] = {};
+		state_array[i] = null;
+	}
+
+	mon.pm_data.push(data_array);
+	mon.pm_old_data.push(old_data_array);
+	mon.pm_state.push(state_array);
+};
+
+
+/*
+ * Start pgstatsmon. The caller can optionally provde a callback to be notified
+ * when it is safe to start using pgstatsmon.
+ */
+PgMon.prototype.start = function (callback)
+{
+	mod_assertplus.optionalFunc(callback, 'callback');
+
+	var mon = this;
+
+	mon.pm_log.info('starting service');
+
+	/*
+	 * discover backends, run initial tick, set up tick interval and
+	 * start metric targets
+	 */
+	mon.connect();
+	mon.tick(function (err) {
+		if (err && callback) {
+			callback(err);
+		}
 
-		mon.tick();
-		mon.pm_intervalObj = setInterval(function () {
+		mon.pm_interval_object = setInterval(function () {
 			mon.tick();
-		}, mon.pm_interval);
+		}, mon.pm_interval_rate);
 
 		mon.pm_targets.forEach(function (target) {
 			target.start();
 		});
+
+		if (callback) {
+			callback();
+		}
 	});
 };
 
+/*
+ * Stop pgstatsmon. This stops the tick interval, stops all metric targets,
+ * and closes all of the database connections, and stops backend discovery.
+ */
 PgMon.prototype.stop = function ()
 {
-	clearInterval(this.pm_intervalObj);
-	this.pm_targets.forEach(function (target) {
+	var mon = this;
+
+	clearInterval(mon.pm_interval_object);
+	mon.pm_targets.forEach(function (target) {
 		target.stop();
 	});
-	this.pm_pgs.forEach(function (db) {
-		db.end(); /* close the db connections */
+	mon.pm_pools.forEach(function (backend, ind) {
+		/* this stops both the pool and the resolver */
+		backend.pool.stop();
+		/* remove all of the data for this instance */
+		mon.remove_connection_data(ind);
 	});
+	mon.pm_resolver.stop();
 };
 
 /*
@@ -239,64 +619,217 @@ PgMon.prototype.stop = function ()
  */
 PgMon.prototype.tick = function (callback)
 {
+	mod_assertplus.optionalFunc(callback);
+
 	var mon = this;
 	var queue;
 
-	queue = mod_vasync.queue(function (pi, cb) {
+	/* up to ten Postgres instances can be queried during a given tick() */
+	var max_backends_in_flight = 10;
+
+	dtrace['tick-start'].fire(function () { return ([]); });
+
+	queue = mod_vasync.queue(function enqueue_queries(pi, cb) {
 		/* kick off the Postgres queries for this instance */
+		var error;
+		var errmetric;
+		var errors = [];
 		var barrier = mod_vasync.barrier();
+		var pool = mon.pm_pools[pi].pool;
+		var backend = mon.pm_pgs[pi].name;
+
+		if (mon.pm_pools[pi].needs_setup) {
+			setImmediate(function () {
+				mon.setup_backend(pi);
+			});
+
+			dtrace['backend-done'].fire(function () {
+				return ([backend]);
+			});
+
+			cb();
+			return; /* skip this round, wait for setup */
+		}
+
 		barrier.start('enqueue queries');
-		mon.pm_queries.forEach(function (query, qi) {
-			/*
-			 * barrier string looks like:
-			 *  'url [pg_index, query_index]'
-			 */
-			var query_id = mod_util.format('%s [%d, %d]',
-			    mon.pm_dbs[pi], pi, qi);
-			barrier.start(query_id);
-			mon.tickPgQuery(pi, qi, function () {
-				barrier.done(query_id);
+
+		pool.claim({
+			'timeout': mon.pm_connect_timeout
+		}, function (err, handle, conn) {
+
+			if (err) {
+				/* couldn't connect, so short circuit */
+				mon.pm_log.warn(err, 'could not connect');
+				errors.push(err);
+				barrier.done('enqueue queries');
+				return;
+			}
+
+			mon.pm_pgs[pi].handle = handle;
+			mon.pm_pgs[pi].conn = conn;
+			backend = mon.pm_pgs[pi].name;
+
+			mon.pm_queries.forEach(function
+			    kick_off_queries(query, qi) {
+
+				/*
+				 * barrier string looks like:
+				 *  'backend [pg_index, query_index]'
+				 */
+				var query_id = mod_util.format('%s [%d, %d]',
+				    backend, pi, qi);
+				barrier.start(query_id);
+
+				dtrace['backend-query-start'].fire(function () {
+					return ([
+					    backend,
+					    query.q_name
+					]);
+				});
+
+				mon.tickPgQuery(pi, qi, function (err2) {
+					dtrace['backend-query-done'].fire(
+					    function () {
+
+						return ([
+						    backend,
+						    query.q_name
+						]);
+					});
+					if (err2) {
+						errors.push(err2);
+					}
+					barrier.done(query_id);
+				});
 			});
+			barrier.done('enqueue queries');
 		});
-		barrier.done('enqueue queries');
-		barrier.on('drain', cb);
-	}, 10); /* 10 Postgres instances outstanding */
+
+		/*
+		 * All of the queries are done executing for a particular
+		 * backend. Now it's time to check for errors and handle them as
+		 * appropriate.
+		 *
+		 * A few different errors can occur:
+		 * - A query times out
+		 *   - Gently close the connection
+		 *   - The timeout has an error metric created in the
+		 *     tickPgQuery function
+		 * - The connection pool failed or connecting took too long
+		 *   - Record that the error occurred
+		 *   - Cueball will continue to try to reconnect
+		 *
+		 * If everything goes well pgstatsmon will release the
+		 * connection back into the pool.
+		 */
+		barrier.on('drain', function barrier_drain() {
+			error = mod_verror.errorFromList(errors);
+			if (error && mod_verror.hasCauseWithName(error,
+			    'QueryTimeoutError')) {
+
+				mon.pm_log.info({
+					'error': error.message,
+					'backend': backend
+				}, 'query timeout, destroying connection');
+
+				mon.pm_pgs[pi].handle.close();
+			} else if (error && (mod_verror.hasCauseWithName(error,
+			    'PoolFailedError') ||
+			    mod_verror.hasCauseWithName(error,
+			    'ClaimTimeoutError') ||
+			    mod_verror.hasCauseWithName(error,
+			    'PoolStoppingError'))) {
+
+				/* no valid handle or connection */
+				mon.pm_log.warn({
+					'error': error.message,
+					'backend': backend
+				}, 'error connecting to backend');
+
+				/* make sure we report the connection error */
+				errmetric = {
+					'name': 'pg_connect_error',
+					'help': 'PG connection failed',
+					'metadata': {
+						'backend': backend
+					}
+				};
+				mon.emitCounter(errmetric, 1);
+
+			} else {
+				/* done with the connection */
+				mon.pm_pgs[pi].handle.release();
+
+				mon.pm_pgs[pi].handle = null;
+				mon.pm_pgs[pi].conn = null;
+			}
+
+			dtrace['backend-done'].fire(function () {
+				return ([backend]);
+			});
+
+			cb(error);
+		});
+	}, max_backends_in_flight);
 
 	/* enqueue all of the Postgres instances */
-	this.pm_pgs.forEach(function (pg, pi) {
+	mon.pm_pgs.forEach(function (pg, pi) {
+		dtrace['backend-start'].fire(function () {
+			return ([pg.name]);
+		});
 		queue.push(pi);
 	});
 
 	queue.close();
-	queue.on('end', function () {
+
+	/* done with all of the work for this tick() */
+	queue.on('end', function (err) {
+		if (err) {
+			mon.pm_log.error(err, 'error running queue');
+		}
+
+		dtrace['tick-done'].fire(function () { return ([]); });
+
 		if (callback) {
-			setImmediate(callback);
+			setImmediate(callback, err);
 		}
 	});
 };
 
+/*
+ * [private] For a given backend and query, run the query and return the
+ * results or errors asynchronously.
+ *
+ * If a query times out an error metric is recorded stating which query timed
+ * out, and for which backend it occurred.
+ */
 PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 {
+	mod_assertplus.number(pi, 'pi');
+	mod_assertplus.number(qi, 'qi');
+	mod_assertplus.func(cb, 'cb');
+
 	var mon = this;
-	var log = this.pm_log;
-	var url = this.pm_dbs[pi];
-	var client = this.pm_pgs[pi];
-	var query = this.pm_queries[qi];
-	var state = this.pm_state[pi][qi];
+	var log = mon.pm_log;
+	var query = mon.pm_queries[qi];
+	var state = mon.pm_state[pi][qi];
+	var client = mon.pm_pgs[pi].conn;
+	var backend = mon.pm_pgs[pi].name; /* for logging */
+
 	var time;
-	var timer, errmetric;
+	var timer, errmetric, res;
+	var aborted = false;
+	var rows = [];
 
 	/*
 	 * If the last check is still running, either the interval is configured
 	 * too short, the database is swamped, or something else has gone
 	 * horribly wrong (e.g., network issue).  Do not initiate another check,
 	 * since that can generally only make things worse.
-	 * XXX should likely have a timeout on the query, disconnect the client,
-	 * and reconnect.
 	 */
 	if (state !== null) {
-		log.error({
-		    'url': url,
+		log.warn({
+		    'backend': mon.pm_pgs[pi].name,
 		    'query': query.q_name,
 		    'last': state
 		}, 'skipping check (still pending)');
@@ -305,46 +838,91 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 	}
 
 	time = process.hrtime();
-	this.pm_state[pi][qi] = new Date().toISOString();
+	mon.pm_state[pi][qi] = new Date().toISOString();
 	log.debug({
-	    'url': url,
+	    'backend': backend,
 	    'query': query.q_name
 	}, 'check: start');
-	client.query(query.q_sql, function (err, result) {
+
+	res = client.query(query.q_sql);
+	res.on('row', function on_query_row(row) {
+		if (aborted) {
+			log.warn({
+				'backend': backend,
+				'query': query.q_name
+			}, 'query was aborted');
+			return;
+		}
 		log.debug({
-		    'url': url,
+		    'backend': backend,
 		    'query': query.q_name
 		}, 'check: done');
+
+		rows.push(row);
+	});
+
+	res.on('error', function on_query_error(err) {
 		mon.pm_state[pi][qi] = null;
-		time = process.hrtime(time);
+		aborted = true;
 
 		/*
-		 * If we see an error running the query, create a metric for
-		 * the query we were running
+		 * Record query timeouts. We could record other errors if we
+		 * think the cardinality of the error type would be low.
 		 */
-		if (err) {
+		if (mod_verror.hasCauseWithName(err, 'QueryTimeoutError')) {
 			errmetric = {
-				'name': 'pg_query_error',
-				'help': 'error performing PG query',
+				'name': 'pg_query_timeout',
+				'help': 'PG query timed out',
 				'metadata': {
+					'backend': backend,
 					'query': query.q_name
 				}
 			};
-			log.error({
-			    'url': url,
-			    'query': query.q_name
-			}, 'query failed');
 			mon.emitCounter(errmetric, 1);
+			setImmediate(cb, err);
+			return;
+		}
+
+		/*
+		 * If we see an error running the query, create a metric
+		 * for the query we were running
+		 */
+		errmetric = {
+			'name': 'pg_query_error',
+			'help': 'error performing PG query',
+			'metadata': {
+				'backend': backend,
+				'query': query.q_name
+			}
+		};
+		log.warn(err, {
+		    'backend': backend,
+		    'query': query.q_name
+		}, 'query failed');
+		mon.emitCounter(errmetric, 1);
+		setImmediate(cb, err);
+		return;
+	});
+
+	res.once('end', function on_query_end() {
+		res.removeAllListeners();
+		if (client.isDestroyed()) {
+			mon.pm_log.info({
+				'backend': backend
+			}, 'client removed during query');
 			setImmediate(cb);
 			return;
 		}
+		time = process.hrtime(time);
 
 		/*
 		 * Record the datapoint, which will emit several counter
 		 * stats, and then emit a separate timer stat for the
 		 * query itself.
 		 */
-		mon.record(pi, qi, result);
+		if (rows.length > 0) {
+			mon.record(pi, qi, rows);
+		}
 		timer = {
 			'attr': 'querytime',
 			'help': 'time to run stat query',
@@ -352,6 +930,7 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 		};
 		mon.emitTimer(mon.qstatname(pi, qi, null, timer),
 		    mod_jsprim.hrtimeMillisec(time));
+		mon.pm_state[pi][qi] = null;
 		setImmediate(cb);
 	});
 };
@@ -363,35 +942,139 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
  */
 PgMon.prototype.record = function (pi, qi, datum)
 {
+	mod_assertplus.number(pi, 'pi');
+	mod_assertplus.number(qi, 'qi');
+	mod_assertplus.object(datum, 'datum');
+
 	var mon = this;
-	var query = this.pm_queries[qi];
-	var url = this.pm_dbs[pi]['url'];
+	var backend = mon.pm_pgs[pi].name;
+	var query = mon.pm_queries[qi];
 	var oldresult, oldrow;
-
-	oldresult = this.pm_data[pi][qi];
-	this.pm_data[pi][qi] = {};
-	datum['rows'].forEach(function (row) {
+	var reset_time;
+	var last_reset_time;
+	var new_value, old_value;
+	var metric;
+
+	oldresult = mon.pm_data[pi][qi];
+	mon.pm_old_data[pi][qi] = oldresult;
+	mon.pm_data[pi][qi] = {};
+	datum.forEach(function record_metrics_for_row(row) {
 		var key = row[query.q_statkey];
 		mon.pm_data[pi][qi][key] = row;
 		oldrow = oldresult[key];
 
+		if (row.stats_reset && oldrow) {
+			/*
+			 * Try to detect a stat reset. Some relations reset
+			 * stats when the PG instance restarts, and then record
+			 * the reset time in the 'stats_reset' attribute. Other
+			 * relations (pg_stat_user_tables in particular) reset
+			 * stats when the PG instance restarts, but don't
+			 * include a 'stats_reset' attribute.
+			 *
+			 * In either case, we've overwritten the pre-reset data
+			 * with new data, so we should just skip this round
+			 * of recording metrics.
+			 */
+			reset_time = Date.parse(row.stats_reset);
+			last_reset_time = Date.parse(oldrow.stats_reset);
+			if (reset_time > last_reset_time) {
+				mon.pm_log.info({
+					'backend': backend,
+					'query': query.q_name,
+					'stats_reset': row.stats_reset
+				}, 'stats reset detected');
+				return;
+			}
+		}
+
 		if (!oldrow) {
 			mon.pm_log.info({
-			    'url': url,
+			    'backend': backend,
+			    'query': query.q_name,
 			    'key': key
 			}, 'row detected');
 			return;
 		}
 
-		query.q_counters.forEach(function (c) {
+		query.q_counters.forEach(function emit_counters(c) {
+			new_value = row[c.attr];
+			old_value = oldrow[c.attr];
+
+			/*
+			 * Ways we can get bad data from Postgres:
+			 * - corruption on the wire
+			 * - our database user can't access certain tables or
+			 *   values within a table
+			 * - we ran a bad query
+			 * In these cases we won't attempt to increment
+			 * counters or gauges, but will log a warning and
+			 * increment a separate counter to track this behavior.
+			 */
+			if (isNaN(new_value) || new_value === null) {
+				metric = mon.qstatname(pi, qi, row, c);
+
+				mon.pm_log.warn({
+					'metric': metric
+				}, 'NaN value observed');
+
+				mon.pm_targets.forEach(function (t) {
+					t.emitCounter({
+						'name': 'pg_NaN_error',
+						'help': 'pgstatsmon read a bad'
+						    + ' value from a SQL query',
+						'metadata': {
+							'name': metric.name,
+							'backend': backend,
+							'query': query.q_name
+						}
+					}, 1);
+				});
+				return;
+			}
+
+			if (old_value > new_value) {
+				/* some relations don't advertise stat resets */
+				mon.pm_log.info({
+					'backend': backend,
+					'key': key,
+					'counter': c
+				}, 'old value greater than new value -'
+				    + ' skipping');
+				return;
+			}
 			mon.emitCounter(
 			    mon.qstatname(pi, qi, row, c),
-			    row[c.attr] - oldrow[c.attr]);
+			    new_value - old_value);
 		});
 
-		query.q_gauges.forEach(function (g) {
+		query.q_gauges.forEach(function emit_gauges(g) {
+			new_value = row[g.attr];
+			/* see previous comment for explanation */
+			if (isNaN(new_value) || new_value === null) {
+				metric = mon.qstatname(pi, qi, row, g);
+
+				mon.pm_log.warn({
+					'metric': metric
+				}, 'NaN value observed');
+
+				mon.pm_targets.forEach(function (t) {
+					t.emitCounter({
+						'name': 'pg_NaN_error',
+						'help': 'pgstatsmon read a bad'
+						    + ' value from a SQL query',
+						'metadata': {
+							'name': metric.name,
+							'backend': backend,
+							'query': query.q_name
+						}
+					}, 1);
+				});
+				return;
+			}
+
 			mon.emitGauge(
-			    mon.qstatname(pi, qi, row, g), row[g.attr]);
+			    mon.qstatname(pi, qi, row, g), new_value);
 		});
 	});
 };
@@ -402,15 +1085,20 @@ PgMon.prototype.record = function (pi, qi, datum)
  */
 PgMon.prototype.qstatname = function (pi, qi, row, field)
 {
-	var dbname = this.pm_dbs[pi]['name'];
-	var query = this.pm_queries[qi];
+	mod_assertplus.number(pi, 'pi');
+	mod_assertplus.number(qi, 'qi');
+	mod_assertplus.optionalObject(row, 'row');
+	mod_assertplus.object(field, 'field');
+
+	var mon = this;
+	var query = mon.pm_queries[qi];
 	var fieldname = field.attr;
 	var help = field.help;
 	var metadata = query.q_metadata;
 	var mdvalues = {};
 	var name;
 
-	mdvalues['name'] = dbname;
+	mdvalues.backend = mon.pm_pgs[pi].name;
 	if (metadata && row) {
 		metadata.forEach(function (attr) {
 			mdvalues[attr] = row[attr];
@@ -440,62 +1128,35 @@ PgMon.prototype.qstatname = function (pi, qi, row, field)
 };
 
 /*
- * Emit the named counter to all targets.
+ * [private] Emit the named counter to all targets.
  */
 PgMon.prototype.emitCounter = function (metric, value)
 {
-	/*
-	 * It's possible that the user we're using to connect to the DB doesn't
-	 * have permissions to view certain tables, or we ran a bad query. In
-	 * these cases we won't attempt to increment counters, but will log
-	 * a warning and increment a separate counter to track this behavior.
-	 */
-	if (value === null) {
-		this.pm_log.warn(metric, 'null value observed');
-		this.pm_targets.forEach(function (t) {
-			t.emitCounter({
-				'name': 'pg_null_value_observed',
-				'help': 'pgstatsmon read a null value from' +
-				    ' a SQL query',
-				'metadata': {
-					'name': metric.name
-				}
-			}, 1);
-		});
-		return;
-	}
-	this.pm_targets.forEach(function (t) {
-		t.emitCounter(metric, value);
-	});
+	mod_assertplus.object(metric, 'metric');
+	mod_assertplus.number(value, 'value');
+
+	this.pm_targets.forEach(function (t) { t.emitCounter(metric, value); });
 };
 
 /*
- * Emit the named gauge to all targets.
+ * [private] Emit the named gauge to all targets.
  */
 PgMon.prototype.emitGauge = function (metric, value)
 {
-	if (value === null) {
-		this.pm_log.warn(metric, 'null value observed');
-		this.pm_targets.forEach(function (t) {
-			t.emitCounter({
-				'name': 'pg_null_value_observed',
-				'help': 'pgstatsmon read a null value from' +
-				    ' a SQL query',
-				'metadata': {
-					'name': metric.name
-				}
-			}, 1);
-		});
-		return;
-	}
+	mod_assertplus.object(metric, 'metric');
+	mod_assertplus.number(value, 'value');
+
 	this.pm_targets.forEach(function (t) { t.emitGauge(metric, value); });
 };
 
 /*
- * Emit the named timer to all targets.
+ * [private] Emit the named timer to all targets.
  */
 PgMon.prototype.emitTimer = function (metric, duration)
 {
+	mod_assertplus.object(metric, 'metric');
+	mod_assertplus.number(duration, 'duration');
+
 	this.pm_targets.forEach(function (t) {
 		t.emitTimer(metric, duration);
 	});
@@ -507,12 +1168,12 @@ PgMon.prototype.emitTimer = function (metric, duration)
  */
 function Query(conf, log)
 {
-	this.q_name = conf['name'];
-	this.q_sql = conf['sql'];
-	this.q_statkey = conf['statkey'] || null;
-	this.q_counters = (conf['counters'] || []).slice(0);
-	this.q_gauges = (conf['gauges'] || []).slice(0);
-	this.q_metadata = (conf['metadata'] || []).slice(0);
+	this.q_sql = conf.sql;
+	this.q_name = conf.name;
+	this.q_statkey = conf.statkey || null;
+	this.q_gauges = (conf.gauges || []).slice(0);
+	this.q_counters = (conf.counters || []).slice(0);
+	this.q_metadata = (conf.metadata || []).slice(0);
 }
 
 
@@ -552,11 +1213,16 @@ LogTarget.prototype.stop = function ()
  */
 function PrometheusTarget(conf, log)
 {
+	mod_assertplus.object(conf, 'conf');
+	mod_assertplus.object(log, 'log');
+
 	this.pe_log = log;
 	this.pe_ip = conf.ip;
 	this.pe_port = conf.port;
 	this.pe_route = conf.route;
-	this.pe_collector = mod_artedi.createCollector();
+	this.pe_collector = mod_artedi.createCollector({
+		'labels': conf.metadata
+	});
 	this.pe_server = mod_restify.createServer({
 		'log': this.pe_log.child({ 'component': 'prometheus_server' }),
 		'name': 'Monitor'
diff --git a/lib/queries.js b/lib/queries.js
index 5bb33b5..077978e 100644
--- a/lib/queries.js
+++ b/lib/queries.js
@@ -3,7 +3,7 @@
  * License, v. 2.0. If a copy of the MPL was not distributed with this
  * file, You can obtain one at http://mozilla.org/MPL/2.0/.
  *
- * Copyright (c) 2017, Joyent, Inc.
+ * Copyright (c) 2018, Joyent, Inc.
  */
 
 var mod_ajv = require('ajv');
@@ -86,38 +86,33 @@ var queries = [ {
     'name': 'pg_stat_replication',
     'statkey': 'application_name',
     'metadata': [ 'sync_state' ],
-    'sql': [
+    'sql': [ /* this only works on Postgres 9.4+ */
 	'SELECT ',
 	'sync_state, ',
-	'pg_xlog_location_diff(sent_location, write_location) AS write_lag, ',
-	'pg_xlog_location_diff(write_location, flush_location) AS flush_lag, ',
-	'pg_xlog_location_diff(flush_location, replay_location) AS replay_lag',
-	'FROM pg_stat_replication'
+	'sent_location - CAST (\'0/0\' AS pg_lsn) AS wal_sent, ',
+	'write_location - CAST (\'0/0\' AS pg_lsn) AS replica_wal_written, ',
+	'flush_location - CAST (\'0/0\' AS pg_lsn) AS replica_wal_flushed, ',
+	'replay_location - CAST (\'0/0\' AS pg_lsn) AS replica_wal_replayed ',
+	'FROM get_stat_replication();'
     ].join('\n'),
-    'gauges': [
-	{ 'attr': 'write_lag', 'help': 'write lag', 'unit': 'bytes' },
-	{ 'attr': 'flush_lag', 'help': 'flush lag', 'unit': 'bytes' },
-	{ 'attr': 'replay_lag', 'help': 'replay lag', 'unit': 'bytes' }
+    'counters': [
+	{ 'attr': 'wal_sent',
+	    'help': 'wal bytes sent to replica', 'unit': 'bytes' },
+	{ 'attr': 'replica_wal_written',
+	    'help': 'wal bytes written by replica', 'unit': 'bytes' },
+	{ 'attr': 'replica_wal_flushed',
+	    'help': 'wal bytes flushed by replica', 'unit': 'bytes' },
+	{ 'attr': 'replica_wal_replayed',
+	    'help': 'wal bytes replayed into database by replica',
+	    'unit': 'bytes' }
     ]
-}, {
-    'name': 'pg_class',
-    'statkey': 'relname',
-    'metadata': [ 'relname' ],
-    'sql': [
-	'SELECT c.oid::regclass AS relname,',
-	'       greatest(age(c.relfrozenxid),age(t.relfrozenxid)) AS freezeage',
-	'FROM pg_class c',
-	'LEFT JOIN pg_class t ON c.reltoastrelid = t.oid',
-	'WHERE c.relkind = \'r\';'
-    ].join('\n'),
-    'gauges': [ { 'attr': 'freezeage', 'help': 'xids since last vacuum' } ]
 }, {
     'name': 'pg_stat_activity',
     'statkey': 'datname',
     'metadata': [ 'datname', 'state' ],
     'sql': [
-	'SELECT datname, state, count(*) AS connections',
-	'FROM pg_stat_activity',
+	'SELECT datname, state, count(*) AS connections ',
+	'FROM get_stat_activity() ',
 	'GROUP BY datname, state;'
     ].join('\n'),
     'gauges': [ { 'attr': 'connections', 'help': 'worker process state' } ]
@@ -126,8 +121,10 @@ var queries = [ {
     'statkey': 'datname',
     'metadata': [ 'datname' ],
     'sql': [
-	'SELECT *',
-	'FROM pg_stat_database'
+	'SELECT * ',
+	'FROM pg_stat_database ',
+	'WHERE datname NOT LIKE \'postgres\' AND ',
+	'datname NOT LIKE \'template%\';'
     ].join('\n'),
     'gauges': [ { 'attr': 'numbackends', 'help': 'number of connections' } ],
     'counters': [
@@ -154,9 +151,9 @@ var queries = [ {
 	'       c.reltuples AS row_estimate,',
 	'       pg_total_relation_size(c.oid) AS total_bytes,',
 	'       pg_indexes_size(c.oid) AS index_bytes,',
-	'       pg_total_relation_size(reltoastrelid) AS toast_bytes',
-	'FROM pg_class c',
-	'LEFT JOIN pg_namespace n ON n.oid = c.relnamespace',
+	'       pg_total_relation_size(reltoastrelid) AS toast_bytes ',
+	'FROM pg_class c ',
+	'LEFT JOIN pg_namespace n ON n.oid = c.relnamespace ',
 	'WHERE relkind = \'r\' AND nspname LIKE \'public\';'
     ].join('\n'),
     'gauges': [
@@ -170,7 +167,7 @@ var queries = [ {
     'statkey': 'bgwriter',
     'metadata': [],
     'sql': [
-	'SELECT *',
+	'SELECT * ',
 	'FROM pg_stat_bgwriter;'
     ].join('\n'),
     'counters': [
@@ -190,7 +187,27 @@ var queries = [ {
 	    ' backends' },
 	{ 'attr': 'buffers_alloc', 'help': 'number of buffers allocated' }
     ]
-} ];
+}, {
+    'name': 'pg_vacuum',
+    'statkey': 'relname',
+    'metadata': ['relname'],
+    'sql': [ /* relowner 10 is hard-coded to be the 'postgres' superuser */
+	'SELECT ',
+	'    relname, age(relfrozenxid) AS xid_age, ',
+	'    (SELECT ',
+	'        setting::int FROM pg_settings ',
+	'        WHERE',
+	'        name = \'autovacuum_freeze_max_age\') - age(relfrozenxid)',
+	'    AS tx_until_wraparound_autovacuum ',
+	'FROM pg_class WHERE relowner != 10 AND relkind = \'r\';'
+    ].join('\n'),
+    'gauges': [
+	{ 'attr': 'xid_age', 'help': 'transactions since last wraparound' +
+	    ' autovacuum' },
+	{ 'attr': 'tx_until_wraparound_autovacuum', 'help': 'transactions' +
+	    ' until the next wraparound autovacuum' }
+    ]
+}];
 
 /*
  * Validate the query schema. Returns the query object if valid.
diff --git a/package.json b/package.json
index 21b69d2..fc2eb4f 100644
--- a/package.json
+++ b/package.json
@@ -7,16 +7,18 @@
 	},
 	"dependencies": {
 		"ajv": "5.5.1",
-		"artedi": "1.2.0",
+		"artedi": "1.3.0",
 		"assert-plus": "1.0.0",
+		"backoff": "2.5.0",
+		"bunyan": "1.8.10",
+		"cueball": "2.5.1",
+		"dtrace-provider": "0.8.6",
 		"jsprim": "2.0.0",
-		"pg": "6.4.2",
+		"pg": "7.4.1",
 		"restify": "6.3.4",
-		"vasync": "2.2.0"
-	},
-	"devDependencies": {
-		"bunyan": "1.8.10",
-		"verror": "1.10.0"
+		"vasync": "2.2.0",
+		"verror": "1.10.0",
+		"vmapi-resolver": "1.0.0"
 	},
 	"author": "Joyent, Inc",
 	"license": "MPL-2.0"
diff --git a/sapi_manifests/pgstatsmon/manifest.json b/sapi_manifests/pgstatsmon/manifest.json
new file mode 100644
index 0000000..7329444
--- /dev/null
+++ b/sapi_manifests/pgstatsmon/manifest.json
@@ -0,0 +1,4 @@
+{
+    "name": "pgstatsmon",
+    "path": "/opt/smartdc/pgstatsmon/etc/config.json"
+}
diff --git a/sapi_manifests/pgstatsmon/template b/sapi_manifests/pgstatsmon/template
new file mode 100644
index 0000000..feb0f01
--- /dev/null
+++ b/sapi_manifests/pgstatsmon/template
@@ -0,0 +1,28 @@
+{
+    "interval": {{SCRAPE_INTERVAL}},
+    "connections": {
+        "query_timeout": 1000,
+        "connect_timeout": 3000,
+        "connect_retries": 3
+    },
+    "user": "{{DATABASE_USER}}",
+    "database": "moray",
+    "backend_port": 5432,
+    "vmapi": {
+        "url": "http://vmapi.{{DATACENTER}}.{{DNS_DOMAIN}}",
+        "pollInterval": {{VMAPI_POLL_INTERVAL}},
+        "tags": {
+            "vm_tag_name": "{{VM_TAG_NAME}}",
+            "vm_tag_value": "{{VM_TAG_VALUE}}",
+            "nic_tag": "{{NIC_TAG}}"
+        }
+    },
+    "target": {
+        "ip": "0.0.0.0",
+        "port": 8881,
+        "route": "/metrics",
+	"metadata": {
+    		"datacenter": "{{DATACENTER}}"
+	}
+    }
+}
diff --git a/smf/manifests/pgstatsmon.xml b/smf/manifests/pgstatsmon.xml
new file mode 100644
index 0000000..4aac1ce
--- /dev/null
+++ b/smf/manifests/pgstatsmon.xml
@@ -0,0 +1,53 @@
+<?xml version='1.0'?>
+<!DOCTYPE service_bundle SYSTEM '/usr/share/lib/xml/dtd/service_bundle.dtd.1'>
+
+<service_bundle type='manifest' name='export'>
+    <service name='manta/application/pgstatsmon' type='service' version='0'>
+        <create_default_instance enabled='true'/>
+        <single_instance/>
+
+        <dependency name='fs'
+                    grouping='require_all'
+                    restart_on='none'
+                    type='service'>
+            <service_fmri value='svc:/system/filesystem/local'/>
+        </dependency>
+        <dependency name='net'
+                    grouping='require_all'
+                    restart_on='none'
+                    type='service'>
+            <service_fmri value='svc:/network/physical'/>
+        </dependency>
+        <dependency name="mdata"
+                    grouping="require_all"
+                    restart_on="none"
+                    type="service">
+            <service_fmri value="svc:/smartdc/mdata:execute" />
+        </dependency>
+        <dependency name="config-agent"
+                    grouping="optional_all"
+                    restart_on="none"
+                    type="service">
+            <service_fmri value="svc:/smartdc/application/config-agent" />
+        </dependency>
+
+        <exec_method name='start'
+                     type='method'
+                     exec='node --abort-on-uncaught-exception bin/pgstatsmon.js etc/config.json &amp;'
+                     timeout_seconds='10'>
+            <method_context working_directory='/opt/smartdc/pgstatsmon'>
+                <method_credential user='root' group='root'/>
+                <method_environment>
+                    <envvar name='PATH' value='/opt/smartdc/pgstatsmon/node/bin:/usr/bin:/bin:/opt/local/bin'/>
+                </method_environment>
+            </method_context>
+        </exec_method>
+
+        <exec_method name='stop'
+                     type='method'
+                     exec=':kill -TERM'
+                     timeout_seconds='60'>
+            <method_context/>
+        </exec_method>
+    </service>
+</service_bundle>
diff --git a/test/badquery.tst.js b/test/badquery.tst.js
index ccdcd0a..be61b21 100644
--- a/test/badquery.tst.js
+++ b/test/badquery.tst.js
@@ -20,6 +20,7 @@ var VError = require('verror').VError;
  */
 var TEST_USER = 'pgstatsmon';
 var TEST_DATABASE = 'pgstatsmon';
+
 function main()
 {
 	var badQuery;
@@ -59,10 +60,12 @@ function BadQuery(callback)
 	mon_args.log = this.log;
 
 	this.mon = helper.getMon(mon_args);
-	this.prom_target = this.mon.getTarget();
 
 	mod_vasync.pipeline({
 		'funcs': [
+			function (_, cb) {
+				self.mon.start(cb);
+			},
 			function (_, cb) {
 				helper.createUser(TEST_USER, cb);
 			},
@@ -81,6 +84,28 @@ function BadQuery(callback)
 					cb();
 				});
 			},
+			function (_, cb) {
+				/*
+				 * pgstatsmon first tries to set up its
+				 * backend(s) by creating a user and some
+				 * functions for that user to call. We don't
+				 * want that user because we want to do weird
+				 * things that the user pgstatsmon creates isn't
+				 * allowed to do. pgstatsmon won't run any
+				 * queries against a backend if it hasn't first
+				 * tried to set up the user.
+				 *
+				 * Anyway, it takes a little while for this
+				 * to happen so we have to sleep for just a few
+				 * ms while pgstatsmon does its thing.
+				 *
+				 * XXX Maybe we can make user creation optional,
+				 * or somehow have pgstatsmon wait to call the
+				 * 'start' function's callback until at least
+				 * one backend is verified set up.
+				 */
+				setTimeout(cb, 500);
+			},
 			function (_, cb) {
 				self.mon.tick(cb);
 			},
@@ -94,6 +119,7 @@ function BadQuery(callback)
 			callback(new VError(err, 'error preparing tests'));
 			return;
 		}
+		self.prom_target = self.mon.getTarget();
 		callback();
 	});
 }
@@ -127,10 +153,10 @@ BadQuery.prototype.run_invalid_query = function (callback)
 	} ];
 
 	var labels = {
-		'query': queries[0].name
+		'query': queries[0].name,
+		'backend': self.mon.pm_pgs[0]['name']
 	};
 
-
 	this.mon.initializeMetrics(queries);
 	/*
 	 * since mon.initializeMetrics() drops all of the data, we need to get
diff --git a/test/basic.tst.js b/test/basic.tst.js
index 93964be..bead4f0 100644
--- a/test/basic.tst.js
+++ b/test/basic.tst.js
@@ -71,7 +71,6 @@ function BasicTest(callback)
 
 	this.table_name = 'pgstatsmon_basic';
 	this.mon = helper.getMon(mon_args);
-	this.prom_target = this.mon.getTarget();
 
 	mod_vasync.pipeline({
 		'funcs': [
@@ -97,7 +96,16 @@ function BasicTest(callback)
 				    cb);
 			},
 			function (_, cb) {
-				self.mon.tick(cb);
+				self.mon.start(cb);
+			},
+			function (_, cb) {
+				/*
+				 * see comment in badquery.js about waiting for
+				 * pgstatsmon to create users
+				 */
+				setTimeout(function () {
+					self.mon.tick(cb);
+				}, 500);
 			}
 		]
 	}, function (err, results) {
@@ -106,6 +114,7 @@ function BasicTest(callback)
 			return;
 		}
 		clearInterval(self.mon.pm_intervalObj);
+		self.prom_target = self.mon.getTarget();
 		callback();
 	});
 }
@@ -150,7 +159,7 @@ BasicTest.prototype.check_tuple_count = function (callback)
 	var initial_value;
 	var q;
 	var labels = {
-		'name': this.mon.pm_dbs[0].name,
+		'backend': this.mon.pm_pgs[0]['name'],
 		'relname': this.table_name
 	};
 
@@ -233,7 +242,7 @@ BasicTest.prototype.check_connections = function (callback)
 	var initial_value;
 	var mclient;
 	var labels = {
-		'name': this.mon.pm_dbs[0].name,
+		'backend': this.mon.pm_pgs[0]['name'],
 		'datname': this.client.database,
 		'state': 'idle'
 	};
diff --git a/test/etc/testconfig.json b/test/etc/testconfig.json
index 661498d..d1a9d21 100644
--- a/test/etc/testconfig.json
+++ b/test/etc/testconfig.json
@@ -1,12 +1,26 @@
 {
     "interval": 2000,
-    "dbs": [ {
-	"name": "test",
-	"url": "postgres://pgstatsmon@localhost:5432/pgstatsmon"
-    } ],
+    "connections": {
+        "query_timeout": 1000,
+        "connect_timeout": 3000,
+        "connect_retries": 3,
+        "max_connections": 10
+    },
+    "user": "pgstatsmon",
+    "database": "pgstatsmon",
+    "backend_port": 5432,
+    "static": {
+        "dbs": [ {
+            "name": "test",
+            "ip": "127.0.0.1"
+        } ]
+    },
     "target": {
-	"ip": "0.0.0.0",
-	"port": 9187,
-	"route": "/metrics"
+        "ip": "0.0.0.0",
+        "port": 9187,
+        "route": "/metrics",
+	"metadata": {
+		"datacenter": "testing-dc"
+	}
     }
 }
diff --git a/test/helper.js b/test/helper.js
index a07e98e..c384877 100644
--- a/test/helper.js
+++ b/test/helper.js
@@ -89,14 +89,14 @@ function getMon(args)
  */
 function createClient(user, database, cb)
 {
-	var url_obj = mod_url.parse(config.dbs[0].url);
-	var url_str = mod_util.format('postgres://%s@%s/%s', user, url_obj.host,
+	var ip = config['static']['dbs'][0].ip;
+	var url_str = mod_util.format('postgres://%s@%s/%s', user, ip,
 	    database);
 
 	var client = new mod_pg.Client(url_str);
 	client.connect(function (err) {
 		if (err) {
-			config.log.error(err, config.dbs[0], 'failed to' +
+			config.log.error(err, url_str, 'failed to' +
 				' create connection to backend Postgres');
 		}
 		cb(err, client);
@@ -109,9 +109,9 @@ function createClient(user, database, cb)
 function createTable(table_name, client, cb)
 {
 	if (table_name_regex.test(table_name) === false) {
-		throw new VError('invalid table name: "%s"', table_name);
+		cb(new VError('invalid table name: "%s"', table_name));
+		return;
 	}
-
 	var query = 'CREATE TABLE ' + table_name + ' (animal text, sound text)';
 	client.query(query, function (err) {
 		cb(err);
@@ -124,7 +124,8 @@ function createTable(table_name, client, cb)
 function dropTable(table_name, client, cb)
 {
 	if (table_name_regex.test(table_name) === false) {
-		throw new VError('invalid table name: "%s"', table_name);
+		cb(new VError('invalid table name: "%s"', table_name));
+		return;
 	}
 
 	var query = 'DROP TABLE IF EXISTS ' + table_name;
diff --git a/tools/mk/Makefile.defs b/tools/mk/Makefile.defs
new file mode 100644
index 0000000..73dd612
--- /dev/null
+++ b/tools/mk/Makefile.defs
@@ -0,0 +1,105 @@
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2018, Joyent, Inc.
+#
+
+#
+# Makefile.defs: common defines.
+#
+# NOTE: This makefile comes from the "eng" repo. It's designed to be dropped
+# into other repos as-is without requiring any modifications. If you find
+# yourself changing this file, you should instead update the original copy in
+# eng.git and then update your repo to use the new version.
+#
+# This makefile defines some useful defines. Include it at the top of
+# your Makefile.
+#
+# Definitions in this Makefile:
+#
+#	TOP 		The absolute path to the project directory. The top dir.
+#	BRANCH 		The current git branch.
+#	TIMESTAMP	The timestamp for the build. This can be set via
+#			the TIMESTAMP envvar (used by MG-based builds).
+#	STAMP		A build stamp to use in built package names.
+#
+#	MAKE_STAMPS_DIR	The directory in which make stamp files are to be
+#			created.  See comments below on expensive targets.
+#
+#	CACHE_DIR	A directory tree in which temporary files may be
+#			collected by download, tarball extraction, etc.  This
+#			directory is completely removed by "make distclean".
+#			Files in this directory are not intended to be shipped.
+#
+
+TOP := $(shell pwd)
+
+#
+# Mountain Gorilla-spec'd versioning.
+# See "Package Versioning" in MG's README.md:
+# <https://mo.joyent.com/mountain-gorilla/blob/master/README.md#L139-200>
+#
+# Need GNU awk for multi-char arg to "-F".
+_AWK := $(shell (which gawk >/dev/null && echo gawk) \
+	|| (which nawk >/dev/null && echo nawk) \
+	|| echo awk)
+BRANCH := $(shell git symbolic-ref HEAD | $(_AWK) -F/ '{print $$3}')
+ifeq ($(TIMESTAMP),)
+	TIMESTAMP := $(shell date -u "+%Y%m%dT%H%M%SZ")
+endif
+_GITDESCRIBE := g$(shell git describe --all --long --dirty | $(_AWK) -F'-g' '{print $$NF}')
+STAMP := $(BRANCH)-$(TIMESTAMP)-$(_GITDESCRIBE)
+
+# node-gyp will print build info useful for debugging with V=1
+export V=1
+
+CACHE_DIR ?=		cache
+DISTCLEAN_FILES +=	$(CACHE_DIR)
+
+#
+# EXPENSIVE TARGETS AND MAKE STAMP FILES
+#
+# Targets which are expensive to run and lack a single file that marks
+# completion are difficult to track with make; e.g., "npm install".  One
+# solution to this problem is to create "stamp" files with symbolic names which
+# are created as the final step in a complex make rule in order to mark
+# completion.
+#
+# In order to make these more uniform, and easier to target with "make clean",
+# we will attempt to store them under a single directory.  Note that these
+# files are never targets for shipping in build artefacts.
+#
+# Stamp-backed targets come in several parts.  First, a macro should be defined
+# which names a file in the MAKE_STAMPS_DIR directory.  Then, a target which
+# creates this stamp file must be provided.  The recipe for this target should
+# use MAKE_STAMP_REMOVE and MAKE_STAMP_CREATE to perform the appropriate stamp
+# management.
+#
+# For example:
+#
+# --- Makefile.*.defs:
+#
+#	$(STAMP_EXPENSIVE_RESULT) := $(MAKE_STAMPS_DIR)/expensive-result
+#
+# --- Makefile.*.targ:
+#
+#	$(STAMP_EXPENSIVE_RESULT): input/file another/input/file
+#		$(MAKE_STAMP_REMOVE)
+#		rm -rf output_tree/  # <--- ensure a clean slate
+#		expensive_program -o output_tree/ $^
+#		$(MAKE_STAMP_CREATE)
+#
+# NOTE: Every stamp file is exposed as an implicit "stamp-$STAMP_NAME" target.
+# The example above could be built manually by invoking:
+#
+#	make stamp-expensive-result
+#
+MAKE_STAMPS_DIR ?=	make_stamps
+CLEAN_FILES +=		$(MAKE_STAMPS_DIR)
+
+MAKE_STAMP_REMOVE =	mkdir -p $(@D); rm -f $(@)
+MAKE_STAMP_CREATE =	mkdir -p $(@D); touch $(@)
diff --git a/tools/mk/Makefile.node_modules.defs b/tools/mk/Makefile.node_modules.defs
new file mode 100644
index 0000000..ad9fb6d
--- /dev/null
+++ b/tools/mk/Makefile.node_modules.defs
@@ -0,0 +1,68 @@
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2018, Joyent, Inc.
+#
+
+#
+# Makefile.node_modules.defs: Makefile for using NPM modules.
+#
+# NOTE: This makefile comes from the "eng" repo. It's designed to be dropped
+# into other repos as-is without requiring any modifications. If you find
+# yourself changing this file, you should instead update the original copy in
+# eng.git and then update your repo to use the new version.
+#
+
+#
+# This Makefile provides a target for building NPM modules from the dependency
+# information in the "package.json" file.  The "npm install" operation is
+# expensive and produces a complex (multi-file) result which is difficult for
+# make to use in dependency analysis.  As such, we use a "stamp" file to track
+# successful completion of module installation.
+#
+# This variable allows the consumer to influence the environment used to run
+# NPM commands.
+#
+#	NPM_ENV			This string should be set to a list of
+#				environment variables in the syntax used
+#				by bash; e.g.,
+#
+#					NPM_ENV =	TESTING=yes V=1
+#
+# Consumers should, for targets which depend on the installation of NPM
+# modules, depend on the stamp file using the $(STAMP_NODE_MODULES) variable,
+# e.g.:
+#
+#	.PHONY: all
+#	all: $(STAMP_NODE_MODULES)
+#
+# A phony target, "make stamp-node-modules", is also provided to allow the
+# engineer to manually perform NPM module installation without invoking other
+# targets.  Note that this target should _not_ be used as a dependency for
+# other targets in consuming Makefiles; using phony targets to represent
+# intermediate build stages can inhibit the ability of make to determine
+# when no additional actions are required.
+#
+
+TOP ?= $(error You must include Makefile.defs before this makefile)
+NPM ?= $(error You must include either Makefile.node.defs or \
+    Makefile.node_prebuilt.defs before this makefile)
+
+BUILD ?=		build
+
+#
+# Invoking "npm install" at the top-level will create a "node_modules"
+# directory into which NPM modules will be installed.
+#
+CLEAN_FILES +=		node_modules
+
+#
+# To avoid repeatedly reinstalling from NPM, we create a "stamp" file to track
+# successful runs of "npm install".  Note that MAKE_STAMPS_DIR is included
+# in CLEAN_FILES already.
+#
+STAMP_NODE_MODULES ?=	$(MAKE_STAMPS_DIR)/node-modules
diff --git a/tools/mk/Makefile.node_modules.targ b/tools/mk/Makefile.node_modules.targ
new file mode 100644
index 0000000..762f3bf
--- /dev/null
+++ b/tools/mk/Makefile.node_modules.targ
@@ -0,0 +1,31 @@
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2018, Joyent, Inc.
+#
+
+#
+# Makefile.node_modules.targ: See comments in Makefile.node_modules.defs.
+#
+# NOTE: This makefile comes from the "eng" repo. It's designed to be dropped
+# into other repos as-is without requiring any modifications. If you find
+# yourself changing this file, you should instead update the original copy in
+# eng.git and then update your repo to use the new version.
+#
+
+STAMP_NODE_MODULES ?= $(error You must include Makefile.node_modules.defs \
+    before this file)
+
+#
+# If the "package.json" file changes, we need to rebuild the contents of
+# the "node_modules" directory.
+#
+$(STAMP_NODE_MODULES): package.json | $(NPM_EXEC)
+	$(MAKE_STAMP_REMOVE)
+	rm -rf node_modules
+	$(NPM_ENV) $(NPM) install
+	$(MAKE_STAMP_CREATE)
diff --git a/tools/mk/Makefile.node_prebuilt.defs b/tools/mk/Makefile.node_prebuilt.defs
new file mode 100644
index 0000000..9edc396
--- /dev/null
+++ b/tools/mk/Makefile.node_prebuilt.defs
@@ -0,0 +1,159 @@
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2018, Joyent, Inc.
+#
+
+#
+# Makefile.node_prebuilt.defs: Makefile for including a prebuilt Node.js build.
+#
+# NOTE: This makefile comes from the "eng" repo. It's designed to be dropped
+# into other repos as-is without requiring any modifications. If you find
+# yourself changing this file, you should instead update the original copy in
+# eng.git and then update your repo to use the new version.
+#
+
+#
+# This Makefile facilitates downloading and bundling a prebuilt node.js
+# build (using the 'sdcnode' distro builds). This is an alternative to
+# the "Makefile.node.*" makefiles for *building* a node from source.
+#
+# Usage:
+#
+# - Define `NODE_PREBUILT_VERSION` in your Makefile to choose a node version.
+#   E.g.: `NODE_PREBUILT_VERSION=v0.6.19`. See other optional variables
+#   below.
+# - `include tools/mk/Makefile.node_prebuilt.defs` after this in your Makefile.
+# - `include tools/mk/Makefile.node_prebuilt.targ` near the end of your
+#   Makefile.
+# - Have at least one of your Makefile targets depend on either `$(NODE_EXEC)`
+#   or `$(NPM_EXEC)`. E.g.:
+#
+#		node_modules/restify: deps/restify $(NPM_EXEC)
+#			$(NPM) install deps/restify
+#
+#   or better, use an order-only dependency to avoid spurious rebuilds:
+#
+#		node_modules/restify: deps/restify | $(NPM_EXEC)
+#			$(NPM) install deps/restify
+#
+# - Use `$(NPM)` or `$(NODE)` to use your node build.
+# - Include the "$(NODE_INSTALL)" tree in your release package.
+#
+#
+# When including this Makefile, you MUST also specify:
+#
+#	NODE_PREBUILT_VERSION 	The node version in the prebuilt 'sdcnode'
+#				package to use. Typically this is one of the
+#				node version tags, e.g. "v0.6.18" but it
+#				can be any commitish.
+#
+# When including this Makefile, you MAY also specify:
+#
+#	NODE_PREBUILT_DIR 	The dir in which to find sdcnode builds. This
+#				can either be a *local directory* or *a
+#				URL* dir (with trailing '/') which serves
+#				Apache/Nginx dir listing HTML.
+#				(default: sdcnode master build dir on stuff)
+#
+#	NODE_PREBUILT_TAG	The 'sdcnode' project supports special
+#				configuration builds of node, e.g. say a
+#				build configured `--without-ssl`. These
+#				special configurations are given a tag, e.g.
+#				'gz', that is used in the filename. Optionally
+#				specify a tag name here.
+#				(default: empty)
+#
+#	NODE_PREBUILT_BRANCH	Specify a particular branch of 'sdcnode' builds
+#				from which to pull. Generally one should stick
+#				with the default.
+#				(default: master)
+#
+#	NODE_PREBUILT_IMAGE		If you have a zone image that differs from that
+#				for an sdcnode build that you want to use (potential compat
+#				issues be damned), then set this to the UUID of the sdcnode
+#				build you want. See here for available build image uuids:
+#				<https://download.joyent.com/pub/build/sdcnode/master-latest/sdcnode/>
+#
+#	BUILD			top-level directory for built binaries
+#				(default: "build")
+#
+#	NODE_INSTALL		where node should install its built items
+#				(default: "$BUILD/node")
+#
+#
+# Dev Notes:
+#
+# This works by getting "NODE_PREBUILT_NAME" from the provided "NODE_PREBUILT_*"
+# vars and the image version (via 'mdata-get sdc:image_uuid'). The image uuid is
+# included to ensure an exact match with the build machine. This name (e.g.
+# "v0.6.18-zone-$uuid") is used to find a matching "sdcnode-$name-*.tgz" build
+# in "NODE_PREBUILT_DIR" (either a local directory or a URL). That tarball is
+# downloaded and extracted into "NODE_INSTALL".
+#
+# The "*_EXEC" vars are set to named symlinks, e.g.
+# "build/prebuilt-node-v0.6.18-$uuid", so that a change of selected node
+# build (say the developer changes NODE_PREBUILT_VERSION) will recreate the
+# node install.
+#
+# See <https://mo.joyent.com/docs/sdcnode/master/> for details on 'sdcnode-*'
+# package naming.
+#
+
+TOP ?= $(error You must include Makefile.defs before this makefile)
+NODE_PREBUILT_VERSION ?= $(error NODE_PREBUILT_VERSION is not set.)
+
+
+BUILD		?= build
+NODE_INSTALL	?= $(BUILD)/node
+DISTCLEAN_FILES	+= $(NODE_INSTALL) \
+	$(BUILD)/prebuilt-node-* $(BUILD)/prebuilt-npm-*
+
+NODE_PREBUILT_BRANCH ?= master
+NODE_PREBUILT_IMAGE ?= $(shell pfexec mdata-get sdc:image_uuid)
+ifeq ($(NODE_PREBUILT_TAG),)
+	NODE_PREBUILT_NAME := $(NODE_PREBUILT_VERSION)-$(NODE_PREBUILT_IMAGE)
+else
+	NODE_PREBUILT_NAME := $(NODE_PREBUILT_VERSION)-$(NODE_PREBUILT_TAG)-$(NODE_PREBUILT_IMAGE)
+endif
+NODE_PREBUILT_PATTERN := sdcnode-$(NODE_PREBUILT_NAME)-$(NODE_PREBUILT_BRANCH)-.*\.tgz
+NODE_PREBUILT_DIR ?= https://download.joyent.com/pub/build/sdcnode/$(NODE_PREBUILT_IMAGE)/$(NODE_PREBUILT_BRANCH)-latest/sdcnode/
+ifeq ($(shell echo $(NODE_PREBUILT_DIR) | cut -c 1-4),http)
+	NODE_PREBUILT_BASE := $(shell curl -ksS --fail --connect-timeout 30 $(NODE_PREBUILT_DIR) | grep 'href=' | cut -d'"' -f2 | grep "^$(NODE_PREBUILT_PATTERN)$$" | sort | tail -1)
+	ifneq ($(NODE_PREBUILT_BASE),)
+		NODE_PREBUILT_TARBALL := $(NODE_PREBUILT_DIR)$(NODE_PREBUILT_BASE)
+	endif
+else
+	NODE_PREBUILT_BASE := $(shell ls -1 $(NODE_PREBUILT_DIR)/ | grep "^$(NODE_PREBUILT_PATTERN)$$" 2>/dev/null | sort | tail -1)
+	ifneq ($(NODE_PREBUILT_BASE),)
+		NODE_PREBUILT_TARBALL := $(NODE_PREBUILT_DIR)/$(NODE_PREBUILT_BASE)
+	endif
+endif
+ifeq ($(NODE_PREBUILT_TARBALL),)
+	NODE_PREBUILT_TARBALL = $(error NODE_PREBUILT_TARBALL is empty: no '$(NODE_PREBUILT_DIR)/$(NODE_PREBUILT_PATTERN)' found)
+endif
+
+
+# Prebuild-specific paths for the "*_EXEC" vars to ensure that
+# a prebuild change (e.g. if master Makefile's NODE_PREBUILT_VERSION
+# choice changes) causes a install of the new node.
+NODE_EXEC	:= $(BUILD)/prebuilt-node-$(NODE_PREBUILT_NAME)
+NODE_WAF_EXEC	:= $(BUILD)/prebuilt-node-waf-$(NODE_PREBUILT_NAME)
+NPM_EXEC	:= $(BUILD)/prebuilt-npm-$(NODE_PREBUILT_NAME)
+
+#
+# These paths should be used during the build process to invoke Node and
+# Node-related build tools like NPM.  All paths are fully qualified so that
+# they work regardless of the current working directory at the point of
+# invocation.
+#
+# Note that where PATH is overridden, the value chosen must cause execution of
+# "node" to find the same binary to which the NODE macro refers.
+#
+NODE		:= $(TOP)/$(NODE_INSTALL)/bin/node
+NODE_WAF	:= $(TOP)/$(NODE_INSTALL)/bin/node-waf
+NPM		:= PATH=$(TOP)/$(NODE_INSTALL)/bin:$(PATH) $(NODE) $(TOP)/$(NODE_INSTALL)/bin/npm
diff --git a/tools/mk/Makefile.node_prebuilt.targ b/tools/mk/Makefile.node_prebuilt.targ
new file mode 100644
index 0000000..0e720a8
--- /dev/null
+++ b/tools/mk/Makefile.node_prebuilt.targ
@@ -0,0 +1,42 @@
+# -*- mode: makefile -*-
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2018, Joyent, Inc.
+#
+
+#
+# Makefile.node_prebuilt.targ: Makefile for including a prebuilt Node.js
+# build.
+#
+# NOTE: This makefile comes from the "eng" repo. It's designed to be dropped
+# into other repos as-is without requiring any modifications. If you find
+# yourself changing this file, you should instead update the original copy in
+# eng.git and then update your repo to use the new version.
+
+
+NODE_PREBUILT_TARBALL ?= $(error NODE_PREBUILT_TARBALL is not set: was Makefile.node_prebuilt.defs included?)
+
+
+# TODO: remove this limitation
+# Limitation: currently presuming that the NODE_INSTALL basename is
+# 'node' and that sdcnode tarballs have a 'node' top-level dir.
+$(NODE_EXEC) $(NPM_EXEC) $(NODE_WAF_EXEC):
+	[[ $(shell basename $(NODE_INSTALL)) == "node" ]] \
+		|| (echo "Limitation: 'basename NODE_INSTALL' is not 'node'" && exit 1)
+	rm -rf $(NODE_INSTALL) \
+		$(BUILD)/prebuilt-node-* $(BUILD)/prebuilt-npm-*
+	mkdir -p $(shell dirname $(NODE_INSTALL))
+	if [[ $(shell echo $(NODE_PREBUILT_TARBALL) | cut -c 1-4) == "http" ]]; then \
+		echo "Downloading '$(NODE_PREBUILT_BASE)'."; \
+		curl -ksS --fail --connect-timeout 30 -o $(shell dirname $(NODE_INSTALL))/$(NODE_PREBUILT_BASE) $(NODE_PREBUILT_TARBALL); \
+		(cd $(shell dirname $(NODE_INSTALL)) && $(TAR) xf $(NODE_PREBUILT_BASE)); \
+	else \
+		(cd $(shell dirname $(NODE_INSTALL)) && $(TAR) xf $(NODE_PREBUILT_TARBALL)); \
+	fi
+	ln -s $(TOP)/$(NODE_INSTALL)/bin/node $(NODE_EXEC)
+	ln -s $(TOP)/$(NODE_INSTALL)/bin/npm $(NPM_EXEC)
diff --git a/Makefile.targ b/tools/mk/Makefile.targ
similarity index 99%
rename from Makefile.targ
rename to tools/mk/Makefile.targ
index b6e642b..8300cdb 100644
--- a/Makefile.targ
+++ b/tools/mk/Makefile.targ
@@ -1,6 +1,6 @@
 # -*- mode: makefile -*-
 #
-# Copyright (c) 2012, Joyent, Inc. All rights reserved.
+# Copyright (c) 2018, Joyent, Inc. All rights reserved.
 #
 # Makefile.targ: common targets.
 #
