From 93db4af2a132be251361398a18bfbdaef0417562 Mon Sep 17 00:00:00 2001
From: Kody A Kantor <kody.kantor@gmail.com>
Date: Mon, 12 Feb 2018 19:40:50 +0000
Subject: [PATCH] joyent/pgstatsmon#4 improve connection handling and backend
 discovery

---
 CHANGES.md               |   1 +
 README.md                |  79 ++++-
 bin/pgstatsmon.js        |   3 +-
 etc/config.json          |  12 -
 etc/static.json          |  22 ++
 etc/vmapi.json           |  25 ++
 lib/dtrace.js            |  71 +++++
 lib/pgclient.js          | 245 +++++++++++++++
 lib/pgstatsmon.js        | 652 +++++++++++++++++++++++++++++++++------
 lib/queries.js           |  23 +-
 package.json             |  10 +-
 test/badquery.tst.js     |  19 +-
 test/basic.tst.js        |   9 +-
 test/etc/testconfig.json |  24 +-
 test/helper.js           |   7 +-
 15 files changed, 1053 insertions(+), 149 deletions(-)
 delete mode 100644 etc/config.json
 create mode 100644 etc/static.json
 create mode 100644 etc/vmapi.json
 create mode 100644 lib/dtrace.js
 create mode 100644 lib/pgclient.js

diff --git a/CHANGES.md b/CHANGES.md
index 5fb6a9e..b4adc78 100644
--- a/CHANGES.md
+++ b/CHANGES.md
@@ -1,6 +1,7 @@
 # pgstatsmon Changelog
 
 ## Not yet released.
+* #4 improve connection handling and backend discovery
 * #2 repo housekeeping
 * #1 support Prometheus-style metric collection
 
diff --git a/README.md b/README.md
index 71aea53..57dbfe8 100644
--- a/README.md
+++ b/README.md
@@ -18,23 +18,36 @@ Then run the monitor with:
 It logs to stdout using bunyan.
 
 ## Example
+
+Using a configuration file for static backends:
 ```
 $ cat etc/myconfig.json
 {
     "interval": 10000,
-    "dbs": [ {
-        "name": "primary",
-        "url": "postgres://postgres@10.99.99.16:5432/moray"
-    } ],
+    "connections": {
+        "query_timeout": 1000,
+        "connect_timeout": 3000,
+        "connect_retries": 3,
+        "max_connections": 100
+    },
+    "static": {
+        "dbs": [{
+            "name": "primary",
+            "ip": "10.99.99.16"
+        }],
+        "backend_port": 5432,
+        "user": "moray"
+    },
     "target": {
         "ip": "0.0.0.0",
         "port": 9187,
         "route": "/metrics"
     }
 }
+
 $ node ./bin/pgstatsmon.js etc/myconfig.json > pgstatsmon.log &
 
-... wait <interval> seconds ...
+... wait <interval / 1000> seconds ...
 
 $ curl http://localhost:9187/metrics
 ...
@@ -61,7 +74,54 @@ pg_stat_bgwriter_checkpoint_sync_time_ms{name="primary"} 19
 ...
 ```
 
+## VMAPI Discovery
+
+pgstatsmon can optionally be configured to use VMAPI for discovery of backend
+Postgres instances. This configuration will cause pgstatsmon to poll VMAPI at
+the given interval for information about running Postgres instances.
+
+The VMAPI discovery configuration takes a number of arguments:
+* 'url' - URL or IP address of the VMAPI server
+* 'pollInterval' - rate (in milliseconds) at which to poll VMAPI
+* 'tags' - an object describing which VMs to discover
+  * 'vm_tag_name' - name of the VM tag key for Postgres VMs
+  * 'vm_tag_value' - value of the VM tag for Postgres VMs
+  * 'nic_tag_name' - NIC tag of interface to use for connecting to Postgres
+* 'backend_port' - port number used to connect to Postgres instances
+* 'user' - Postgres user for pgstatsmon
+
+Example VMAPI configuration file:
+```
+$ cat etc/vmapiconfig.json
+{
+    "interval": 10000,
+    "connections": {
+        "query_timeout": 1000,
+        "connect_timeout": 3000,
+        "connect_retries": 3,
+        "max_connections": 100
+    },
+    "vmapi": {
+        "url": "http://vmapi.coal-1.example.com",
+        "pollInterval": 600000,
+        "tags": {
+            "vm_tag_name": "manta_role",
+            "vm_tag_value": "postgres",
+            "nic_tag_name": "manta"
+        },
+        "backend_port": 5432,
+        "user": "moray"
+    },
+    "target": {
+        "ip": "0.0.0.0",
+        "port": 9187,
+        "route": "/metrics"
+    }
+}
+```
+
 ## Prometheus
+
 pgstatsmon makes metrics available in the Prometheus text format.  A user can
 issue `GET /metrics` to retrieve all of the metrics pgstatsmon collects from
 every Postgres instance being monitored.
@@ -119,6 +179,15 @@ to run the tests, your configuration file may look like this:
 }
 ```
 
+## DTrace
+
+There are a number of useful DTrace probes built in to pgstatsmon.  The full
+listing of probes specific to pgstatsmon and their arguments can be found in
+the [lib/dtrace.js](./lib/dtrace.js) file.
+
+[node-artedi](https://github.com/joyent/node-artedi), which pgstatsmon uses to
+perform aggregation and serialize metrics, also exposes DTrace probes.
+
 ## License
 MPL-v2. See the LICENSE file.
 
diff --git a/bin/pgstatsmon.js b/bin/pgstatsmon.js
index 2bf181d..6ba9190 100644
--- a/bin/pgstatsmon.js
+++ b/bin/pgstatsmon.js
@@ -47,8 +47,9 @@ function main()
 	});
 
 	log.info('config', config);
+
 	config['log'] = log;
-	pgstatsmon(config);
+	pgstatsmon(config).start();
 }
 
 main();
diff --git a/etc/config.json b/etc/config.json
deleted file mode 100644
index a9af07d..0000000
--- a/etc/config.json
+++ /dev/null
@@ -1,12 +0,0 @@
-{
-    "interval": 10000,
-    "dbs": [ {
-	"name": "primary",
-	"url": "postgres://postgres@10.99.99.16:5432/moray"
-    } ],
-    "target": {
-	"ip": "0.0.0.0",
-	"port": 9187,
-	"route": "/metrics"
-    }
-}
diff --git a/etc/static.json b/etc/static.json
new file mode 100644
index 0000000..a93b68f
--- /dev/null
+++ b/etc/static.json
@@ -0,0 +1,22 @@
+{
+    "interval": 10000,
+    "connections": {
+        "query_timeout": 1000,
+        "connect_timeout": 3000,
+        "connect_retries": 3,
+        "max_connections": 100
+    },
+    "static": {
+        "dbs": [{
+            "name": "primary",
+            "ip": "10.99.99.16"
+        }],
+        "backend_port": 5432,
+        "user": "moray"
+    },
+    "target": {
+        "ip": "0.0.0.0",
+        "port": 9187,
+        "route": "/metrics"
+    }
+}
diff --git a/etc/vmapi.json b/etc/vmapi.json
new file mode 100644
index 0000000..1115d9b
--- /dev/null
+++ b/etc/vmapi.json
@@ -0,0 +1,25 @@
+{
+    "interval": 10000,
+    "connections": {
+        "query_timeout": 1000,
+        "connect_timeout": 3000,
+        "connect_retries": 3,
+        "max_connections": 100
+    },
+    "vmapi": {
+        "url": "http://vmapi.coal-1.example.com",
+        "pollInterval": 600000,
+        "tags": {
+            "vm_tag_name": "manta_role",
+            "vm_tag_value": "postgres",
+            "nic_tag_name": "manta"
+        },
+        "backend_port": 5432,
+        "user": "moray"
+    },
+    "target": {
+        "ip": "0.0.0.0",
+        "port": 9187,
+        "route": "/metrics"
+    }
+}
diff --git a/lib/dtrace.js b/lib/dtrace.js
new file mode 100644
index 0000000..e4145cc
--- /dev/null
+++ b/lib/dtrace.js
@@ -0,0 +1,71 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ *
+ * Copyright (c) 2018, Joyent, Inc.
+ */
+
+/*
+ * dtrace.js: DTrace probe definitions.
+ */
+
+var mod_dtrace_provider = require('dtrace-provider');
+
+var PROBES = {
+	/* Postgres client probes */
+	/* sql, backend name */
+	'query-start': ['char *', 'char *'],
+
+	/* sql, backend name */
+	'query-row': ['char *', 'json', 'char *'],
+
+	/* sql, error message, backend name */
+	'query-error': ['char *', 'char *', 'char *'],
+
+	/* sql, backend name */
+	'query-timeout': ['char *', 'char *'],
+
+	/* sql, end data, backend name */
+	'query-done': ['char *', 'json', 'char *'],
+
+	/* pgstatsmon probes */
+	/* no arguments */
+	'tick': [],
+
+	/* backend url */
+	'collect-start': ['char *'],
+
+	/* sql, backend name */
+	'collect-query-start': ['char *', 'char *'],
+
+	/* sql, backend name */
+	'collect-query-done': ['char *', 'char *'],
+
+	/* metric, value */
+	'emit-counter': ['json', 'int'],
+
+	/* metric, value */
+	'emit-gauge': ['json', 'int'],
+
+	/* metric, value */
+	'emit-timer': ['json', 'int']
+
+};
+var PROVIDER;
+
+module.exports = function exportStaticProvider() {
+	if (!PROVIDER) {
+		PROVIDER = mod_dtrace_provider.createDTraceProvider(
+		    'pgstatsmon');
+
+		Object.keys(PROBES).forEach(function (p) {
+			var args = PROBES[p].splice(0);
+			args.unshift(p);
+
+			PROVIDER.addProbe.apply(PROVIDER, args);
+		});
+		PROVIDER.enable();
+	}
+	return (PROVIDER);
+}();
diff --git a/lib/pgclient.js b/lib/pgclient.js
new file mode 100644
index 0000000..12925a1
--- /dev/null
+++ b/lib/pgclient.js
@@ -0,0 +1,245 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ *
+ * Copyright (c) 2018, Joyent, Inc.
+ */
+
+/*
+ * pgclient.js: a Postgres client wrapper intended to be used with node-cueball.
+ */
+
+var mod_cueball = require('cueball');
+var mod_dtrace = require('dtrace-provider');
+var mod_pg = require('pg');
+var mod_assertplus = require('assert-plus');
+var mod_util = require('util');
+var EventEmitter = require('events').EventEmitter;
+var WError = require('verror').WError;
+
+var dtrace = require('./dtrace');
+
+function QueryTimeoutError(cause, sql) {
+	WError.call(this, cause, 'query timeout: %s', sql);
+	this.name = 'QueryTimeoutError';
+}
+mod_util.inherits(QueryTimeoutError, WError);
+
+function pgCreate(opts) {
+	var queryTimeout = opts.queryTimeout;
+	var user = opts.user;
+	var log = opts.log;
+
+	function _pgCreate(backend) {
+		mod_assertplus.object(backend, 'backend');
+		mod_assertplus.string(backend.name, 'backend.name');
+		mod_assertplus.string(backend.address, 'backend.address');
+		mod_assertplus.number(backend.port, 'backend.port');
+
+		/* construct the connection url */
+		var url = mod_util.format('postgres://%s@%s:%d/%s',
+		    user, backend.address, backend.port, user);
+
+		return (new PGClient({
+			'url': url,
+			'name': backend.name,
+			'queryTimeout': queryTimeout,
+			'log': log
+		}));
+	}
+	return (_pgCreate);
+}
+
+function PGClient(options) {
+	mod_assertplus.object(options, 'options');
+	mod_assertplus.string(options.url, 'options.url');
+	mod_assertplus.string(options.name, 'options.name');
+	mod_assertplus.object(options.log, 'options.log');
+	mod_assertplus.number(options.queryTimeout, 'options.queryTimeout');
+
+	var self = this;
+	this.client = new mod_pg.Client({
+		connectionString: options.url,
+		keepAlive: true
+	});
+	this.client.on('error', this._handleClientError.bind(this));
+
+	this.client.connect();
+	this.client.on('connect', function () {
+		self.log.info({
+			'backend': options.name
+		}, 'connected');
+		self.emit('connect');
+	});
+
+	this.url = options.url;
+	this.user = options.user;
+	this.name = options.name;
+
+	this.queryTimeout = options.queryTimeout;
+	this.client_had_err = null;
+	this.destroyed = false;
+
+	this.log = options.log.child({
+		component: 'PGClient'
+	}, true);
+
+	EventEmitter.call(this);
+}
+mod_util.inherits(PGClient, EventEmitter);
+
+PGClient.prototype.getBackend = function () {
+	return (this.url);
+};
+
+PGClient.prototype.getName = function () {
+	return (this.name);
+};
+
+PGClient.prototype.isDestroyed = function () {
+	return (this.destroyed);
+};
+
+/*
+ * The underlying Postgres will emit errors when it has a connection
+ * problem. This can fire multiple times: once when the connection goes
+ * away, and again if we try to make a query using this client. When
+ * this happens, we mark this client as having failed so that the pool
+ * will remove us once we're released.
+ */
+PGClient.prototype._handleClientError = function (err) {
+	this.log.error({
+		err: err
+	}, 'pg: client emitted an error');
+
+	this.client_had_err = err;
+};
+
+PGClient.prototype.query = function clientQuery(sql, args) {
+	mod_assertplus.string(sql, 'sql');
+	mod_assertplus.optionalArray(args, 'args');
+
+	/* Clean up whitespace so queries are normalized to DTrace */
+	sql = sql.replace(/(\r\n|\n|\r)/gm, '').replace(/\s+/, ' ');
+
+	var log = this.log;
+	var req;
+	var res = new EventEmitter();
+	var self = this;
+	var timer;
+
+	var aborted = false;
+
+	function done(event, arg) {
+		if (aborted) {
+			return;
+		}
+
+		res.emit(event, arg);
+		clearTimeout(timer);
+	}
+
+	req = new mod_pg.Query(sql, args);
+
+	req.on('row', function onRow(row) {
+		dtrace['query-row'].fire(function () {
+			return ([sql, row, self.url]);
+		});
+
+		log.debug({
+			row: row
+		}, 'query: row');
+
+		if (aborted) {
+			return;
+		}
+
+		clearTimeout(timer);
+		res.emit('row', row);
+	});
+
+	req.on('end', function onQueryEnd(arg) {
+		dtrace['query-done'].fire(function () {
+			return ([sql, arg, self.url]);
+		});
+
+
+		log.debug({
+			res: arg
+		}, 'query: done');
+
+		done('end', arg);
+	});
+
+	req.on('error', function onQueryError(err) {
+		dtrace['query-error'].fire(function () {
+			return ([sql, err.toString(), self.url]);
+		});
+
+
+		log.debug({
+			err: err
+		}, 'query: failed');
+
+		/*
+		 * node-postgres's client.query() will fire "error"
+		 * synchronously, resulting in this handler firing in the same
+		 * tick as the client.query() call. Since the PGClient.query()
+		 * caller won't have had an opportunity to set up their own
+		 * "error" listener, we delay firing the event until the next
+		 * tick.
+		 */
+		setImmediate(done, 'error', err);
+	});
+
+	if (this.queryTimeout > 0) {
+		timer = setTimeout(function onRowTimeout() {
+			var err = new QueryTimeoutError(sql);
+			self.client_had_err = err;
+			dtrace['query-timeout'].fire(function () {
+				return ([sql, self.url]);
+			});
+
+			/*
+			 * We're timing out the query, but
+			 * the Postgres query is still running. It may
+			 * still return rows, return a SQL error, or end due
+			 * to connection problems. We don't emit anything
+			 * after this point, since we've already emitted an
+			 * "error" and will have replied to the client. We
+			 * do continue logging and firing DTrace probes for
+			 * anyone who's observing the process, though.
+			 */
+			aborted = true;
+
+			res.emit('error', err);
+		}, this.queryTimeout);
+	}
+
+	this.client.query(req);
+
+	dtrace['query-start'].fire(function () {
+		return ([sql, self.url]);
+	});
+
+	log.debug({
+		sql: sql,
+		args: args
+	}, 'pg.query: started');
+
+	return (res);
+};
+
+PGClient.prototype.destroy = function closePGClient() {
+	var self = this;
+
+	this.destroyed = true;
+	self.client.end(function () {
+		self.emit('close');
+	});
+};
+
+module.exports = {
+	pgCreate: pgCreate
+};
diff --git a/lib/pgstatsmon.js b/lib/pgstatsmon.js
index 0550866..16416df 100644
--- a/lib/pgstatsmon.js
+++ b/lib/pgstatsmon.js
@@ -13,13 +13,20 @@
 
 var mod_artedi = require('artedi');
 var mod_assertplus = require('assert-plus');
+var mod_cueball = require('cueball');
 var mod_jsprim = require('jsprim');
 var mod_pg = require('pg');
 var mod_restify = require('restify');
 var mod_util = require('util');
+var mod_url = require('url');
 var mod_vasync = require('vasync');
+var mod_verror = require('verror');
+var mod_vmapi_resolver = require('vmapi-resolver');
+
+var mod_pgclient = require('./pgclient');
 
 var queries = require('./queries');
+var dtrace = require('./dtrace');
 
 /* Public interface */
 module.exports = pgstatsmon;
@@ -53,27 +60,61 @@ module.exports = pgstatsmon;
  */
 function pgstatsmon(config)
 {
+	/* XXX use a JSON schema validator? */
 	mod_assertplus.number(config['interval'], 'config.interval');
-	mod_assertplus.arrayOfObject(config['dbs'], 'config.dbs');
+	mod_assertplus.object(config['connections'], 'config.connections');
+	mod_assertplus.number(config['connections']['query_timeout'],
+	    'config.connections.query_timeout');
+	mod_assertplus.number(config['connections']['connect_timeout'],
+	    'config.connections.connect_timeout');
+	mod_assertplus.number(config['connections']['connect_retries'],
+	    'config.connections.connect_retries');
+	mod_assertplus.number(config['connections']['max_connections'],
+	    'config.connections.max_connections');
 	mod_assertplus.object(config['target'], 'config.target');
 	mod_assertplus.object(config['log'], 'config.log');
 
-	config['dbs'].forEach(function (dbconf, pi) {
-		mod_assertplus.string(dbconf['name'],
-		    'config.dbs[' + pi + '].name');
-		mod_assertplus.string(dbconf['url'],
-		    'config.dbs[' + pi + '].url');
-	});
+	if (config['vmapi']) {
+		/* default to using VMAPI to discover backends */
+		var vmapi = config['vmapi'];
+		mod_assertplus.object(vmapi, 'config.vmapi');
+		mod_assertplus.string(vmapi['url'], 'config.vmapi.url');
+		mod_assertplus.number(vmapi['pollInterval'],
+		    'config.vmapi.pollInterval');
+		mod_assertplus.number(vmapi['backend_port'],
+		    'config.vmapi.backend_port');
+		mod_assertplus.string(vmapi['user'], 'config.vmapi.user');
+		mod_assertplus.object(vmapi['tags'], 'config.vmapi.tags');
+		mod_assertplus.string(vmapi['tags']['vm_tag_name'],
+		    'config.vmapi.tags.vm_tag_name');
+		mod_assertplus.string(vmapi['tags']['vm_tag_value'],
+		    'config.vmapi.tags.vm_tag_value');
+		mod_assertplus.string(vmapi['tags']['nic_tag_name'],
+		    'config.vmapi.tags.nic_tag_name');
+	} else {
+		/* use static backends if not using VMAPI */
+		var static_conf = config['static'];
+		mod_assertplus.object(static_conf, 'config.static');
+		mod_assertplus.arrayOfObject(static_conf['dbs'],
+		    'config.static.dbs');
+		static_conf['dbs'].forEach(function (dbconf, pi) {
+			mod_assertplus.string(dbconf['name'],
+			    'config.static.dbs[' + pi + '].name');
+			mod_assertplus.string(dbconf['ip'],
+			    'config.static.dbs[' + pi + '].ip');
+		});
+		mod_assertplus.number(static_conf['backend_port'],
+		    'config.static.backend_port');
+		mod_assertplus.string(static_conf['user'],
+		    'config.static.user');
+	}
 
 	var target = config['target'];
 	mod_assertplus.string(target['ip'], 'config.target.ip');
 	mod_assertplus.number(target['port'], 'config.target.port');
 	mod_assertplus.string(target['route'], 'config.target.route');
 
-	mod_pg.defaults.parseInt8 = true; /* parse int8 into a numeric value */
-	var mon = new PgMon(config);
-	mon.start();
-	return (mon);
+	return (new PgMon(config));
 }
 
 /*
@@ -83,18 +124,53 @@ function pgstatsmon(config)
  */
 function PgMon(config)
 {
-	var log = config['log'];
+	mod_pg.defaults.parseInt8 = true; /* parse int8 into a numeric value */
 
 	/* Save log and configuration */
-	this.pm_log = log;
-	this.pm_dbs = mod_jsprim.deepCopy(config['dbs']);
+	this.pm_log = config['log'];
 	this.pm_targetconf = mod_jsprim.deepCopy(config['target']);
 	this.pm_interval = config['interval'];
 	this.pm_targets = [];
-	this.pm_prometheus_target = null;
+	this.pm_prom_target = null;
+
+	this.pm_max_connections = config['connections']['max_connections'];
+	this.pm_query_timeout = config['connections']['query_timeout'];
+	this.pm_connect_timeout = config['connections']['connect_timeout'];
+	this.pm_connect_retries = config['connections']['connect_retries'];
+
+	if (config['vmapi']) {
+		/* VMAPI Resolver configuration */
+		this.pm_vmapi = config['vmapi'];
+		this.pm_vmapi.log = this.pm_log.child({
+			'component': 'VMResolver'
+		});
+		this.pm_dbuser = config['vmapi']['user'];
+	} else {
+		/* list of static backends */
+		this.pm_static = config['static'];
+		this.pm_dbuser = this.pm_static['user'];
+
+		var port = this.pm_static['backend_port'];
+		var backends = [];
+		this.pm_static['dbs'].forEach(function (db) {
+			backends.push({
+				'address': db.ip,
+				'port': port,
+				'name': db.name
+			});
+		});
+		this.pm_backend_list = backends;
+		this.pm_log.info(this.pm_backend_list, 'static backends');
+	}
+
+	this.pm_client_constructor = mod_pgclient.pgCreate({
+		'queryTimeout': this.pm_query_timeout,
+		'user': this.pm_dbuser,
+		'log': this.pm_log
+	});
 
 	/* interval returned from setInterval */
-	this.pm_intervalObj = -1;
+	this.pm_intervalObj = null;
 
 	/* queries to run */
 	this.pm_queries = [];
@@ -102,9 +178,12 @@ function PgMon(config)
 	this.pm_state =	[];
 	/* last-seen datapoints for each instance */
 	this.pm_data = [];
+	/* postgres instance data */
+	this.pm_pgs = [];
+	/* cueball connection pools for each instance */
+	this.pm_pools = [];
+
 
-	/* postgres client objects */
-	this.pm_pgs = new Array(this.pm_dbs.length);
 	this.initializeMetrics(queries.getQueries());
 }
 
@@ -152,16 +231,6 @@ PgMon.prototype.initializeMetrics = function (query_list)
 	this.pm_queries = query_list.map(
 	    function (q) { return (new Query(q, mon.pm_log)); });
 
-	this.pm_state = this.pm_dbs.map(function () {
-		/* JSSTYLED */
-		return (mon.pm_queries.map(function () { return (null); }));
-	});
-
-	this.pm_data = this.pm_dbs.map(function () {
-		/* JSSTYLED */
-		return (mon.pm_queries.map(function () { return ({}); }));
-	});
-
 	/* Prometheus target */
 	this.pm_targets.push(mon.createTarget(this.pm_targetconf));
 
@@ -169,54 +238,260 @@ PgMon.prototype.initializeMetrics = function (query_list)
 	this.pm_targets.push(new LogTarget(this.pm_log));
 };
 
-PgMon.prototype.start = function ()
+PgMon.prototype.connect = function ()
 {
 	var mon = this;
 	var log = this.pm_log;
-	var barrier = mod_vasync.barrier();
+	var resolver, pool;
+
+	if (this.pm_vmapi) {
+		/* default to the VMAPI resolver */
+		mon.pm_resolver =
+			new mod_vmapi_resolver.VmapiResolver(this.pm_vmapi);
+	} else {
+		/* use the cueball static IP resolver */
+		mon.pm_resolver = new mod_cueball.StaticIpResolver({
+			'backends': mon.pm_backend_list
+		});
+	}
 
-	log.info('starting service');
+	/*
+	 * We'd like to maintain exactly one connection to each backend.
+	 * Cueball provides us with nice connection management, but we don't
+	 * need or want more than one connection per backend Postgres. We'll
+	 * create one pool per backend Postgres instance.
+	 */
+	mon.pm_resolver.on('added', function (key, backend) {
+		log.info('backend discovered', backend);
+		/*
+		 * Start a staticIpResolver for each backend.
+		 */
+		resolver = new mod_cueball.StaticIpResolver({
+			'backends': [ {
+				'address': backend.address,
+				'port': backend.port
+			} ]
+		});
+		resolver.start();
+
+		pool = new mod_cueball.ConnectionPool({
+			'constructor': mon.pm_client_constructor,
+			'domain': 'unused.domain',
+			'recovery': {
+				'default': {
+					'timeout': 2000,
+					'maxTimeout': 2000,
+					'retries': 3,
+					'delay': 1000,
+					'maxDelay': 3000
+				}
+			},
+			'spares': 1,
+			'maximum': 1,
+			'log': log.child({'component': 'cueball'}),
+			'resolver': resolver
+		});
 
-	this.pm_dbs.forEach(function (pgconf, pi) {
-		var client;
+		mon.pm_pools.push({
+			'key': key,
+			'pool': pool,
+			'resolver': resolver
+		});
+		mon.add_connection_data({
+			'key': key,
+			'name': backend.name
+		});
+	});
 
-		barrier.start(pgconf['url']);
-		client = mon.pm_pgs[pi] = new mod_pg.Client(pgconf['url']);
-		client.connect(function (err) {
-			if (err) {
-				log.error(err, pgconf, 'failed to connect');
-				barrier.done(pgconf['url']);
-				return;
+	/*
+	 * When a backend is removed:
+	 *  - Find the backend in the list of PG instances
+	 *  - Stop the cueball connection pool, which destroys all connections
+	 *  - Remove the data associated with the backend:
+	 *    - Remove entry from the PG instance list
+	 *    - Remove the data from the previous queries
+	 *    - Remove the state value
+	 */
+	mon.pm_resolver.on('removed', function (key) {
+		mon.pm_pgs.forEarch(function (pg, pi) {
+			if (pg.key === key) {
+				/*
+				 * Stop the pool and it's staticIpResolver.
+				 * This also destroys all open connections.
+				 */
+				pg.pool.stop();
+
+				/* remove this connection's data */
+				mon.wait_and_remove(key);
 			}
-
-			log.info(pgconf, 'connected');
-			barrier.done(pgconf['url']);
 		});
+
 	});
 
-	barrier.on('drain', function () {
-		log.info('all clients connected');
+	mon.pm_resolver.start();
+};
 
-		mon.tick();
-		mon.pm_intervalObj = setInterval(function () {
-			mon.tick();
-		}, mon.pm_interval);
 
-		mon.pm_targets.forEach(function (target) {
-			target.start();
+/*
+ * Wait for queries to finish, then remove connection data.
+ * 'key' is used to find the PG instance in the pm_pgs structure
+ */
+PgMon.prototype.wait_and_remove = function (key)
+{
+	var mon = this;
+	var log = mon.pm_log;
+	var interval;
+	var tries = 0;
+	var max_tries = 3;
+
+	/*
+	 * Wait up to (max_tries * query_timeout) milliseconds for
+	 * connections to stop being used, otherwise forcibly remove the
+	 * connection data.
+	 */
+	interval = setInterval(function () {
+		var still_running = false;
+		var pi = -1;
+		tries++;
+
+		/* find the PG instance in question */
+		mon.pm_pgs.forEach(function (pg, ind) {
+			if (pg.key === key) {
+				pi = ind;
+			}
+		});
+
+		if (pi === -1) {
+			/* key not found, data must be deleted already */
+			clearInterval(interval);
+			return;
+		}
+
+		/*
+		 * Iterate through the query state array. If any query
+		 * has state (a timestamp), the query is still running.
+		 */
+		mon.pm_state[pi].forEach(function (st) {
+			if (st) {
+				still_running = true;
+			}
 		});
+
+		/* safely delete the data for this backend */
+		if (!still_running) {
+			clearInterval(interval);
+			mon.remove_connection_data(pi);
+		}
+
+		/*
+		 * For some reason the queries are still running and
+		 * the query timeout mechanism isn't working. Forcibly
+		 * remove the connection.
+		 *
+		 * This is relatively dangerous, but query timeouts
+		 * should stop this from being reached.
+		 */
+		if (still_running && tries >= max_tries) {
+			clearInterval(interval);
+			log.warn('waited ~%d ms for queries to stop, moving on',
+			    (tries+1) * mon.pm_query_timeout);
+			mon.remove_connection_data(pi);
+		}
+	}, mon.pm_query_timeout);
+};
+
+/* remove data associated with a given PG instance */
+PgMon.prototype.remove_connection_data = function (pi)
+{
+	var mon = this;
+	var backend = mon.pm_pgs[pi]['name'];
+
+	mon.pm_pools.splice(pi, 1);
+	mon.pm_pgs.splice(pi, 1);
+	mon.pm_state.splice(pi, 1);
+	mon.pm_data.splice(pi, 1);
+
+	mon.pm_log.info({
+		'backend': backend
+	}, 'removed connection data');
+};
+
+/* add data associated with a given PG instance */
+PgMon.prototype.add_connection_data = function (backend)
+{
+	var mon = this;
+	/*
+	 * When a backend is added:
+	 *  - Add it to the list of PG instances
+	 *  - Add an object to keep track of the observed data points
+	 *  - Add a 'state' array to identify in-flight queries
+	 */
+	mon.pm_pgs.push({
+		'key': backend.key,
+		'name': backend.name,
+		'conn': null,
+		'handle': null
+	});
+	mon.pm_data.push(mon.pm_queries.map(function () {
+		return ({});
+	}));
+	mon.pm_state.push(mon.pm_queries.map(function () {
+		return (null);
+	}));
+};
+
+
+/*
+ * Start pgstatsmon. The caller can optionally provde a callback to be notified
+ * when it is safe to start using pgstatsmon.
+ */
+PgMon.prototype.start = function (callback)
+{
+	var mon = this;
+
+	this.pm_log.info('starting service');
+	/*
+	 * connect to backends, run initial tick, set up tick interval and
+	 * start metric targets
+	 */
+	mod_vasync.pipeline({
+		'funcs': [
+			function (_, cb) { mon.connect(); mon.tick(cb); },
+			function (_, cb) {
+				mon.pm_intervalObj = setInterval(function () {
+					mon.tick();
+				}, mon.pm_interval);
+				mon.pm_targets.forEach(function (target) {
+					target.start();
+				});
+				cb();
+			}
+		]
+	}, function (err, result) {
+		if (callback) {
+			callback(err);
+		}
 	});
 };
 
+/*
+ * Stop pgstatsmon. This stops the tick interval, stops all metric targets,
+ * and closes all of the database connections.
+ */
 PgMon.prototype.stop = function ()
 {
+	var mon = this;
 	clearInterval(this.pm_intervalObj);
-	this.pm_targets.forEach(function (target) {
+	mon.pm_targets.forEach(function (target) {
 		target.stop();
 	});
-	this.pm_pgs.forEach(function (db) {
-		db.end(); /* close the db connections */
+	mon.pm_pools.forEach(function (pool_obj, ind) {
+		/* this stops both the pool and the resolver */
+		pool_obj['pool'].stop();
+		/* remove all of the data for this instance */
+		mon.remove_connection_data(ind);
 	});
+	mon.pm_resolver.stop();
 };
 
 /*
@@ -234,35 +509,118 @@ PgMon.prototype.tick = function (callback)
 	var mon = this;
 	var queue;
 
-	queue = mod_vasync.queue(function (pi, cb) {
+	dtrace['tick'].fire(function () { return ([]); });
+
+	queue = mod_vasync.queue(function enqueue_queries(pi, cb) {
 		/* kick off the Postgres queries for this instance */
+		var errors = [];
+		var error;
+		var errmetric;
+		var pool = mon.pm_pools[pi]['pool'];
 		var barrier = mod_vasync.barrier();
+		var backend = mon.pm_pgs[pi]['name'];
+
 		barrier.start('enqueue queries');
-		mon.pm_queries.forEach(function (query, qi) {
-			/*
-			 * barrier string looks like:
-			 *  'url [pg_index, query_index]'
-			 */
-			var query_id = mod_util.format('%s [%d, %d]',
-			    mon.pm_dbs[pi], pi, qi);
-			barrier.start(query_id);
-			mon.tickPgQuery(pi, qi, function () {
-				barrier.done(query_id);
+
+		pool.claim({
+			'timeout': mon.pm_connect_timeout
+		}, function (err, handle, conn) {
+
+			if (err) {
+				mon.pm_log.error(err, 'could not connect');
+				errors.push(err);
+				barrier.done('enqueue queries');
+				return;
+			}
+			mon.pm_pgs[pi]['handle'] = handle;
+			mon.pm_pgs[pi]['conn'] = conn;
+			backend = mon.pm_pgs[pi]['name'];
+
+			mon.pm_queries.forEach(function
+			    kick_off_queries(query, qi) {
+
+				/*
+				 * barrier string looks like:
+				 *  'backend [pg_index, query_index]'
+				 */
+				var query_id = mod_util.format('%s [%d, %d]',
+				    backend, pi, qi);
+				barrier.start(query_id);
+				dtrace['collect-query-start'].fire(function () {
+					return ([mon.pm_queries[qi], backend]);
+				});
+
+				mon.tickPgQuery(pi, qi, function (err2) {
+					dtrace['collect-query-done'].fire(
+					    function () {
+
+						return ([mon.pm_queries[qi],
+						    backend]);
+					});
+					if (err2) {
+						errors.push(err2);
+					}
+					barrier.done(query_id);
+				});
 			});
+			barrier.done('enqueue queries');
+		});
+
+		barrier.on('drain', function barrier_drain() {
+			error = mod_verror.errorFromList(errors);
+			if (error && mod_verror.hasCauseWithName(error,
+			    'QueryTimeoutError')) {
+
+				mon.pm_log.info({
+					'error': error.message,
+					'backend': backend
+				}, 'query timeout, destroying connection');
+
+				mon.pm_pgs[pi]['handle'].close();
+			} else if (error && (mod_verror.hasCauseWithName(error,
+			    'PoolFailedError') ||
+			    mod_verror.hasCauseWithName(error,
+			    'ClaimTimeoutError'))) {
+
+				/* no valid handle or connection */
+				mon.pm_log.warn({
+					'error': error.message,
+					'backend': backend
+				}, 'error connecting to backend');
+
+				/* make sure we report the connection error */
+				errmetric = {
+					'name': 'pg_connect_error',
+					'help': 'PG connection failed',
+					'metadata': {
+						'backend': backend
+					}
+				};
+				mon.emitCounter(errmetric, 1);
+
+			} else {
+				/* done with the connection */
+				mon.pm_pgs[pi]['handle'].release();
+			}
+			cb(error);
 		});
-		barrier.done('enqueue queries');
-		barrier.on('drain', cb);
 	}, 10); /* 10 Postgres instances outstanding */
 
 	/* enqueue all of the Postgres instances */
 	this.pm_pgs.forEach(function (pg, pi) {
+		dtrace['collect-start'].fire(function () {
+			return ([pg['conn'].getBackend()]);
+		});
 		queue.push(pi);
 	});
 
 	queue.close();
-	queue.on('end', function () {
+	queue.on('end', function (err) {
+		if (err) {
+			mon.pm_log.error(err, 'error running queue');
+		}
 		if (callback) {
-			setImmediate(callback);
+			setImmediate(callback, err);
 		}
 	});
 };
@@ -271,12 +629,15 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 {
 	var mon = this;
 	var log = this.pm_log;
-	var url = this.pm_dbs[pi];
-	var client = this.pm_pgs[pi];
+	var client = this.pm_pgs[pi]['conn'];
+	var backend = mon.pm_pgs[pi]['name']; /* for logging */
 	var query = this.pm_queries[qi];
 	var state = this.pm_state[pi][qi];
 	var time;
 	var timer, errmetric;
+	var aborted = false;
+	var rows = [];
+	var res;
 
 	/*
 	 * If the last check is still running, either the interval is configured
@@ -288,7 +649,7 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 	 */
 	if (state !== null) {
 		log.error({
-		    'url': url,
+		    'backend': mon.pm_pgs[pi]['name'],
 		    'query': query.q_name,
 		    'last': state
 		}, 'skipping check (still pending)');
@@ -299,44 +660,82 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 	time = process.hrtime();
 	this.pm_state[pi][qi] = new Date().toISOString();
 	log.debug({
-	    'url': url,
+	    'backend': backend,
 	    'query': query.q_name
 	}, 'check: start');
-	client.query(query.q_sql, function (err, result) {
+
+	res = client.query(query.q_sql);
+	res.on('row', function on_query_row(row) {
+		if (aborted) {
+			log.warn({
+				'backend': backend,
+				'query': query.q_name
+			}, 'query was aborted');
+			return;
+		}
 		log.debug({
-		    'url': url,
+		    'backend': backend,
 		    'query': query.q_name
 		}, 'check: done');
-		mon.pm_state[pi][qi] = null;
-		time = process.hrtime(time);
 
-		/*
-		 * If we see an error running the query, create a metric for
-		 * the query we were running
-		 */
-		if (err) {
+		rows.push(row);
+	});
+
+	res.once('error', function on_query_error(err) {
+		mon.pm_state[pi][qi] = null;
+		aborted = true;
+		if (mod_verror.hasCauseWithName(err, 'QueryTimeoutError')) {
 			errmetric = {
-				'name': 'pg_query_error',
-				'help': 'error performing PG query',
+				'name': 'pg_query_timeout',
+				'help': 'PG query timed out',
 				'metadata': {
+					'backend': backend,
 					'query': query.q_name
 				}
 			};
-			log.error({
-			    'url': url,
-			    'query': query.q_name
-			}, 'query failed');
 			mon.emitCounter(errmetric, 1);
-			setImmediate(cb);
+			setImmediate(cb, err);
 			return;
 		}
+		/*
+		 * If we see an error running the query, create a metric
+		 * for the query we were running
+		 */
+		errmetric = {
+			'name': 'pg_query_error',
+			'help': 'error performing PG query',
+			'metadata': {
+				'backend': backend,
+				'query': query.q_name
+			}
+		};
+		log.error(err, {
+		    'backend': backend,
+		    'query': query.q_name
+		}, 'query failed');
+		mon.emitCounter(errmetric, 1);
+		setImmediate(cb, err);
+		return;
+	});
 
+	res.once('end', function on_query_end() {
+		res.removeAllListeners();
+		if (client.isDestroyed()) {
+			mon.pm_log.info({
+				'backend': backend
+			}, 'client removed during query');
+			setImmediate(cb);
+			return;
+		}
+		time = process.hrtime(time);
 		/*
 		 * Record the datapoint, which will emit several counter
 		 * stats, and then emit a separate timer stat for the
 		 * query itself.
 		 */
-		mon.record(pi, qi, result);
+		if (rows.length > 0) {
+			mon.record(pi, qi, rows);
+		}
 		timer = {
 			'attr': 'querytime',
 			'help': 'time to run stat query',
@@ -344,6 +743,7 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 		};
 		mon.emitTimer(mon.qstatname(pi, qi, null, timer),
 		    mod_jsprim.hrtimeMillisec(time));
+		mon.pm_state[pi][qi] = null;
 		setImmediate(cb);
 	});
 };
@@ -356,32 +756,69 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 PgMon.prototype.record = function (pi, qi, datum)
 {
 	var mon = this;
+	var backend = this.pm_pgs[pi]['name'];
 	var query = this.pm_queries[qi];
-	var url = this.pm_dbs[pi]['url'];
 	var oldresult, oldrow;
+	var reset_time;
+	var last_reset_time;
+	var new_value, old_value;
 
 	oldresult = this.pm_data[pi][qi];
 	this.pm_data[pi][qi] = {};
-	datum['rows'].forEach(function (row) {
+	datum.forEach(function record_metrics_for_row(row) {
 		var key = row[query.q_statkey];
 		mon.pm_data[pi][qi][key] = row;
 		oldrow = oldresult[key];
 
+		if (row['stats_reset'] && oldrow) {
+			/*
+			 * Try to detect a stat reset. Some relations reset
+			 * stats when the PG instance restarts, and then record
+			 * the reset time in the 'stats_reset' attribute. Other
+			 * relations (pg_stat_user_tables in particular) reset
+			 * stats when the PG instance restarts, but don't
+			 * include a 'stats_reset' attribute.
+			 */
+			reset_time = Date.parse(row['stats_reset']);
+			last_reset_time = Date.parse(oldrow['stats_reset']);
+			if (reset_time > last_reset_time) {
+				mon.pm_log.info({
+					'backend': backend,
+					'query': query.q_name,
+					'stats_reset': row['stats_reset']
+				}, 'stats reset detected');
+				return;
+			}
+		}
+
 		if (!oldrow) {
 			mon.pm_log.info({
-			    'url': url,
+			    'backend': backend,
+			    'query': query.q_name,
 			    'key': key
 			}, 'row detected');
 			return;
 		}
 
-		query.q_counters.forEach(function (c) {
+		query.q_counters.forEach(function emit_counters(c) {
+			new_value = row[c.attr];
+			old_value = oldrow[c.attr];
+			if (old_value > new_value) {
+				/* some relations don't advertise stat resets */
+				mon.pm_log.info({
+					'backend': backend,
+					'key': key,
+					'counter': c
+				}, 'old value greater than new value -'
+				    + ' skipping');
+				return;
+			}
 			mon.emitCounter(
 			    mon.qstatname(pi, qi, row, c),
-			    row[c.attr] - oldrow[c.attr]);
+			    new_value - old_value);
 		});
 
-		query.q_gauges.forEach(function (g) {
+		query.q_gauges.forEach(function emit_gauges(g) {
 			mon.emitGauge(
 			    mon.qstatname(pi, qi, row, g), row[g.attr]);
 		});
@@ -394,7 +831,6 @@ PgMon.prototype.record = function (pi, qi, datum)
  */
 PgMon.prototype.qstatname = function (pi, qi, row, field)
 {
-	var dbname = this.pm_dbs[pi]['name'];
 	var query = this.pm_queries[qi];
 	var fieldname = field.attr;
 	var help = field.help;
@@ -402,7 +838,7 @@ PgMon.prototype.qstatname = function (pi, qi, row, field)
 	var mdvalues = {};
 	var name;
 
-	mdvalues['name'] = dbname;
+	mdvalues['name'] = this.pm_pgs[pi]['name'];
 	if (metadata && row) {
 		metadata.forEach(function (attr) {
 			mdvalues[attr] = row[attr];
@@ -436,6 +872,10 @@ PgMon.prototype.qstatname = function (pi, qi, row, field)
  */
 PgMon.prototype.emitCounter = function (metric, value)
 {
+	dtrace['emit-counter'].fire(function () {
+		return ([metric, value]);
+	});
+
 	/*
 	 * It's possible that the user we're using to connect to the DB doesn't
 	 * have permissions to view certain tables, or we ran a bad query. In
@@ -443,7 +883,9 @@ PgMon.prototype.emitCounter = function (metric, value)
 	 * a warning and increment a separate counter to track this behavior.
 	 */
 	if (value === null) {
-		this.pm_log.warn(metric, 'null value observed');
+		this.pm_log.info({
+			'metric': metric
+		}, 'null value observed');
 		this.pm_targets.forEach(function (t) {
 			t.emitCounter({
 				'name': 'pg_null_value_observed',
@@ -466,8 +908,14 @@ PgMon.prototype.emitCounter = function (metric, value)
  */
 PgMon.prototype.emitGauge = function (metric, value)
 {
+	dtrace['emit-gauge'].fire(function () {
+		return ([metric, value]);
+	});
+
 	if (value === null) {
-		this.pm_log.warn(metric, 'null value observed');
+		this.pm_log.info({
+			'metric': metric
+		}, 'null value observed');
 		this.pm_targets.forEach(function (t) {
 			t.emitCounter({
 				'name': 'pg_null_value_observed',
@@ -488,6 +936,10 @@ PgMon.prototype.emitGauge = function (metric, value)
  */
 PgMon.prototype.emitTimer = function (metric, duration)
 {
+	dtrace['emit-timer'].fire(function () {
+		return ([metric, duration]);
+	});
+
 	this.pm_targets.forEach(function (t) {
 		t.emitTimer(metric, duration);
 	});
diff --git a/lib/queries.js b/lib/queries.js
index 5bb33b5..812b3e3 100644
--- a/lib/queries.js
+++ b/lib/queries.js
@@ -59,6 +59,7 @@ var mod_ajv = require('ajv');
  * The query schema is validated when pgstatsmon starts.
  */
 
+/* XXX did node-postgres change the way it handles newline chars in queries? */
 var queries = [ {
     'name': 'pg_stat_user_tables',
     'sql': 'SELECT * FROM pg_stat_user_tables',
@@ -91,7 +92,7 @@ var queries = [ {
 	'sync_state, ',
 	'pg_xlog_location_diff(sent_location, write_location) AS write_lag, ',
 	'pg_xlog_location_diff(write_location, flush_location) AS flush_lag, ',
-	'pg_xlog_location_diff(flush_location, replay_location) AS replay_lag',
+	'pg_xlog_location_diff(flush_location, replay_location) AS replay_lag ',
 	'FROM pg_stat_replication'
     ].join('\n'),
     'gauges': [
@@ -105,9 +106,9 @@ var queries = [ {
     'metadata': [ 'relname' ],
     'sql': [
 	'SELECT c.oid::regclass AS relname,',
-	'       greatest(age(c.relfrozenxid),age(t.relfrozenxid)) AS freezeage',
-	'FROM pg_class c',
-	'LEFT JOIN pg_class t ON c.reltoastrelid = t.oid',
+	'      greatest(age(c.relfrozenxid),age(t.relfrozenxid)) AS freezeage ',
+	'FROM pg_class c ',
+	'LEFT JOIN pg_class t ON c.reltoastrelid = t.oid ',
 	'WHERE c.relkind = \'r\';'
     ].join('\n'),
     'gauges': [ { 'attr': 'freezeage', 'help': 'xids since last vacuum' } ]
@@ -116,8 +117,8 @@ var queries = [ {
     'statkey': 'datname',
     'metadata': [ 'datname', 'state' ],
     'sql': [
-	'SELECT datname, state, count(*) AS connections',
-	'FROM pg_stat_activity',
+	'SELECT datname, state, count(*) AS connections ',
+	'FROM pg_stat_activity ',
 	'GROUP BY datname, state;'
     ].join('\n'),
     'gauges': [ { 'attr': 'connections', 'help': 'worker process state' } ]
@@ -126,7 +127,7 @@ var queries = [ {
     'statkey': 'datname',
     'metadata': [ 'datname' ],
     'sql': [
-	'SELECT *',
+	'SELECT * ',
 	'FROM pg_stat_database'
     ].join('\n'),
     'gauges': [ { 'attr': 'numbackends', 'help': 'number of connections' } ],
@@ -154,9 +155,9 @@ var queries = [ {
 	'       c.reltuples AS row_estimate,',
 	'       pg_total_relation_size(c.oid) AS total_bytes,',
 	'       pg_indexes_size(c.oid) AS index_bytes,',
-	'       pg_total_relation_size(reltoastrelid) AS toast_bytes',
-	'FROM pg_class c',
-	'LEFT JOIN pg_namespace n ON n.oid = c.relnamespace',
+	'       pg_total_relation_size(reltoastrelid) AS toast_bytes ',
+	'FROM pg_class c ',
+	'LEFT JOIN pg_namespace n ON n.oid = c.relnamespace ',
 	'WHERE relkind = \'r\' AND nspname LIKE \'public\';'
     ].join('\n'),
     'gauges': [
@@ -170,7 +171,7 @@ var queries = [ {
     'statkey': 'bgwriter',
     'metadata': [],
     'sql': [
-	'SELECT *',
+	'SELECT * ',
 	'FROM pg_stat_bgwriter;'
     ].join('\n'),
     'counters': [
diff --git a/package.json b/package.json
index 21b69d2..6adeb89 100644
--- a/package.json
+++ b/package.json
@@ -7,12 +7,16 @@
 	},
 	"dependencies": {
 		"ajv": "5.5.1",
-		"artedi": "1.2.0",
+		"artedi": "1.3.0",
 		"assert-plus": "1.0.0",
+		"cueball": "2.5.1",
+		"dtrace-provider": "0.8.6",
 		"jsprim": "2.0.0",
-		"pg": "6.4.2",
+		"mooremachine": "2.2.0",
+		"pg": "7.4.1",
 		"restify": "6.3.4",
-		"vasync": "2.2.0"
+		"vasync": "2.2.0",
+		"vmapi-resolver": "1.0.0"
 	},
 	"devDependencies": {
 		"bunyan": "1.8.10",
diff --git a/test/badquery.tst.js b/test/badquery.tst.js
index 6b78c23..4eee5e4 100644
--- a/test/badquery.tst.js
+++ b/test/badquery.tst.js
@@ -58,12 +58,19 @@ function BadQuery(callback)
 	mon_args.log = this.log;
 
 	this.mon = helper.getMon(mon_args);
-	this.prom_target = this.mon.getTarget();
 	this.client = helper.createClient();
 
-	this.mon.tick(function () {
-		clearInterval(self.mon.pm_intervalObj);
-		callback();
+	/* make sure we know when it's safe to use pgstatsmon */
+	this.mon.start(function (err) {
+		if (err) {
+			self.log.error(err, 'could not start pgstatsmon');
+			process.exit(1);
+		}
+		self.mon.tick(function () {
+			clearInterval(self.mon.pm_intervalObj);
+			callback();
+		});
+		self.prom_target = self.mon.getTarget();
 	});
 }
 
@@ -96,10 +103,10 @@ BadQuery.prototype.run_invalid_query = function (callback)
 	} ];
 
 	var labels = {
-		'query': queries[0].name
+		'query': queries[0].name,
+		'backend': self.mon.pm_pgs[0]['name']
 	};
 
-
 	this.mon.initializeMetrics(queries);
 	/*
 	 * since mon.initializeMetrics() drops all of the data, we need to get
diff --git a/test/basic.tst.js b/test/basic.tst.js
index b0b180f..9d06c57 100644
--- a/test/basic.tst.js
+++ b/test/basic.tst.js
@@ -69,7 +69,6 @@ function BasicTest(callback)
 
 	this.table_name = 'pgstatsmon_basic';
 	this.mon = helper.getMon(mon_args);
-	this.prom_target = this.mon.getTarget();
 	this.client = helper.createClient();
 
 	mod_vasync.pipeline({
@@ -82,6 +81,9 @@ function BasicTest(callback)
 				helper.createTable(self.table_name, self.client,
 				    cb);
 			},
+			function (_, cb) {
+				self.mon.start(cb);
+			},
 			function (_, cb) {
 				self.mon.tick(cb);
 			}
@@ -92,6 +94,7 @@ function BasicTest(callback)
 			return;
 		}
 		clearInterval(self.mon.pm_intervalObj);
+		self.prom_target = self.mon.getTarget();
 		callback();
 	});
 }
@@ -136,7 +139,7 @@ BasicTest.prototype.check_tuple_count = function (callback)
 	var initial_value;
 	var q;
 	var labels = {
-		'name': this.mon.pm_dbs[0].name,
+		'name': this.mon.pm_pgs[0]['name'],
 		'relname': this.table_name
 	};
 
@@ -213,7 +216,7 @@ BasicTest.prototype.check_connections = function (callback)
 	var initial_value;
 	var mclient;
 	var labels = {
-		'name': this.mon.pm_dbs[0].name,
+		'name': this.mon.pm_pgs[0]['name'],
 		'datname': this.client.database,
 		'state': 'idle'
 	};
diff --git a/test/etc/testconfig.json b/test/etc/testconfig.json
index 661498d..7d5190d 100644
--- a/test/etc/testconfig.json
+++ b/test/etc/testconfig.json
@@ -1,12 +1,22 @@
 {
     "interval": 2000,
-    "dbs": [ {
-	"name": "test",
-	"url": "postgres://pgstatsmon@localhost:5432/pgstatsmon"
-    } ],
+    "connections": {
+        "query_timeout": 1000,
+        "connect_timeout": 3000,
+        "connect_retries": 3,
+        "max_connections": 10
+    },
+    "static": {
+        "dbs": [ {
+            "name": "test",
+            "ip": "127.0.0.1"
+        } ],
+        "user": "pgstatsmon",
+        "backend_port": 5432
+    },
     "target": {
-	"ip": "0.0.0.0",
-	"port": 9187,
-	"route": "/metrics"
+        "ip": "0.0.0.0",
+        "port": 9187,
+        "route": "/metrics"
     }
 }
diff --git a/test/helper.js b/test/helper.js
index b233dcb..e1340c5 100644
--- a/test/helper.js
+++ b/test/helper.js
@@ -10,6 +10,7 @@ var mod_assert = require('assert-plus');
 var mod_bunyan = require('bunyan');
 var mod_fs = require('fs');
 var mod_pg = require('pg');
+var mod_util = require('util');
 var mod_vasync = require('vasync');
 var pgstatsmon = require('../lib/pgstatsmon');
 
@@ -84,7 +85,11 @@ function getMon(args)
  */
 function createClient()
 {
-	var client = new mod_pg.Client(config.dbs[0].url);
+	var conf = config['static'];
+	var url = mod_util.format('postgresql://%s@%s:%d/%s',
+	    conf['user'], conf['dbs'][0]['ip'], conf['backend_port'],
+	    conf['user']);
+	var client = new mod_pg.Client(url);
 	client.connect(function (err) {
 		if (err) {
 			config.log.error(err, config.dbs[0], 'failed to' +
-- 
2.21.0

