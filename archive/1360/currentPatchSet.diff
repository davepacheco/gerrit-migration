commit 643715648dd4153d4bd0a063a11ee75ff836e2e8 (refs/changes/60/1360/3)
Author: David Pacheco <dap@joyent.com>
Date:   2017-01-30T18:13:41-08:00 (2 years, 8 months ago)
    
    MORAY-382 moray repo docs need work
    Reviewed by: Cody Peter Mello <cody.mello@joyent.com>
    Approved by: Robert Mustacchi <rm@joyent.com>

diff --git a/README.md b/README.md
index 310b6e0..c0e7c2c 100644
--- a/README.md
+++ b/README.md
@@ -8,28 +8,109 @@
     Copyright 2017, Joyent, Inc.
 -->
 
-# Moray, the highly-available key/value store
+# Moray, the highly-available key-value store
 
-This repo contains Moray, the highly-available key/value store from Joyent.
-Moray offers a simple put/get/del/search protocol on top of Postgres 9.x,
-using the [node-fast](https://github.com/joyent/node-fast) RPC protocol.
+This repository contains Moray, the highly-available key-value store from
+Joyent.  The Moray service provides a simple put/get/search/delete abstraction
+on top of Postgres 9.x, over plain TCP using
+[node-fast](https://github.com/joyent/node-fast).
 
 This repository is part of the Joyent Manta and Triton projects. For
 contribution guidelines, issues, and general documentation, visit the main
 [Triton](http://github.com/joyent/triton) and
 [Manta](http://github.com/joyent/manta) project pages.
 
-## Development
-
-You'll want a [Manatee](https://github.com/joyent/manatee) instance up and
-running first (which itself requires ZooKeeper),  so the easiest way is to
-point at an existing COAL or Manta standup.  Once you have that, you'll need
-to create a database and the minimal schema necessary to bootstrap moray
-(issue these against whatever DB is currently `primary` in manatee):
-
-    createdb -U postgres -O moray moray
-    psql -U postgres moray
 
+## Introduction
+
+For basic information about how to use Moray and what operations Moray supports,
+see the moray(1) and related manual pages in the [Moray
+client](https://github.com/joyent/node-moray/) repository.
+
+For reference documentation about the RPC calls and the node-moray library calls
+used to invoke them, see the [developer reference](./docs/index.md)
+documentation inside this repository.
+
+The rest of this README describes how Moray is deployed and how to build, run,
+and test Moray.
+
+
+## Overview
+
+Moray implements a JSON key-value interface to a PostgreSQL database.  It serves
+several functions:
+
+* Moray provides a reasonably simple key-value abstraction over PostgreSQL,
+  serving a similar role as an ORM.  _Buckets_ are implemented with tables,
+  _objects_ are implemented as rows, and operations are translated into SQL
+  queries.  The abstractions provided are oriented around very large buckets
+  and attempt to avoid exposing operations that would not scale accordingly.
+* Moray provides pooling of PostgreSQL connections (similar to pgbouncer).
+  Moray clients maintain persistent TCP connections that can be idle for
+  extended periods.  The Moray server multiplexes incoming requests over some
+  fixed number of PostgreSQL connections.
+* In a database cluster deployed using
+  [Manatee](https://github.com/joyent/manatee), Moray is responsible for
+  tracking the cluster state so that queries are always dispatched to the
+  current PostgreSQL primary.
+
+In Triton and Manta, PostgreSQL is typically deployed atop Manatee, which
+provides high availability for PostgreSQL using synchronous replication and
+automated failover.  Multiple Moray instances are typically deployed atop that.
+Here's a diagram of the containers used in a typical Manatee/Moray _shard_:
+
+             +-------+     +-------+     +-------+
+             | moray |     | moray |     | moray | ...          
+             +-+---+-+     +-+---+-+     +-+---+-+              
+               |   |         |   |         |   +--------------------------+
+               |   |         |   +-------- | ---------------------------+ |
+               |   +-------- | ----------- | -------------------------+ | |
+               |             |             |                          | | |
+            PG |+------------+             |                          | | |
+       queries ||+-------------------------+                    +-----+-+-+-+
+               |||                                              |           |
+               |||   +----------------------------------------- + ZooKeeper |
+               |||   |             +--------------------------- + Cluster   |
+               |||   |             |             +------------- +           |
+            +--+++---+-+  +--------+-+  +--------+-+            +-----------+
+            | manatee  |  | manatee  |  | manatee  |  
+            |----------+  |----------+  |----------+ ...
+            | postgres |  | postgres |  | postgres |  
+            | primary  |  | sync     |  | async    |
+            +----------+  +----------+  +----------+  
+
+This works as follows:
+
+* Incoming Moray data requests (e.g., for reading and writing of key-value
+  pairs) are translated into SQL queries against the underlying PostgreSQL
+  database.
+* The Manatee component supervises the underlying PostgreSQL database instances
+  and ensures that only one of them is writable at any given time.  The
+  non-writable instances are used to support rapid failover in the event of
+  failure of the primary instance.  Manatee automatically records the current
+  cluster state (including which peer is the primary) in ZooKeeper.
+* Each Moray instance reads the cluster state from ZooKeeper to make sure that
+  incoming requests are dispatched only to the current PostgreSQL primary.
+
+All state is stored in ZooKeeper and PostgreSQL.  As a result, additional
+Moray instances can be deployed for both horizontal scalability and fault
+tolerance of Moray itself.
+
+
+## Building and running Moray
+
+Be sure to read the Introduction section above that describes how Moray is
+deployed.  It's also assumed that you're familiar with setting up a Triton or
+Manta development zone (container).
+
+To work on Moray, you'll need Manatee and ZooKeeper clusters available.  You can
+set these up on your own, but usually it's easier to configure your development
+Moray instance to use the Manatee and ZooKeeper clusters in an existing Triton
+or Manta deployment.  You can even use the same PostgreSQL database, or you can
+create your own by executing this on the primary Manatee instance:
+
+    # createdb -U postgres -O moray moray
+    # psql -U postgres moray
     moray=# CREATE TABLE buckets_config (
         name text PRIMARY KEY,
         index text NOT NULL,
@@ -41,34 +122,51 @@ to create a database and the minimal schema necessary to bootstrap moray
 
     moray=# alter table buckets_config owner to moray
 
-Note if you want to use a different database name than `moray`, you can, you
-just need to set the environment variable `MORAY_DB_NAME` to whatever you want
-before starting the server.
+Note if you want to use a different database name than `moray`, you can; you
+just need to modify the above commands accordingly and then set the environment
+variable `MORAY_DB_NAME` to the desired database name before starting the
+server.
+
+Once you've got a Manatee and ZooKeeper cluster available and created the
+database, you'll need to create a Moray server configuration file.  Again, you
+can create your own from scratch, but it's usually easier to copy the
+configuration file from `/opt/smartdc/moray/etc/config.json` in a Moray zone
+inside an existing Triton or Manta deployment.  If you want to create your own,
+you can start with the template configuration file in ./sapi\_manifests, but the
+configuration file properties are currently not documented.
+
+Now, in your development zone, build this repository:
+
+    $ make
 
-Once the above is done, edit one of the JSON files in `./etc/` (based on whether
-you're using an SDC or Manatee, and whether COAL or lab) to have the correct
-ZooKeeper endpoint(s) and domain name (note the domain name is the DNS name of
-the manatee to back this Moray instance - that DNS name is "mapped" into
-ZooKeeper). If in doubt, compare against the configuration for the Moray zone
-deployed atop the Manatee you're deploying against. Then, source in ./env.sh
-(this ensures you have the moray node et al) and run:
+Now source ./env.sh so that you've got a PATH that includes the right version of
+Node:
 
-    . ./env.sh
-    server
+    $ source ./env.sh
+
+and run Moray using your configuration file:
+
+    node main.js -f YOUR_CONFIG_FILE -v 2>&1 | bunyan
+
+By default, Moray listens on port 2020.  You can use the CLI tools in
+[node-moray](https://github.com/joyent/node-moray) to start working with the
+server.  Those tools have detailed manual pages.
 
-Which will open up Moray on port 2020.  You can now use the CLI in
-`node-moray.git` or whatever other means you want of talking to the server.
 
 ## Testing
 
 For testing, see the separate
 [moray-test-suite](https://github.com/joyent/moray-test-suite) repository.  You
-will need to supply your own server configuration file.  You should consider
-pointing the config file to a different DB than `moray`. There is a script at
-`tools/coal-test-env.sh` which will create a `moray_test` DB for you and run an
-additional `moray-test` instance listening at port `2222`. Just scping into GZ
-and executing it should work.  You will need to configure the test suite
-appropriately (see the README.md in the moray-test-suite repository).
+will need to build and run a Moray instance as described above, and then follow
+the instructions in that repository to test it.
+
+You should consider pointing your testing instance at a different DB than
+`moray` to avoid interfering with operations in your Triton or Manta deployment.
+There is an unsupported script in `tools/coal-test-env.sh` that will create a
+`moray_test` DB for you and run an additional `moray-test` instance listening at
+port `2222`.  Just scp'ing it into the global zone and executing it should work.
+You will need to configure the test suite appropriately (see the README.md in
+the moray-test-suite repository).
 
 ## Running in standalone mode
 
@@ -97,4 +195,4 @@ server. The server will emit a `ready` event once it's started up.
 This Source Code Form is subject to the terms of the Mozilla Public License, v.
 2.0.  For the full license text see LICENSE, or http://mozilla.org/MPL/2.0/.
 
-Copyright (c) 2016, Joyent, Inc.
+Copyright (c) 2017, Joyent, Inc.
diff --git a/docs/index.md b/docs/index.md
index 9972644..d846480 100644
--- a/docs/index.md
+++ b/docs/index.md
@@ -6,9 +6,10 @@ apisections: Buckets, Objects
 
 # Moray
 
-This is the reference documentation for Moray, which is a key/value store built
-on top of [Manatee](https://mo.joyent.com/docs/manatee/master/), and
-[fast](https://github.com/mcavage/node-fast).
+This is the reference documentation for Moray, which is a key-value store built
+on top of [Manatee](https://github.com/joyent/manatee/) and
+[fast](https://github.com/joyent/node-fast).  For general information about
+Moray, see the [README](https://github.com/joyent/moray).
 
 This documentation provides descriptions of the APIs that Moray offers, as well
 as how to use the (node.js) [client SDK](https://mo.joyent.com/node-moray) to
@@ -27,22 +28,32 @@ information are formatted like this:
         assert.ifError(err);
     });
 
+
 # Overview
 
-Moray allows you to store arbitrary data as a JSON document in a `bucket`. A
-bucket is essentially a namespace, such that bucket `foo` can have a key `mykey`
-and bucket `bar` can have a key `mykey` with different values.  There is no
-limit on the number of buckets, nor on the number of keys in a bucket.  That
-said, a single Moray instance is backed by a single logical Postgres instance,
-so you are practically limited to how much data you can maintain.  The basic
-operations on objects in a bucket are `put`, `get`, and `delete`.
-
-Upon creating a bucket you are allowed to define the bucket to have indexes,
-which allow you to later `search` for multiple records that match those
-indexes.  If indexes are defined on a bucket, when you write a key/value pair,
-the value is automatically indexed server-side in Moray for you.  Indexes can
-be defined to be of type `number`, `boolean`, `string`, `ip` or `subnet`. They
-can optionally be defined to enforce uniqueness of a value. Index names must:
+Moray allows you to store arbitrary JSON documents (called **objects**) inside
+namespaces called **buckets**.  Buckets can have **indexes**, which are
+top-level fields that are extracted so that search operations using those fields
+can be executed efficiently.  Indexes can also be used to enforce uniqueness of
+a value.  Since buckets are essentially namespaces, bucket `foo` can have a key
+`mykey` and bucket `bar` can have a key `mykey` with different values.
+
+There is no limit on the number of buckets, nor on the number of keys in a
+bucket.  That said, a single Moray instance is backed by a single logical
+Postgres instance, so you are practically limited to how much data you can
+maintain.
+
+The basic operations on objects in a bucket are `put`, `get`, and `delete`.
+`search`, `update`, and `deleteMany` are also supported using filter strings
+that operate on indexed fields.
+
+You define a bucket's indexes when you initially create the bucket.  When you
+write an object, the value of each indexed field is updated in the server-side
+index for that field.  You can update a bucket's indexes later, but objects
+already written in that bucket will need to be reindexed.
+
+Indexes can be defined to be of type `number`, `boolean`, `string`, `ip` or
+`subnet`.  Index names must:
 
 - Contain only Latin letters, Arabic numerals, and underscores.
 - Start with a Latin letter or an underscore.
@@ -52,174 +63,29 @@ can optionally be defined to enforce uniqueness of a value. Index names must:
 - Not be a reserved name (`_etag`, `_id`, `_key`, `_atime`, `_ctime`, `_mtime`,
   `_rver`, `_txn_snap`, `_value`, or `_vnode`).
 
-## Arrays
-
-As of release 20130822 (integrated in Git commit `#ccb80c9`), Moray now supports
-the ability to define multi-valued entries such that indexing still works
-(mostly) as expected. A small example is given, and then explained:
-
-    var assert = require('assert-plus');
-    var moray = require('moray');
-
-    var cfg = {
-        index: {
-            name: {
-                type: '[string]'
-            }
-        }
-    };
-    var client = moray.createClient({...});
-    client.putBucket('foo', cfg, function (bucket_err) {
-        assert.ifError(bucket_err);
-
-        var data = {
-            something_irrelevant: 'blah blah',
-            name: ['foo', 'bar', 'baz']
-        };
-        client.putObject('foo', 'bar', data, function (put_err) {
-            assert.ifError(put_err);
-
-            var req = client.findObjects('foo', '(name=bar)');
-            req.once('error', assert.ifError.bind(assert));
-            req.on('record', ...);
-            req.once('end', ...);
-        });
-    });
-
-Array types work just like regular types, except they are defined by `[:type:]`,
-and on writes, moray knows how to index those properly.  On searches, the
-contract is to return the record if _any_ of the array values match the filter
-subclause.  There is one caveat:  _wildcard_ searches do not work (and can't).
-So doing `(name=f*)` will return an error to you.  The reason is that Postgres
-does not have any sane way of doing this (it is technically possible, but
-expensive, and not currently implemented in Moray).
-
-## Triggers
-
-While a quasi-advanced topic, Moray does support `pre` and `post` "triggers" on
-buckets, which are guaranteed to run before and after a write
-(so put|del object) is performed.  You write them as just plain old JS functions
-when you create your bucket, and note that they are guaranteed to run as part of
-the transaction scope.  You typically will use `pre` triggers to transform
-and/or validate data before being saved, and use `post` triggers to write an
-audit log or some such thing (UFDS leverages `post` triggers to generate the
-changelog).  The are guaranteed to run in the order you define them.  Note that
-`pre` triggers only run before `putObject`, whereas `post` runs after both
-`putObject` and `delObject`.
-
-In general, you need to be super careful with these, as they run in the same VM
-that Moray does, so a null pointer deref will bring down the whole server.  This
-is intentional for performance reasons.  So basically, don't screw up.
-
-### pre
-
-The definition for a pre trigger looks like this:
+Moray also supports multi-valued entries such that indexing still works (mostly)
+as expected.  There's an example later in this document under "Using Arrays".
 
-    function myPreTrigger(req, cb) {
-        ....
-        cb();
-    }
-
-Where `req` is an object like this:
-
-    {
-        bucket: 'foo',
-        key: 'bar',
-        log: <logger>,
-        pg: <postgres handle>,
-        schema: <index configuration>,
-        value: {
-            email: 'mark.cavage@joyent.com'
-        }
-    }
-
-That is, you are passed in the name of the bucket, the name of the key, the
-value (as a JS object) the user wanted to write, the bucket configuration, a
-bunyan instance you can use to log to the Moray log, and the current Postgres
-connection (which is a raw [node-pg](https://github.com/brianc/node-postgres/)
-handle).
-
-### post
-
-The definition for a post trigger is nearly identical:
-
-
-    function myPostTrigger(req, cb) {
-        ....
-        cb();
-    }
-
-Where `req` is an object like this:
-
-    {
-        bucket: 'foo',
-        key: 'bar',
-        id: 123,
-        log: <logger>,
-        pg: <postgres handle>,
-        schema: <index configuration>,
-        value: {
-            email: 'mark.cavage@joyent.com'
-        }
-    }
-
-The `req` object is basically the same, except you will now also have the
-database `id` (a monotonically increasing sequence number) tacked in as well,
-should you want to use that for something.
 
 # Basic MorayClient usage
 
-In order to interact with Moray, you are assumed to be using the `node-moray`
-package, which you can install like:
-
-    $ npm install git+ssh://git@github.com:joyent/node-moray.git
+You can install the node-moray client library and CLI tools using:
 
-In order to then use it, here's some sample code:
+    $ npm install moray
 
-    var bunyan = require('bunyan');
-    var moray = require('moray');
-
-    var client = moray.createClient({
-        host: '127.0.0.1',
-        port: 2020,
-        log: bunyan.createLogger({
-            name: 'moray',
-            level: 'INFO',
-            stream: process.stdout,
-            serializers: bunyan.stdSerializers
-        })
-    });
-
-    client.on('error', function (err) {
-         // client is no longer usable
-         // ...
-    });
-
-    client.on('connect', function () {
-        client.close();
-    });
+Or, to install the CLI tools and manual pages on your path:
 
-Note that connect timeout and backoff/retry are options that are currently not
-supported, but are on the (very) short-term TODO list.
+    $ npm install -g moray
 
-All of the APIs shown below have an optional `options` argument that is always
-second to last (callback being last). Typically, you would use this to pass in
-a `req_id` that we can use to correlate logs from one service to the
-interactions of moray.  Some APIs (namely put/get/del object) have additional
-logic there to allow cache bypassing, etc.  For example, both of these are
-valid:
+The CLI tools have detailed manual pages.  Start with `moray(1)` for an
+overview.
 
-    client.getObject('foo', 'bar', function (err, obj) {
-        ...
-    });
+The library interface has an overview manual page `moray(3)` that describes how
+to initialize the client for Triton and Manta services and CLI tools.  The API
+documentation below includes examples of using both the CLI tools and Node
+client library interfaces.  It's worth reviewing `moray(1)` and `moray(3)` to
+understand the basic conventions used below.
 
-    var opts = {
-        req_id: 1,
-        noCache: true
-    };
-    client.getObject('foo', 'bar', opts, function (err, obj) {
-        ...
-    });
 
 # Buckets
 
@@ -232,7 +98,7 @@ updates will fail if the version rolls backwards.
 
 ### API
 
- A "fully loaded" config (w/o post triggers) would look like this:
+A "fully loaded" config (without post triggers) would look like this:
 
     var cfg = {
         index: {
@@ -389,7 +255,6 @@ Returns the configuration for all buckets.
 ### CLI
 
     $ listbuckets
-
     [{
       "name": foo",
       "index": {
@@ -438,7 +303,7 @@ version in the database.
 Note that if you *add* new indexes via `updateBucket`, then any *new* data will
 be indexed accordingly, but _old_ data will not.  See the ReindexObjects
 function for a method to update old rows. (A bucket version _must_ be specified
-for `reindexObjects` to operation properly.)
+for `reindexObjects` to work.)
 
 Also, note that all operations involving the bucket configuration in Moray will
 use a cached copy of the bucket configuration, so it may take a few minutes for
@@ -496,7 +361,7 @@ Or alternatively:
 
 ## DeleteBucket
 
-Deletes a bucket, *and all data in that bucket!* No real options to speak of.
+Deletes a bucket, *and all data in that bucket!*
 
 ### API
 
@@ -593,27 +458,18 @@ Plus any currently unhandled Postgres Errors (such as relation already exists).
 
 ## GetObject
 
-Retrieves an object by bucket/key.  An important option to keep in mind is the
-`noCache` option.  By default, Moray keeps a fairly small in-memory cache for
-`GetObject` specifically, so if you want strong consistency on reads, set
-`noCache: true` in the client.  Note also that if noCache is not set, Moray will
-try to read from the Postgres asynchronous slave.  So, if you want guaranteed read
-after write consistency, set `noCache`.  This can be done on a request by
-request basis.
-
-Also, another important note is regarding the magic column `_txn_snap`.  This
-field represents the postgres internal transaction *snapshot* that corresponds
-to when this record was written (specifically the `xmin` component).  To really
-get this, you probably want to go read up on how
-[MVCC is implemented in postgres](http://momjian.us/main/writings/pgsql/internalpics.pdf);
-specifically pp56-58.  The short of it is, for records in moray, it is
-possible/probable that more than one record will have the same `_txn_snap`
-value, so you cannot rely on it as a unique id.  However, you can rely on it to
-be the only thing that will increase over time, and if your query's `_txn_snap`
-is not the *latest* one, you won't see phantom reads.  This field was built into
-Moray to solve the "sliding window" problem where consumers use Moray as a queue
-(it makes sense if you want durable H/A...); what it means is that consumers
-doing this need to use both the `_id` and `_txn_snap` fields.
+Retrieves an object by bucket and key.
+
+The `_txn_snap` field represents the postgres internal transaction *snapshot*
+that corresponds to when this record was written (specifically the `xmin`
+component).  To really understand this, you probably want to go read up on how
+[MVCC is implemented in
+postgres](http://momjian.us/main/writings/pgsql/internalpics.pdf); specifically
+pp56-58.  The short of it is, for records in Moray, it is possible that more
+than one record will have the same `_txn_snap` value, so you cannot rely on it
+as a unique id.  You also cannot rely on this value to increase over time.
+Objects with later `_txn_snap` values can be visible before objects with earlier
+`_txn_snap` values.
 
 ### API
 
@@ -664,20 +520,24 @@ Plus any currently unhandled Postgres Errors (such as relation already exists).
 ## FindObjects
 
 Allows you to query a bucket for a set of records that match indexed fields.
-Note this is not a streaming API (there is no pagination. do that yourself if
-you want it).
+Note this is not a streaming API.  (Pagination must be implemented by consumers,
+if desired.)
 
-Search filters are fully specified according to LDAP search filter rules (which
-is one of the few sane parts of LDAP).  Whatever you search on must be part of
-the index on the bucket config; this is non-negotiable.
+Search filters are fully specified according to search filters resembling LDAP
+filter rules.  All fields included in your search query should be indexed in the
+bucket config, though the server only enforces that at least one field that's
+used to limit the result set is indexed (to avoid an obvious case where a table
+scan is required).  **Surprising behavior results when searching with
+non-indexed fields, and this is strongly discouraged.**  For details, see
+`findobjects(1)`.
 
 In addition to the search filter, you can specify `limit`, `offset`, and `sort`;
-the first two act like they usually do in DBs, and `sort` must be a JS object
+the first two act like they usually do in DBs, and `sort` must be a JSON object
 specifying the attribute to sort by, and an order, which is one of `ASC` or
-`DESC` (so also like DBs).  The default `limit` setting is `1000`. There is
-no default `offset`.  The default `sort` order is `ASC`, and the default
-attribute is `_id` (which means records are returned in the order they were
-*created*).
+`DESC` (so also like DBs).  The default `limit` setting is `1000`. There is no
+default `offset`.  The default `sort` order is `ASC`, and the default attribute
+is `_id` (which means records are returned in the order they were created or
+updated).
 
 ### API
 
@@ -805,15 +665,14 @@ Plus any currently unhandled Postgres Errors (such as relation already exists).
 
 Allows you to bulk update a set of objects in one transaction.  Note you can
 only update indexed fields.  You call this API with a bucket, a list of fields
-to update and a filter, that is exactly the same syntax as `findObjects`.  To
-use this API, you must set the bucket options `syncUpdates: true` at
-`putBucket` time, otherwise you'll effectively have data corruption.
+to update and a filter, that is exactly the same syntax as `findObjects`.
 
 A few caveats:
 
 - All objects affected by the update will have the same `_etag`.
-- The `JSON value` will *not* be updated.  You will have to pay on every get
-  request to merge them (the bucket options field drives this logic).
+- The `JSON value` will *not* be updated in situ, though Moray hides this fact
+  from you.  Subsequent "get" operations will merge the results into the
+  returned value.
 
 ### API
 
@@ -833,7 +692,7 @@ A few caveats:
 | Field   | Type   | Description                                            |
 | ------- | ------ | ------------------------------------------------------ |
 | bucket  | string | bucket to write this key in                            |
-| fields  | object | key/values to update                                   |
+| fields  | object | keys and values to update                              |
 | filter  | string | search filter string                                   |
 | options | object | any optional parameters (req\_id, limit, offset, sort) |
 
@@ -847,10 +706,10 @@ parameter to set the max rows per iteration, `reindexObjects` must be called
 repeatedly until it reports 0 rows processed.  Only then will the added indexes
 be made available for use.
 
-The selected `count` value should be chosen with the expected bucket object
-size in mind.  Too large a value may cause Moray to consume excessive amounts
-of memory since it's unable to exert backpressure on PostgreSQL when fetching
-rows.
+The selected `count` value should be chosen with the expected bucket object size
+in mind.  Too large a value can cause transactions to remain open for extended
+periods, and may also cause Moray to consume excessive amounts of memory since
+it's unable to exert backpressure on PostgreSQL when fetching rows.
 
 It is safe to make multiple simultaneous calls to `reindexObjects` acting on
 the same bucket but it's likely to race on rows and incur rollbacks/slowdowns.
@@ -978,6 +837,8 @@ An API that really only exists for two reasons:
 2. for systems like UFDS that need to create extra tables et al in Moray for
    `post` triggers.
 
+The latter use-case (filling in gaps in the Moray API) is considered deprecated.
+
 ### API
 
     var req = client.sql('select * from buckets_config');
@@ -1005,3 +866,122 @@ An API that really only exists for two reasons:
     {
         "now": "2012-08-01T15:50:33.291Z"
     }
+
+
+# Additional features
+
+## Using Arrays
+
+Here's a small example demonstrating how to use arrays:
+
+    var assert = require('assert-plus');
+    var moray = require('moray');
+
+    var cfg = {
+        index: {
+            name: {
+                type: '[string]'
+            }
+        }
+    };
+    var client = moray.createClient({...});
+    client.putBucket('foo', cfg, function (bucket_err) {
+        assert.ifError(bucket_err);
+
+        var data = {
+            something_irrelevant: 'blah blah',
+            name: ['foo', 'bar', 'baz']
+        };
+        client.putObject('foo', 'bar', data, function (put_err) {
+            assert.ifError(put_err);
+
+            var req = client.findObjects('foo', '(name=bar)');
+            req.once('error', assert.ifError.bind(assert));
+            req.on('record', ...);
+            req.once('end', ...);
+        });
+    });
+
+Array types work just like regular types, except they are defined by `[:type:]`,
+and on writes, Moray knows how to index those properly.  On searches, the
+contract is to return the record if _any_ of the array values match the filter
+subclause.  There is one caveat:  _wildcard_ searches do not work (and can't).
+So doing `(name=f*)` will return an error to you.  The reason is that Postgres
+does not have any sane way of doing this (it is technically possible, but
+expensive, and not currently implemented in Moray).
+
+## Triggers
+
+While a quasi-advanced topic, Moray does support `pre` and `post` "triggers" on
+buckets, which are guaranteed to run before and after a write
+(so put|del object) is performed.  You write them as just plain old JS functions
+when you create your bucket, and note that they are guaranteed to run as part of
+the transaction scope.  You typically will use `pre` triggers to transform
+and/or validate data before being saved, and use `post` triggers to write an
+audit log or some such thing (UFDS leverages `post` triggers to generate the
+changelog).  They are guaranteed to run in the order you define them.  Note that
+`pre` triggers only run before `putObject`, whereas `post` runs after both
+`putObject` and `delObject`.
+
+In general, you need to be super careful with these, as they run in the same VM
+that Moray does, so any uncaught exception will bring down the whole server.
+This is intentional for performance reasons.  Additionally, they run in the same
+callback chain as the rest of the operation, so failure to invoke the callback
+will stall the operation and leak the Postgres handle.  So basically, don't
+screw up.
+
+### pre
+
+The definition for a `pre` trigger looks like this:
+
+    function myPreTrigger(req, cb) {
+        ....
+        cb();
+    }
+
+Where `req` is an object like this:
+
+    {
+        bucket: 'foo',
+        key: 'bar',
+        log: <logger>,
+        pg: <postgres handle>,
+        schema: <index configuration>,
+        value: {
+            email: 'mark.cavage@joyent.com'
+        }
+    }
+
+That is, you are passed in the name of the bucket, the name of the key, the
+value (as a JS object) the user wanted to write, the bucket configuration, a
+bunyan instance you can use to log to the Moray log, and the current Postgres
+connection (which is a raw [node-pg](https://github.com/brianc/node-postgres/)
+handle).
+
+### post
+
+The definition for a `post` trigger is nearly identical:
+
+
+    function myPostTrigger(req, cb) {
+        ....
+        cb();
+    }
+
+Where `req` is an object like this:
+
+    {
+        bucket: 'foo',
+        key: 'bar',
+        id: 123,
+        log: <logger>,
+        pg: <postgres handle>,
+        schema: <index configuration>,
+        value: {
+            email: 'mark.cavage@joyent.com'
+        }
+    }
+
+The `req` object is basically the same, except you will now also have the
+database `id` (a monotonically increasing sequence number) tacked in as well,
+should you want to use that for something.
diff --git a/env.sh b/env.sh
index 2c6187f..e652223 100644
--- a/env.sh
+++ b/env.sh
@@ -5,10 +5,10 @@
 #
 
 #
-# Copyright (c) 2014, Joyent, Inc.
+# Copyright (c) 2017, Joyent, Inc.
 #
 
+set -o xtrace
 export PATH=$PWD/build/node/bin:$PWD/node_modules/.bin:node_modules/moray/bin:$PATH
-
-alias server='node main.js -f ./etc/config.coal.json -v 2>&1 | bunyan'
 alias npm='node `which npm`'
+set +o xtrace
