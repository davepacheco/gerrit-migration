From 2e5c618e939de07499a6ba327a3fdb89623824dc Mon Sep 17 00:00:00 2001
From: David Pacheco <dap@joyent.com>
Date: Thu, 18 Apr 2019 16:27:56 -0700
Subject: [PATCH] MANTA-4234 document how to construct a request timeline for
 debugging

---
 0002-investigation-tasks.adoc | 189 ++++++++++++++++++++++--
 TODO.md                       |   2 +
 docs/index.html               | 269 +++++++++++++++++++++++++++++++++-
 3 files changed, 448 insertions(+), 12 deletions(-)

diff --git a/0002-investigation-tasks.adoc b/0002-investigation-tasks.adoc
index 0b2380e..58fdbfa 100644
--- a/0002-investigation-tasks.adoc
+++ b/0002-investigation-tasks.adoc
@@ -795,16 +795,185 @@ client, and we also know which Muskie zone it was.
 
 === Build a request timeline
 
-// XXX
-// loadbalancer log: accept time, completion time
-// muskie log:
-// - completion time
-// - client "Date" header (note: different clock)
-// - latency (only for time-to-first-byte)
-// - request timers
-// - shark information
-// object metadata (e.g., mtime on record)
-// mako access log entries
+Building a request timeline can be incredibly valuable for understanding why a
+particular request failed or why a request took as long as it did.  Timelines
+are especially useful for understanding apparently mysterious failure modes
+like request timeouts, "socket hang-up" errors, "connection reset" errors, and
+the like.
+
+A request timeline is simply a list of events related to a request and the
+precise timestamps when they were reported.  You can include as much or as
+little detail as is relevant to your problem.  It's common to focus on the
+phases that happen inside Muskie.  These are the simplest to collect and they're
+often sufficient when failure modes are explicit (e.g., an authentication
+failure or a 404 response).  A pretty complete request timeline could include a
+lot more than that about what happens at the load balancer and storage nodes.
+
+NOTE: Keep in mind that wall clocks vary across physical machines.  When
+comparing timestamps recorded from different systems, beware that differences
+may simply be a result of differences in the wall clock time on those systems.
+Within Manta, clocks are usually synchronized within a few hundred
+milliseconds.  (You can check this for the servers you're looking at using NTP
+tools.)  Timestamps reported by clients can be much further off, but if they're
+also using NTP to synchronize their clocks, they may well be pretty close.
+
+==== A brief review of request handling
+
+A typical GET request for an object stored in Manta runs as follows:
+
+* The client establishes a TCP connection to the Manta load balancer.  As part
+  of this process, the load balancer establishes a TCP connection to a Muskie
+  instance to handle any requests that arrive over this connection.
+* The client constructs an HTTP request and sends it over the TCP connection.
+* The load balancer forwards the request to Muskie.
+* Muskie begins processing the request.
+** There are a number of (usually very fast) setup phases within Muskie.
+** Muskie authenticates the request.  This may involve fetching authentication
+   and authorization information from Mahi, in which case an HTTP request will
+   be sent over the network to Mahi to fetch basic information about the
+   account.
+** Muskie fetches metadata associated with the object.  This step involves a few
+   network RPC calls to the metadata tier: first to an Electric-Moray instance,
+   which forwards the request to a Moray instance, which executes a SQL query
+   against a PostgreSQL primary.
+** Muskie authorizes the request.  This involves comparing the authenticated
+   user's credentials to the credentials required to access the object
+   (specified in its metadata).
+** Muskie makes an HTTP request to a storage node hosting a copy of the object.
+** Assuming this request succeeds, Muskie reports a successful response header
+   to the client.  Then data is sent from the storage node, through Muskie,
+   through the load balancer, and to the client.
+** The request concludes when the last data is sent to the client.
+
+// XXX make a visual of the above steps?
+
+PUT requests to upload objects are similar except that there's an additional
+metadata RPC after all the data has streamed to the storage nodes.  Other types
+of requests (e.g., creation and deletion of directories) are largely similar,
+but generally don't involve storage nodes.
+
+This is a simplification.  For details, see the Muskie source code.  (Don't be
+afraid to crack it open!)
+
+There are 1-2 dozen phases of request handling within Muskie, but most of the
+elapsed time of a request happens in only a handful of phases that relate to
+making requests to external services.  These are described below.
+
+==== Key events during a request lifetime
+
+The table below explains a number of events that happen while processing a
+request and where you can find more information about it.  Many of these entries
+refer to entries in logs that are documented elsewhere in this guide.  See
+<<_understanding_a_muskie_log_entry>> and
+<<_understanding_a_load_balancer_log_entry>>.
+
+Remember, it's not necessary to collect all of these to start!  Start with the
+basics and flesh out what looks relevant.  Some of what's below won't apply to
+every request.  However, if you're stumped about a strange failure mode, it's
+often helpful to construct a pretty complete timeline, as you'll often find
+surprising gaps or unusual intervals (e.g., exactly 60 seconds from when
+something started until when some obscure error message was reported, which
+might suggest a timeout).
+
+If you don't know where to start, consider a timeline that just includes:
+
+- when the request was constructed on the client
+- when the request was received by the load balancer (if possible)
+- when the request was received by Muskie
+- when the metadata phases of request handling were completed in Muskie
+- when the request was completed by Muskie
+
+It's common to start there, skim the `req.timers` field (mentioned below) to
+look for unusually long phases (e.g., those taking upwards of 1 second), and
+add those to the timeline as needed.
+
+[cols="4*",options="header"]
+|===
+|Event
+|Component where it happens
+|Where to find the timestamp
+|Notes
+
+|Client establishes a TCP connection to the load balancer.
+|Load balancer
+|Load balancer log entry, `accept_date` field.  Note that this is not the date
+at the very start of the load balancer log entry.
+|Both the client and server may use timeouts that measure from this timestamp.  It can be helpful to check for intervals of round numbers (e.g., exactly 1 or 2 minutes).
+
+|Client constructs the HTTP request.
+|Client (outside Manta)
+|Muskie log entry, `req.headers.date` (the `Date` header supplied by the client with the HTTP request)
+|Large gaps between when the client constructed the request and when Muskie began processing it could indicate queueing in Muskie or somewhere in the network before reaching Muskie.
+
+|Muskie begins processing the request.
+|Muskie
+|Muskie log entry.  There's no discrete field for this, but you can compute this by taking the timestamp when Muskie completed the request (see below) and subtracting the total latency of the request (including time to transfer data).  To calculate the total latency, sum all the timers in `req.phases`.
+|This is a very useful point at which to divide an investigation, since large gaps in time _before_ this point indicate queueing prior to the request reaching Muskie, which generally involve different tools and data sources than latency after this point.
+
+|Muskie makes HTTP request to Mahi (authcache) to load information needed to authenticate and authorize this request.
+|Muskie
+|Muskie log entry, `req.timers`.  These requests generally occur during the `loadCaller` and `loadOwner` phases, though they can also happen during `parseHttpAuthToken` (if the `x-auth-token` header is provided by the client), or `getActiveRoles`.  You can generally infer that if these HTTP requests were made, they started after one of these phases began and completed before the same phase ended.  To determine the actual start and end timestamps, you have compute them based on the latency of the previous phases and the time when Muskie began processing the request (or the latency of the subsequent phases and the time when Muskie finished processing the request).
+|Most requests do not actually involve any of these HTTP requests because the information is usually cached in Muskie.  However, latency here may indicate a problem with Mahi or the network.
+
+|Muskie makes RPCs to load metadata.
+|Muskie
+|Muskie log entry, `req.timers`.  These requests most commonly occur during the `getMetadata` phase.  For the actual start and end timestamps, you must compute them based on the latency of the other phases and the time the request completed (or finished).
+|High latency here indicates a slow read RPC to the metadata tier.
+
+|Muskie issues HTTP requests to storage nodes.
+|Muskie
+|Muskie log entry, `req.timers` and `req.sharksContacted`.  For GET requests, this happens during the `streamFromSharks` phase.  For PUT requests, this happens during the `startSharkStreams` phase.  Since multiple storage nodes may be contacted, sometimes in parallel and sometimes sequentially, the `sharksContacted` field has more fine-grained information about the time for each particular storage node.
+|This step is often forgotten, but it's important to complete before Muskie sends successful response headers to the client.  High latency between when these requests are issued and when storage nodes send headers back may indicate a network problem or a storage node that's offline or overloaded.
+
+|Muskie sends response headers.
+|Muskie
+|Muskie log entry, `latency` field (elapsed time in milliseconds, which must be added to the initial timestamp)
+a|Sometimes called latency-to-first-byte, this is a significant point in the request because until this point, the client has heard nothing from the server while the server authenticates the client, loads metadata, and authorizes the request.  Many clients have timeouts (sometimes as short as a few seconds) from when they send the request until they see response headers.  If there's a long gap in this range and then Muskie subsequently reports a client error (e.g., timed out waiting for the client or a closed connection from the client), the client may have given up.
+
+Also, the work associated with the rest of this request depends on how large the end user's object is and how much network bandwidth they have available to Manta.  As a result, it's common to summarize this latency-to-first-byte rather than the whole latency of the request, as this number is more comparable across different types of requests.
+
+|Muskie begins streaming data between storage nodes and the client.
+|Muskie
+|Muskie log entry, `req.timers` field.  For GET operations, the phase where data is transferred is called `streamFromSharks`.  For PUT operations, the phase is called `sharkStreams`.
+|This should be about the same time as the response headers are sent.
+
+|Storage nodes process HTTP requests
+|Mako
+|`mako-access` log.  Entries in the mako-access log include both a timestamp when they completed and a latency, which allows you to compute the end timestamp.
+|High latency between when Muskie began transferring data to or from storage nodes and when storage nodes report having started may indicate overload on the storage node or a network problem.
+
+|Muskie finishes streaming data between storage nodes and the client.
+|Muskie
+|See the row above where Muskie begins streaming data.  The end of the corresponding phase indicates when this finished.
+|The time for this phase is highly variable depending on how much data is requested and the bandwidth available to the client.  It's more useful to compute throughput (as the total bytes sent divided by the latency of this phase) than look at the latency directly.  Low throughput here can be indicative of almost anything: a slow client, a slow network, overloaded Muskie, or an overloaded storage node.
+
+|Muskie makes RPCs to save metadata.
+|Muskie
+a|Muskie log entry, `req.timers`.  These requests most commonly occur during the `saveMetadata` phase.  For the actual start and end timestamps, you must compute them based on the latency of the other phases and the time the request completed (or finished).
+
+Another possible indicator of this timestamp may be given by the `mtime` stored in the object metadata, assuming it hasn't been changed since the request you're investigating completed.
+
+
+|High latency here indicates a slow write RPC to the metadata tier.
+
+|Load balancer indicates the TCP session ended.
+|Load balancer
+|Load balancer log, `syslog_date` field.
+|This is generally the last time the client heard anything from the server associated with this request.  Normally, this indicates the end of the request.  In pathological cases where metadata operation takes several minutes, the load balancer may terminate the request, logging this entry and sending a failure response to the client, even though Muskie is still processing the request.  This is typically followed by a Muskie log entry with a timestamp _after_ this point.  The latency of the various phases in the Muskie request point to what took so long.
+|===
+
+// XXX add a table of other useful timestamps: `uptime`, `last`, process up
+// time, process crash time, kernel panic time?
+
+Depending on the situation, you might need even more information: if you're
+debugging a problem that happened at the same time as a kernel panic, you might
+want the high-resolution timestamp when the kernel panicked.  Or you might want
+to include information about a hardware failure or a process crash.
+
+
+// XXX example!  that includes load balancer, muskie, 
+
+
 
 === Details about specific error messages
 
diff --git a/TODO.md b/TODO.md
index b745c3d..6e8a435 100644
--- a/TODO.md
+++ b/TODO.md
@@ -15,6 +15,8 @@ path through the decision tree goes where it should.
 - load balancer problems
 - muskie slowness
 
+- overview of object metadata, using mlocate
+
 - message:
 {"phase":"0","what":"phase 0: input \"/khangngu/stor/books/treasure_island.txt\"","code":"InvalidArgumentError","message":"failed to dispatch task: requested image is not available","input":"/khangngu/stor/books/treasure_island.txt","p0input":"/khangngu/stor/books/treasure_island.txt"}
 from: https://chatlogs.joyent.us/logs/manta/2018/07/26#00:11:10.748Z
diff --git a/docs/index.html b/docs/index.html
index e7e70e6..e6ffbf4 100644
--- a/docs/index.html
+++ b/docs/index.html
@@ -485,7 +485,12 @@ body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-b
 </ul>
 </li>
 <li><a href="#_understanding_a_load_balancer_log_entry">Understanding a load balancer log entry</a></li>
-<li><a href="#_build_a_request_timeline">Build a request timeline</a></li>
+<li><a href="#_build_a_request_timeline">Build a request timeline</a>
+<ul class="sectlevel3">
+<li><a href="#_a_brief_review_of_request_handling">A brief review of request handling</a></li>
+<li><a href="#_key_events_during_a_request_lifetime">Key events during a request lifetime</a></li>
+</ul>
+</li>
 <li><a href="#_details_about_specific_error_messages">Details about specific error messages</a>
 <ul class="sectlevel3">
 <li><a href="#_request_has_exceeded_bytes">"Request has exceeded &#8230;&#8203; bytes"</a></li>
@@ -2491,7 +2496,267 @@ client, and we also know which Muskie zone it was.</p>
 </div>
 <div class="sect2">
 <h3 id="_build_a_request_timeline">Build a request timeline</h3>
-
+<div class="paragraph">
+<p>Building a request timeline can be incredibly valuable for understanding why a
+particular request failed or why a request took as long as it did.  Timelines
+are especially useful for understanding apparently mysterious failure modes
+like request timeouts, "socket hang-up" errors, "connection reset" errors, and
+the like.</p>
+</div>
+<div class="paragraph">
+<p>A request timeline is simply a list of events related to a request and the
+precise timestamps when they were reported.  You can include as much or as
+little detail as is relevant to your problem.  It&#8217;s common to focus on the
+phases that happen inside Muskie.  These are the simplest to collect and they&#8217;re
+often sufficient when failure modes are explicit (e.g., an authentication
+failure or a 404 response).  A pretty complete request timeline could include a
+lot more than that about what happens at the load balancer and storage nodes.</p>
+</div>
+<div class="admonitionblock note">
+<table>
+<tr>
+<td class="icon">
+<div class="title">Note</div>
+</td>
+<td class="content">
+Keep in mind that wall clocks vary across physical machines.  When
+comparing timestamps recorded from different systems, beware that differences
+may simply be a result of differences in the wall clock time on those systems.
+Within Manta, clocks are usually synchronized within a few hundred
+milliseconds.  (You can check this for the servers you&#8217;re looking at using NTP
+tools.)  Timestamps reported by clients can be much further off, but if they&#8217;re
+also using NTP to synchronize their clocks, they may well be pretty close.
+</td>
+</tr>
+</table>
+</div>
+<div class="sect3">
+<h4 id="_a_brief_review_of_request_handling">A brief review of request handling</h4>
+<div class="paragraph">
+<p>A typical GET request for an object stored in Manta runs as follows:</p>
+</div>
+<div class="ulist">
+<ul>
+<li>
+<p>The client establishes a TCP connection to the Manta load balancer.  As part
+of this process, the load balancer establishes a TCP connection to a Muskie
+instance to handle any requests that arrive over this connection.</p>
+</li>
+<li>
+<p>The client constructs an HTTP request and sends it over the TCP connection.</p>
+</li>
+<li>
+<p>The load balancer forwards the request to Muskie.</p>
+</li>
+<li>
+<p>Muskie begins processing the request.</p>
+<div class="ulist">
+<ul>
+<li>
+<p>There are a number of (usually very fast) setup phases within Muskie.</p>
+</li>
+<li>
+<p>Muskie authenticates the request.  This may involve fetching authentication
+and authorization information from Mahi, in which case an HTTP request will
+be sent over the network to Mahi to fetch basic information about the
+account.</p>
+</li>
+<li>
+<p>Muskie fetches metadata associated with the object.  This step involves a few
+network RPC calls to the metadata tier: first to an Electric-Moray instance,
+which forwards the request to a Moray instance, which executes a SQL query
+against a PostgreSQL primary.</p>
+</li>
+<li>
+<p>Muskie authorizes the request.  This involves comparing the authenticated
+user&#8217;s credentials to the credentials required to access the object
+(specified in its metadata).</p>
+</li>
+<li>
+<p>Muskie makes an HTTP request to a storage node hosting a copy of the object.</p>
+</li>
+<li>
+<p>Assuming this request succeeds, Muskie reports a successful response header
+to the client.  Then data is sent from the storage node, through Muskie,
+through the load balancer, and to the client.</p>
+</li>
+<li>
+<p>The request concludes when the last data is sent to the client.</p>
+</li>
+</ul>
+</div>
+</li>
+</ul>
+</div>
+<div class="paragraph">
+<p>PUT requests to upload objects are similar except that there&#8217;s an additional
+metadata RPC after all the data has streamed to the storage nodes.  Other types
+of requests (e.g., creation and deletion of directories) are largely similar,
+but generally don&#8217;t involve storage nodes.</p>
+</div>
+<div class="paragraph">
+<p>This is a simplification.  For details, see the Muskie source code.  (Don&#8217;t be
+afraid to crack it open!)</p>
+</div>
+<div class="paragraph">
+<p>There are 1-2 dozen phases of request handling within Muskie, but most of the
+elapsed time of a request happens in only a handful of phases that relate to
+making requests to external services.  These are described below.</p>
+</div>
+</div>
+<div class="sect3">
+<h4 id="_key_events_during_a_request_lifetime">Key events during a request lifetime</h4>
+<div class="paragraph">
+<p>The table below explains a number of events that happen while processing a
+request and where you can find more information about it.  Many of these entries
+refer to entries in logs that are documented elsewhere in this guide.  See
+<a href="#_understanding_a_muskie_log_entry">Understanding a Muskie log entry</a> and
+<a href="#_understanding_a_load_balancer_log_entry">Understanding a load balancer log entry</a>.</p>
+</div>
+<div class="paragraph">
+<p>Remember, it&#8217;s not necessary to collect all of these to start!  Start with the
+basics and flesh out what looks relevant.  Some of what&#8217;s below won&#8217;t apply to
+every request.  However, if you&#8217;re stumped about a strange failure mode, it&#8217;s
+often helpful to construct a pretty complete timeline, as you&#8217;ll often find
+surprising gaps or unusual intervals (e.g., exactly 60 seconds from when
+something started until when some obscure error message was reported, which
+might suggest a timeout).</p>
+</div>
+<div class="paragraph">
+<p>If you don&#8217;t know where to start, consider a timeline that just includes:</p>
+</div>
+<div class="ulist">
+<ul>
+<li>
+<p>when the request was constructed on the client</p>
+</li>
+<li>
+<p>when the request was received by the load balancer (if possible)</p>
+</li>
+<li>
+<p>when the request was received by Muskie</p>
+</li>
+<li>
+<p>when the metadata phases of request handling were completed in Muskie</p>
+</li>
+<li>
+<p>when the request was completed by Muskie</p>
+</li>
+</ul>
+</div>
+<div class="paragraph">
+<p>It&#8217;s common to start there, skim the <code>req.timers</code> field (mentioned below) to
+look for unusually long phases (e.g., those taking upwards of 1 second), and
+add those to the timeline as needed.</p>
+</div>
+<table class="tableblock frame-all grid-all stretch">
+<colgroup>
+<col style="width: 25%;">
+<col style="width: 25%;">
+<col style="width: 25%;">
+<col style="width: 25%;">
+</colgroup>
+<thead>
+<tr>
+<th class="tableblock halign-left valign-top">Event</th>
+<th class="tableblock halign-left valign-top">Component where it happens</th>
+<th class="tableblock halign-left valign-top">Where to find the timestamp</th>
+<th class="tableblock halign-left valign-top">Notes</th>
+</tr>
+</thead>
+<tbody>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Client establishes a TCP connection to the load balancer.</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Load balancer</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Load balancer log entry, <code>accept_date</code> field.  Note that this is not the date
+at the very start of the load balancer log entry.</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Both the client and server may use timeouts that measure from this timestamp.  It can be helpful to check for intervals of round numbers (e.g., exactly 1 or 2 minutes).</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Client constructs the HTTP request.</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Client (outside Manta)</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie log entry, <code>req.headers.date</code> (the <code>Date</code> header supplied by the client with the HTTP request)</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Large gaps between when the client constructed the request and when Muskie began processing it could indicate queueing in Muskie or somewhere in the network before reaching Muskie.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie begins processing the request.</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie log entry.  There&#8217;s no discrete field for this, but you can compute this by taking the timestamp when Muskie completed the request (see below) and subtracting the total latency of the request (including time to transfer data).  To calculate the total latency, sum all the timers in <code>req.phases</code>.</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">This is a very useful point at which to divide an investigation, since large gaps in time <em>before</em> this point indicate queueing prior to the request reaching Muskie, which generally involve different tools and data sources than latency after this point.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie makes HTTP request to Mahi (authcache) to load information needed to authenticate and authorize this request.</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie log entry, <code>req.timers</code>.  These requests generally occur during the <code>loadCaller</code> and <code>loadOwner</code> phases, though they can also happen during <code>parseHttpAuthToken</code> (if the <code>x-auth-token</code> header is provided by the client), or <code>getActiveRoles</code>.  You can generally infer that if these HTTP requests were made, they started after one of these phases began and completed before the same phase ended.  To determine the actual start and end timestamps, you have compute them based on the latency of the previous phases and the time when Muskie began processing the request (or the latency of the subsequent phases and the time when Muskie finished processing the request).</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Most requests do not actually involve any of these HTTP requests because the information is usually cached in Muskie.  However, latency here may indicate a problem with Mahi or the network.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie makes RPCs to load metadata.</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie log entry, <code>req.timers</code>.  These requests most commonly occur during the <code>getMetadata</code> phase.  For the actual start and end timestamps, you must compute them based on the latency of the other phases and the time the request completed (or finished).</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">High latency here indicates a slow read RPC to the metadata tier.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie issues HTTP requests to storage nodes.</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie log entry, <code>req.timers</code> and <code>req.sharksContacted</code>.  For GET requests, this happens during the <code>streamFromSharks</code> phase.  For PUT requests, this happens during the <code>startSharkStreams</code> phase.  Since multiple storage nodes may be contacted, sometimes in parallel and sometimes sequentially, the <code>sharksContacted</code> field has more fine-grained information about the time for each particular storage node.</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">This step is often forgotten, but it&#8217;s important to complete before Muskie sends successful response headers to the client.  High latency between when these requests are issued and when storage nodes send headers back may indicate a network problem or a storage node that&#8217;s offline or overloaded.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie sends response headers.</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie log entry, <code>latency</code> field (elapsed time in milliseconds, which must be added to the initial timestamp)</p></td>
+<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
+<p>Sometimes called latency-to-first-byte, this is a significant point in the request because until this point, the client has heard nothing from the server while the server authenticates the client, loads metadata, and authorizes the request.  Many clients have timeouts (sometimes as short as a few seconds) from when they send the request until they see response headers.  If there&#8217;s a long gap in this range and then Muskie subsequently reports a client error (e.g., timed out waiting for the client or a closed connection from the client), the client may have given up.</p>
+</div>
+<div class="paragraph">
+<p>Also, the work associated with the rest of this request depends on how large the end user&#8217;s object is and how much network bandwidth they have available to Manta.  As a result, it&#8217;s common to summarize this latency-to-first-byte rather than the whole latency of the request, as this number is more comparable across different types of requests.</p>
+</div></div></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie begins streaming data between storage nodes and the client.</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie log entry, <code>req.timers</code> field.  For GET operations, the phase where data is transferred is called <code>streamFromSharks</code>.  For PUT operations, the phase is called <code>sharkStreams</code>.</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">This should be about the same time as the response headers are sent.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Storage nodes process HTTP requests</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Mako</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock"><code>mako-access</code> log.  Entries in the mako-access log include both a timestamp when they completed and a latency, which allows you to compute the end timestamp.</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">High latency between when Muskie began transferring data to or from storage nodes and when storage nodes report having started may indicate overload on the storage node or a network problem.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie finishes streaming data between storage nodes and the client.</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">See the row above where Muskie begins streaming data.  The end of the corresponding phase indicates when this finished.</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">The time for this phase is highly variable depending on how much data is requested and the bandwidth available to the client.  It&#8217;s more useful to compute throughput (as the total bytes sent divided by the latency of this phase) than look at the latency directly.  Low throughput here can be indicative of almost anything: a slow client, a slow network, overloaded Muskie, or an overloaded storage node.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie makes RPCs to save metadata.</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Muskie</p></td>
+<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
+<p>Muskie log entry, <code>req.timers</code>.  These requests most commonly occur during the <code>saveMetadata</code> phase.  For the actual start and end timestamps, you must compute them based on the latency of the other phases and the time the request completed (or finished).</p>
+</div>
+<div class="paragraph">
+<p>Another possible indicator of this timestamp may be given by the <code>mtime</code> stored in the object metadata, assuming it hasn&#8217;t been changed since the request you&#8217;re investigating completed.</p>
+</div></div></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">High latency here indicates a slow write RPC to the metadata tier.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Load balancer indicates the TCP session ended.</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Load balancer</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Load balancer log, <code>syslog_date</code> field.</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">This is generally the last time the client heard anything from the server associated with this request.  Normally, this indicates the end of the request.  In pathological cases where metadata operation takes several minutes, the load balancer may terminate the request, logging this entry and sending a failure response to the client, even though Muskie is still processing the request.  This is typically followed by a Muskie log entry with a timestamp <em>after</em> this point.  The latency of the various phases in the Muskie request point to what took so long.</p></td>
+</tr>
+</tbody>
+</table>
+<div class="paragraph">
+<p>Depending on the situation, you might need even more information: if you&#8217;re
+debugging a problem that happened at the same time as a kernel panic, you might
+want the high-resolution timestamp when the kernel panicked.  Or you might want
+to include information about a hardware failure or a process crash.</p>
+</div>
+</div>
 </div>
 <div class="sect2">
 <h3 id="_details_about_specific_error_messages">Details about specific error messages</h3>
-- 
2.21.0

