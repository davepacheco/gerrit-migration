commit d6f07cf55b34b383459318ff953bb3d1ae5772d1 (refs/changes/19/1919/4)
Author: Pedro Palazon Candel <pedro@joyent.com>
Date:   2017-05-11T18:29:50+02:00 (2 years, 5 months ago)
    
    TOOLS-1762 sdcadm update binder should determine instances to update before executing the update procedure

diff --git a/lib/procedures/index.js b/lib/procedures/index.js
index b40e7d8..40d801d 100644
--- a/lib/procedures/index.js
+++ b/lib/procedures/index.js
@@ -518,9 +518,10 @@ function coordinatePlan(opts, cb) {
             changes.forEach(function (change) {
                 if (change.type === 'update-instance' &&
                     change.service.name === 'manatee') {
-                    return cb(new UsageError(
+                    next(new UsageError(
                         'Individual update of manatee instances ' +
                         'is not allowed'));
+                    return;
                 }
 
                 if ((change.type === 'update-service' ||
@@ -560,9 +561,10 @@ function coordinatePlan(opts, cb) {
             changes.forEach(function (change) {
                 if (change.type === 'update-instance' &&
                     change.service.name === 'binder') {
-                    return cb(new UsageError(
+                    next(new UsageError(
                         'Individual update of binder instances ' +
                         'is not allowed'));
+                    return;
                 }
 
                 if ((change.type === 'update-service' ||
@@ -571,7 +573,12 @@ function coordinatePlan(opts, cb) {
                 {
                     var svcInsts = instsFromSvcName[change.service.name] || [];
                     if (svcInsts.length && svcInsts.length > 1) {
-                        change.insts = svcInsts;
+                        var chInsts = forceSameImage ? svcInsts :
+                                svcInsts.filter(function (ins) {
+                                    return (ins.image !==
+                                        change.image.uuid);
+                                });
+                        change.insts = chInsts;
                     } else {
                         change.inst = svcInsts[0];
                     }
diff --git a/lib/procedures/update-binder-v2.js b/lib/procedures/update-binder-v2.js
index 4958d88..b06f555 100644
--- a/lib/procedures/update-binder-v2.js
+++ b/lib/procedures/update-binder-v2.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright 2016 Joyent, Inc.
+ * Copyright 2017 Joyent, Inc.
  */
 
 var assert = require('assert-plus');
@@ -27,30 +27,43 @@ var s = require('./shared');
  *
  * This is the second replacement for "upgrade-binder.sh" from the
  * incr-upgrade scripts.
+ *
+ * While "update binder" does not support individual instances, there's a
+ * scenario where the update process could fail leaving some instances
+ * outdated. Due to this possibility, we're fetching every binder instance
+ * into the SDC setup using `sdcadm.listInsts` and storing into `allInsts`
+ * variable, instead of just moving forward with the collection of instances
+ * provided by `change.insts`.
+ *
+ * Given that update of individual agent instances is not supported, we're
+ * replacing the "this.changes" variable in other update classes with just
+ * "this.change" for UpdateBinderV2, after some quick assertion regarding
+ * the length of the provided changes.
  */
 function UpdateBinderV2(options) {
     assert.arrayOfObject(options.changes, 'options.changes');
-    this.changes = options.changes;
+    assert.equal(options.changes.length, 1, 'options.changes.length');
+    this.change = options.changes[0];
 }
 util.inherits(UpdateBinderV2, Procedure);
 
 UpdateBinderV2.prototype.summarize = function ushiSummarize() {
-    var word = (this.changes[0].type === 'rollback-service') ?
+    var word = (this.change.type === 'rollback-service') ?
         'rollback' : 'update';
-    var c0 = this.changes[0];
-    var img = c0.image;
+    var img = this.change.image;
+
     var out = [sprintf('%s "%s" service to image %s', word,
-                    c0.service.name, img.uuid),
+                    this.change.service.name, img.uuid),
                 common.indent(sprintf('(%s@%s)', img.name, img.version))];
-    if (c0.insts) {
+    if (this.change.insts) {
         out[0] += ':';
-        out = out.concat(c0.insts.map(function (inst) {
+        out = out.concat(this.change.insts.map(function (inst) {
             if (inst.image === img.uuid) {
                 return common.indent(sprintf(
                     'instance "%s" (%s) is already at version %s',
                     inst.zonename, inst.alias, img.version));
             } else {
-                return common.indent(sprintf('instance "%s" (%s) in server %s',
+                return common.indent(sprintf('instance "%s" (%s) on server %s',
                     inst.zonename, inst.alias, inst.server));
             }
         }));
@@ -67,314 +80,357 @@ UpdateBinderV2.prototype.execute = function ushiExecute(opts, cb) {
     assert.string(opts.wrkDir, 'opts.wrkDir');
     assert.func(cb, 'cb');
     var self = this;
+    var sdcadm = opts.sdcadm;
     var log = opts.log;
     var progress = opts.progress;
     var rollback = opts.plan.rollback ||Â false;
 
-    function updateBinder(change, nextSvc) {
-        var insts = change.insts || [change.inst];
-        var leader;
-        var standalone = false;
-        var followers = [];
-
-        var arg = {
-            change: change,
-            opts: opts,
-            userScript: false,
-            HA: false
-        };
-
-        if (insts && insts.length > 1) {
-            arg.HA = true;
-        }
-
-        if (opts.plan.changes.length > 1) {
-            progress('');
-            progress('--- Updating %s ...', change.service.name);
-        }
+    var insts = self.change.insts || [self.change.inst];
+
+    var arg = {
+        change: self.change,
+        opts: opts,
+        userScript: false,
+        HA: false,
+        /*
+         * UUIDs of the binder instances with a zookeeper role of followers
+         *
+         */
+        followers: [],
+        /*
+         * UUID of the binder instance with zookeeper role of leader
+         */
+        leader: null,
+        /*
+         * Instance Object. Leader instance, in case that it will be
+         * updated during the current process.
+         */
+        leaderInst: null,
+        /*
+         * Array of Instance Objects. Follower instances that will be
+         * updated. Note that it might not be all of them.
+         */
+        followerInsts: [],
+        /*
+         * Is zookeeper running in standalone mode?
+         */
+        standalone: false
+    };
+
+    if (insts && insts.length > 1) {
+        arg.HA = true;
+    }
 
-        var funcs = [
-            function findZkLeader(_, next) {
-                progress('Looking for zk leader');
-                vasync.forEachParallel({
-                    inputs: insts,
-                    func: function zkInstStatus(inst, next_) {
-                        var c = format(
-                            'echo stat | nc %s 2181 | grep -i "mode"', inst.ip);
-                        common.execPlus({
-                            cmd: c,
-                            log: opts.log
-                        }, function (err, stdout, stderr) {
-                            if (err) {
-                                // The command throws an error while ZK is
-                                // transitioning from standalone to cluster
-                                next_(null, {
-                                    instance: inst,
-                                    mode: 'transitioning'
-                                });
-                            } else {
-                                next_(null, {
-                                    instance: inst,
-                                    mode: stdout.trim().replace(/^Mode:\s/, '')
-                                });
-                            }
-                        });
-                    }
-                }, function (err, res) {
-                    if (err) {
-                        return next(err);
-                    }
+    if (opts.plan.changes.length > 1) {
+        progress('');
+        progress('--- Updating %s ...', self.change.service.name);
+    }
 
-                    res.successes.filter(function (r) {
-                        if (r.mode === 'leader') {
-                            leader = r.instance;
-                        } else if (r.mode === 'standalone') {
-                            leader = r.instance;
-                            standalone = true;
+    var funcs = [
+        /*
+         * Do not rely into instances provided by change, make sure we get
+         * all the binder instances from VMAPI to find ZK leader.
+         */
+        function getAllBinderVms(ctx, next) {
+            progress('Getting Triton\'s binder instances');
+            sdcadm.listInsts({
+                svcs: ['binder']
+            }, function (instsErr, instances) {
+                if (instsErr) {
+                    next(instsErr);
+                    return;
+                }
+                ctx.allInsts = instances;
+                next();
+            });
+        },
+        /*
+         * We'll take advantage of this function to fill ctx.binderIps
+         * variable, since we'll need it for several steps below.
+         */
+        function findZkLeader(ctx, next) {
+            ctx.binderIps = [];
+            progress('Looking for zk leader');
+            vasync.forEachParallel({
+                inputs: ctx.allInsts,
+                func: function zkInstStatus(vm, next_) {
+                    ctx.binderIps.push(vm.ip);
+                    var c = format(
+                        'echo stat | nc %s 2181 | grep -i "mode"', vm.ip);
+                    common.execPlus({
+                        cmd: c,
+                        log: opts.log
+                    }, function (err, stdout, stderr) {
+                        if (err) {
+                            // The command throws an error while ZK is
+                            // transitioning from standalone to cluster
+                            next_(null, {
+                                instance: vm.zonename,
+                                mode: 'transitioning'
+                            });
                         } else {
-                            followers.push(r.instance);
+                            next_(null, {
+                                instance: vm.zonename,
+                                mode: stdout.trim().replace(/^Mode:\s/, '')
+                            });
                         }
                     });
+                }
+            }, function (err, res) {
+                if (err) {
+                    next(err);
+                    return;
+                }
 
-                    return next();
+                res.successes.filter(function (r) {
+                    if (r.mode === 'leader') {
+                        ctx.leader = r.instance;
+                    } else if (r.mode === 'standalone') {
+                        ctx.leader = r.instance;
+                        ctx.standalone = true;
+                    } else {
+                        ctx.followers.push(r.instance);
+                    }
                 });
-            }
-        ];
-
-        if (rollback) {
-            funcs.push(s.getOldUserScript);
-        } else {
-            funcs.push(s.getUserScript);
-            funcs.push(s.writeOldUserScriptForRollback);
+                next();
+            });
+        },
+        /*
+         * Once we know zookeeper roles, we can populate the variables
+         * ctx.leaderInst and ctx.followerInsts appropriately.
+         */
+        function classifyInstances(ctx, next) {
+            insts.forEach(function (ins) {
+                if (ins.zonename === ctx.leader) {
+                    ctx.leaderInst = ins;
+                } else {
+                    ctx.followerInsts.push(ins);
+                }
+            });
+            next();
         }
+    ];
 
+    if (rollback) {
+        funcs.push(s.getOldUserScript);
+    } else {
+        funcs.push(s.getUserScript);
+        funcs.push(s.writeOldUserScriptForRollback);
+    }
 
-        funcs.push(function bailIfNoDelegateDataset(_, next) {
-            vasync.forEachParallel({
-                func: function bailIfBinderHasNoDelegate(inst, next_) {
-                    var argv = [
-                        '/opt/smartdc/bin/sdc-oneachnode',
-                        '-j',
-                        format('-n %s ', inst.server),
-                        format('/usr/sbin/vmadm get %s', inst.zonename)
-                    ];
-                    var env = common.objCopy(process.env);
-                    // Get 'debug' level logging in imgadm >=2.6.0 without
-                    // triggering trace level logging in imgadm versions before
-                    // that. Trace level logging is too much here.
-                    env.IMGADM_LOG_LEVEL = 'debug';
-                    var execOpts = {
-                        encoding: 'utf8',
-                        env: env
-                    };
-                    log.trace({argv: argv}, 'Looking for vm dataset');
-                    function remoteCb(err, stdout, stderr) {
-                        if (err) {
-                            var msg = format(
-                                'error looking for vm dataset %s:\n' +
-                                '\targv: %j\n' +
-                                '\texit status: %s\n' +
-                                '\tstdout:\n%s\n' +
-                                '\tstderr:\n%s', inst.alias,
-                                argv, err.code, stdout.trim(), stderr.trim());
-                            return next_(new errors.InternalError({
-                                message: msg,
-                                cause: err
-                            }));
-                        }
-
-                        var expectedDs = sprintf('zones/%s/data',
-                                inst.zonename);
-                        var res = JSON.parse(stdout.trim())[0].result.stdout;
-                        var vm = JSON.parse(res);
-                        log.debug({
-                            expectedDs: expectedDs,
-                            vm: vm
-                        }, 'binder vm');
-
-                        if (vm.datasets.indexOf(expectedDs) === -1) {
-                            return next_(new errors.UpdateError(format(
-                                'binder vm %s has no "%s" delegate dataset, ' +
-                                'upgrading it would lose image file data',
-                                vm.uuid, expectedDs)));
-                        }
-                        next_();
 
+    funcs.push(function bailIfNoDelegateDataset(_, next) {
+        vasync.forEachParallel({
+            func: function bailIfBinderHasNoDelegate(inst, next_) {
+                var argv = [
+                    '/opt/smartdc/bin/sdc-oneachnode',
+                    '-j',
+                    format('-n %s ', inst.server),
+                    format('/usr/sbin/vmadm get %s', inst.zonename)
+                ];
+                var env = common.objCopy(process.env);
+                // Get 'debug' level logging in imgadm >=2.6.0 without
+                // triggering trace level logging in imgadm versions before
+                // that. Trace level logging is too much here.
+                env.IMGADM_LOG_LEVEL = 'debug';
+                var execOpts = {
+                    encoding: 'utf8',
+                    env: env
+                };
+                log.trace({argv: argv}, 'Looking for vm dataset');
+                function remoteCb(err, stdout, stderr) {
+                    if (err) {
+                        var msg = format(
+                            'error looking for vm dataset %s:\n' +
+                            '\targv: %j\n' +
+                            '\texit status: %s\n' +
+                            '\tstdout:\n%s\n' +
+                            '\tstderr:\n%s', inst.alias,
+                            argv, err.code, stdout.trim(), stderr.trim());
+                        next_(new errors.InternalError({
+                            message: msg,
+                            cause: err
+                        }));
+                        return;
                     }
-                    execFile(argv[0], argv.slice(1), execOpts, remoteCb);
-                },
-                inputs: insts
-            }, next);
-        });
-
-        funcs = funcs.concat([
-            s.updateSvcUserScript,
-
-            function updateVmsUserScript(_, next) {
-                vasync.forEachParallel({
-                    func: function (inst, next_) {
-                        s.updateVmUserScriptRemote({
-                            service: change.service,
-                            progress: progress,
-                            zonename: inst.zonename,
-                            log: opts.log,
-                            server: inst.server,
-                            userScript: arg.userScript
-                        }, next_);
-                    },
-                    inputs: insts
-                }, next);
-            },
 
-            s.updateSapiSvc,
-
-            function installVmsImg(_, next) {
-                // Pipeline, not parallel, just in case we have several
-                // instances on the same server:
-                vasync.forEachPipeline({
-                    inputs: insts,
-                    func: function installVmImg(inst, next_) {
-                        s.imgadmInstallRemote({
-                            progress: progress,
-                            img: change.image,
-                            log: opts.log,
-                            server: inst.server
-                        }, next_);
+                    var expectedDs = sprintf('zones/%s/data',
+                            inst.zonename);
+                    var res = JSON.parse(stdout.trim())[0].result.stdout;
+                    var vm = JSON.parse(res);
+                    log.debug({
+                        expectedDs: expectedDs,
+                        vm: vm
+                    }, 'binder vm');
+
+                    if (vm.datasets.indexOf(expectedDs) === -1) {
+                        next_(new errors.UpdateError(format(
+                            'binder vm %s has no "%s" delegate dataset, ' +
+                            'upgrading it would lose image file data',
+                            vm.uuid, expectedDs)));
+                        return;
                     }
-                }, next);
-            },
-
-            // Update everything but leader:
+                    next_();
 
-            function reprovisionFollowers(_, next) {
-                if (!arg.HA) {
-                    return next();
                 }
-                vasync.forEachPipeline({
-                    inputs: followers,
-                    func: function reprovFollower(inst, next_) {
-                        if (inst.image === change.image.uuid) {
-                            next_();
-                            return;
-                        }
-                        s.reprovisionRemote({
-                            server: inst.server,
-                            img: change.image,
-                            zonename: inst.zonename,
-                            progress: progress,
-                            log: opts.log,
-                            sdcadm: opts.sdcadm
-                        }, next_);
-                    }
-                }, next);
+                execFile(argv[0], argv.slice(1), execOpts, remoteCb);
             },
+            inputs: insts
+        }, next);
+    });
 
-            function waitFollowers(_, next) {
-                if (!arg.HA) {
-                    return next();
-                }
-                vasync.forEachPipeline({
-                    inputs: followers,
-                    func: function waitFollower(inst, next_) {
-                        progress('Wait (sleep) for %s instance %s to come up',
-                            change.service.name, inst.zonename);
-                        setTimeout(next_, 60 * 1000);
-                    }
-                }, next);
-            },
+    funcs = funcs.concat([
+        s.updateSvcUserScript,
 
-            function checkAllInstancesJoinedZkCluster(_, next) {
-                if (!arg.HA) {
-                    return next();
+        function updateVmsUserScript(ctx, next) {
+            vasync.forEachParallel({
+                func: function (inst, next_) {
+                    s.updateVmUserScriptRemote({
+                        service: ctx.change.service,
+                        progress: progress,
+                        zonename: inst.zonename,
+                        log: opts.log,
+                        server: inst.server,
+                        userScript: arg.userScript
+                    }, next_);
+                },
+                inputs: insts
+            }, next);
+        },
+
+        s.updateSapiSvc,
+
+        function installVmsImg(ctx, next) {
+            // Pipeline, not parallel, just in case we have several
+            // instances on the same server:
+            vasync.forEachPipeline({
+                inputs: insts,
+                func: function installVmImg(inst, next_) {
+                    s.imgadmInstallRemote({
+                        progress: progress,
+                        img: ctx.change.image,
+                        log: opts.log,
+                        server: inst.server
+                    }, next_);
                 }
-                progress('Waiting for zk instances to re-join ZK cluster');
-                var ips = insts.map(function (inst) {
-                    return (inst.ip);
-                });
+            }, next);
+        },
 
-                s.wait4ZkCluster({
-                    ips: ips,
-                    log: opts.log
-                }, next);
-            },
+        // Update everything but leader:
 
-            function waitForZkClusterOk(_, next) {
-                if (!arg.HA) {
-                    return next();
+        function reprovisionFollowers(ctx, next) {
+            if (!ctx.HA || !ctx.followers.length) {
+                next();
+                return;
+            }
+            vasync.forEachPipeline({
+                inputs: ctx.followerInsts,
+                func: function reprovFollower(inst, next_) {
+                    s.reprovisionRemote({
+                        server: inst.server,
+                        img: ctx.change.image,
+                        zonename: inst.zonename,
+                        progress: progress,
+                        log: opts.log,
+                        sdcadm: opts.sdcadm
+                    }, next_);
                 }
-                progress('Waiting for ZK cluster to reach a steady state');
-                var ips = insts.map(function (inst) {
-                    return (inst.ip);
-                });
-
-                s.wait4ZkOk({
-                    ips: ips,
-                    log: opts.log
-                }, next);
-            },
+            }, next);
+        },
 
-            function reprovisionLeader(_, next) {
-                if (!leader || leader.image === change.image.uuid) {
-                    next();
-                    return;
+        function waitFollowers(ctx, next) {
+            if (!ctx.HA || !ctx.followers.length) {
+                next();
+                return;
+            }
+            vasync.forEachPipeline({
+                inputs: ctx.followerInsts,
+                func: function waitFollower(inst, next_) {
+                    progress('Wait (sleep) for %s instance %s to come up',
+                        ctx.change.service.name, inst.zonename);
+                    setTimeout(next_, 60 * 1000);
                 }
-                progress('Updating ZK leader');
-                s.reprovisionRemote({
-                    server: leader.server,
-                    img: change.image,
-                    zonename: leader.zonename,
-                    progress: progress,
-                    log: opts.log,
-                    sdcadm: opts.sdcadm
-                }, next);
-            },
+            }, next);
+        },
 
-            function waitForLeader(_, next) {
-                if (!leader) {
-                    return next();
-                }
-                progress('Wait (sleep) for %s instance %s to come up',
-                    change.service.name, leader.zonename);
-                setTimeout(next, 60 * 1000);
-            },
+        function checkAllInstancesJoinedZkCluster(ctx, next) {
+            if (!ctx.HA) {
+                next();
+                return;
+            }
+            progress('Waiting for zk instances to re-join ZK cluster');
 
-            function checkAgainAllInstancesJoinedZkCluster(_, next) {
-                if (!leader || standalone) {
-                    return next();
-                }
-                progress('Waiting for zk leader to re-join ZK cluster');
-                var ips = insts.map(function (inst) {
-                    return (inst.ip);
-                });
+            s.wait4ZkCluster({
+                ips: ctx.binderIps,
+                log: opts.log
+            }, next);
+        },
 
-                s.wait4ZkCluster({
-                    ips: ips,
-                    log: opts.log
-                }, next);
-            },
+        function waitForZkClusterOk(ctx, next) {
+            if (!ctx.HA) {
+                next();
+                return;
+            }
+            progress('Waiting for ZK cluster to reach a steady state');
 
-            function waitAgainForZkClusterOk(_, next) {
-                if (!leader || standalone) {
-                    return next();
-                }
-                progress('Waiting for zk leader to reach a steady state');
-                var ips = insts.map(function (inst) {
-                    return (inst.ip);
-                });
+            s.wait4ZkOk({
+                ips: ctx.binderIps,
+                log: opts.log
+            }, next);
+        },
 
-                s.wait4ZkOk({
-                    ips: ips,
-                    log: opts.log
-                }, next);
+        function reprovisionLeader(ctx, next) {
+            if (!ctx.leader) {
+                next();
+                return;
             }
-        ]);
-        vasync.pipeline({funcs: funcs, arg: arg}, nextSvc);
-    }
+            progress('Updating ZK leader');
+            s.reprovisionRemote({
+                server: ctx.leaderInst.server,
+                img: ctx.change.image,
+                zonename: ctx.leaderInst.zonename,
+                progress: progress,
+                log: opts.log,
+                sdcadm: opts.sdcadm
+            }, next);
+        },
 
-    vasync.forEachPipeline({
-        inputs: self.changes,
-        func: updateBinder
-    }, cb);
+        function waitForLeader(ctx, next) {
+            if (!ctx.leader) {
+                return next();
+            }
+            progress('Wait (sleep) for %s instance %s to come up',
+                ctx.change.service.name, ctx.leader);
+            setTimeout(next, 60 * 1000);
+        },
+
+        function checkAgainAllInstancesJoinedZkCluster(ctx, next) {
+            if (!ctx.leader || ctx.standalone) {
+                next();
+                return;
+            }
+            progress('Waiting for zk leader to re-join ZK cluster');
+
+            s.wait4ZkCluster({
+                ips: ctx.binderIps,
+                log: opts.log
+            }, next);
+        },
+
+        function waitAgainForZkClusterOk(ctx, next) {
+            if (!ctx.leader || ctx.standalone) {
+                next();
+                return;
+            }
+            progress('Waiting for zk leader to reach a steady state');
+
+            s.wait4ZkOk({
+                ips: ctx.binderIps,
+                log: opts.log
+            }, next);
+        }
+    ]);
+    vasync.pipeline({funcs: funcs, arg: arg}, cb);
 };
 //---- exports
 
