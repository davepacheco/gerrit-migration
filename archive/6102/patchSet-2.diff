From 634ab320761fbf35b23c71268dba46bfc1ee84f3 Mon Sep 17 00:00:00 2001
From: David Pacheco <dap@joyent.com>
Date: Tue, 16 Apr 2019 16:34:02 -0700
Subject: [PATCH] MANTA-4218 document SharksExhaustedError in debugging guide

---
 0002-investigation-tasks.adoc | 101 +++++++++++++++++++++++
 0003-quick-reference.adoc     |   2 +-
 docs/index.html               | 149 +++++++++++++++++++++++++++++++++-
 3 files changed, 249 insertions(+), 3 deletions(-)

diff --git a/0002-investigation-tasks.adoc b/0002-investigation-tasks.adoc
index fce0d82..0b2380e 100644
--- a/0002-investigation-tasks.adoc
+++ b/0002-investigation-tasks.adoc
@@ -346,6 +346,10 @@ also use the `json` tool to filter and aggregate them.
 
 **For more information, see <<_muskie_log_entry_properties>>.**
 
+**If you have a particular Muskie log entry with an error message in it and you
+want to know more about the error, see
+<<_details_about_specific_error_messages>>.**
+
 ==== Contents of a GET log entry
 
 Below is an example log entry for a **GET request**, formatted using the `bunyan`
@@ -844,6 +848,103 @@ HTTP requests on the TCP connection.
 
 ==== "No storage nodes available for this request"
 
+Certain underlying conditions (described below) result in the following
+symptoms:
+
+- an error (reported either from the client or server) with message "No storage
+  nodes available for this request"
+- a 503 response code (reported either from the client or server), likely having
+  a "Retry-After" header
+- a client-reported error called "InternalError"
+- a server-reported error called "SharksExhaustedError"
+
+If you see these symptoms, read on for more information.
+
+NOTE: This section does not apply to all 503 response codes nor all instances of
+InternalError.  However, the condition described here always produces a 503
+response code, an InternalError reported to the client, and a
+SharksExhaustedError reported internally, and both the client and the server
+errors always include this message.
+
+These symptoms occur when Manta gave up on an object upload request (i.e., a PUT
+request) while trying to connect to storage nodes to store copies of the
+requested object.  In many cases, retrying the request is likely to succeed
+because different storage nodes will be selected that are likely to be
+available.  For that reason, the response generally includes a `Retry-After`
+HTTP header.  If the problem is persistent, there may be an unusual problem
+affecting a lot of storage nodes (e.g., a whole datacenter partition).
+
+More specifically: for each upload request, Manta selects up to three _sets_ of
+storage nodes that will be used to store copies of the object.  Each set
+contains as many storage nodes as there will be copies of the object (two by
+default, but this can be overridden by the client).  Manta first attempts to
+initiate an object upload to all the storage nodes in the first set.  If any of
+those fails, it moves on to the second set, and so on until all sets are
+exhausted, at which point the request fails.
+
+Based on the way this works, it's possible for Manta to report this failure even
+when most of the system is functioning normally -- it only takes three storage
+node failures.  Keep in mind, though, that when Manta selects storage nodes for
+these sets (before even attempting to connect to them), it only selects storage
+nodes that have reported their own capacity relatively recently (typically
+within the last minute or so), so it's expected that most of them should be
+online and functioning.  That's why retries are likely to succeed and why
+persistent occurrence of this problem may indicate a more unusual network issue.
+
+**To understand the problem in more detail:**
+
+- First, follow the steps to
+  <<_investigating_a_specific_request_that_has_failed,find the Muskie log entry
+  for the failed request>>.
+- The log entry for object uploads includes a property called `sharksContacted`,
+  which describes each of the sharks (storage nodes) that Muskie attempted to
+  reach.  Each entry indicates the time Manta started trying to reach it, how
+  long it took (if successful), and what the result was (e.g., `ok`).  See the
+  entry for `sharksContacted` under
+  <<_muskie_provided_properties_for_debugging_only>> for an example.  Also see
+  <<_understanding_a_muskie_log_entry>>.
+
+Using the `sharksContacted` property, you can <<_build_a_request_timeline>>.
+You should be able to tell from this property which storage nodes were part of
+which set (e.g., the first set of two, the second set of two, and so on).  This
+will allow you to confirm that Manta failed to reach at least one of the storage
+nodes in each set, and it will indicate which storage nodes it failed to
+contact.
+
+**What if there are fewer than three sets of storage nodes?**  This can happen
+when there aren't enough storage nodes meeting the desired storage requirements
+(e.g., that copies are in different datacenters).  In small deployments, this
+can lead to https://smartos.org/bugview/MANTA-3454[higher error rates than
+expected].
+
+Once you've determined which storage nodes Manta failed to reach, you can dig
+further into why Manta failed to reach them.  This is largely beyond the
+scope of this guide, but below are some questions you might start with:
+
+- **Was the storage node actually online at this time?**  You may be able to use
+  logs in the zone (e.g., `/var/log/mako-access.log`) or the output of commands
+  like `last reboot` or `uptime` to demonstrate that the zone was online (or
+  not).  Similarly, you could look for evidence that the server itself was
+  online using logs or these same commands from the global zone.  You could look
+  for crash dumps (indicating an operating system panic) in
+  `/var/crash/volatile` in the global zone or looking at logs in `/var/log` in
+  the global zone.
+- **Was the storage node reachable at this time?**  This can be difficult to
+  debug after the fact, but you can look for evidence in some of these same logs
+  (e.g., `/var/log/mako-access.log`).  Evidence of serving requests indicates at
+  least _some_ remote systems were able to reach it.  A sudden drop in request
+  activity is consistent with network problems.  However, many of the symptoms
+  of network issues (including disconnects, timeouts, and reset connections) are
+  also consistent with systemic problems that caused important programs to stop
+  running for an extended period (e.g., major I/O problems or problems with the
+  operating system).
+
+If you're debugging the problem right after it happened or if it's currently
+reproducible, these questions are easier to answer.  For example, if you can log
+into a zone, then it's not offline.  You can check for network connectivity with
+tools like `ping(1)`.
+
+
 ==== "Not enough free space for ... MB"
 
 This message (associated with 507 errors) indicates that Manta does not have
diff --git a/0003-quick-reference.adoc b/0003-quick-reference.adoc
index 9ae3386..3e6157d 100644
--- a/0003-quick-reference.adoc
+++ b/0003-quick-reference.adoc
@@ -253,7 +253,7 @@ a|
 
 |`err`
 |`false`
-|Error associated with this request, if any.
+|Error associated with this request, if any.  See <<_details_about_specific_error_messages>>.
 
 |`objectId`
 |`"bf54fb8a-6cb5-4683-8655-f9ad90b984d4"`
diff --git a/docs/index.html b/docs/index.html
index fe394a5..e7e70e6 100644
--- a/docs/index.html
+++ b/docs/index.html
@@ -1953,6 +1953,11 @@ also use the <code>json</code> tool to filter and aggregate them.</p>
 <div class="paragraph">
 <p><strong>For more information, see <a href="#_muskie_log_entry_properties">Muskie log entry properties</a>.</strong></p>
 </div>
+<div class="paragraph">
+<p><strong>If you have a particular Muskie log entry with an error message in it and you
+want to know more about the error, see
+<a href="#_details_about_specific_error_messages">Details about specific error messages</a>.</strong></p>
+</div>
 <div class="sect3">
 <h4 id="_contents_of_a_get_log_entry">Contents of a GET log entry</h4>
 <div class="paragraph">
@@ -2547,7 +2552,147 @@ HTTP requests on the TCP connection.</p>
 </div>
 <div class="sect3">
 <h4 id="_no_storage_nodes_available_for_this_request">"No storage nodes available for this request"</h4>
-
+<div class="paragraph">
+<p>Certain underlying conditions (described below) result in the following
+symptoms:</p>
+</div>
+<div class="ulist">
+<ul>
+<li>
+<p>an error (reported either from the client or server) with message "No storage
+nodes available for this request"</p>
+</li>
+<li>
+<p>a 503 response code (reported either from the client or server), likely having
+a "Retry-After" header</p>
+</li>
+<li>
+<p>a client-reported error called "InternalError"</p>
+</li>
+<li>
+<p>a server-reported error called "SharksExhaustedError"</p>
+</li>
+</ul>
+</div>
+<div class="paragraph">
+<p>If you see these symptoms, read on for more information.</p>
+</div>
+<div class="admonitionblock note">
+<table>
+<tr>
+<td class="icon">
+<div class="title">Note</div>
+</td>
+<td class="content">
+This section does not apply to all 503 response codes nor all instances of
+InternalError.  However, the condition described here always produces a 503
+response code, an InternalError reported to the client, and a
+SharksExhaustedError reported internally, and both the client and the server
+errors always include this message.
+</td>
+</tr>
+</table>
+</div>
+<div class="paragraph">
+<p>These symptoms occur when Manta gave up on an object upload request (i.e., a PUT
+request) while trying to connect to storage nodes to store copies of the
+requested object.  In many cases, retrying the request is likely to succeed
+because different storage nodes will be selected that are likely to be
+available.  For that reason, the response generally includes a <code>Retry-After</code>
+HTTP header.  If the problem is persistent, there may be an unusual problem
+affecting a lot of storage nodes (e.g., a whole datacenter partition).</p>
+</div>
+<div class="paragraph">
+<p>More specifically: for each upload request, Manta selects up to three <em>sets</em> of
+storage nodes that will be used to store copies of the object.  Each set
+contains as many storage nodes as there will be copies of the object (two by
+default, but this can be overridden by the client).  Manta first attempts to
+initiate an object upload to all the storage nodes in the first set.  If any of
+those fails, it moves on to the second set, and so on until all sets are
+exhausted, at which point the request fails.</p>
+</div>
+<div class="paragraph">
+<p>Based on the way this works, it&#8217;s possible for Manta to report this failure even
+when most of the system is functioning normally&#8201;&#8212;&#8201;it only takes three storage
+node failures.  Keep in mind, though, that when Manta selects storage nodes for
+these sets (before even attempting to connect to them), it only selects storage
+nodes that have reported their own capacity relatively recently (typically
+within the last minute or so), so it&#8217;s expected that most of them should be
+online and functioning.  That&#8217;s why retries are likely to succeed and why
+persistent occurrence of this problem may indicate a more unusual network issue.</p>
+</div>
+<div class="paragraph">
+<p><strong>To understand the problem in more detail:</strong></p>
+</div>
+<div class="ulist">
+<ul>
+<li>
+<p>First, follow the steps to
+<a href="#_investigating_a_specific_request_that_has_failed">find the Muskie log entry
+for the failed request</a>.</p>
+</li>
+<li>
+<p>The log entry for object uploads includes a property called <code>sharksContacted</code>,
+which describes each of the sharks (storage nodes) that Muskie attempted to
+reach.  Each entry indicates the time Manta started trying to reach it, how
+long it took (if successful), and what the result was (e.g., <code>ok</code>).  See the
+entry for <code>sharksContacted</code> under
+<a href="#_muskie_provided_properties_for_debugging_only">Muskie-provided properties for debugging only</a> for an example.  Also see
+<a href="#_understanding_a_muskie_log_entry">Understanding a Muskie log entry</a>.</p>
+</li>
+</ul>
+</div>
+<div class="paragraph">
+<p>Using the <code>sharksContacted</code> property, you can <a href="#_build_a_request_timeline">Build a request timeline</a>.
+You should be able to tell from this property which storage nodes were part of
+which set (e.g., the first set of two, the second set of two, and so on).  This
+will allow you to confirm that Manta failed to reach at least one of the storage
+nodes in each set, and it will indicate which storage nodes it failed to
+contact.</p>
+</div>
+<div class="paragraph">
+<p><strong>What if there are fewer than three sets of storage nodes?</strong>  This can happen
+when there aren&#8217;t enough storage nodes meeting the desired storage requirements
+(e.g., that copies are in different datacenters).  In small deployments, this
+can lead to <a href="https://smartos.org/bugview/MANTA-3454">higher error rates than
+expected</a>.</p>
+</div>
+<div class="paragraph">
+<p>Once you&#8217;ve determined which storage nodes Manta failed to reach, you can dig
+further into why Manta failed to reach them.  This is largely beyond the
+scope of this guide, but below are some questions you might start with:</p>
+</div>
+<div class="ulist">
+<ul>
+<li>
+<p><strong>Was the storage node actually online at this time?</strong>  You may be able to use
+logs in the zone (e.g., <code>/var/log/mako-access.log</code>) or the output of commands
+like <code>last reboot</code> or <code>uptime</code> to demonstrate that the zone was online (or
+not).  Similarly, you could look for evidence that the server itself was
+online using logs or these same commands from the global zone.  You could look
+for crash dumps (indicating an operating system panic) in
+<code>/var/crash/volatile</code> in the global zone or looking at logs in <code>/var/log</code> in
+the global zone.</p>
+</li>
+<li>
+<p><strong>Was the storage node reachable at this time?</strong>  This can be difficult to
+debug after the fact, but you can look for evidence in some of these same logs
+(e.g., <code>/var/log/mako-access.log</code>).  Evidence of serving requests indicates at
+least <em>some</em> remote systems were able to reach it.  A sudden drop in request
+activity is consistent with network problems.  However, many of the symptoms
+of network issues (including disconnects, timeouts, and reset connections) are
+also consistent with systemic problems that caused important programs to stop
+running for an extended period (e.g., major I/O problems or problems with the
+operating system).</p>
+</li>
+</ul>
+</div>
+<div class="paragraph">
+<p>If you&#8217;re debugging the problem right after it happened or if it&#8217;s currently
+reproducible, these questions are easier to answer.  For example, if you can log
+into a zone, then it&#8217;s not offline.  You can check for network connectivity with
+tools like <code>ping(1)</code>.</p>
+</div>
 </div>
 <div class="sect3">
 <h4 id="_not_enough_free_space_for_mb">"Not enough free space for &#8230;&#8203; MB"</h4>
@@ -3406,7 +3551,7 @@ HTTP request.  Other log entries have other fields.)</p>
 <tr>
 <td class="tableblock halign-left valign-top"><p class="tableblock"><code>err</code></p></td>
 <td class="tableblock halign-left valign-top"><p class="tableblock"><code>false</code></p></td>
-<td class="tableblock halign-left valign-top"><p class="tableblock">Error associated with this request, if any.</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Error associated with this request, if any.  See <a href="#_details_about_specific_error_messages">Details about specific error messages</a>.</p></td>
 </tr>
 <tr>
 <td class="tableblock halign-left valign-top"><p class="tableblock"><code>objectId</code></p></td>
-- 
2.21.0

