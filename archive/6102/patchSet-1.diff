commit 2647a057087bacc82ec461d6d6894484164c2074
Author: David Pacheco <dap@joyent.com>
Date:   2019-04-11T14:28:04-07:00 (6 months ago)
    
    MANTA-4218 document SharksExhaustedError in debugging guide

diff --git a/0002-investigation-tasks.adoc b/0002-investigation-tasks.adoc
index fce0d82..bf82698 100644
--- a/0002-investigation-tasks.adoc
+++ b/0002-investigation-tasks.adoc
@@ -844,6 +844,95 @@ HTTP requests on the TCP connection.
 
 ==== "No storage nodes available for this request"
 
+Under some conditions, you may see the following:
+
+- a 503 response code (reported either from the client or server), likely having
+  a "Retry-After" header
+- a client-reported error called "InternalError"
+- a server-reported error called "SharksExhaustedError"
+- an error with message "No storage nodes available for this request"
+
+NOTE: This section does not apply to all 503 response codes nor all instances of
+InternalError.  However, the condition described here always produces a 503
+response code, an InternalError reported to the client, and a
+SharksExhaustedError reported internally, and both errors always have this
+message.
+
+This error is associated with object uploads (i.e., PUT operations).  The error
+means that Manta gave up trying to connect to storage nodes to store the
+requested object.  In many cases, retrying the request is likely to succeed.
+For that reason, the response generally includes a `Retry-After` HTTP header.
+
+For each upload request, Manta selects up to three _sets_ of storage nodes that
+will be used to store copies of the object.  Each set contains as many storage
+nodes as there will be copies of the object (two by default, but this can be
+overridden by the client).  Manta first attempts to initiate an object upload to
+all the storage nodes in the first set.  If any of those fails, it moves on to
+the second set, and so on until all sets are exhausted, at which point the
+request fails.
+
+Based on the way this works, it's possible for Manta to report this failure even
+when most of the system is functioning normally -- it only takes three storage
+node failures.  Keep in mind, though, that when Manta selects storage nodes for
+these sets (before even attempting to connect to them), it only selects storage
+nodes that have reported their own capacity relatively recently (typically
+within the last minute or so), so it's expected that most of them should be
+online and functioning.
+
+**To understand the problem in more detail:**
+
+- First, follow the steps to
+  <<_investigating_a_specific_request_that_has_failed,find the Muskie log entry
+  for the failed request>>.
+- The log entry for object uploads includes a property called `sharksContacted`,
+  which describes each of the sharks (storage nodes) that Muskie attempted to
+  reach.  Each entry indicates the time Manta started trying to reach it, how
+  long it took (if successful), and what the result was (e.g., `ok`).  See the
+  entry for `sharksContacted` under
+  <<_muskie_provided_properties_for_debugging_only>> for an example.  Also see
+  <<_understanding_a_muskie_log_entry>>.
+
+Using the `sharksContacted` property, you can <<_build_a_request_timeline>>.
+You should be able to tell from this property which storage nodes were part of
+which set (e.g., the first set of two, the second set of two, and so on).  This
+will allow you to confirm that Manta failed to reach at least one of the storage
+nodes in each set, and it will indicate which storage nodes it failed to
+contact.
+
+**What if there are fewer than three sets of storage nodes?**  This can happen
+when there aren't enough storage nodes meeting the desired storage requirements
+(e.g., that copies are in different datacenters).  In small deployments, this
+can lead to https://smartos.org/bugview/MANTA-3454[higher error rates than
+expected].
+
+Once you've determined which storage nodes Manta failed to reach, you can dig
+further into why Manta failed to reach them, but this is largely beyond the
+scope of this guide.  Questions to ask:
+
+- **Was the storage node actually online at this time?**  You may be able to use
+  logs in the zone (e.g., `/var/log/mako-access.log`) or the output of commands
+  like `last reboot` or `uptime` to demonstrate that the zone was online (or
+  not).  Similarly, you could look for evidence that the server itself was
+  online using logs or these same commands from the global zone.  You could look
+  for crash dumps (indicating an operating system panic) in
+  `/var/crash/volatile` in the global zone or looking at logs in `/var/log` in
+  the global zone.
+- **Was the storage node reachable at this time?**  This can be difficult to
+  debug after the fact, but you can look for evidence in some of these same logs
+  (e.g., `/var/log/mako-access.log`).  Evidence of serving requests indicates at
+  least _some_ remote systems were able to reach it.  A sudden drop in request
+  activity is consistent with network problems.  However, many of the symptoms
+  of network issues (including disconnects, timeouts, and reset connections) are
+  also consistent with systemic problems that caused important programs to stop
+  running for an extended period (e.g., major I/O problems or problems with the
+  operating system).
+
+If you're debugging the problem right after it happened or if it's currently
+reproducible, these questions are easier to answer.  For example, if you can log
+into a zone, then it's not offline.  You can check for network connectivity with
+tools like `ping(1)`.
+
+
 ==== "Not enough free space for ... MB"
 
 This message (associated with 507 errors) indicates that Manta does not have
diff --git a/docs/index.html b/docs/index.html
index fe394a5..8d64725 100644
--- a/docs/index.html
+++ b/docs/index.html
@@ -2547,7 +2547,138 @@ HTTP requests on the TCP connection.</p>
 </div>
 <div class="sect3">
 <h4 id="_no_storage_nodes_available_for_this_request">"No storage nodes available for this request"</h4>
-
+<div class="paragraph">
+<p>Under some conditions, you may see the following:</p>
+</div>
+<div class="ulist">
+<ul>
+<li>
+<p>a 503 response code (reported either from the client or server), likely having
+a "Retry-After" header</p>
+</li>
+<li>
+<p>a client-reported error called "InternalError"</p>
+</li>
+<li>
+<p>a server-reported error called "SharksExhaustedError"</p>
+</li>
+<li>
+<p>an error with message "No storage nodes available for this request"</p>
+</li>
+</ul>
+</div>
+<div class="admonitionblock note">
+<table>
+<tr>
+<td class="icon">
+<div class="title">Note</div>
+</td>
+<td class="content">
+This section does not apply to all 503 response codes nor all instances of
+InternalError.  However, the condition described here always produces a 503
+response code, an InternalError reported to the client, and a
+SharksExhaustedError reported internally, and both errors always have this
+message.
+</td>
+</tr>
+</table>
+</div>
+<div class="paragraph">
+<p>This error is associated with object uploads (i.e., PUT operations).  The error
+means that Manta gave up trying to connect to storage nodes to store the
+requested object.  In many cases, retrying the request is likely to succeed.
+For that reason, the response generally includes a <code>Retry-After</code> HTTP header.</p>
+</div>
+<div class="paragraph">
+<p>For each upload request, Manta selects up to three <em>sets</em> of storage nodes that
+will be used to store copies of the object.  Each set contains as many storage
+nodes as there will be copies of the object (two by default, but this can be
+overridden by the client).  Manta first attempts to initiate an object upload to
+all the storage nodes in the first set.  If any of those fails, it moves on to
+the second set, and so on until all sets are exhausted, at which point the
+request fails.</p>
+</div>
+<div class="paragraph">
+<p>Based on the way this works, it&#8217;s possible for Manta to report this failure even
+when most of the system is functioning normally&#8201;&#8212;&#8201;it only takes three storage
+node failures.  Keep in mind, though, that when Manta selects storage nodes for
+these sets (before even attempting to connect to them), it only selects storage
+nodes that have reported their own capacity relatively recently (typically
+within the last minute or so), so it&#8217;s expected that most of them should be
+online and functioning.</p>
+</div>
+<div class="paragraph">
+<p><strong>To understand the problem in more detail:</strong></p>
+</div>
+<div class="ulist">
+<ul>
+<li>
+<p>First, follow the steps to
+<a href="#_investigating_a_specific_request_that_has_failed">find the Muskie log entry
+for the failed request</a>.</p>
+</li>
+<li>
+<p>The log entry for object uploads includes a property called <code>sharksContacted</code>,
+which describes each of the sharks (storage nodes) that Muskie attempted to
+reach.  Each entry indicates the time Manta started trying to reach it, how
+long it took (if successful), and what the result was (e.g., <code>ok</code>).  See the
+entry for <code>sharksContacted</code> under
+<a href="#_muskie_provided_properties_for_debugging_only">Muskie-provided properties for debugging only</a> for an example.  Also see
+<a href="#_understanding_a_muskie_log_entry">Understanding a Muskie log entry</a>.</p>
+</li>
+</ul>
+</div>
+<div class="paragraph">
+<p>Using the <code>sharksContacted</code> property, you can <a href="#_build_a_request_timeline">Build a request timeline</a>.
+You should be able to tell from this property which storage nodes were part of
+which set (e.g., the first set of two, the second set of two, and so on).  This
+will allow you to confirm that Manta failed to reach at least one of the storage
+nodes in each set, and it will indicate which storage nodes it failed to
+contact.</p>
+</div>
+<div class="paragraph">
+<p><strong>What if there are fewer than three sets of storage nodes?</strong>  This can happen
+when there aren&#8217;t enough storage nodes meeting the desired storage requirements
+(e.g., that copies are in different datacenters).  In small deployments, this
+can lead to <a href="https://smartos.org/bugview/MANTA-3454">higher error rates than
+expected</a>.</p>
+</div>
+<div class="paragraph">
+<p>Once you&#8217;ve determined which storage nodes Manta failed to reach, you can dig
+further into why Manta failed to reach them, but this is largely beyond the
+scope of this guide.  Questions to ask:</p>
+</div>
+<div class="ulist">
+<ul>
+<li>
+<p><strong>Was the storage node actually online at this time?</strong>  You may be able to use
+logs in the zone (e.g., <code>/var/log/mako-access.log</code>) or the output of commands
+like <code>last reboot</code> or <code>uptime</code> to demonstrate that the zone was online (or
+not).  Similarly, you could look for evidence that the server itself was
+online using logs or these same commands from the global zone.  You could look
+for crash dumps (indicating an operating system panic) in
+<code>/var/crash/volatile</code> in the global zone or looking at logs in <code>/var/log</code> in
+the global zone.</p>
+</li>
+<li>
+<p><strong>Was the storage node reachable at this time?</strong>  This can be difficult to
+debug after the fact, but you can look for evidence in some of these same logs
+(e.g., <code>/var/log/mako-access.log</code>).  Evidence of serving requests indicates at
+least <em>some</em> remote systems were able to reach it.  A sudden drop in request
+activity is consistent with network problems.  However, many of the symptoms
+of network issues (including disconnects, timeouts, and reset connections) are
+also consistent with systemic problems that caused important programs to stop
+running for an extended period (e.g., major I/O problems or problems with the
+operating system).</p>
+</li>
+</ul>
+</div>
+<div class="paragraph">
+<p>If you&#8217;re debugging the problem right after it happened or if it&#8217;s currently
+reproducible, these questions are easier to answer.  For example, if you can log
+into a zone, then it&#8217;s not offline.  You can check for network connectivity with
+tools like <code>ping(1)</code>.</p>
+</div>
 </div>
 <div class="sect3">
 <h4 id="_not_enough_free_space_for_mb">"Not enough free space for &#8230;&#8203; MB"</h4>
