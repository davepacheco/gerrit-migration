From fef69c82fbe023f7d89761f616d21412404527f4 Mon Sep 17 00:00:00 2001
From: Cody Peter Mello <cody.mello@joyent.com>
Date: Wed, 5 Apr 2017 20:35:34 +0000
Subject: [PATCH] joyent/node-fast#5 Server shutdown crashes when a connection
 had a socket error

---
 CHANGES.md         |  4 +++
 lib/fast_server.js |  6 ++---
 package.json       |  2 +-
 test/tst.server.js | 61 ++++++++++++++++++++++++++++++++++++++++++++++
 4 files changed, 69 insertions(+), 4 deletions(-)

diff --git a/CHANGES.md b/CHANGES.md
index 6d9a720..a88da5d 100644
--- a/CHANGES.md
+++ b/CHANGES.md
@@ -4,6 +4,10 @@
 
 No changes.
 
+## v2.2.3
+
+* #5 server could crash in certain conditions after a connection error
+
 ## v2.2.2
 
 * #4 client should preserve "ase\_errors" for Errors
diff --git a/lib/fast_server.js b/lib/fast_server.js
index 7013da3..5282982 100644
--- a/lib/fast_server.js
+++ b/lib/fast_server.js
@@ -31,7 +31,7 @@
  * The RPC context argument provided to RPC method handlers is itself an
  * object-mode stream.  Objects written to the stream are sent to the client.
  * When the stream is ended, an "end" message is written to the client, which
- * signifies the successfuly completion of the RPC call.  The stream pipeline
+ * signifies the successful completion of the RPC call.  The stream pipeline
  * from the RPC context to the client socket fully supports flow control, so the
  * stream will used a bounded amount of memory as long as the RPC method
  * respects Node's flow-control semantics (e.g., stops writing to the stream
@@ -60,7 +60,7 @@
  * in an O(N^2) factor in the overall time to complete N requests.  The Node API
  * does not seem fixable.  We could implement a tree of EventEmitters that
  * funnel into the socket, but it's not clear at this point that this
- * considertaion is worthwhile.  In practice, Fast servers are more likely to
+ * consideration is worthwhile.  In practice, Fast servers are more likely to
  * see many clients making a small number of requests each rather than a small
  * number of clients making a large number of requests each, and this deployment
  * model is recommended to avoid this scaling limiter.
@@ -501,7 +501,7 @@ FastServer.prototype.connTerminate = function (conn, err)
 	mod_assertplus.ok(conn instanceof FastRpcConnection);
 	mod_assertplus.ok(err instanceof Error);
 
-	if (conn.fc_server_error === null) {
+	if (conn.fc_server_error === null && conn.fc_socket_error === null) {
 		conn.fc_log.warn(err, 'gracefully terminating connection');
 		conn.fc_server_error = err;
 		this.connDisconnectRequests(conn);
diff --git a/package.json b/package.json
index be5c652..66d7c62 100644
--- a/package.json
+++ b/package.json
@@ -1,7 +1,7 @@
 {
 	"name": "fast",
 	"description": "streaming JSON RPC over TCP",
-	"version": "2.2.2",
+	"version": "2.2.3",
 	"main": "./lib/fast.js",
 	"repository": {
 		"type": "git",
diff --git a/test/tst.server.js b/test/tst.server.js
index af24292..357e33a 100644
--- a/test/tst.server.js
+++ b/test/tst.server.js
@@ -52,6 +52,7 @@ function main()
 function ServerTestContext()
 {
 	this.ts_log = null;	/* bunyan logger */
+	this.ts_closed = false;	/* whether we've already done cleanup */
 	this.ts_socket = null;	/* server net socket */
 	this.ts_server = null;	/* fast server object */
 	this.ts_clients = [];	/* array of clients, each having properties */
@@ -93,6 +94,12 @@ ServerTestContext.prototype.firstFastClient = function ()
 
 ServerTestContext.prototype.cleanup = function ()
 {
+	if (this.ts_closed) {
+		return;
+	}
+
+	this.ts_closed = true;
+
 	this.ts_clients.forEach(function (c) {
 		c.tsc_client.detach();
 		c.tsc_socket.destroy();
@@ -656,6 +663,60 @@ serverTestCases = [ {
 	}, callback);
     }
 
+}, {
+    'name': 'connection error followed by server shutdown',
+    'run': function (tctx, callback) {
+	var c = tctx.ts_clients.shift();
+	var log = tctx.ts_log;
+	var rpc;
+
+	/* Save the RPC handle, and reply so we know we can continue. */
+	function saveRPC(r) {
+		r.write({ start: true });
+		rpc = r;
+	}
+
+	tctx.ts_server.registerRpcMethod({
+		rpcmethod: 'block',
+		rpchandler: saveRPC
+	});
+
+	/* Kick off an RPC, and wait until it's started. */
+	var req = c.tsc_client.rpc({
+		rpcmethod: 'block',
+		rpcargs: [ ]
+	});
+
+	/* Ignore (but log) the expected transport error. */
+	req.on('error', function (err) {
+		log.info({ err: err }, 'received expected error');
+	});
+
+	/* Kill the RPC once it's started. */
+	req.once('data', function () {
+		setImmediate(killRPC);
+	});
+
+	function killRPC() {
+		/* Disconnect client so we'll get EPIPE below. */
+		c.tsc_client.detach();
+		c.tsc_socket.destroy();
+
+		/* Attempt socket write so onConnectionError() gets called. */
+		rpc.write({ kaboom: 'error' });
+		rpc.end();
+
+		/* Now run the server clean up, which shouldn't crash. */
+		tctx.ts_socket.on('close', function () {
+			tctx.ts_server.close();
+			tctx.ts_closed = true;
+
+			callback();
+		});
+		tctx.ts_socket.close();
+	}
+    }
+
 }, {
     'name': 'basic RPC with immediate end-of-stream',
     'run': function (tctx, callback) {
-- 
2.21.0

