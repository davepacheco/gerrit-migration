commit 84cdf9c42efbad766a4629a67c132694038174c9 (refs/changes/93/2893/1)
Author: Marsell Kukuljevic <marsell@joyent.com>
Date:   2017-11-02T00:23:53+13:00 (1 year, 11 months ago)
    
    DOCKER-1054: Provide provisioning limits plugin for sdc-docker similar to cloudapi functionality

diff --git a/Makefile b/Makefile
index 3e67c0b..049e2ea 100644
--- a/Makefile
+++ b/Makefile
@@ -16,7 +16,7 @@ RESTDOWN_FLAGS   = --brand-dir=deps/restdown-brand-remora
 
 TAPE	:= ./node_modules/.bin/tape
 
-JS_FILES	:= $(shell find lib test -name '*.js' | grep -v '/tmp/')
+JS_FILES	:= $(shell find lib plugins test -name '*.js' | grep -v '/tmp/')
 JSL_CONF_NODE	 = tools/jsl.node.conf
 JSL_FILES_NODE	 = $(JS_FILES)
 JSSTYLE_FILES	 = $(JS_FILES)
diff --git a/lib/backends/sdc/containers.js b/lib/backends/sdc/containers.js
index fa934ec..b9e4bb9 100644
--- a/lib/backends/sdc/containers.js
+++ b/lib/backends/sdc/containers.js
@@ -1253,10 +1253,10 @@ function getMemoryTarget(opts, container) {
  *
  * The callback is called:
  *
- *   callback(err, package_uuid)
+ *   callback(err, package)
  *
  * Where err is an Error object if no package was found that matches the
- * criteria, and package_uuid is passed if a package was found.
+ * criteria, and package is passed if a package was found.
  */
 function getClosestMemoryPackage(opts, pkgs, memory /* MiB */, callback) {
     assert.object(opts, 'opts');
@@ -1328,7 +1328,7 @@ function getClosestMemoryPackage(opts, pkgs, memory /* MiB */, callback) {
     }
 
     log.debug({pkg: candidate}, 'selected package for VM');
-    callback(null, candidate.uuid);
+    callback(null, candidate);
 }
 
 /*
@@ -1347,10 +1347,10 @@ function getClosestMemoryPackage(opts, pkgs, memory /* MiB */, callback) {
  *
  * The callback is called:
  *
- *   callback(err, package_uuid)
+ *   callback(err, package)
  *
  * Where err is an Error object if no package was found that matches the
- * criteria, and package_uuid is passed if a package was found.
+ * criteria, and package is passed if a package was found.
  */
 function getSpecifiedPackage(opts, pkgs, specifiedPackage, callback) {
     assert.object(opts, 'opts');
@@ -1360,7 +1360,7 @@ function getSpecifiedPackage(opts, pkgs, specifiedPackage, callback) {
     assert.func(callback, 'callback');
 
     var constraint = {};
-    var foundUuid;
+    var foundPkg;
     var log = opts.log;
     var nameMatches = [];
     var uuidMatches = [];
@@ -1372,30 +1372,30 @@ function getSpecifiedPackage(opts, pkgs, specifiedPackage, callback) {
         // use the one where the name matches.
         pkgs.forEach(function _onEachPkg(pkg) {
             if (pkg.name === specifiedPackage) {
-                nameMatches.push(pkg.uuid);
+                nameMatches.push(pkg);
             } else if (specifiedPackage.match(/^[0-9a-f]{8}$/)
                 && (pkg.uuid.substr(0, 8) === specifiedPackage)) {
 
-                uuidMatches.push(pkg.uuid);
+                uuidMatches.push(pkg);
             } else if (pkg.uuid === specifiedPackage) {
-                uuidMatches.push(pkg.uuid);
+                uuidMatches.push(pkg);
             }
         });
 
         // We're trying to uniquely identify a package, so only allow one match
         if (nameMatches.length === 1) {
-            foundUuid = nameMatches[0];
+            foundPkg = nameMatches[0];
         } else if (uuidMatches.length === 1) {
-            foundUuid = uuidMatches[0];
+            foundPkg = uuidMatches[0];
         }
 
         // If there are more than 1 match of either, or if there are no matches,
         // we can't determine the package to use and will fail.
     }
 
-    if (foundUuid) {
-        log.debug({foundPackage: foundUuid}, 'found package');
-        callback(null, foundUuid);
+    if (foundPkg) {
+        log.debug({foundPackage: foundPkg.uuid}, 'found package');
+        callback(null, foundPkg);
         return;
     }
 
@@ -1478,10 +1478,10 @@ function buildPackageFilter(opts) {
  *
  * Whether a package is found or not, the callback is called:
  *
- *   callback(err, package_uuid)
+ *   callback(err, package)
  *
- * with err being an Error object when a package was not found, and package_uuid
- * being the UUID of the correct package for this VM if this was able to be
+ * with err being an Error object when a package was not found, and package
+ * being the object for the correct package for this VM if this was able to be
  * determined.
  */
 function getPackage(opts, container, callback) {
@@ -1558,7 +1558,7 @@ function getPackage(opts, container, callback) {
         // memory value). So now we dispatch to the appropriate caller which
         // will call the callback with:
         //
-        //  callback(err, package_uuid)
+        //  callback(err, package)
         //
 
         if (specifiedPackage) {
@@ -2195,9 +2195,10 @@ function buildVmPayload(opts, container, callback) {
         },
 
         function selectPackage(_, cb) {
-            getPackage(opts, container, function (err, package_uuid) {
+            getPackage(opts, container, function (err, pkg) {
                 if (!err) {
-                    payload.billing_id = package_uuid;
+                    payload.pkg = pkg;
+                    payload.billing_id = pkg.uuid;
                 }
                 cb(err);
             });
@@ -2924,6 +2925,7 @@ function createContainer(opts, callback) {
 
     vasync.waterfall([
         _buildPayload,
+        _checkProvisionAllowed,
         _addLinks,
         _createVm,
         _saveLinks
@@ -2962,6 +2964,17 @@ function createContainer(opts, callback) {
         });
     }
 
+    function _checkProvisionAllowed(cb) {
+        var pkg = vm_payload.pkg;
+        delete vm_payload.pkg; // only used in this function; not needed further
+
+        opts.app.plugins.allowProvision({
+            account: opts.account,
+            image: opts.image,
+            pkg: pkg
+        }, cb);
+    }
+
     function _addLinks(cb) {
         var configLinks = container.HostConfig.Links;
         getLinkDetails(opts, configLinks, vm_payload, function (err, details) {
diff --git a/lib/plugin-manager.js b/lib/plugin-manager.js
index e52b260..1e37c76 100644
--- a/lib/plugin-manager.js
+++ b/lib/plugin-manager.js
@@ -78,10 +78,38 @@ function init(app) {
     // the plugins it imports.
     var pluginApi = {
         log: app.log,
+        datacenterName: app.config.datacenterName,
         getNapiNetworksForAccount: function getNapiShim(obj, cb) {
+            assert.object(obj, 'obj');
+            assert.func(cb, 'cb');
+
             obj = jsprim.deepCopy(obj);
             obj.config = { napi: app.config.napi };
             app.backend.getNapiNetworksForAccount(obj, cb);
+        },
+        getActiveVmsForAccount: function getActiveVmsShim(opts, cb) {
+            assert.object(opts, 'opts');
+            assert.uuid(opts.accountUuid, 'opts.accountUuid');
+            assert.optionalString(opts.fields, 'opts.fields');
+            assert.func(cb, 'cb');
+
+            app.vmapi.listVms({
+                owner_uuid: opts.accountUuid,
+                fields: opts.fields,
+                state: 'active'
+            }, cb);
+        },
+        getImage: function getImageShim(imgUuid, cb) {
+            assert.uuid(imgUuid, 'imgUuid');
+            assert.func(cb, 'cb');
+
+            app.imgapi.getImage(imgUuid, cb);
+        },
+        listImages: function listImageShim(opts, cb) {
+            assert.object(opts, 'opts');
+            assert.func(cb, 'cb');
+
+            app.imgapi.listImages(opts, cb);
         }
     };
 
@@ -100,7 +128,12 @@ function init(app) {
         var plugin = require(pPath);
 
         Object.keys(plugin).forEach(function (apiName) {
-            assert.notEqual(supportedPluginHooks[apiName], -1,
+            // skip if first character in exported is _
+            if (apiName[0] === '_') {
+                return;
+            }
+
+            assert.notEqual(supportedPluginHooks.indexOf(apiName), -1,
                 'supportedPluginFunctions[apiName]');
 
             var initedPlugin = plugin[apiName](pluginApi, pluginCfg);
@@ -165,3 +198,28 @@ function findOwnerExternalNetwork(opts, cb) {
         return p.bind(null, opts);
     }), cb);
 };
+
+
+/*
+ * This hook is run before the creation of a contain is initiated. It checks
+ * that various preconditions have been fulfilled before allowing the creation
+ * to proceed further.
+ */
+PluginManager.prototype.allowProvision =
+function allowProvision(opts, cb) {
+    assert.object(opts, 'opts');
+    assert.func(cb, 'cb');
+
+    var hooks = this.hooks.allowProvision;
+    var funcs = hooks.map(function wrapFunc(func) {
+        return function (_, next) {
+            func(opts, next);
+        };
+    });
+
+    // Runs every plugin (if any) until a plugin fails. Any failure indicates
+    // that the provision should not be allowed.
+    vasync.pipeline({ funcs: funcs }, function (err, results) {
+        cb(err);
+    });
+};
diff --git a/plugins/filter_owner_networks.js b/plugins/filter_owner_networks.js
index dd1c2ec..f0e5cc3 100644
--- a/plugins/filter_owner_networks.js
+++ b/plugins/filter_owner_networks.js
@@ -129,16 +129,16 @@ function findOwnerExternalNetwork(api, cfg) {
             var owned = networks.filter(function filterOwner(network) {
                 var owners = network.owner_uuids;
                 return owners && owners.indexOf(accountUuid) !== -1;
-            })
+            });
 
             var external = owned.filter(function filterExternal(network) {
                 var tags = network.nic_tags_present;
-                return network.nic_tag === EXTERNAL_NIC_TAG ||
-                    (tags && tags.indexOf(EXTERNAL_NIC_TAG) !== -1);
+                return network.nic_tag === EXTERNAL_NIC_TAG
+                    || (tags && tags.indexOf(EXTERNAL_NIC_TAG) !== -1);
             });
 
             if (external.length === 0) {
-                var msg = 'Found no external network accessible to account'
+                var msg = 'Found no external network accessible to account';
                 return cb(new Error(msg));
             }
 
diff --git a/plugins/provision_limits.js b/plugins/provision_limits.js
new file mode 100644
index 0000000..1bd1704
--- /dev/null
+++ b/plugins/provision_limits.js
@@ -0,0 +1,777 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+/*
+ * This applies limits specified by operators across a datacenter, either for
+ * all accounts or for a specific account. It is possible to limit an account
+ * based on three sums: total number of account VMs, total sum of those VM's
+ * RAM, and/or the total sum of those VM's disk quota. Each of these three sums
+ * can be optionally constrainted by: VM OS (specifically, the "os" attribute
+ * in the VM's image) and/or VM image name.
+ *
+ * Examples are worth a lot, so here are some examples of limits before going
+ * into the specifics:
+ *
+ * { "value": 200,  "by": "quota" }
+ * { "value": 1024, "by": "ram", "check": "os",    "os": "windows" }
+ * { "value": 25 }
+ * { "value": 8192, "by": "ram", "check": "image", "image": "base64-lts" }
+ * { "value": 50,                "check": "os",    "os": "any" }
+ *
+ * Now the specifics.
+ *
+ * Limit comes in the following JSON format:
+ * { "value": <number> }
+ *
+ * Where <number> is either a number, or a 10-base string encoding of a number.
+ * E.g. 10 or "10".
+ *
+ * By default, a limit counts the number of VMs across a datacenter. So to set
+ * the maximum number of VMs for an account across a datacenter to 25, use:
+ * { "value": 25 }
+ *
+ * We can modify what the "value" counts by adding a "by" clause:
+ * { "value": <number>, "by": "<dimension>" }
+ *
+ * Where currently-supported dimensions are "ram" (in MiB) or "quota" (in GiB).
+ * Its possible to use something beyond "ram" and "quota" (e.g. "count"), but
+ * that will be ignored and treated as the default: counting the number of VMs
+ * across a datacenter; this is for compatibility with cloudapi's plugin.
+ *
+ * As an example, to limit the total amount of RAM an account can use across a
+ * datacenter to 10240MiB, use the following limit:
+ * { "value": 10240, "by": "ram" }
+ *
+ * It's possible to constrain a limit to specific image names or operating
+ * systems, instead of the entire datacenter. This is done with the "check"
+ * attribute. It comes in two forms:
+ * { ..., "check": "os", "os": "<name of image operating system>" }
+ * { ..., "check": "image", "image": "<name of image>" }
+ *
+ * So to limit the total amount of RAM used by VMs running Windows images to
+ * 8192MiB:
+ * { "value": 8192, "by": "ram", "check": "os", "os": "windows" }
+ *
+ * IMPORTANT: since this plugin is in sdc-docker, the "os" you'd be interested
+ * in is "other". This is what all Docker images have as their "os".
+ *
+ * You can use "any" in place of the image OS or name. Like so:
+ * { "value" 25, "check": "image", "image": "any" }
+ *
+ * "any" flags in "image" or "os" are commonly added by adminui, yet while "any"
+ * is supported, its effect is the same as not using "check" in the first place.
+ * E.g. these two are equivalent, both limiting the amount of disk used across
+ * an entire datacenter to 900GiB:
+ * { "value": 900, "by": "quota", "check": "os", "os": "any" }
+ * { "value": 900, "by": "quota" }
+ *
+ * Several limits can apply to the same account at once. All the examples above
+ * were meant as one-liners, but adding several limits to an account will work
+ * as desired. Each limit is applied to a new provision, and if any of the
+ * limits, the provision is rejected.
+ *
+ * As an example, to allow an account to have up to 25 VMs, a maximum of
+ * 25600MiB RAM and 2.5TiB disk across the datacenter, and specifically only
+ * allow them to use 2048MiB RAM for the heretical penguin-loving Linux,
+ * add the following four limits to the account:
+ * { "value": 25 }
+ * { "value": 25600, "by": "ram" }
+ * { "value": 2560, "by": "quota" }
+ * { "value": 2048, "by": "ram", "check": "os", "os": "linux" }
+ *
+ * There are two places that limits can be stored, and this is also reflected in
+ * their use case:
+ *
+ * 1. sapi, both for sdc-docker and cloudapi. This is where default limits and
+ *    categories of limits for large numbers of users are kept. These limits
+ *    typically rarely change.
+ * 2. ufds, which is for individual accounts. These are used to add exceptions
+ *    to the defaults and categories stored in sapi.
+ *
+ * A typical use-case is to prevent all accounts from using more than a limited
+ * amount of RAM of VMs across a datacenter, until their account has been vetted
+ * by support (e.g. credit card number isn't fraudulent). After vetting, the
+ * limit is bumped substantially. In this use-case, small limits would be set in
+ * sdc-docker's (and additionally cloudapi's) sapi configuration to serve as
+ * defaults. Once support has vetted the account, they can add a limit in ufds
+ * for that account to override the defaults, thus bumping the amount of RAM or
+ * VMs the account can provision.
+ *
+ * Limits are added to sdc-docker through sapi by adding a configuration for
+ * this sdc-docker plugin:
+ *
+ * DOCKER_UUID=$(sdc-sapi /services?name=docker | json -Ha uuid)
+ * sdc-sapi /services/$DOCKER_UUID -X PUT -d '{
+ *     "metadata": {
+ *         "DOCKER_PLUGINS": "[{\"name\":\"provision_limits\", \
+ *         \"enabled\": true,\"config\":{\"defaults\":[{\"value\":2 }]}}]"
+ *     }
+ * }'
+ *
+ * The above completely replaces DOCKER_PLUGINS, so make sure to check that
+ * you're not overwriting the configurations for other plugins in the process.
+ *
+ * Looking at this plugin's configuration:
+ * { "defaults": [<limits>] }
+ *
+ * Limits in "defaults" are applied to all provisions unless specifically
+ * overridden with a ufds limit. Additional categories can be added in the
+ * plugin's configuration, and their names are up to you. E.g.:
+ * {
+ *     "defaults": [
+ *         { "value": 2 },
+ *         { "value": 1024, "by": "ram" }
+ *     ]
+ *     "small": [
+ *         { "value": 20 },
+ *         { "value": 10, "check": "os", "os": "windows" },
+ *         { "value": 327680, "by": "ram" },
+ *         { "value": 2000, "by": "quota" }
+ *     ]
+ *     "whale": [
+ *         { "value": 10000 },
+ *         { "value": 327680000, "by": "ram" },
+ *         { "value": 1000000, "by" :"quota" }
+ *     ]
+ * }
+ *
+ * The above configuration has defaults which are applied to all accounts that
+ * do not have a category set in "tenant" (see below). There are two added
+ * category of users: "small" and "whale". The "small" category allows accounts
+ * to have up to 20 VMs, up to 10 Windows VMs, and a total of 320GiB RAM and
+ * 2000GiB disk across the datacenter. The "whale" category is much, much
+ * higher.
+ *
+ * Which category an account falls in is determined by the "tenant" attribute on
+ * that account in ufds. If the attribute is blank or absent (or a category
+ * that doesn't exist in the configuration), the account uses "defaults" limits.
+ * If the attribute is present and matches a category in the plugin
+ * those are the limits used. For example, this account is a whale:
+ *
+ * $ sdc-ufds search '(login=megacorp)' | json tenant
+ * whale
+ *
+ * To override any of these defaults or categories in ufds, add a capilimit
+ * entry. It takes the general form of:
+ *
+ * sdc-ufds add '
+ * {
+ *   "dn": "dclimit=$DATACENTER, uuid=$ACCOUNT_UUID, ou=users, o=smartdc",
+ *   "datacenter": "$DATACENTER",
+ *   "objectclass": "capilimit",
+ *   "limit": ["<JSON limit>", "<JSON limit>", ...]
+ * }'
+ *
+ * Or you could use adminui, which lets operators do the same with a
+ * friendly discoverable GUI.
+ *
+ * This plugin is compatible with cloudapi's provision_limits, with one
+ * minor difference: cloudapi's plugin only works correctly with lower-case
+ * image names, while this one works regardles of casing of image
+ * names (makes no attempt at case insensitivity). For now, to keep safe, only
+ * use lower-case image names.
+ */
+
+var assert = require('assert-plus');
+var vasync = require('vasync');
+
+
+// --- Globals
+
+var QUOTA_ERR = 'Quota exceeded; to have your limits raised please contact '
+    + 'Support';
+var IMAGE = 'image';
+var OS = 'os';
+var RAM = 'ram';
+var QUOTA = 'quota';
+var ANY = 'any';
+
+
+/*
+ * DC limits come in two formats, as a result of how ufds (LDAP) works:
+ * a single JSON string, or an array of JSON strings. Each string represents
+ * a single limit. We deserialize these strings here.
+ *
+ * Returns an array of limit objects.
+ */
+function convertFromCapi(log, dcUserLimits) {
+    assert.object(log, 'log');
+    assert.optionalObject(dcUserLimits, 'dcUserLimits');
+
+    if (!dcUserLimits) {
+        return [];
+    }
+
+    var rawLimits = dcUserLimits.limit;
+    if (!rawLimits) {
+        return [];
+    }
+
+    if (typeof (rawLimits) === 'string') {
+        rawLimits = [rawLimits];
+    }
+
+    var parsedLimits = [];
+    rawLimits.forEach(function (raw) {
+        try {
+            parsedLimits.push(JSON.parse(raw));
+        } catch (e) {
+            log.warn({
+                failed_json_string: raw
+            }, 'Failed to deserialize DC provision limit!');
+        }
+    });
+
+    return parsedLimits;
+}
+
+
+/*
+ * Take an array of limit objects and convert their value attributes to numbers.
+ *
+ * Mutates passed argument; returns nothing.
+ */
+function atoiValues(limits) {
+    assert.arrayOfObject(limits, 'limits');
+
+    limits.forEach(function (limit) {
+        if (limit.value !== undefined) {
+            limit.value = parseInt(limit.value, 10) || 0;
+        }
+    });
+}
+
+
+/*
+ * Given limits specified in sdc-docker's config file, and limits placed on an
+ * account in this DC, merge the two sets of limits into one. DC limits take
+ * priority over config limits, and if any of the DC limits has a image/os value
+ * of 'any' we skip config limits altogether (matching the cloudapi plugin's
+ * behaviour).
+ *
+ * We also do some cleaning/a few rudimentary optimizations here.
+ *
+ * Returns an array of limit objects.
+ */
+function filterLimits(log, cfgUserLimits, rawDcUserLimits) {
+    assert.object(log, 'log');
+    assert.arrayOfObject(cfgUserLimits, 'cfgUserLimits');
+    assert.optionalObject(rawDcUserLimits, 'dcUserLimits');
+
+    var dcUserLimits = convertFromCapi(log, rawDcUserLimits);
+
+    // Convert any value attributes to numbers
+    atoiValues(dcUserLimits);
+    atoiValues(cfgUserLimits);
+
+    // If the user has any DC-wide wildcard limits specified, we skip any limits
+    // specified in the sapi config.
+    var hasDcWildcards = dcUserLimits.some(function (limit) {
+        return limit.image === ANY || limit.os === ANY || !limit.check;
+    });
+
+    // Union of the set of DC and config limits
+    var unionUserLimits = dcUserLimits.slice();
+
+    if (!hasDcWildcards) {
+        // Add any config limit which hasn't been overridden by a DC limit
+        cfgUserLimits.forEach(function (cfgLimit) {
+            var collision = dcUserLimits.some(function (dcLimit) {
+                return dcLimit.check && dcLimit.check === cfgLimit.check
+                    && dcLimit.by && dcLimit.by === cfgLimit.by;
+            });
+
+            if (!collision) {
+                unionUserLimits.push(cfgLimit);
+            }
+        });
+    }
+
+    // {image: 'any'} and {os: 'any'} are equivalent to {}: they're limits that
+    // apply to everything.
+    unionUserLimits.forEach(function simplifyAny(limit) {
+        if (limit.image === ANY || limit.os === ANY) {
+            limit.check = undefined;
+            limit.image = undefined;
+            limit.os    = undefined;
+        }
+    });
+
+    // Remove and log invalid limits
+    unionUserLimits = unionUserLimits.filter(function validateLimit(l) {
+        if ((l.check === IMAGE && !l.image) || (l.check === OS && !l.os)) {
+            log.warn({ limit: l }, 'Invalid limit; entry is incomplete');
+            return false;
+        }
+
+        return true;
+    });
+
+    // Any limit with a value of 0 means 'unlimited', so we remove such limits
+    // here since they're effectively a nop when filtering on them.
+    return unionUserLimits.filter(function filterZero(limit) {
+        return limit.value !== 0;
+    });
+}
+
+
+function sum(a, b) {
+    return a + b;
+}
+
+
+/*
+ * Takes a look at all of a user's VMs, and determines whether this provision
+ * will shoot over any limits set. The three possible limits are for the sum of
+ * all RAM across the DC (in MiB), the sum of all disk across the DC (in GiB),
+ * and the total number of an account's VMs. Each of these limits can be
+ * optionally be restricted to VMs made using an image with the given name, or
+ * VMs that contain a certain OS.
+ *
+ * Some examples:
+ *
+ * - account limited to total 2GiB RAM across whole DC:
+ *   { "by": "ram", "value": 2048 }
+ *
+ * - account limited to total 1TiB disk across whole DC for VMs with "other"
+ *   OS; "other" is usually used for Docker:
+ *   { "check": "os", "os": "other", "by": "quota", "value": 1024 }
+ *
+ * - account limited to 1GiB RAM across DC, 25GiB disk across DC, and can have
+ *   no more than four VMs:
+ *   { "by": "ram", "value": 1024 }
+ *   { "by": "quota", "value": 25 }
+ *   { "value": 4 }
+ *
+ * Unknown checks (i.e. not "ram" or "quota") are treated as the default case:
+ * counting VMs. Not great, but this is to keep consistent with the cloudapi
+ * plugin's behaviour.
+ *
+ * Returns a boolean: true means provision is a go, false means provision should
+ * be rejected.
+ */
+function canProvision(log, pkg, vms, image, limits) {
+    assert.object(log, 'log');
+    assert.object(pkg, 'pkg');
+    assert.arrayOfObject(vms, 'vms');
+    assert.object(image, 'image');
+    assert.arrayOfObject(limits, 'limits');
+
+    // All VMs matching new provision's image name
+    var imgVms = vms.filter(function imgFilter(vm) {
+        return vm.image_name === image.name;
+    });
+
+    // All VMs matching new provision's OS
+    var osVms = vms.filter(function osFilter(vm) {
+        return vm.os === image.os;
+    });
+
+    // Loop through each limit and ensure that it passes. If any limit fails,
+    // this provision fails.
+    for (var i = 0; i < limits.length; i++) {
+        var limit = limits[i];
+
+        log.debug({ limit: limit }, 'Applying provision limit');
+
+        var machines = vms;
+        if (limit.check === IMAGE) {
+            machines = imgVms;
+        }
+
+        if (limit.check === OS) {
+            machines = osVms;
+        }
+
+        // Default is this
+        var count = machines.length + 1;
+
+        if (limit.by === RAM) {
+            // RAM; in MiB
+            count = machines.map(function (vm) {
+                return vm.ram;
+            }).reduce(sum, pkg.max_physical_memory);
+
+        } else if (limit.by === QUOTA) {
+            // Disk; VMs and limits are in GiB, but packages in MiB
+            count = machines.map(function (vm) {
+                return vm.quota;
+            }).reduce(sum, pkg.quota / 1024);
+        }
+
+        if (count > limit.value) {
+            log.info({ limit: limit }, 'Provision limit applied');
+            return false;
+        }
+    }
+
+    return true;
+}
+
+
+/*
+ * Look at what the set of limits will be filtering on, and determine what
+ * are the minimal number of fields we need vmapi to populate each VM object
+ * with; this reduces serialization/deserialization time on both ends.
+ *
+ * One major limitation in vmapi is that it doesn't recognize "image_uuid" as
+ * a field, so if we need any information that can only be found in imgapi, we
+ * have no choice but to load complete vmapi objects.
+ *
+ * Returns a query string to use with vmapi's ListVms ?field=. Returns undefined
+ * if we'll use the default object layout instead.
+ */
+function findMinimalFields(limits) {
+    assert.arrayOfObject(limits, 'limits');
+
+    var needImageUuid = limits.some(function (limit) {
+        return limit.check === IMAGE || limit.check === OS;
+    });
+
+    if (needImageUuid) {
+        // Cannot use fields because vmapi doesn't understand
+        // ?fields=image_uuid, so we have to load everything :(
+        return undefined;
+    }
+
+    var needRam = limits.some(function (limit) {
+        return limit.by === RAM;
+    });
+
+    var needQuota = limits.some(function (limit) {
+        return limit.by === QUOTA;
+    });
+
+    if (needRam && needQuota) {
+        return 'ram,quota';
+    } else if (needQuota) {
+        return 'quota';
+    } else {
+        // vmapi won't return empty objects, so we need at least one attribute
+        // regardless of whether any limit applies to ram or not
+        return 'ram';
+    }
+}
+
+
+/*
+ * Fetch all the VMs from vmapi that we'll need to apply the given limits. If
+ * any of the limits require that VM objects are populated with details of their
+ * image's OS or name, we need to fetch those from imgapi to do that population.
+ *
+ * If we'll be filtering by image name or OS, we can throw away all image or OS
+ * limits that don't apply to this provision once we know the provision's image
+ * OS or name. After all, at that point the only limits that apply either match
+ * the provision's name and OS, or aren't matching on name or OS.
+ *
+ * Calls cb(err, vms, vmImage, limits), where vms is the list of VMs
+ * (populated with "image_name" and "os" if required by the limits), vmImage
+ * (also populated with "name" and "os" if required by the limits), and limits
+ * (a new set of limits once we've throw away now-irrelevant limits).
+ */
+function getVms(log, api, account, morayImage, limits, cb) {
+    assert.object(log, 'log');
+    assert.object(api, 'api');
+    assert.object(account, 'account');
+    assert.object(morayImage, 'morayImage');
+    assert.arrayOfObject(limits, 'limits');
+    assert.func(cb, 'cb');
+
+    var relevantLimits = limits;
+    var vmImage = morayImage;
+    var imageLookup = {};
+    var vms = [];
+
+    // The image object that sdc-docker stores in Moray doesn't have the
+    // information we need if any of the limits will be checking by either
+    // image name or OS, thus we load it here.
+    //
+    // NB: the ImageV2 object in Moray does store OS information, but it differs
+    // from what is in imgapi. We could potentially skip this call for image OS
+    // if we assume that long-term all Docker images in imgapi have
+    // "os": "other".
+    function getVmImage(_, next) {
+        log.trace('Running getVmImage');
+
+        var needVmImage = limits.some(function (limit) {
+            return limit.check === IMAGE || limit.check === OS;
+        });
+
+        if (!needVmImage) {
+            log.debug('VM image is not needed for limit filtering; skipping');
+            return next();
+        }
+
+        var vmImgUuid = morayImage.image_uuid;
+        log.debug('Loading VM image for limit filtering:', vmImgUuid);
+
+        api.getImage(vmImgUuid, function getImageCb(err, image) {
+            if (err) {
+                return next(err);
+            }
+
+            log.debug({ vm_image: image }, 'Loaded VM\'s image');
+
+            vmImage = image;
+
+            // relevantLimits can contain at most one image name and one os
+            // name to query, so we will have at most two imgapi queries later.
+            relevantLimits = limits.filter(function filterRelevant(limit) {
+                if (limit.check === OS) {
+                    return limit.os === image.os;
+                }
+
+                if (limit.check === IMAGE) {
+                    return limit.image === image.name;
+                }
+
+                return true;
+            });
+
+            log.debug({ limits: relevantLimits }, 'Found applicable limits');
+
+            return next();
+        });
+    }
+
+    // Helper function used by getOsImages() and getNameImages()
+    function getImages(opts, next) {
+        assert.object(opts, 'opts');
+        assert.func(next, 'next');
+
+        opts.state = 'all';
+
+        api.listImages(opts, function listImagesCb(err, images) {
+            if (err) {
+                return next(err);
+            }
+
+            log.debug({ opts: opts }, 'Loaded images');
+
+            images.forEach(function (image) {
+                imageLookup[image.uuid] = image;
+            });
+
+            return next();
+        });
+    }
+
+    // Search for images that match the VM image's OS, but only if needed
+    function getOsImages(_, next) {
+        log.trace('Running getOsImages');
+
+        var needOsDetails = relevantLimits.some(function (limit) {
+            return limit.check === OS;
+        });
+
+        if (needOsDetails) {
+            return getImages({ os: vmImage.os }, next);
+        }
+
+        log.debug('VMs\' OS not needed needed for limit filtering; skipping');
+
+        return next();
+    }
+
+    // Search for images that match the VM image's name, but only if needed
+    function getNameImages(_, next) {
+        log.trace('Running getNameImages');
+
+        var needNameDetails = relevantLimits.some(function (limit) {
+            return limit.check === IMAGE;
+        });
+
+        if (needNameDetails) {
+            return getImages({ name: vmImage.name }, next);
+        }
+
+        log.debug('Img names not needed needed for limit filtering; skipping');
+
+        return next();
+    }
+
+    // Unfortunately, vmapi VMs don't have an 'os' attribute, nor do they store
+    // image names. Therefore we're stuck always loading all of an account's
+    // active VMs. This is really Not Great.
+    //
+    // There are various convoluted optimizations we could try and pull (e.g.
+    // we can make individual vmapi queries for each ?image_uuid=, iff all
+    // applicable limits involve the image name), but if the current approach
+    // becomes too expensive it'd be simplest to have vmapi store the 'os' and
+    // 'image_name' attributes. '?fields=' needs to be extended to support
+    // image_uuid as well. And if vmapi grew a fast path for HEAD with an object
+    // count, that would be pretty handy...
+    //
+    // XXX Another problem here pops up if we're dealing with more than 1000
+    // VMs, since vmapi ListVms has an upper limit of 1000 VMs per call. This
+    // code doesn't handle that.
+    //
+    // Calls cb(err, vms, image, limits), where vms is an array of VMs loaded
+    // from vmapi, image comes from imgapi and matches the current provision,
+    // and limits are a new set of limits filtered to match the current
+    // provision given new information about the provision's OS and image name
+    // (if relevant).
+    function getAccountVms(_, next) {
+        log.trace('Running getAccountVms');
+
+        var opts = {
+            accountUuid: account.uuid,
+            fields: findMinimalFields(relevantLimits)
+        };
+
+        api.getActiveVmsForAccount(opts, function getAccountVmsCb(err, _vms) {
+            if (err) {
+                return (err);
+            }
+
+            vms = _vms;
+
+            // Add 'os' and 'image_name' fields to vms when available. VMs which
+            // don't have a matching image are not under consideration for any
+            // "check":"image"/"os" (if applicable) in any case, which was why
+            // we didn't load those images earlier.
+            vms.forEach(function addVmAttr(vm) {
+                var image = imageLookup[vm.image_uuid];
+                if (image) {
+                    vm.image_name = image.name;
+                    vm.os = image.os;
+                }
+            });
+
+            if (vms.length === 1000) {
+                log.warn('Loaded 1,000 VMs; plugin may be missing others!');
+            }
+
+            log.debug('VMs loaded');
+
+            return next();
+        });
+    }
+
+    vasync.pipeline({
+        funcs: [getVmImage, getOsImages, getNameImages, getAccountVms]
+    }, function vasyncCb(err) {
+        cb(err, vms, vmImage, relevantLimits);
+    });
+}
+
+
+/*
+ * Given a new provision, load all limits that apply to the current account
+ * both in sdc-docker's config and in ufds, determine which limits are relevant
+ * to this provision, and check that the provision won't violate any of those
+ * limits.
+ *
+ * Calls cb(err), where no error means that the provision can proceed. An error
+ * should halt the provision.
+ */
+function checkLimits(api, cfg) {
+    assert.object(api, 'api');
+    assert.object(api.log, 'api.log');
+    assert.object(cfg, 'cfg');
+    assert.arrayOfObject(cfg.defaults, 'cfg.defaults');
+
+    var log = api.log;
+
+    return function checkProvisionLimits(opts, cb) {
+        assert.object(opts, 'opts');
+        assert.object(opts.account, 'opts.account');
+        assert.object(opts.image, 'opts.image');
+        assert.object(opts.pkg, 'opts.pkg');
+        assert.func(cb, 'cb');
+
+        var account = opts.account;
+        var morayImage = opts.image;
+        var pkg = opts.pkg;
+
+        log.debug('Running', checkProvisionLimits.name);
+
+        if (account.isAdmin()) {
+            log.debug('Account %s is an admin; skipping provision limits',
+                account.uuid);
+                return cb();
+        }
+
+        // fetch all of this account's DC limits from ufds
+        return account.listLimits(function listLimitsCb(err, globalUserLimits) {
+            if (err) {
+                return cb(err);
+            }
+
+            // Since ufds replicates between DCs, we're only interested in any
+            // limits that apply to this DC specifically.
+            var dcUserLimits = (globalUserLimits || []).find(function (limit) {
+                return limit.datacenter === api.datacenterName;
+            });
+
+            // We use a specific class of sapi-specified limits if the account
+            // has that class, otherwise fall back to defaults.
+            var cfgUserLimits = cfg[account.tenant] || cfg.defaults || [];
+
+            // Merge and optimize a bit the two sets of limits.
+            var limits = filterLimits(log, cfgUserLimits, dcUserLimits);
+
+            if (!limits.length) {
+                log.debug('No limits to be applied; skipping provision limits');
+                return cb();
+            }
+
+            log.debug({ provisioning_limits: limits }, 'Will apply limits');
+
+            var disallow = limits.some(function (limit) {
+                return limit.value <= -1;
+            });
+
+            if (disallow) {
+                log.info('Disallowing provision because -1 limit value found');
+                return cb(new Error(QUOTA_ERR));
+            }
+
+            // Load and populate any required VMs from imgapi to check against
+            // the given limits. Narrow the limits based on new information
+            // available from those queries.
+            getVms(log, api, account, morayImage, limits,
+                function (err2, vms, image, fittedLimits) {
+
+                if (err2) {
+                    return cb(err2);
+                }
+
+                log.info({
+                    vm_count: vms.length,
+                    limits: fittedLimits,
+                    img_os: image.os,
+                    img_name: image.name
+                }, 'VMs loaded and provision limits adjusted');
+
+                var allow = canProvision(log, pkg, vms, image, fittedLimits);
+                if (!allow) {
+                    return cb(new Error(QUOTA_ERR));
+                }
+
+                return cb();
+            });
+        });
+    };
+}
+
+
+module.exports = {
+    // hook loaded by PluginManager
+    allowProvision: checkLimits,
+
+    // and these are additionally exported for tests
+    _convertFromCapi: convertFromCapi,
+    _atoiValues: atoiValues,
+    _filterLimits: filterLimits,
+    _canProvision: canProvision,
+    _findMinimalFields: findMinimalFields,
+    _getVms: getVms
+};
