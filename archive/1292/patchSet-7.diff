commit 38fa9739e76e613f4aa651038baea4ff7b675396 (refs/changes/92/1292/7)
Author: Jordan Hendricks <jordan.hendricks@joyent.com>
Date:   2017-02-07T21:38:54+00:00 (2 years, 8 months ago)
    
    MANTA-2169 Support multipart upload of a single file to Manta

diff --git a/.gitignore b/.gitignore
index d416fc4..408b0e2 100644
--- a/.gitignore
+++ b/.gitignore
@@ -11,3 +11,4 @@ cscope.out
 smf/manifests/*.xml
 *.tar.bz2
 *.log
+*.swp
diff --git a/lib/audit.js b/lib/audit.js
index 36bfa92..908da68 100644
--- a/lib/audit.js
+++ b/lib/audit.js
@@ -232,6 +232,9 @@ function auditLogger(options) {
         }
         obj.sharksContacted = req.sharksContacted;
         obj.shard = req.shard;
+        if (req.route) {
+            obj.route = req.route.name;
+        }
 
         if (req._timeToLastByte !== undefined &&
             req._totalBytes !== undefined) {
diff --git a/lib/auth.js b/lib/auth.js
index c368e28..fa9dd14 100644
--- a/lib/auth.js
+++ b/lib/auth.js
@@ -689,13 +689,24 @@ function parseHttpAuthToken(req, res, next) {
 
 
 function loadOwner(req, res, next) {
+    var p = req.path();
+    loadOwnerFromPath(req, p, next);
+}
+
+
+/*
+ * Extract the owner of a resource based on the input path, verify that
+ * the account exists, and set the `owner` field on the request object
+ * to the object returned from Mahi.
+ */
+function loadOwnerFromPath(req, p, next) {
     req.log.debug('loadOwner: entered');
 
     var account;
     try {
-        account = decodeURIComponent(req.path().split('/', 2).pop());
+        account = decodeURIComponent(p.split('/', 2).pop());
     } catch (e) {
-        next(new InvalidPathError(req.path()));
+        next(new InvalidPathError(p));
         return;
     }
 
@@ -718,7 +729,7 @@ function loadOwner(req, res, next) {
         req.owner = owner;
 
         if (req.caller.anonymous && !owner.user) {
-            next(new AuthorizationError(common.ANONYMOUS_USER, req.path(),
+            next(new AuthorizationError(common.ANONYMOUS_USER, p,
                 'owner ' + account + ' has no anonymous user'));
             return;
         } else if (req.caller.anonymous) {
@@ -872,7 +883,7 @@ function storageContext(req, res, next) {
      * authorization checks and return the correct error.
      */
     if (!req.metadata.type && !req.parentMetadata.type) {
-        next(new DirectoryDoesNotExistError(req));
+        next(new DirectoryDoesNotExistError(req.path()));
         return;
     }
 
@@ -1009,6 +1020,8 @@ module.exports = {
         ]);
     },
 
+    loadOwnerFromPath: loadOwnerFromPath,
+
     gatherContext: gatherContext,
     storageContext: storageContext,
     createAuthToken: createAuthToken,
diff --git a/lib/common.js b/lib/common.js
index 8996f9f..6b6e93f 100644
--- a/lib/common.js
+++ b/lib/common.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright (c) 2014, Joyent, Inc.
+ * Copyright (c) 2017, Joyent, Inc.
  */
 
 var EventEmitter = require('events').EventEmitter;
@@ -44,6 +44,8 @@ var JOBS_PATH = /^\/([a-zA-Z][a-zA-Z0-9_\.@%]+)\/jobs\/([a-f0-9]{8}-[a-f0-9]{4}-
 /* JSSTYLED */
 var JOBS_ROOT_PATH = /^\/([a-zA-Z][a-zA-Z0-9_\.@%]+)\/jobs\/?.*/;
 /* JSSTYLED */
+var UPLOADS_ROOT_PATH = /^\/([a-zA-Z][a-zA-Z0-9_\.@%]+)\/uploads\/?.*/;
+/* JSSTYLED */
 var JOBS_STOR_PATH = /^\/([a-zA-Z][a-zA-Z0-9_\-\.@%]+)\/jobs\/[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}\/stor/;
 var PUBLIC_STOR_PATH = /^\/([a-zA-Z][a-zA-Z0-9_\-\.@%]+)\/public(\/(.*)|$)/;
 var REPORTS_STOR_PATH = /^\/([a-zA-Z][a-zA-Z0-9_\-\.@%]+)\/reports(\/(.*)|$)/;
@@ -57,6 +59,7 @@ var ROOT_REGEXPS = [
     new RegExp('^\\/[a-zA-Z0-9_\\-\\.@%]+\\/public\\/?$'), // public
     new RegExp('^\\/[a-zA-Z0-9_\\-\\.@%]+\\/stor\\/?$'), // storage
     new RegExp('^\\/[a-zA-Z0-9_\\-\\.@%]+\\/jobs\\/?$'), // jobs (list)
+    new RegExp('^\\/[a-zA-Z0-9_\\-\\.@%]+\\/uploads\\/?$'), // uploads (list)
 
     // jobs storage
     new RegExp('^\\/[a-zA-Z0-9_\\-\\.@%]+\\/jobs\\/[\\w-]+\\/stor\\/?$'),
@@ -146,7 +149,7 @@ HttpRequest.isRootDirectory = function isRootDirectory(d) {
     return (_test(d));
 };
 
-
+//TODO: think uploads should go here
 HttpRequest.isRestrictedWrite = function isRestrictedWrite() {
     if (this.method !== 'PUT')
         return (false);
@@ -159,6 +162,7 @@ HttpRequest.isRestrictedWrite = function isRestrictedWrite() {
 
 ///--- API
 
+//TODO: comment this.
 function createMetadata(req, type, cb) {
     var prev = req.metadata || {};
     // Override the UpdateMetadata type, as this flows in via PUT Object
@@ -362,7 +366,7 @@ function ensureParent(req, res, next) {
         req.log.debug('ensureParent: done');
         next();
     } else if (!req.parentMetadata || req.parentMetadata.type === null) {
-        next(new DirectoryDoesNotExistError(req));
+        next(new DirectoryDoesNotExistError(req.path()));
     } else if (req.parentMetadata.type !== 'directory') {
         next(new ParentNotDirectoryError(req));
     } else {
@@ -371,7 +375,7 @@ function ensureParent(req, res, next) {
     }
 }
 
-
+//TODO: comment this
 function getMetadata(req, res, next) {
     var log = req.log;
 
@@ -443,6 +447,7 @@ function getMetadata(req, res, next) {
                 req.metadata._etag = r.etag || null;
                 req.metadata.headers =
                     req.metadata.headers || {};
+                // TODO: comment this for conditional requests
                 if (r.metadata.etag)
                     res.set('Etag', r.metadata.etag);
                 if (r.metadata.mtime) {
@@ -468,7 +473,7 @@ function getMetadata(req, res, next) {
     });
 }
 
-
+//TODO: comment this.
 function loadMetadata(req, opts, callback) {
     req.moray.getMetadata(opts, function (err, md, wrap) {
         if (err) {
@@ -700,23 +705,36 @@ module.exports = {
 
     PATH_LOGIN_RE: PATH_LOGIN_RE,
 
-    StoragePaths: {
-        'public': {
-            'name': 'Public',
-            'regex': PUBLIC_STOR_PATH
-        },
-        'stor': {
-            'name': 'Storage',
-            'regex': STOR_PATH
-        },
-        'jobs': {
-            'name': 'Jobs',
-            'regex': JOBS_ROOT_PATH
-        },
-        'reports': {
-            'name': 'Reports',
-            'regex': REPORTS_STOR_PATH
+    UPLOADS_ROOT_PATH: UPLOADS_ROOT_PATH,
+
+    storagePaths: function storagePaths(cfg) {
+        var StoragePaths = {
+            'public': {
+                'name': 'Public',
+                'regex': PUBLIC_STOR_PATH
+            },
+            'stor': {
+                'name': 'Storage',
+                'regex': STOR_PATH
+            },
+            'jobs': {
+                'name': 'Jobs',
+                'regex': JOBS_ROOT_PATH
+            },
+            'reports': {
+                'name': 'Reports',
+                'regex': REPORTS_STOR_PATH
+            }
+        };
+
+        if (cfg.enableMPU) {
+            StoragePaths.uploads = {
+                'name': 'Uploads',
+                'regex': UPLOADS_ROOT_PATH
+            };
         }
+
+        return (StoragePaths);
     },
 
     createMetadata: createMetadata,
@@ -903,23 +921,6 @@ module.exports = {
         }
 
         return (setup);
-    },
-
-    // Not used anymore
-    debugRequestHandler: function () {
-        function _debugLogRequest(req, res, next) {
-            var log = req.log;
-            var str = req.method + ' ' +
-                req.url + ' ' +
-                req.httpVersion + '\n';
-            Object.keys(req.headers).sort().forEach(function (k) {
-                str += k + ': ' + req.headers[k] + '\n';
-            });
-            log.debug('handling request:\n%s\n', str);
-            return (next());
-        }
-
-        return (_debugLogRequest);
     }
 
 };
diff --git a/lib/dir.js b/lib/dir.js
index d2d3bba..ab09320 100644
--- a/lib/dir.js
+++ b/lib/dir.js
@@ -174,7 +174,7 @@ function getRootDirectory(req, res, next) {
     if (req.method !== 'HEAD' && req.method !== 'GET')
         return (next(new RootDirectoryError(req)));
 
-    var storagePaths = Object.keys(common.StoragePaths).sort();
+    var storagePaths = Object.keys(common.storagePaths(req.config)).sort();
     // Pagination is useless, but things like mfind rely on it.
     if (req.params.marker) {
         while (storagePaths.length > 0 &&
@@ -265,7 +265,6 @@ module.exports = {
 
     putDirectoryHandler: function putDirectoryHandler() {
         var chain = [
-            // common.ensureNotRootHandler(),
             common.ensureParentHandler(),
             mkdir
         ];
diff --git a/lib/errors.js b/lib/errors.js
index bd5c156..4a51259 100644
--- a/lib/errors.js
+++ b/lib/errors.js
@@ -5,9 +5,10 @@
  */
 
 /*
- * Copyright (c) 2014, Joyent, Inc.
+ * Copyright (c) 2017, Joyent, Inc.
  */
 
+//TODO: comment that you have to change the conf for lint to add errors
 var assert = require('assert');
 var fs = require('fs');
 var path = require('path');
@@ -115,11 +116,11 @@ function ContentLengthError() {
 util.inherits(ContentLengthError, MuskieError);
 
 
-function DirectoryDoesNotExistError(req) {
+function DirectoryDoesNotExistError(p) {
     MuskieError.call(this, {
         restCode: 'DirectoryDoesNotExist',
         statusCode: 404,
-        message: sprintf('%s does not exist', path.dirname(req.path()))
+        message: sprintf('%s does not exist', path.dirname(p))
     });
 }
 util.inherits(DirectoryDoesNotExistError, MuskieError);
@@ -439,6 +440,112 @@ function MissingPermissionError(perm) {
 util.inherits(MissingPermissionError, MuskieError);
 
 
+function MultipartUploadAbortedError(id) {
+    MuskieError.call(this, {
+        restCode: 'MultipartUploadAborted',
+        statusCode: 409,
+        message: sprintf('upload %s aborted', id)
+    });
+}
+util.inherits(MultipartUploadAbortedError, MuskieError);
+
+
+
+function MultipartUploadCommitInProgressError(id) {
+    MuskieError.call(this, {
+        restCode: 'MultipartUploadCommitInProgress',
+        statusCode: 409,
+        message: sprintf('upload %s already has a commit in progress', id)
+    });
+}
+util.inherits(MultipartUploadCommitInProgressError, MuskieError);
+
+
+function MultipartUploadContentLengthError(id, expected, actual) {
+    MuskieError.call(this, {
+        restCode: 'MultipartUploadContentLengthError',
+        statusCode: 400,
+        message: sprintf('upload %s: expected content-length %d, ' +
+            'but object was: %d', id, expected, actual)
+    });
+}
+util.inherits(MultipartUploadContentLengthError, MuskieError);
+
+
+function MultipartUploadFinalizeConflictError(id, action) {
+    MuskieError.call(this, {
+        restCode: 'MultipartUploadFinalizeConflict',
+        statusCode: 409,
+        message: sprintf('cannot %s upload %s; already finalized', action, id)
+    });
+}
+util.inherits(MultipartUploadFinalizeConflictError, MuskieError);
+
+
+function MultipartUploadMissingObjecPathError() {
+    MuskieError.call(this, {
+        restCode: 'MultipartUploadMissingObjecPathError',
+        statusCode: 400,
+        message: 'an objectPath is required'
+    });
+}
+util.inherits(MultipartUploadMissingObjecPathError, MuskieError);
+
+
+function MultipartUploadMissingPartError(id, index, etag) {
+    MuskieError.call(this, {
+        restCode: 'MultipartUploadMissingPartError',
+        statusCode: 400,
+        message: sprintf('upload %s, part %d is missing,' +
+            ' or no etag was provided (input etag: \"%s\")', id, index, etag)
+    });
+}
+util.inherits(MultipartUploadMissingPartError, MuskieError);
+
+
+function MultipartUploadPartEtagError(id, index, etag) {
+    MuskieError.call(this, {
+        restCode: 'MultipartUploadPartEtag',
+        statusCode: 400,
+        message: sprintf('upload %s, part %s has invalid etag: %s',
+            id, index, etag)
+    });
+}
+util.inherits(MultipartUploadPartEtagError, MuskieError);
+
+
+function MultipartUploadPartLimitError(id, numParts) {
+    MuskieError.call(this, {
+        restCode: 'MultipartUploadPartLimitError',
+        statusCode: 400,
+        message: sprintf('invalid commit for upload %s: too many parts (%s)',
+            id, numParts)
+    });
+}
+util.inherits(MultipartUploadPartLimitError, MuskieError);
+
+
+function MultipartUploadPartNumError(id, partNum) {
+    MuskieError.call(this, {
+        restCode: 'MultipartUploadPartNum',
+        statusCode: 400,
+        message: sprintf('invalid partNum for upload %s: %s', id, partNum)
+    });
+}
+util.inherits(MultipartUploadPartNumError, MuskieError);
+
+
+function MultipartUploadPartSizeError(id, index, size) {
+    MuskieError.call(this, {
+        restCode: 'MultipartUploadPartSize',
+        statusCode: 400,
+        message: sprintf('upload %s, part %d is too small (size %d)',
+            id, index, size)
+    });
+}
+util.inherits(MultipartUploadPartSizeError, MuskieError);
+
+
 function NotAcceptableError(req, type) {
     MuskieError.call(this, {
         restCode: 'NotAcceptable',
diff --git a/lib/obj.js b/lib/obj.js
index 31f38cb..dca9f72 100644
--- a/lib/obj.js
+++ b/lib/obj.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright (c) 2014, Joyent, Inc.
+ * Copyright (c) 2017, Joyent, Inc.
  */
 
 //
@@ -912,5 +912,11 @@ module.exports = {
             deletePointer
         ];
         return (chain);
-    }
+    },
+
+    parseArguments: parseArguments,
+    startSharkStreams: startSharkStreams,
+    sharkStreams: sharkStreams,
+    saveMetadata: saveMetadata,
+    DEF_MAX_LEN: DEF_MAX_LEN
 };
diff --git a/lib/other.js b/lib/other.js
index 5a91a42..c53333f 100644
--- a/lib/other.js
+++ b/lib/other.js
@@ -376,7 +376,6 @@ module.exports = {
             common.ensureParentHandler(),
             preflightPUTRequest,
             common.ensureEntryExistsHandler(),
-            // common.assertMetadataHandler(),
             preflightRequest
         ];
 
diff --git a/lib/server.js b/lib/server.js
index 1bf8b7e..d3643fb 100644
--- a/lib/server.js
+++ b/lib/server.js
@@ -24,10 +24,11 @@ var common = require('./common');
 var dir = require('./dir');
 var jobs = require('./jobs');
 var link = require('./link');
+var medusa = require('./medusa');
 var obj = require('./obj');
 var other = require('./other');
 var picker = require('./picker');
-var medusa = require('./medusa');
+var uploads = require('./uploads');
 
 // injects into the global namespace
 require('./errors');
@@ -284,6 +285,12 @@ function createServer(options, clearProxy) {
         authAction: 'mlogin'
     }, medusa.getMedusaAttachHandler());
 
+
+    // Multipart Uploads
+    if (options.enableMPU) {
+        addMultipartUploadRoutes(server);
+    }
+
     server.use(common.getMetadataHandler());
     server.use(auth.storageContext);
     server.use(auth.authorizationHandler());
@@ -303,6 +310,15 @@ function createServer(options, clearProxy) {
                common.assertMetadataHandler(),
                dir.getDirectoryHandler());
 
+    // /jhendricks/uploads/:prefix/:id/:partNum
+    //      - PUT
+    //      - HEAD
+    //      - Forbidden: GET/POST/DELETE
+    if (options.enableMPU) {
+        addMultipartUploadDataPlaneRoutes(server);
+    }
+
+
     // Root dir
 
     server.get({
@@ -334,10 +350,19 @@ function createServer(options, clearProxy) {
         authAction: 'deletedirectory'
     }, dir.rootDirHandler());
 
-    Object.keys(common.StoragePaths).forEach(function (k) {
+    // creates routes for everything in StoragePaths array
+    // put___directory
+    // put___link
+    // put___object
+    // options___storage
+    // get___storage
+    // head___storage
+    // delete___storage
+    var storagePaths = common.storagePaths(options);
+    Object.keys(storagePaths).forEach(function (k) {
 
-        var _p = common.StoragePaths[k].regex;
-        var _n = common.StoragePaths[k].name;
+        var _p = storagePaths[k].regex;
+        var _n = storagePaths[k].name;
 
         // Otherwise in audit/dtrace we'll see GetStorageStorage
         if (_n === 'Storage')
@@ -410,7 +435,180 @@ function createServer(options, clearProxy) {
     return (server);
 }
 
+function forbiddenHandler(req, res, next) {
+        req.log.info('Method ' + req.method + ' disallowed for ' + req.url);
+        res.send(403);
+        next(false);
+}
+
+
+function addMultipartUploadRoutes(server) {
+        server.post({
+        path: '/:account/uploads',
+        name: 'CreateUpload',
+        contentType: 'application/json'
+    }, uploads.createHandler());
+
+    server.put({
+        path: '/:account/uploads'
+    }, forbiddenHandler);
+
+    server.del({
+        path: '/:account/uploads'
+    }, forbiddenHandler);
+
+    /* JSSTYLED */
+    var uploadsRedirectPath = '/:account/uploads/[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}';
+    /* JSSTYLED */
+    var uploadsRedirectPathPart = '/:account/uploads/[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}/:partNum';
+    server.get({
+        path: uploadsRedirectPath,
+        contentType: '*/*'
+    }, uploads.redirectHandler());
+
+    server.put({
+        path: uploadsRedirectPath,
+        contentType: '*/*'
+    }, uploads.redirectHandler());
+
+    server.head({
+        path: uploadsRedirectPath,
+        contentType: '*/*'
+    }, uploads.redirectHandler());
+
+    server.del({
+        path: uploadsRedirectPath,
+        contentType: '*/*'
+    }, uploads.redirectHandler());
+
+    server.post({
+        path: uploadsRedirectPath,
+        contentType: '*/*'
+    }, uploads.redirectHandler());
+
+    server.get({
+        path: uploadsRedirectPathPart,
+        contentType: '*/*'
+    }, uploads.redirectHandler());
+
+    server.put({
+        path: uploadsRedirectPathPart,
+        contentType: '*/*'
+    }, uploads.redirectHandler());
+
+    server.head({
+        path: uploadsRedirectPathPart,
+        contentType: '*/*'
+    }, uploads.redirectHandler());
+
+    server.del({
+        path: uploadsRedirectPathPart,
+        contentType: '*/*'
+    }, uploads.redirectHandler());
+
+    server.post({
+        path: uploadsRedirectPathPart,
+        contentType: '*/*'
+    }, uploads.redirectHandler());
+
+    server.put({
+        path: '/:account/uploads/[0-f]+/:id'
+    }, forbiddenHandler);
+
+    server.post({
+        path: '/:account/uploads/[0-f]+/:id'
+    }, forbiddenHandler);
+
+    /* BEGIN JSSTYLED */
+    /* server.del({
+        path: '/:account/uploads/[0-f]+/:id'
+    }, forbiddenHandler); */
+    /* END JSSTYLED */
+
+    server.get({
+        path: '/:account/uploads/[0-f]+/:id/state',
+        name: 'GetUpload'
+    }, uploads.getHandler());
+
+    server.head({
+        path: '/:account/uploads/[0-f]+/:id/state'
+    }, forbiddenHandler);
+
+    server.put({
+        path: '/:account/uploads/[0-f]+/:id/state'
+    }, forbiddenHandler);
+
+    server.post({
+        path: '/:account/uploads/[0-f]+/:id/state'
+    }, forbiddenHandler);
+
+    server.del({
+        path: '/:account/uploads/[0-f]+/:id/state'
+    }, forbiddenHandler);
+
+    server.post({
+        path: '/:account/uploads/[0-f]+/:id/abort',
+        name: 'AbortUpload'
+    }, uploads.abortHandler());
+
+    server.get({
+        path: '/:account/uploads/[0-f]+/:id/abort'
+    }, forbiddenHandler);
+
+    server.put({
+        path: '/:account/uploads/[0-f]+/:id/abort'
+    }, forbiddenHandler);
+
+    server.head({
+        path: '/:account/uploads/[0-f]+/:id/abort'
+    }, forbiddenHandler);
+
+    server.del({
+        path: '/:account/uploads/[0-f]+/:id/abort'
+    }, forbiddenHandler);
+
+    server.post({
+        path: '/:account/uploads/[0-f]+/:id/commit',
+        name: 'CommitUpload',
+        contentType: 'application/json'
+    }, uploads.commitHandler());
+
+    server.get({
+        path: '/:account/uploads/[0-f]+/:id/commit'
+    }, forbiddenHandler);
+
+    server.put({
+        path: '/:account/uploads/[0-f]+/:id/commit'
+    }, forbiddenHandler);
+
+    server.head({
+        path: '/:account/uploads/[0-f]+/:id/commit'
+    }, forbiddenHandler);
+
+    server.del({
+        path: '/:account/uploads/[0-f]+/:id/commit'
+    }, forbiddenHandler);
+}
+
+function addMultipartUploadDataPlaneRoutes(server) {
+        server.put({
+            path: '/:account/uploads/[0-f]+/:id/:partNum',
+            name: 'UploadPart',
+            contentType: '*/*'
+        }, uploads.uploadPartHandler());
 
+        server.get({
+            path: '/:account/uploads/[0-f]+/:id/:partNum'
+        }, forbiddenHandler);
+
+            server.post({
+            path: '/:account/uploads/[0-f]+/:id/:partNum'
+        }, forbiddenHandler);
+
+        server.del({
+            path: '/:account/uploads/[0-f]+/:id/:partNum'
+        }, forbiddenHandler);
+}
 
 ///--- Exports
 
diff --git a/lib/shark_client.js b/lib/shark_client.js
index 98db362..b9c223d 100644
--- a/lib/shark_client.js
+++ b/lib/shark_client.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright (c) 2014, Joyent, Inc.
+ * Copyright (c) 2017, Joyent, Inc.
  */
 
 var EventEmitter = require('events').EventEmitter;
@@ -55,8 +55,11 @@ util.inherits(SharkResponseError, Error);
 
 function _request(opts, cb) {
     cb = once(cb);
-
     var req = http.request(opts);
+
+    if (opts.body) {
+        req.write(JSON.stringify(opts.body));
+    }
     /*
      * This timer represents the timeout for connecting to the shark
      * for this request, so it is important that it is cleared only once
@@ -85,6 +88,10 @@ function _request(opts, cb) {
         cb(new ConnectTimeoutError(opts.hostname, opts.connectTimeout));
     }, opts.connectTimeout);
 
+    if (opts.method === 'POST') {
+        clearTimeout(connectionTimer);
+    }
+
     req.once('error', function onRequestError(err) {
         clearTimeout(connectionTimer);
         cb(err);
@@ -117,8 +124,9 @@ function _request(opts, cb) {
     });
 
     req.once('response', onResponse);
-    if (opts.method !== 'PUT')
+    if (opts.method !== 'PUT') {
         req.end();
+    }
 }
 
 
@@ -135,11 +143,19 @@ function request(thisp, method, opts, cb) {
         },
         hostname: thisp.hostname,
         method: method,
-        path: '/' + (opts.creator || opts.owner) + '/' + opts.objectId,
-        port: thisp.port,
-        requestId: opts.requestId
+        port: thisp.port
     };
 
+    if (!opts.path) {
+        _opts.path = '/' + (opts.creator || opts.owner) + '/' + opts.objectId;
+    } else {
+        _opts.path = opts.path;
+    }
+
+    if (opts.body) {
+        _opts.body = opts.body;
+    }
+
     log.debug(_opts, 'request: entered');
 
     // don't log this
@@ -227,7 +243,7 @@ util.inherits(SharkClient, EventEmitter);
 
 
 /**
- * Wraps up the restify http_client.get request.
+ * Wraps node's http request.
  *
  * Options needs:
  *   - objectId
@@ -251,7 +267,7 @@ SharkClient.prototype.get = function get(opts, cb) {
 
 
 /**
- * Wraps up the restify http_client.head request.
+ * Wraps node's http request.
  *
  * Options needs:
  *   - objectId
@@ -274,8 +290,8 @@ SharkClient.prototype.head = function head(opts, cb) {
 };
 
 
-/**
- * Wraps up the restify http_client.put request.
+/*
+ * Wraps up node's http request.
  *
  * Options needs:
  *   - contentLength
@@ -321,12 +337,39 @@ SharkClient.prototype.put = function put(opts, cb) {
 };
 
 
+/*
+ * Wraps up node's http request.
+ *
+ * Options needs:
+ *   - objectId
+ *   - contentType
+ *   - objectId
+ *   - owner
+ *   - requestId
+ *
+ * @param {object} options see above
+ * @param {body} JSON blob to send in POST request
+ * @param {function} callback => f(err, req)
+ */
+SharkClient.prototype.post = function post(opts, body, cb) {
+    assert.object(opts, 'options');
+    assert.object(body, 'body');
+    assert.string(opts.objectId, 'options.objectId');
+    assert.string(opts.owner, 'options.owner');
+    assert.string(opts.requestId, 'options.requestId');
+    assert.func(cb, 'callback');
+
+    opts.body = body;
+
+    request(this, 'POST', opts, cb);
+};
+
+
 SharkClient.prototype.toString = function toString() {
     return ('[object SharkClient<' + this.hostname + '>]');
 };
 
 
-
 ///--- Exports
 
 module.exports = {
diff --git a/lib/uploads/abort.js b/lib/uploads/abort.js
new file mode 100644
index 0000000..e489b09
--- /dev/null
+++ b/lib/uploads/abort.js
@@ -0,0 +1,103 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+var assert = require('assert-plus');
+var restify = require('restify');
+
+var auth = require('../auth');
+var uploadsCommon = require('./common');
+require('../errors');
+
+
+///--- API
+
+
+
+/*
+ * Ensures that the upload is in the correct state: either created or aborted,
+ * and updates the upload record's metadata if needed to reflect this state.
+ *
+ */
+function finalizingState(req, res, next) {
+    var log = req.log;
+    var upload = req.upload;
+    var id = upload.id;
+
+    var states = uploadsCommon.uploadStates;
+    var types = uploadsCommon.uploadTypes;
+    var state = req.upload.get(uploadsCommon.mdKeys.STATE);
+    var type = req.upload.get(uploadsCommon.mdKeys.TYPE);
+
+    log.info({
+        uploadId: id,
+        uploadState: state,
+        finalizingType: type ? type : 'N/A'
+    }, 'abort: requested');
+
+    if (state === states.CREATED) {
+        assert.ok(!type);
+        upload.finalizeUploadRecord(types.ABORT, null, function (err2) {
+                if (err2) {
+                    next(err2);
+                } else {
+                    next();
+                }
+        });
+    } else if ((state === states.FINALIZING) &&
+        (type === types.ABORT)) {
+
+        log.info('abort already in progress for upload ' + id);
+        next();
+
+    } else if ((state === states.FINALIZING) &&
+        (type === types.COMMIT)) {
+
+        next(new MultipartUploadFinalizeConflictError(id, types.ABORT));
+
+    } else {
+        assert.fail('Invalid state/type combination for upload: '
+            + state + '/' + type);
+     }
+}
+
+
+function abort(req, res, next) {
+    var log = req.log;
+
+    req.upload.abortUpload(function (err) {
+        if (err) {
+            next(err);
+        } else {
+            log.info({
+                uploadId: req.upload.id
+            }, 'abort: completed');
+
+            res.setHeader('Content-Length', '0');
+            res.send(204);
+            next();
+        }
+    });
+}
+
+
+///--- Exports
+
+module.exports = {
+    abortHandler: function abortHandler() {
+        var chain = [
+            uploadsCommon.loadUpload,
+            uploadsCommon.uploadContext,
+            auth.authorizationHandler(),
+            finalizingState,
+            abort
+        ];
+        return (chain);
+    }
+};
diff --git a/lib/uploads/commit.js b/lib/uploads/commit.js
new file mode 100644
index 0000000..dc67898
--- /dev/null
+++ b/lib/uploads/commit.js
@@ -0,0 +1,522 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+var assert = require('assert-plus');
+var crypto = require('crypto');
+var libmanta = require('libmanta');
+var libuuid = require('libuuid');
+var restify = require('restify');
+var path = require('path');
+var util = require('util');
+var vasync = require('vasync');
+var verror = require('verror');
+
+var auth = require('../auth');
+var common = require('../common');
+var obj = require('../obj');
+var sharkClient = require('../shark_client');
+var uploadsCommon = require('./common');
+var utils = require('../utils');
+require('../errors');
+
+
+///--- Globals
+
+var clone = utils.shallowCopy;
+var sprintf = util.format;
+
+// 5 MB
+var MIN_PART_SIZE = 5242880;
+
+
+///--- Helpers
+
+/*
+ * Invokes the mako-finalize operation on a single shark.
+ *
+ * Parameters:
+ * - req the current request
+ * - body: body to POST to the shark
+ * - opts: an options blob to pass to the shark client
+ * - shark: a shark object
+ */
+function invokeMakoFinalize(req, body, opts, shark, cb) {
+    var client = sharkClient.getClient({
+        connectTimeout: req.sharkConfig.connectTimeout,
+        log: req.log,
+        retry: req.sharkConfig.retry,
+        shark: shark,
+        agent: req.sharkAgent
+    });
+    assert.ok(client, 'sharkClient returned null');
+
+    var start = Date.now();
+    var hostname = shark.manta_storage_id;
+
+    client.post(opts, body, function (err, _, res) {
+        /*
+         * Similar to PUTs, log information about sharks we contacted over
+         * the course of the commit request.
+         */
+        var sharkInfo = {
+            shark: hostname,
+            timeTotal: Date.now() - start,
+            result: 'fail',
+            _startTime: start
+        };
+        req.sharksContacted.push(sharkInfo);
+
+        var s = {
+            shark: hostname,
+            md5: null
+        };
+        if (err) {
+            cb(err, s);
+        } else {
+            s.md5 = res.headers['x-joyent-computed-content-md5'];
+            if (!s.md5) {
+                cb(new InternalError('mako failed to return an MD5 sum'), s);
+            } else {
+                sharkInfo.result = 'ok';
+                cb(null, s);
+            }
+        }
+    });
+}
+
+
+// Given an array of etags, computes the md5 sum of all etags concatenated.
+function computePartsMD5(parts) {
+    var hash = crypto.createHash('md5');
+    parts.forEach(function (p) {
+        hash.update(p);
+    });
+
+    return (hash.digest('base64'));
+}
+
+
+///--- API
+
+
+// Get the owner of the resource of the input object path.
+function loadOwner(req, res, next) {
+    var p = req.upload.get(uploadsCommon.mdKeys.OBJECT_PATH);
+    auth.loadOwnerFromPath(req, p, next);
+}
+
+
+/*
+ * For authorizing the use of the object path on commit, we rely on
+ * existing handlers for other muskie endpoints: in particular,
+ * getMetadata, which loads the metadata for the parent directory, if
+ * needed, and the metadata for the object at the path stored at req.key; and
+ * storageContext, with sets up the authContext used by mahi to authorize
+ * the caller to perform the given action. In order to use these handlers,
+ * we need to set some state on the request object, specifically:
+ *      - req.key: the key of the object path (for this commit)
+ *      - req.parentKey: the key of the object path's parent directory
+ */
+function setupMetadataState(req, res, next) {
+    var log = req.log;
+
+    req._path = req.upload.get(uploadsCommon.mdKeys.OBJECT_PATH);
+
+    req.key = req.upload.get(uploadsCommon.mdKeys.OBJECT_PATH_KEY);
+    if (!req.isRootDirectory(req.key)) {
+        req.parentKey = path.dirname(req.key);
+    }
+
+    log.info({
+        objectPathKey: req.key,
+        parentKey: req.parentKey
+    }, 'passing keys to getMetadata');
+
+    next();
+}
+
+
+/*
+ * Ensures that the upload is in a proper state before proceeding: either
+ * CREATED or COMMIT. It it is in state COMMIT, the request must specify the
+ * same set of parts as is recorded in the upload record.
+ */
+function validateUploadState(req, res, next) {
+    var log = req.log;
+    var parts = req.body.parts || [];
+    var id = req.upload.id;
+
+    var states = uploadsCommon.uploadStates;
+    var types = uploadsCommon.uploadTypes;
+
+    var state = req.upload.get(uploadsCommon.mdKeys.STATE);
+    var type = req.upload.get(uploadsCommon.mdKeys.TYPE);
+    var computedPartsMD5 = computePartsMD5(parts);
+
+    req.upload._partsMD5 = computedPartsMD5;
+
+    log.info({
+        uploadId: id,
+        parts: parts,
+        uploadState: state,
+        finalizingType: type ? type : 'N/A',
+        parts: parts,
+        computedPartsMD5: computedPartsMD5
+    }, 'commit: requested');
+
+    if (state === states.FINALIZING) {
+        if (type === types.ABORT) {
+            // Abort already in progress
+            next(new MultipartUploadAbortedError(id));
+
+        } else if (type === types.COMMIT) {
+            // Not an error, but we need to verify the parts are the
+            // same as the input ones before proceeding
+            var p = req.upload.get(uploadsCommon.mdKeys.PARTS_MD5);
+            if (computedPartsMD5 !== p) {
+                log.info({
+                    userSpecified: computedPartsMD5,
+                    expected: p
+                }, 'mismatch of parts md5');
+
+                next(new MultipartUploadCommitInProgressError(id));
+            } else {
+                next();
+            }
+
+        } else {
+            assert.fail('Invalid type: ' + type);
+        }
+    } else if (state === states.CREATED) {
+        next();
+    } else {
+        assert.fail('Invalid state: ' + state);
+    }
+}
+
+
+/*
+ * Ensures that the input parts set for the commit has the etags of the parts
+ * as they exist now. This step also checks that all parts have a size that
+ * exceeds the minimum part size (excluding the last part).
+ */
+function validateParts(req, res, next) {
+    var log = req.log;
+    var id = req.upload.id;
+    var parts = req.body.parts;
+    log.info('validating parts for upload');
+
+    if (!parts) {
+        req.body.parts = [];
+        log.info('empty parts array');
+        next();
+        return;
+    } else if (parts.length > 10000) {
+        next(new MultipartUploadPartLimitError(req.upload.id, parts.length));
+        return;
+    }
+
+    var errors = [];
+    var sum = 0;
+
+    /*
+     * This function verifies that:
+     * - the etag exists
+     * - the etag matches the current etag for the part
+     * - the size of the part is at least the minimum size, unless
+     *   it's the last part
+     */
+    function validateEtag(part, cb) {
+        var index = part.index;
+        var etag = part.etag;
+
+        var record = req.upload.uploadMd;
+        var key = record.key + '/' + index;
+
+        if (etag === '') {
+            cb(new MultipartUploadMissingPartError(id, index, etag));
+            return;
+        }
+
+        var opts = {
+            key: key,
+            requestId: req.getId()
+        };
+
+        req.moray.getMetadata(opts, function (err, md) {
+            log.info('part index: ' + index + ', etag: ' + etag);
+            if (err) {
+                if (verror.hasCauseWithName(err, 'ObjectNotFoundError')) {
+                    //  No part with that part number has been uploaded.
+                    errors.push(new MultipartUploadMissingPartError(id, index,
+                        etag));
+                } else {
+                    errors.push(new InternalError(err));
+                }
+            } else {
+                var size = parseInt(md.contentLength, 10);
+                var isFinalPart = index === (parts.length - 1);
+
+                if (md.etag !== etag) {
+                    // Uploaded part has a different etag than the input one.
+                    errors.push(new MultipartUploadPartEtagError(id, index,
+                        etag));
+                } else if (!isFinalPart && (size < MIN_PART_SIZE)) {
+                    if (!(parts.length > 1 && (index === (parts.length - 2)))) {
+                        errors.push(new MultipartUploadPartSizeError(id, index,
+                            size));
+                    }
+                }
+
+                sum += size;
+            }
+
+            cb();
+        });
+    }
+
+    var queue = vasync.queue(validateEtag, 10);
+    parts.forEach(function (val, i) {
+        queue.push({
+            index: i,
+            etag: val
+        });
+    });
+    queue.close();
+
+    queue.on('end', function () {
+        log.info('part validation completed');
+        if (errors.length > 0) {
+
+            // Even if there's an internal error, still send the user any uesr
+            // errors, so they can be corrected and retried.
+            for (var i = 0; i < errors.length; i++) {
+                var e = errors[i];
+                if (e.statusCode >= 500) {
+                    log.error('internal error: ' + e);
+                } else if (e.statusCode >= 400) {
+                    next(e);
+                    return;
+                } else {
+                    assert.fail('invalid error: ' + e);
+                }
+            }
+
+            next(new InternalError('commit error'));
+
+        } else {
+            if (sum > obj.DEF_MAX_LEN) {
+                // TODO: the error message for this is a litle misleading
+                next(new MaxContentLengthError(sum));
+            } else {
+                req.upload.checkSize(sum, function (valid, expected) {
+                    if (!valid) {
+                        next(new MultipartUploadContentLengthError(id, expected,
+                            sum));
+                    } else {
+                        req.upload._size = sum;
+                        next();
+                    }
+                });
+            }
+        }
+    });
+}
+
+
+/*
+ * Saves the upload record with its state set to FINALIZING.
+ */
+function finalizingState(req, res, next) {
+    req.upload.finalizeUploadRecord(
+        uploadsCommon.uploadTypes.COMMIT,
+        req.upload._partsMD5,
+        function (err) {
+            if (err) {
+                next(err);
+            } else {
+                next();
+            }
+    });
+}
+
+
+/*
+ * Invokes the mako-finalize operation on each shark selected at the beginning
+ * of the upload. If there is a problem with any of the sharks, this operation
+ * will fail.
+ *
+ * The mako node expects a JSON blob of the form:
+ * {
+ *      version,        // the version of multipart upload this is
+ *      owner,          // string uuid of the owner of the upload object
+ *      nbytes,         // expected size of the object
+ *      objectId,       // string uuid of object
+ *      parts,          // array of string uuids for each part
+ * }
+ *
+ * This handler is also expected to set the following on the uploads object:
+ *  - contentMD5
+ *  - objectId (if it does not exist yet)
+ */
+function finalizeUpload(req, res, next) {
+    var log = req.log;
+
+    var objectId = req.upload.get(uploadsCommon.mdKeys.OBJECT_ID);
+    var sharks = req.upload.get(uploadsCommon.mdKeys.SHARKS);
+    var nbytes = req.upload._size;
+
+    // Skip mako-finalize for zero-byte uploads.
+    if (nbytes === 0) {
+        log.info('zero-byte object; skipping mako-finalize');
+        req.upload._md5 = '1B2M2Y8AsgTpgAmY7PhCfg==';
+        req.upload._size = 0;
+        next();
+        return;
+    }
+
+    var body = {
+        version: 1,
+        nbytes: nbytes,
+        account: req.owner.account.uuid,
+        objectId: objectId,
+        parts: req.body.parts
+    };
+    log.info('mako request body: ' + JSON.stringify(body));
+
+    var opts = {
+        objectId: objectId,
+        owner: req.owner.account.uuid,
+        requestId: req.getId(),
+        path: '/mpu/v1/commit'
+    };
+
+    req.sharksContacted = [];
+
+    vasync.forEachParallel({
+        func: function finalize(shark, cb) {
+            var _opts = clone(opts);
+            var _body = clone(body);
+            invokeMakoFinalize(req, _body, _opts, shark, cb);
+        },
+        inputs: sharks
+    },  function (err, results) {
+            log.info('mako-finalize: completed on all sharks');
+
+            if (err) {
+                results.operations.forEach(function (r) {
+                    log.error('error with shark ' + r.result.shark +
+                        ': ' + r.err);
+                });
+                // TODO: this should probably be a new type of error.
+                next(new SharksExhaustedError());
+            } else {
+                var md5 = null;
+
+                // Validate that all makos returned the same md5 sum.
+                var mismatch = false;
+                results.operations.forEach(function (r) {
+                    assert.ok(r.status === 'ok');
+                    assert.ok(r.result);
+
+                    if (md5 && (md5 !== r.result.md5)) {
+                        mismatch = true;
+                    } else {
+                        md5 = r.result.md5;
+                    }
+                });
+
+                if (mismatch) {
+                    log.error('mako nodes returned different md5 sums for ' +
+                        'the same object');
+
+                    results.operations.forEach(function (r) {
+                        log.error(sprintf('shark \"%s\", md5: %s',
+                            r.result.shark, r.result.md5));
+                    });
+
+                    next(new InternalError());
+                } else {
+                    // Validate user-provided md5 sums.
+                    req.upload.checkMD5(md5, function (valid, expected) {
+                        if (!valid) {
+                            next(new ChecksumError(expected, md5));
+                        } else {
+                            req.upload._md5 = md5;
+                            next();
+                        }
+                    });
+                }
+            }
+    });
+}
+
+
+/*
+ * This step makes the committed upload visible from Manta by atomically
+ * inserting a commit record and object record on the shard associated
+ * with the object. Most of the heavy lifting is done by the req.uploads
+ * object here.
+ */
+function commit(req, res, next) {
+    var log = req.log;
+
+    var size = req.upload._size;
+    var md5 = req.upload._md5;
+    var partsMD5 = req.upload._partsMD5;
+
+    assert.number(size);
+    assert.string(md5);
+
+    req.upload.commitUpload(partsMD5, size, md5, function (err) {
+        if (err) {
+            next(err);
+        } else {
+            var p = req.upload.get(uploadsCommon.mdKeys.OBJECT_PATH);
+
+            log.info({
+                uploadId: req.upload.id,
+                objectPath: p
+            }, 'commit: completed');
+
+            res.setHeader('Location', p);
+            res.send(201);
+            next();
+        }
+    });
+}
+
+
+///--- Exports
+
+module.exports = {
+    commitHandler: function commitHandler() {
+        var chain = [
+            uploadsCommon.loadUpload,
+            uploadsCommon.uploadContext,
+            auth.authorizationHandler(),
+            loadOwner,
+            setupMetadataState,
+            common.getMetadataHandler(),
+            auth.storageContext,
+            auth.authorizationHandler(),
+            restify.jsonBodyParser({
+                mapParams: false,
+                maxBodySize: 500000
+            }),
+            validateUploadState,
+            validateParts,
+            finalizingState,
+            finalizeUpload,
+            commit
+        ];
+        return (chain);
+    }
+};
diff --git a/lib/uploads/common.js b/lib/uploads/common.js
new file mode 100644
index 0000000..83f7b68
--- /dev/null
+++ b/lib/uploads/common.js
@@ -0,0 +1,1064 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+var util = require('util');
+
+var assert = require('assert-plus');
+var jsprim = require('jsprim');
+var libmanta = require('libmanta');
+var libuuid = require('libuuid');
+var path = require('path');
+var verror = require('verror');
+
+var common = require('../common');
+var obj = require('../obj');
+require('../errors');
+
+
+/*
+ *  This file contains the majority of the logic for handling multipart uploads.
+ *
+ *
+ *  API OVERVIEW:
+ *
+ *  The Manta multipart upload API allows clients to upload a Manta object
+ *  by splitting it into parts and uploading the parts individually. When all
+ *  parts are uploaded, the client signifies that the upload is completed by
+ *  "committing" the upload through the API, which creates a Manta object
+ *  that is the sum of the uploaded parts and is indistiguisable from an
+ *  object created through a normal Manta PUT. If a client decides not to
+ *  finish the upload, it may also abort the upload process.
+ *
+ *  The possible operations in the mulitpart upload API are:
+ *      - create: establish a multipart upload
+ *      - upload-part: upload a part of the object
+ *      - abort: cancel the upload
+ *      - commit: complete the upload
+ *      - get: get information about an ongoing upload
+ *
+ *  There is an additional API endpoint designed for client usability purposes
+ *  that redirects all requests sent to the path /:account/upload/:id to the
+ *  correct upload path.
+ *
+ *
+ *  TERMS:
+ *
+ *  There is some terminology that is used consistently throughout the
+ *  multipart upload implementation that is useful to know:
+ *
+ *   - Upload ID: a uuid representing a multipart upload request, selected
+ *     when the upload is created.
+ *
+ *   - Upload path: The path where parts of an upload are uploaded to and
+ *     stored in Manta.
+ *
+ *   - Upload record: The Manta directory record for the upload path. This
+ *     record contains state about the upload recording in an additional
+ *     "upload" blob tacked onto the record. The upload record gives us a
+ *     mechanism of passing state about an upload across various upload
+ *     requests.
+ *
+ *   - Object path: An input to creating an upload, this refers to the path
+ *     the object (created from the uploaded parts) will be stored at in
+ *     Manta.
+ *
+ *   - Object record: The Manta object record for the upload's object path.
+ *
+ *   - Finalizing record: A Manta record (stored in a different
+ *     bucket than object and directory metadata -- namely, "manta_uploads")
+ *     and on the same shard as the object record. This upload is identified
+ *     by both the object path and upload ID (both of which are used to
+ *     construct the key used to insert the record into Moray). The presence of
+ *     a finalizing record for a given object path and upload id indicates that
+ *     either a commit or abort has begun for the upload. The finalizing record
+ *     stores which type the record is, the upload ID and the etags for the
+ *     parts.
+ *
+ *
+ *  METADATA STRUCTURE:
+ *
+ *  Because most of the state about an upload is stored in metadata records in
+ *  Moray, it is important to have a well-defined structure for what this
+ *  information looks like.
+ *
+ *   - Upload record: This record has the same structure as a typical Manta
+ *     directory record, with an additional object called "upload" that has
+ *     the following structure:
+ *
+ *          upload {
+ *              id,             // upload id
+ *              state,          // state of the upload: CREATED or FINALIZING
+ *              type,           // if state is FINALIZING, then ABORT or COMMIT
+ *              objectPath,     // object path
+ *              uploadPath,     // upload path
+ *              headers,        // headers to store on object record
+ *              sharks,         // mako sharks the object is stored on
+ *              parts,          // when commit has started, etags of each part
+ *              objectId        // object ID for the uploaded object
+ *          }
+ *
+ *    - Finalizing record: This record has the same structure as a typical Manta
+ *     directory record, with an additional object called "upload" that has
+ *     the following structure:
+ *          upload {
+ *              id,             // upload id
+ *              type,           // ABORT or COMMIT
+ *              parts,          // when type is COMMIT, etags of each part
+ *              objectPath      // object path
+ *          }
+ *
+ *    - Object record: The object record is a normal Manta object record, but
+ *      there are a few fields on the object that are set explitily by the
+ *      multipart upload code, instead of the common metadata code.
+ *
+ *      In particular, the following fields are set explicitly:
+ *        - objectId: This is generated when the object is created and is
+ *          needed for mako-finalize.
+ *        - contentLength: This is set either by the user when creating the
+ *          upload (and validated by the commit endpoint), or it is
+ *          calculated on commit.
+ *        - contentMD5: This is set either by the user when creating the
+ *          upload (and validated by the commit endpoint), or it is
+ *          calculated on commit.
+ *        - contentMD5: This is set either by the user when creating the
+ *          upload, or set to a default value.
+ *        - headers: This is set by the user when creating the upload.
+ *        - sharks: These are selected when the upload is created.
+ *
+ *
+ *  IMPLEMENTATION DETAILS:
+ *
+ *  The logic of this API is implemented as methods on the MultipartUpload
+ *  object defined in this file. When a multipart upload related request comes
+ *  in to muskie, a new MultipartUpload request is constructed, and sets up
+ *  some state the various handlers will need.
+ *
+ *  After an upload's creation, most of the state about the upload is stored in
+ *  the upload record. After validating inputs to a request, the first thing a
+ *  multipart upload API endpoint should do is call the method uploadState(),
+ *  which will load the upload record from Moray and allow the handlers to
+ *  fetch state from the record using the get() method, and modify the record
+ *  using the set() method.
+ *
+ *  Once the API handlers have completed the relevant logic based on the
+ *  upload's state, they each call a relevant method on the upload object
+ *  ({create,abort,commit,get}Upload()). These methods take care of saving
+ *  the upload record back to Moray, and perform any additional metadata
+ *  transformations as needed.
+ *
+ *  // TODO: explain how authorization works
+ *
+ */
+
+
+///--- Globals
+
+var sprintf = util.format;
+
+var ID_REGEX = /^[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}$/;
+var PREFIX_LENGTH = 1;
+
+// Upload states
+var states = {
+    CREATED: 'created',
+    FINALIZING: 'finalizing'
+};
+
+// Finalizing types
+var types = {
+    COMMIT: 'commit',
+    ABORT: 'abort'
+};
+
+// Used to lookup values in a loaded upload record
+var mdKeys = {
+    STATE: 'state',
+    TYPE: 'type',
+    OBJECT_PATH: 'objectPath',
+    OBJECT_PATH_KEY: 'objectPathKey',
+    UPLOAD_PATH: 'uploadPath',
+    HEADERS: 'headers',
+    SHARKS: 'sharks',
+    PARTS_MD5: 'partsMD5',
+    OBJECT_ID: 'objectId'
+};
+
+
+
+///--- Helpers
+
+/*
+ * Creates the upload record for the upload path.
+ * (e.g., /jhendricks/uploads/c/c46ac2b1-fcc3-4e12-8c46-c935808ed59f)
+ *
+ * Parameters:
+ *  - upload: MultipartUpload object
+ *  - opts: options blob that must have the following items:
+ *      - objectPath
+ *      - sharks
+ *      - headers
+ *  - cb: function that is passed an error and the metadata blob
+ */
+function createUploadRecord(upload, opts, cb) {
+    assert.object(opts);
+    assert.string(opts.objectPath);
+    assert.string(opts.objectPathKey);
+    assert.object(opts.sharks);
+    assert.object(opts.headers);
+    assert.string(upload.uploadPath);
+    assert.string(upload.id);
+    assert.func(cb);
+
+    var req = upload.req;
+
+    /*
+     * createMetadata assumes that the key for the metadata it should create
+     * is saved in req.key, which is true for most requests. In this case,
+     * we save the key stored in req.key (which corresponds to the path
+     * /:account/uploads, instead of the upload record key) and restore it
+     * after the metadata is created. This is a bit janky, but allows us to
+     * reuse existing code to create metadata for directories much more
+     * easily.
+     */
+    var savedKey = req.key;
+    assert.ok(upload.uploadMd.key);
+    req.key = upload.uploadMd.key;
+
+    common.createMetadata(req, 'directory', function (err, md) {
+        req.key = savedKey;
+
+        if (err) {
+            cb(err);
+        } else {
+            //md._etag = null;
+            md.upload = {
+                id: upload.id,
+                state: states.CREATED,
+                type: null,  // used only for finalizing uploads
+                objectPath: opts.objectPath,
+                objectPathKey: opts.objectPathKey,
+                uploadPath: upload.uploadPath,
+                headers: opts.headers,
+                sharks: opts.sharks,
+                partsMD5: null, // used only for commits
+                objectId: libuuid.create()
+            };
+
+            cb(null, md);
+        }
+    });
+}
+
+/*
+ * Creates the metadata for the finalizing record for the upload, for both
+ * commit and aborts.
+ *
+ * Parameters:
+ *  - upload: MultipartUpload object
+ *  - type: finalizing type
+ *  - md5: if a commit, the md5 sum of the object
+ *  - cb: function that is passed the metadata blob
+ */
+function createFinalizingRecord(upload, type, md5, cb) {
+    assert.ok(type === types.COMMIT || type === types.ABORT);
+    assert.func(cb);
+
+    var req = upload.req;
+
+    var md = {
+        uploadId: upload.id,
+        finalizingType: type,
+        owner: req.owner.account.uuid,
+        requestId: req.getId(),
+        objectPath: upload.get(mdKeys.OBJECT_PATH),
+        objectId: upload.get(mdKeys.OBJECT_ID),
+        md5: md5,
+        _etag: null
+    };
+
+    cb(md);
+}
+
+
+/*
+ * Creates the object record.
+ *
+ * Parameters:
+ *  - upload: MultipartUpload object
+ *  - size: size of the object
+ *  - md5: md5 sum (calculated in mako-finalize)
+ *  - cb: function that is passed an error and the metadata blob
+ */
+
+function createObjectRecord(upload, size, md5, cb) {
+    var req = upload.req;
+    var objPath = upload.get(mdKeys.OBJECT_PATH);
+
+    normalize(upload.req, objPath, function (err, objKey) {
+        if (err) {
+            cb(err);
+        } else {
+            //TODO: comment
+            var savedKey = req.key;
+            var savedHeaders = req.headers;
+
+            req.key = objKey;
+            req.headers = upload.get(mdKeys.HEADERS);
+            req.query.metadata = null;
+
+            common.createMetadata(req, 'object', function (err2, md) {
+                if (err2) {
+                    cb(err2);
+                } else {
+                    req.key = savedKey;
+                    req.headers = savedHeaders;
+
+                    // createMetadata does most of the work for us here, but
+                    // a few values need to be overwritten.
+                    md.objectId = upload.get(mdKeys.OBJECT_ID);
+                    md.contentLength = size;
+                    md.contentMD5 = md5;
+
+                    var ct = upload.get(mdKeys.HEADERS)['content-type'];
+                    if (ct) {
+                        md.contentType = ct;
+                    } else {
+                        md.contentType = 'application/octet-stream';
+                    }
+
+                    md.sharks = upload.get(mdKeys.SHARKS);
+                    //TODO: _etag.
+
+                    cb(null, md);
+                }
+            });
+        }
+    });
+}
+
+
+/*
+ * Saves the upload record to moray with the given state and type.
+ *
+ * Parameters:
+ *  - upload: MultipartUpload object
+ *  - state: upload state
+ *  - type: if applicable, finalizing type
+ *  - cb: function
+ */
+function persistUploadRecord(upload, state, type, cb) {
+    assert.ok(state === states.CREATED || state === states.FINALIZING);
+    assert.func(cb);
+    if (state === states.CREATED) {
+        assert.ok(!type);
+    } else {
+        assert.ok(type === types.COMMIT || type === types.ABORT);
+    }
+    assert.ok(upload.uploadMd.toSave, 'no upload record to save');
+
+    var log = upload.req.log;
+
+    upload.set(mdKeys.STATE, state);
+    upload.set(mdKeys.TYPE, type);
+
+    upload.uploadMd.toSave.requestId = upload.req.getId();
+    upload.req.moray.putMetadata(upload.uploadMd.toSave, function (err, md) {
+        if (err) {
+            cb(err);
+        } else {
+            log.debug({
+                uploadId: upload.id,
+                record: md
+            }, 'persistUploadRecord: done');
+            cb();
+        }
+    });
+}
+
+
+/*
+ * Loads the upload record, saves a copy of it, and creates a new
+ * copy that can be modified throughout the request.
+ */
+function loadUploadRecord(upload, cb) {
+    var record = upload.uploadMd;
+
+    var options = {
+        key: record.key,
+        requestId: upload.req.getId()
+    };
+
+    upload.req.moray.getMetadata(options, function (err, md) {
+        if (err) {
+            cb(err);
+        } else {
+            assert.ok(md);
+            assert.ok(md.upload, '\"upload\" not present in upload record md');
+
+            record.loaded = md;
+            record.toSave = jsprim.deepCopy(record.loaded);
+debugger;
+            cb(null, record.loaded.upload);
+        }
+    });
+}
+
+/*
+ * Loads the finalizing record.
+ */
+function loadFinalizingMetadata(upload, cb) {
+    var record = upload.finalizingMd;
+
+    function load(u, key, r, lcb) {
+        assert.ok(key, 'key');
+
+        r.key = key;
+        var options = {
+            key: key,
+            requestId: upload.req.getId()
+        };
+
+        upload.req.moray.getFinalizingMetadata(options, function (err, md) {
+            if (err) {
+                lcb(err);
+            } else {
+                assert.ok(md);
+                r.loaded = md;
+                lcb(null, r.loaded);
+            }
+        });
+    }
+
+    load(upload, upload.constructKey(), record, cb);
+}
+
+
+// Normalizes a path in Manta.
+function normalize(req, mPath, cb) {
+    var opts = {
+        account: req.owner.account,
+        path: mPath
+    };
+
+    libmanta.normalizeMantaPath(opts, function (err, p) {
+        if (err) {
+            req.log.debug({
+                url: path,
+                err: err
+            }, 'failed to normalize URL');
+            cb(err);
+        } else {
+            cb(null, p);
+        }
+    });
+}
+
+
+/*
+ * Given an upload ID, returns the prefix to use for the parent directory
+ * of the upload directory. For now, this is the just the first character of
+ * the upload uuid, but we may want to use more characters later to allow for
+ * more simulataneous uploads.
+ *
+ * For example, for the input id '0bb83e47-32df-4833-a6fd-94d77e8c7dd3' and a
+ * prefix length of 1, this function will return '0'.
+ */
+function idToPrefix(id) {
+    assert.string(id);
+    assert.ok(id.match(ID_REGEX));
+
+    return (id.substring(0, PREFIX_LENGTH));
+}
+
+
+///--- Routes
+
+function loadUpload(req, res, next) {
+    var log = req.log;
+
+    /*
+     * Multipart upload is not supported for subusers. For now, if the
+     * caller of this operation is a subuser of an account, we disallow
+     * any use of the multipart upload API.
+     */
+    if (req.caller.user) {
+        next(new AuthorizationError(req.caller.user.login, req.url));
+        return;
+    }
+
+
+    var id = req.params.id;
+    req.upload = new MultipartUpload(req, id);
+
+    loadUploadRecord(req.upload, function (err, upload) {
+        if (err) {
+            next(err);
+        } else {
+            log.info(sprintf('loaded upload record for %s: state %s',
+                req.upload.id, upload.state));
+            next();
+        }
+    });
+}
+
+
+/*
+ * For existing uploads, treat the upload directory as the resource.
+ */
+function uploadContext(req, res, next) {
+    var log = req.log;
+    log.debug('uploadContext: completed');
+
+    var opts = {
+        // keys like /<account uuid>/uploads/[0-f]/<upload id>
+        key: req.key.split('/').slice(0, 4).join('/'),
+        requestId: req.getId()
+    };
+
+    common.loadMetadata(req, opts, function (md_err, md) {
+        if (md_err) {
+            next(md_err);
+            return;
+        }
+        var o = req.upload.uploadOwner();
+        req.mahi.getAccountById(o, function (auth_err, owner) {
+            if (auth_err) {
+                next(auth_err);
+                return;
+            }
+
+            if (owner.account.uuid !== req.owner.account.uuid) {
+                next(new ResourceNotFoundError(req.path()));
+                return;
+            }
+
+            req.authContext.resource = {
+                owner: owner,
+                key: md.key || req.key,
+                roles: md.roles || []
+            };
+
+            log.debug('uploadContext: completed');
+            next();
+        });
+    });
+}
+
+
+///--- API
+
+/*
+ * Constructor for the MultipartUpload object, which is instantiated at
+ * the beginning of each multipart upload related request and attached to the
+ * request object at `req.upload`.
+ *
+ * The inputs to the constructor are:
+ *      - id, the upload uuid
+ *      - req, the request object for this multipart upload related request
+ *
+ *
+ * The structure of this object is as follows:
+ *
+ * {
+ *    id,                     // upload uuid
+ *    req,                    // pointer to the request this upload is for
+ *    uploadPath,             // upload path
+ *
+ *
+ *    // Private fields used to share state across a specific upload request.
+ *    // They aren't always used or set.
+ *    _headers,
+ *    _size,
+ *    _copies,
+ *    _md5,
+ *
+ *
+ *    // These objects represent some of the relevant metadata for the upload.
+ *
+ *    // When an upload or finalizing record is first loaded during a request,
+ *    // it is saved on the MultipartUpload object at
+ *    // `{upload,finalizing}Md.loaded`. Changes to metadata are made in a copy
+ *    // of the metadata that is saved at {upload,finalizing}Md.toSave.
+ *
+ *    // Additionally, each object contains the moray bucket and the key
+ *    // for the metadata record it represents.
+ *
+ *    uploadMd {          // upload record metadata object
+ *        key,            // normalized uploadPath
+ *        bucket,         // bucket for upload records (normal manta records)
+ *        loaded {        // current metadata for this upload
+ *        toSave          // new metadata for this upload
+ *    },
+ *
+ *    finalizingMd {      // finalizing record metadata object
+ *        key,            // normalized objectPath
+ *        bucket,         // bucket for finalizing records
+ *        loaded,         // current metadata for this upload
+ *        toSave          // new metadata for this upload
+ *    }
+ * }
+ *
+ */
+function MultipartUpload(req, id) {
+    var self = this;
+    self.id = id;
+    self.req = req;
+    self.uploadPath = '/' + req.owner.account.login + '/uploads/' +
+        idToPrefix(id) + '/' + id;
+
+    self._headers = null;
+    self._size = null;
+    self._copies = null;
+    self._md5 = null;
+    self._objectPathKey = null;
+
+    self.uploadMd = {
+        key: null,
+        bucket: 'manta',
+        loaded: null,
+        toSave: null
+    };
+
+    normalize(req, self.uploadPath, function (err, p) {
+        if (err) {
+            throw (new InvalidPathError(self.uploadPath));
+        } else {
+            self.uploadMd.key = p;
+            req.log.info('upload path key: ' + self.uploadMd.key);
+        }
+    });
+
+    self.finalizingMd = {
+        key: null,
+        bucket: 'manta_uploads',
+        loaded: null,
+        toSave: null
+    };
+
+    return (self);
+}
+
+
+///--- Create
+
+/*
+ * Creates the multipart upload by creating the upload record and inserting
+ * it into Moray.
+ *
+ * Parameters:
+ *  - opts: options blob that expects:
+ *      - objectPath
+ *      - sharks: array of shark objects returned from the picker
+ *      - headers: user-specified headers object (or an empty object)
+ */
+MultipartUpload.prototype.createUpload = function createUpload(opts, cb) {
+    assert.func(cb);
+    assert.string(opts.objectPath);
+    assert.string(opts.objectPathKey);
+    assert.ok(opts.sharks);
+    assert.object(opts.headers);
+
+    var self = this;
+
+    createUploadRecord(self, opts, function (err, uploadMd) {
+        if (err) {
+            cb(err);
+        } else {
+                    /* BEGIN JSSTYLED */
+                    /* if (exists) {
+                        uploadMd.etag = upload.etag;
+                    } else {
+                        uploadMd.etag = null;
+                    } */
+                    /* END JSSTYLED */
+debugger;
+            self.uploadMd.toSave = uploadMd;
+            persistUploadRecord(self, states.CREATED, null, function (err2) {
+                if (err2) {
+                    cb(err2);
+                } else {
+                    cb(null, self.uploadPath);
+                }
+            });
+        }
+    });
+};
+
+
+
+
+/*
+ * Aborts an upload.
+ *
+ * First validates that no commit record exists for the upload, then inserts
+ * an abort record for the upload on the object shard.
+ */
+MultipartUpload.prototype.abortUpload = function abortUpload(cb) {
+    var log = this.req.log;
+    var self = this;
+
+    self.finalizingRecordExists(function (err, exists, upload) {
+        if (err) {
+            cb(err);
+        } else if (exists) {
+            // This is only an error if the record isn't an abort record.
+            var type = self.finalizingMd.loaded.finalizingType;
+            if (type === types.ABORT) {
+                log.info('abort record exists for upload ' + self.id);
+                cb();
+            } else {
+                cb(new MultipartUploadFinalizeConflictError(upload.id,
+                    types.ABORT));
+            }
+
+        } else {
+            log.info('upload ' + self.id + ' has no finalizing record yet');
+            createFinalizingRecord(self, types.ABORT, null, function (md) {
+                var record = self.finalizingMd;
+                record.toSave = md;
+
+                self.req.log.info('saving finalizing record: ' + md);
+                self.req.moray.putFinalizingMetadata(record.key,
+                    record.toSave, function (err2) {
+                    if (err2) {
+                        cb(err2);
+                    } else {
+                        cb();
+                    }
+                });
+            });
+        }
+    });
+};
+
+
+/*
+ * Commits an upload.
+ *
+ * First checks for the existence of a finalizing record, then saves the
+ * upload record as finalizing, and atomically inserts a commit record
+ * and object record on the object's shard.
+ */
+MultipartUpload.prototype.commitUpload =
+function commitUpload(partsMD5, size, md5, cb) {
+    assert.string(partsMD5);
+    assert.number(size);
+    assert.string(md5);
+    assert.func(cb);
+
+    var log = this.req.log;
+    var self = this;
+
+    self.finalizingRecordExists(function (err, exists, upload) {
+        if (err) {
+            cb(err);
+        } else if (exists) {
+            // This is valid only for a commit record with matching parts.
+            var type = self.finalizingMd.loaded.finalizingType;
+            if (type === types.ABORT) {
+                cb(new MultipartUploadFinalizeConflictError(upload.id,
+                    types.ABORT));
+            } else {
+                if (self.get(mdKeys.PARTS_MD5) !== partsMD5) {
+                    cb(new MultipartUploadFinalizeConflictError(self.id,
+                        types.COMMIT));
+                } else {
+                    log.info('valid commit record already exists for upload ' +
+                        self.id);
+                    cb();
+                }
+            }
+        } else {
+            createObjectRecord(self, size, md5, function (err2, objectMd) {
+                if (err2) {
+                    cb(err2);
+                } else {
+                    createFinalizingRecord(self, types.COMMIT, md5,
+                    function (finalizingMd) {
+                        var batch = [ {
+                            bucket: self.finalizingMd.bucket,
+                            key: self.finalizingMd.key,
+                            value: finalizingMd,
+                            operation: 'put',
+                            opts: {
+                                req_id: self.req.getId(),
+                                etag: null
+                            }
+                        }, {
+                            bucket: self.uploadMd.bucket,
+                            key: self.get(mdKeys.OBJECT_PATH_KEY),
+                            value: objectMd,
+                            operation: 'put'
+                        } ];
+                        var opts = {
+                            req_id: self.req.getId()
+                        };
+
+                        log.info('batch created: '  + JSON.stringify(batch));
+                        self.req.moray.client.batch(batch, opts,
+                        function (err3, meta) {
+                            if (err3) {
+                                log.error('error batching data: ' + err);
+                                cb(err3);
+                            } else {
+                                log.info('batch successful');
+                                 cb();
+                            }
+                        });
+                    });
+                }
+            });
+        }
+    });
+};
+
+//--- Get
+/*
+ * Returns a object representation of the upload that can be serialized as JSON
+ * and sent to the client.
+ */
+MultipartUpload.prototype.getUpload = function getUpload(cb) {
+    var self = this;
+    loadUploadRecord(self, function (err, md) {
+        if (err) {
+            cb(err);
+        } else {
+            //XXX: Other useful things to include here?
+            var upload = {
+                id: self.id,
+                uploadPath: self.uploadPath,
+                objectPath: md.objectPath,
+                state: md.state,
+                // TODO: this is useful for debugging, but it may be something
+                // we don't want to expose to outside clients.
+                sharks: md.sharks,
+                headers: md.headers,
+                copies: md.copies
+            };
+
+            if (md.type === types.FINALIZING) {
+                upload.type = md.type;
+                upload.parts = md.parts;
+            }
+
+            if (md.contentMD5 !== '') {
+                upload.contentMD5 = md.contentMD5;
+            }
+
+            //cb(null, upload);
+            cb(null, md);
+        }
+    });
+};
+
+
+///--- Common methods for API endpoints
+
+
+/*
+ * Attemtps to load the upload's upload record, and if it exists,
+ * passes the callback the record.
+ */
+MultipartUpload.prototype.uploadRecordExists = function uploadRecordExists(cb) {
+    loadUploadRecord(this, function (err, upload) {
+        if (err) {
+            if (verror.hasCauseWithName(err, 'ObjectNotFoundError')) {
+                cb(null, false);
+            } else {
+                cb(err);
+            }
+        } else {
+            cb(null, true, upload);
+        }
+    });
+};
+
+
+/*
+ * Attemtps to load the upload's finalizing record, and if it exists,
+ * passes the callback the record. This is useful for both committing
+ * and aborting uploads.
+ */
+MultipartUpload.prototype.finalizingRecordExists =
+function finalizingRecordExists(cb) {
+    loadFinalizingMetadata(this, function (err, upload) {
+        if (err) {
+            if (verror.hasCauseWithName(err, 'ObjectNotFoundError')) {
+                cb(null, false);
+            } else {
+                cb(err);
+            }
+        } else {
+            cb(null, true, upload);
+        }
+    });
+};
+
+
+/*
+ * Saves the upload record with state set to FINALIZING.
+ *
+ * Parameters:
+ *  - type: finalizing type
+ *  - parts: if a commit, array of etags representing the parts
+ *  - cb: function
+ */
+MultipartUpload.prototype.finalizeUploadRecord =
+function finalizeUploadRecord(type, md5, cb) {
+    assert.ok(type === types.COMMIT || type === types.ABORT);
+    assert.ok(this.uploadMd.loaded, 'upload record not loaded');
+
+    this.set(mdKeys.PARTS_MD5, md5);
+    persistUploadRecord(this, states.FINALIZING, type, cb);
+};
+
+
+/*
+ * Used by API handlers to set an item in the upload record.
+ * The input key should be one of the keys specifeid in mdKeys.
+ */
+MultipartUpload.prototype.set = function set(k, v) {
+    assert.ok(this.uploadMd.toSave);
+
+    this.uploadMd.toSave.upload[k] = v;
+};
+
+
+/*
+ * Looks up a value in the loaded upload record.
+ */
+MultipartUpload.prototype.get = function get(k) {
+    assert.ok(this.uploadMd.loaded);
+
+    return (this.uploadMd.loaded.upload[k]);
+};
+
+
+/*
+ * Returns the size of the object if specifed on create, or a default value.
+ */
+MultipartUpload.prototype.uploadSize = function uploadSize() {
+    assert.ok(this.uploadMd.loaded);
+    var u = this.uploadMd.loaded.upload;
+    assert.ok(u);
+
+    var size = parseInt((u.headers['content-length'] || obj.DEF_MAX_LEN), 10);
+    assert.ok(size >= 0);
+
+    return (size);
+};
+
+
+/*
+ * Verifies that if a size was specified on create, the input expected value
+ * matches this size.
+ */
+MultipartUpload.prototype.checkSize = function checkSize(expected, cb) {
+    assert.ok(this.uploadMd.loaded);
+    var u = this.uploadMd.loaded.upload;
+    assert.ok(u);
+
+    if (!u.headers['content-length']) {
+        cb(true);
+    } else {
+        var size = parseInt(u.headers['content-length'], 10);
+        assert.ok(size >= 0);
+        if (size !== expected) {
+            cb(false, size);
+        } else {
+            cb(true);
+        }
+    }
+};
+
+/*
+ * Verifies that if an md5 was specified on create, the input expected value
+ * matches this md5.
+ */
+MultipartUpload.prototype.checkMD5 = function checkMD5(expected, cb) {
+    assert.ok(this.uploadMd.loaded);
+    var u = this.uploadMd.loaded.upload;
+    assert.ok(u);
+
+    var md5 = u.headers['content-md5'];
+
+    if (!md5) {
+        cb(true);
+    } else {
+        if (md5 !== expected) {
+            cb(false, md5);
+        } else {
+            cb(true);
+        }
+    }
+};
+
+
+/*
+ * Returns the number of sharks selected for this upload on create.
+ */
+MultipartUpload.prototype.numSharks = function numSharks() {
+    assert.ok(this.uploadMd.loaded);
+    var u = this.uploadMd.loaded.upload;
+    assert.ok(u);
+
+    return (u.sharks.length);
+};
+
+
+/*
+ * Used to create the key for the batch request to moray on commit.
+ * The key is of the form: <upload id>:<object path>
+ */
+MultipartUpload.prototype.constructKey = function constructKey() {
+    var o = this.get(mdKeys.OBJECT_PATH_KEY);
+    assert.ok(o, 'objectPathKey not saved in upload');
+
+    var key = this.id + ':' + o;
+    this.req.log.info('key: ' + key);
+    return (key);
+};
+
+
+// Returns the key for the upload path (for use in Moray).
+MultipartUpload.prototype.uploadPathKey = function uploadPathKey() {
+    var k = null;
+    if (this.uploadMd) {
+        k = this.uploadMd.key;
+    }
+    assert.ok(k);
+    return (k);
+};
+
+
+// Returns the owner of the upload.
+MultipartUpload.prototype.uploadOwner = function uploadOwner() {
+    assert.ok(this.uploadMd.loaded, 'upload record not loaded');
+    assert.ok(this.uploadMd.loaded.owner, 'no owner found in upload record');
+
+    return (this.uploadMd.loaded.owner);
+};
+
+
+///--- Exports
+
+module.exports = {
+
+    ID_REGEX: ID_REGEX,
+
+    mdKeys: mdKeys,
+    uploadStates: states,
+    uploadTypes: types,
+
+    MultipartUpload: MultipartUpload,
+
+    loadUpload: loadUpload,
+    uploadContext: uploadContext
+};
diff --git a/lib/uploads/create.js b/lib/uploads/create.js
new file mode 100644
index 0000000..4f166d9
--- /dev/null
+++ b/lib/uploads/create.js
@@ -0,0 +1,306 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+var assert = require('assert-plus');
+var jsprim = require('jsprim');
+var libmanta = require('libmanta');
+var libuuid = require('libuuid');
+var path = require('path');
+var restify = require('restify');
+var util = require('util');
+var vasync = require('vasync');
+var verror = require('verror');
+
+var sprintf = util.format;
+
+var auth = require('../auth');
+var common = require('../common');
+var obj = require('../obj');
+var uploadsCommon = require('./common');
+require('../errors');
+
+
+///--- Helpers
+
+/*
+ * Selects the sharks for the upload through the picker.choose interface.
+ *
+ * The number of sharks needed and the size of the sharks are specified by
+ * the durability-level and the content-length headers, respectively, or
+ * set to a default value.
+ */
+function chooseSharks(req, size, copies, cb) {
+    var log = req.log;
+
+    if (size === 0) {
+        cb(null, {});
+    } else {
+        var opts = {
+            requestId: req.getId(),
+            replicas: copies,
+            size: size
+        };
+        req.picker.choose(opts, function (err, sharks) {
+            if (err) {
+                cb(err);
+            } else {
+                log.info('upload: sharks chosen');
+                cb(null, sharks[0]);
+            }
+        });
+    }
+}
+
+
+///--- API
+
+
+/*
+ * For upload creation, use the root /uploads directory as the resource.
+ */
+function uploadContextRoot(req, res, next) {
+    // Disallow subusers from creating uploads
+    if (req.caller.user) {
+        next(new AuthorizationError(req.caller.user.login, req.url));
+        return;
+    }
+
+    var opts = {
+        key: req.key.split('/').slice(0, 3).join('/'),
+        requestId: req.getId()
+    };
+
+    common.loadMetadata(req, opts, function (err, md) {
+        if (err) {
+            next(err);
+            return;
+        }
+
+        req.authContext.resource = {
+            owner: req.owner,
+            key: md.key || req.key,
+            roles: md.roles || []
+        };
+
+        next();
+    });
+}
+
+
+// Instantiates the uploads object.
+function setupUpload(req, res, next) {
+    var id = libuuid.create();
+    req.upload = new uploadsCommon.MultipartUpload(req, id);
+
+    next();
+}
+
+/*
+ * Validates that all parameters needed for creating an upload exist, including:
+ *   - objectPath (the final path the uploaded object resides)
+ *
+ * Also validates optional headers, if they exist:
+ *   - durability-level
+ *   - content-length
+ *   - content-md5
+ *
+ * This handler is expected to set the following state on the upload object:
+ * - objectPath
+ * - size
+ * - copies
+ * - headers
+ * - contentMD5
+ * - contentType
+ */
+function validateParams(req, res, next) {
+    var log = req.log;
+
+    if (!req.body.objectPath) {
+        next(new MultipartUploadMissingObjecPathError());
+    } else {
+        var opts = {
+            account: req.owner.account,
+            path: req.body.objectPath
+        };
+        libmanta.normalizeMantaPath(opts, function (err, p) {
+            if (err) {
+                log.debug({
+                    url: path,
+                    err: err
+                }, 'failed to normalize URL');
+
+                next(err);
+            } else {
+                var inputHeaders, headers, size, copies;
+
+                inputHeaders = req.body.headers || {};
+                var maxObjectCopies = req.config.maxObjectCopies;
+
+                headers = {};
+                // TODO check size of headers here.
+                Object.keys(inputHeaders).forEach(function (k) {
+                    headers[k.toLowerCase()] = inputHeaders[k];
+                });
+
+                size = parseInt((headers['content-length'] ||
+                    obj.DEF_MAX_LEN), 10);
+                if ((size < 0) || (size > obj.DEF_MAX_LEN)) {
+                    next(new MaxContentLengthError(size));
+                    return;
+                }
+
+                if (jsprim.hasKey(headers, 'durability-level')) {
+                    copies = parseInt(headers['durability-level'], 10);
+                } else if (jsprim.hasKey(headers, 'x-durability-level')) {
+                    copies = parseInt(headers['x-durability-level'], 10);
+                } else {
+                    copies = 2;
+                }
+
+                if (isNaN(copies) || copies < 1 ||
+                    copies > (maxObjectCopies || 9)) {
+
+                    next(new InvalidDurabilityLevelError(1, maxObjectCopies));
+                    return;
+                }
+
+                req.upload._objectPathKey = p;
+                req.upload._headers = headers;
+                req.upload._size = size;
+                req.upload._copies = copies;
+
+                log.info({
+                    objectPath: req.body.objectPath,
+                    headers: headers,
+                    size: size,
+                    copies: copies
+                }, 'create-mpu: requested');
+
+                next();
+            }
+        });
+    }
+}
+
+
+/*
+ * Checks if the parent of the upload directory exists, and if it doesn't,
+ * creates the directory.
+ *
+ * For example,if the prefix length for an upload ID is 1, and the id is abcdef,
+ * the prefix directory is of the form: /account/uploads/a.
+ */
+function ensurePrefixDir(req, res, next) {
+    var log = req.log;
+    var requestId = req.getId();
+    var id = req.upload.id;
+    log.info('creating upload ' + id + ' for path: \"' +
+        req.body.objectPath + '\"');
+
+    var parentOpts = {
+        key: path.dirname(req.upload.uploadPathKey()),
+        requestId: requestId
+    };
+    log.info('upload path directory key: ' + parentOpts.key);
+
+    req.moray.getMetadata(parentOpts, function (err, md, _) {
+        if (err) {
+            if (verror.hasCauseWithName(err, 'ObjectNotFoundError')) {
+                // If the directory doesn't exist yet, create it.
+                parentOpts.dirname = path.dirname(parentOpts.key);
+                parentOpts.mtime = Date.now();
+                parentOpts.owner = req.owner.account.uuid;
+                parentOpts.requestId = req.getId();
+                parentOpts.type = 'directory';
+                //TODO: headers, roles, _etag
+
+                req.moray.putMetadata(parentOpts, function (err2) {
+                    if (err2) {
+                        next(err2);
+                    } else {
+                        //TODO: need to save parent metadata here?
+                        log.info('prefix directory \"' + parentOpts.key +
+                            '\" created');
+                        next();
+                    }
+                });
+            } else {
+                next(err);
+            }
+        } else {
+            log.info('prefix directory \"' + parentOpts.key +
+                '\" already created');
+            next();
+        }
+    });
+}
+
+
+/*
+ * Actually create the upload in the sense that the upload record exists.
+ * To do so, we must first choose the sharks that the final object will
+ * live on and save the metadata for the upload record.
+ */
+function createUpload(req, res, next) {
+    var log = req.log;
+
+    var s = req.upload._size;
+    var c = req.upload._copies;
+
+    chooseSharks(req, s, c, function (err, sharks) {
+        if (err) {
+            next(err);
+        } else {
+            var opts = {
+                objectPath: req.body.objectPath,
+                objectPathKey: req.upload._objectPathKey,
+                sharks: sharks,
+                headers: req.upload._headers
+            };
+            req.upload.createUpload(opts, function (err2, partsDirectory) {
+                    if (err2) {
+                        next(err2);
+                    } else {
+                        log.info({
+                            id: req.upload.id,
+                            sharks: sharks
+                        }, 'create-mpu: competed');
+
+                        res.send(201, {
+                            id: req.upload.id,
+                            partsDirectory: partsDirectory
+                        });
+                        next();
+                    }
+            });
+        }
+    });
+}
+
+
+///--- Exports
+
+module.exports = {
+    createHandler: function createHandler() {
+        var chain = [
+            uploadContextRoot,
+            auth.authorizationHandler(),
+            restify.jsonBodyParser({
+                mapParams: false,
+                maxBodySize: 100000
+            }),
+            setupUpload,
+            validateParams,
+            ensurePrefixDir,
+            createUpload
+        ];
+        return (chain);
+    }
+};
diff --git a/lib/uploads/get.js b/lib/uploads/get.js
new file mode 100644
index 0000000..cf3c8eb
--- /dev/null
+++ b/lib/uploads/get.js
@@ -0,0 +1,51 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+var uploadsCommon = require('./common');
+
+
+///--- API
+
+
+function getUpload(req, res, next) {
+    var log = req.log;
+
+    log.info({
+        id: req.upload.id
+    }, 'get-mpu: requested');
+
+    req.upload.getUpload(function (err, upload) {
+        if (err) {
+            next(err);
+        } else {
+            log.info({
+                id: req.upload.id,
+                upload: upload
+            }, 'get-mpu:completed');
+
+            res.send(200, upload);
+            next();
+        }
+    });
+}
+
+
+///--- Exports
+
+module.exports = {
+
+    getHandler: function getHandler() {
+        var chain = [
+            uploadsCommon.loadUpload,
+            getUpload
+        ];
+        return (chain);
+    }
+};
diff --git a/lib/uploads/index.js b/lib/uploads/index.js
new file mode 100644
index 0000000..da67ab5
--- /dev/null
+++ b/lib/uploads/index.js
@@ -0,0 +1,29 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+///--- Helpers
+
+function reExport(obj) {
+    Object.keys(obj || {}).forEach(function (k) {
+        module.exports[k] = obj[k];
+    });
+}
+
+
+
+///--- Exports
+
+module.exports = {};
+reExport(require('./create'));
+reExport(require('./upload'));
+reExport(require('./commit'));
+reExport(require('./abort'));
+reExport(require('./get'));
+reExport(require('./redirect'));
diff --git a/lib/uploads/redirect.js b/lib/uploads/redirect.js
new file mode 100644
index 0000000..09072e1
--- /dev/null
+++ b/lib/uploads/redirect.js
@@ -0,0 +1,85 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+var uploadsCommon = require('./common');
+
+var assert = require('assert-plus');
+var libuuid = require('libuuid');
+var path = require('path');
+var restify = require('restify');
+
+
+
+///--- API
+
+/*
+ * Gets the ID from the request URL, which is either of the form:
+ *      /<account>/uploads/<id>
+ * or
+ *      /<account>/uploads/<id>/<partNum>
+ */
+function parseId(req, res, next) {
+    var log = req.log;
+
+    log.info({
+        id: req.params.id,
+        url: req.url,
+        method: req.method
+    }, 'redirect: requested');
+
+    if (req.params && req.params.partNum) {
+        req.params.id = path.basename(path.dirname(req.url));
+    } else {
+        req.params.id = path.basename(req.url);
+    }
+
+    next();
+}
+
+
+/*
+ * Redirects the request by looking up the upload path using the upload ID.
+ */
+function redirect(req, res, next) {
+    var log = req.log;
+
+    // We want to get the upload path from the loaded metadata of the upload,
+    // as opposed to what's on the object itself.
+    var url = req.upload.get(uploadsCommon.mdKeys.UPLOAD_PATH);
+    if (req.params.partNum) {
+        url += '/' + req.params.partNum;
+    }
+
+    log.info({
+        id: req.params.id,
+        url: req.url,
+        method: req.method,
+        redirectLocation: url
+    }, 'redirect: completed');
+
+    res.setHeader('Location', url);
+    res.send(301);
+    next();
+}
+
+
+///--- Exports
+
+module.exports = {
+    redirectHandler: function redirectHandler() {
+        var chain = [
+            parseId,
+            uploadsCommon.loadUpload,
+            redirect
+        ];
+        return (chain);
+    }
+
+};
diff --git a/lib/uploads/upload.js b/lib/uploads/upload.js
new file mode 100644
index 0000000..d0b46c8
--- /dev/null
+++ b/lib/uploads/upload.js
@@ -0,0 +1,123 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+var libuuid = require('libuuid');
+var path = require('path');
+var restify = require('restify');
+var vasync = require('vasync');
+
+var auth = require('../auth');
+var common = require('../common');
+var obj = require('../obj');
+var uploadsCommon = require('./common');
+require('../errors');
+
+//TODO: enforce max individual part size based on object size
+
+///--- API
+
+/*
+ * Does some basic validation on the part before proceeding to the normal PUT
+ * path, including:
+ *   - ensuring the partNum is valid
+ *   - ensuring the upload hasn't been finalized yet
+ *   - ensuring client isn't trying to change the number of copies of the object
+ */
+function validate(req, res, next) {
+    var log = req.log;
+    var id = req.upload.id;
+    log.info('validate');
+
+    var regex = /^([0-9]|[1-9][0-9]{0,3})$/;
+    var partNum = req.params.partNum;
+    var valid = regex.test(partNum);
+
+    if (!valid) {
+        next(new MultipartUploadPartNumError(req.upload.id, partNum));
+    } else {
+        var state = req.upload.get(uploadsCommon.mdKeys.STATE);
+
+        if (state !== uploadsCommon.uploadStates.CREATED) {
+            next(new MultipartUploadFinalizeConflictError(id,
+                'upload part for'));
+        } else {
+                log.info({
+                    uploadId: id,
+                    partNum: partNum,
+                    headers: req.headers
+                }, 'upload-part: requested');
+
+                next();
+        }
+    }
+}
+
+
+/*
+ * The PUT handling code relies on some state being set up on the request
+ * object that is done by handlers not used for uploading parts.
+ *
+ * This handler ensures that the state needed for the PUT handling code
+ * is available so that the PUT handling code we do use for uploading
+ * parts works seamlessly.
+ */
+function setupPutState(req, res, next) {
+    var log = req.log;
+    var upload = req.upload;
+
+    // Ensure zero-byte objects aren't streamed to mako.
+    if (req.upload.uploadSize() === 0) {
+        log.info('zero-byte part');
+        req._zero = true;
+    }
+
+    // Ensure that the PUT handling code can find the correct sharks to use.
+    req._sharks = [upload.get(uploadsCommon.mdKeys.SHARKS)];
+
+    // Fake a durability-level header that matches the header
+    // specified on upload creation.
+    req.headers['durability-level'] = req.upload.numSharks();
+
+    next();
+}
+
+
+///--- Exports
+
+//  PUT handlers not included in this chain, in order:
+//      - conditionalRequest()
+//      - ensureNotRootHandler()
+//      [parseArguments]
+//      - ensureNotDirectoryHandler()
+//      - ensureParentHandler()
+//      - enforceDirectoryCount
+//      [other PUT handlers]
+//
+//      TODO: I think I need: conditionalRequest, enforceDirectoryCount
+module.exports = {
+    uploadPartHandler: function uploadPartHandler() {
+        var chain = [
+            uploadsCommon.loadUpload,
+            uploadsCommon.uploadContext,
+            auth.authorizationHandler(),
+            validate,
+            setupPutState,
+
+            // Piggybacking on existing PUT code.
+            //restify.conditionalRequest,//TODO: this makes request hang
+            obj.parseArguments,
+            obj.startSharkStreams,
+            obj.sharkStreams,
+            obj.saveMetadata
+        ];
+
+        return (chain);
+    }
+};
diff --git a/package.json b/package.json
index 3dc3839..afeffbf 100644
--- a/package.json
+++ b/package.json
@@ -1,52 +1,52 @@
 {
-    "name": "muskie",
-    "description": "Manta public-facing WebAPI",
-    "version": "1.0.0",
-    "author": "Joyent (joyent.com)",
-    "private": true,
-    "repository": {
-        "type": "git",
-        "url": "git+ssh://git@github.com:joyent/muskie.git"
-    },
-    "dependencies": {
-        "aperture-config": "git+ssh://git@github.com:joyent/aperture-config.git#master",
-        "assert-plus": "0.1.5",
-        "backoff": "2.3.0",
-        "bunyan": "0.22.1",
-        "bunyan-syslog": "0.2.2",
-        "cueball": "1.3.0",
-        "dashdash": "1.3.2",
-        "deep-equal": "0.0.0",
-        "dtrace-provider": "0.2.8",
-        "http-signature": "1.1.0",
-	"jsprim": "^1.3.1",
-        "kang": "1.1.0",
-        "keep-alive-agent": "0.0.1",
-        "keyapi": "git+ssh://git@github.com:joyent/keyapi.git#c30dd2710ad2175095dc0e96479686fa774b8063",
-        "libmanta": "git+ssh://git@github.com:joyent/node-libmanta.git#master",
-        "libuuid": "0.1.2",
-        "lru-cache": "2.3.1",
-        "lstream": "0.0.4",
-        "mahi": "2.0.1",
-        "marlin": "git+ssh://git@github.com:joyent/manta-marlin.git#master",
-        "mime": "1.2.11",
-        "moray": "3.0.0",
-        "once": "1.3.0",
-        "restify": "2.6.3",
-        "vasync": "1.4.3",
-        "verror": "^1.7.0",
-        "watershed": "0.3.0",
-        "xtend": "2.1.1"
-    },
-    "devDependencies": {
-        "smartdc": "git+ssh://git@github.com:joyent/node-smartdc.git#v7.3",
-        "manta": "1.4.5",
-        "nodeunit": "0.8.6",
-        "node-uuid": "1.4.1"
-    },
-    "scripts": {
-        "start": "./build/node/bin/node ./main.js",
-        "test": "./node_modules/.bin/nodeunit test/*.test.js 2>&1 | ./node_modules/.bin/bunyan"
-    },
-    "license": "MPL-2.0"
+  "name": "muskie",
+  "description": "Manta public-facing WebAPI",
+  "version": "1.0.0",
+  "author": "Joyent (joyent.com)",
+  "private": true,
+  "repository": {
+    "type": "git",
+    "url": "git+ssh://git@github.com:joyent/muskie.git"
+  },
+  "dependencies": {
+    "aperture-config": "git+ssh://git@github.com:joyent/aperture-config.git#master",
+    "assert-plus": "0.1.5",
+    "backoff": "2.3.0",
+    "bunyan": "0.22.1",
+    "bunyan-syslog": "0.2.2",
+    "cueball": "1.3.0",
+    "dashdash": "1.3.2",
+    "deep-equal": "0.0.0",
+    "dtrace-provider": "0.2.8",
+    "http-signature": "1.1.0",
+    "jsprim": "1.3.1",
+    "kang": "1.1.0",
+    "keep-alive-agent": "0.0.1",
+    "keyapi": "git+ssh://git@github.com:joyent/keyapi.git#c30dd2710ad2175095dc0e96479686fa774b8063",
+    "libmanta": "git+ssh://git@github.com:joyent/node-libmanta#dev-MANTA-2169",
+    "libuuid": "0.1.2",
+    "lru-cache": "2.3.1",
+    "lstream": "0.0.4",
+    "mahi": "2.0.1",
+    "marlin": "git+ssh://git@github.com:joyent/manta-marlin.git#master",
+    "mime": "1.2.11",
+    "moray": "^2.0.0",
+    "once": "1.3.0",
+    "restify": "2.6.3",
+    "vasync": "1.4.3",
+    "verror": "1.9.0",
+    "watershed": "0.3.0",
+    "xtend": "2.1.1"
+  },
+  "devDependencies": {
+    "smartdc": "git+ssh://git@github.com:joyent/node-smartdc.git#v7.3",
+    "manta": "1.4.5",
+    "nodeunit": "0.8.6",
+    "node-uuid": "1.4.1"
+  },
+  "scripts": {
+    "start": "./build/node/bin/node ./main.js",
+    "test": "./node_modules/.bin/nodeunit test/*.test.js 2>&1 | ./node_modules/.bin/bunyan"
+  },
+  "license": "MPL-2.0"
 }
diff --git a/sapi_manifests/muskie/template b/sapi_manifests/muskie/template
index 61d6f7e..8de6ebc 100644
--- a/sapi_manifests/muskie/template
+++ b/sapi_manifests/muskie/template
@@ -134,6 +134,10 @@
     "maximum": 2000
   },
 
+  {{#MPU_ENABLE}}
+  "enableMPU": true,
+  {{/MPU_ENABLE}}
+
   "medusa": {
     "moray": {
       "srvDomain": "{{ELECTRIC_MORAY}}",
@@ -177,4 +181,5 @@
       },
       "clientTimeout": 120000
   }
+
 }
diff --git a/tools/jsl.node.conf b/tools/jsl.node.conf
index 72d31c3..5c8f330 100644
--- a/tools/jsl.node.conf
+++ b/tools/jsl.node.conf
@@ -170,6 +170,16 @@
 +define MaxContentLengthError
 +define MaxSizeExceededError
 +define MissingPermissionError
++define MultipartUploadAbortedError
++define MultipartUploadCommitInProgressError
++define MultipartUploadContentLengthError
++define MultipartUploadFinalizeConflictError
++define MultipartUploadMissingObjecPathError
++define MultipartUploadMissingPartError
++define MultipartUploadPartEtagError
++define MultipartUploadPartLimitError
++define MultipartUploadPartNumError
++define MultipartUploadPartSizeError
 +define NoMatchingRoleTagError
 +define NotAcceptableError
 +define NotEnoughSpaceError
