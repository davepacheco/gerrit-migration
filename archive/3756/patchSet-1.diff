From 7b892f6f4d8d513e12ebbd71bd21137fce2bef84 Mon Sep 17 00:00:00 2001
From: Patrick Mooney <pmooney@pfmooney.com>
Date: Mon, 2 Apr 2018 00:20:46 +0000
Subject: [PATCH] OS-6864 bhyve should guard against going off-cpu OS-6865
 bhyve could be lazy about FPU state

---
 usr/src/uts/i86pc/io/vmm/amd/svm.c   |  15 +++-
 usr/src/uts/i86pc/io/vmm/intel/vmx.c |  41 ++++++++++
 usr/src/uts/i86pc/io/vmm/intel/vmx.h |   3 +-
 usr/src/uts/i86pc/io/vmm/vmm.c       | 115 +++++++++++++++++++++++++--
 usr/src/uts/i86pc/sys/vmm.h          |   9 +++
 5 files changed, 174 insertions(+), 9 deletions(-)

diff --git a/usr/src/uts/i86pc/io/vmm/amd/svm.c b/usr/src/uts/i86pc/io/vmm/amd/svm.c
index c6e3330779..8bf363faf5 100644
--- a/usr/src/uts/i86pc/io/vmm/amd/svm.c
+++ b/usr/src/uts/i86pc/io/vmm/amd/svm.c
@@ -24,6 +24,10 @@
  * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  */
 
+/*
+ * Copyright 2018 Joyent, Inc.
+ */
+
 #include <sys/cdefs.h>
 __FBSDID("$FreeBSD$");
 
@@ -2268,5 +2272,14 @@ struct vmm_ops vmm_ops_amd = {
 	svm_npt_alloc,
 	svm_npt_free,
 	svm_vlapic_init,
-	svm_vlapic_cleanup	
+	svm_vlapic_cleanup,
+
+#ifndef __FreeBSD__
+	/*
+	 * When SVM support is wired up and tested, it is likely to require
+	 * savectx/restorectx functions similar to VMX.
+	 */
+	NULL,
+	NULL,
+#endif
 };
diff --git a/usr/src/uts/i86pc/io/vmm/intel/vmx.c b/usr/src/uts/i86pc/io/vmm/intel/vmx.c
index 4bf335a7c5..c9a6a7a28b 100644
--- a/usr/src/uts/i86pc/io/vmm/intel/vmx.c
+++ b/usr/src/uts/i86pc/io/vmm/intel/vmx.c
@@ -2800,6 +2800,11 @@ vmx_run(void *arg, int vcpu, register_t rip, pmap_t pmap,
 
 	VMPTRLD(vmcs);
 
+#ifndef __FreeBSD__
+	VERIFY(!vmx->ctx_loaded[vcpu] && curthread->t_preempt != 0);
+	vmx->ctx_loaded[vcpu] = B_TRUE;
+#endif
+
 	/*
 	 * XXX
 	 * We do this every time because we may setup the virtual machine
@@ -2914,6 +2919,11 @@ vmx_run(void *arg, int vcpu, register_t rip, pmap_t pmap,
 	VMCLEAR(vmcs);
 	vmx_msr_guest_exit(vmx, vcpu);
 
+#ifndef __FreeBSD__
+	VERIFY(vmx->ctx_loaded[vcpu] && curthread->t_preempt != 0);
+	vmx->ctx_loaded[vcpu] = B_FALSE;
+#endif
+
 	return (0);
 }
 
@@ -3705,6 +3715,32 @@ vmx_vlapic_cleanup(void *arg, struct vlapic *vlapic)
 	free(vlapic, M_VLAPIC);
 }
 
+#ifndef __FreeBSD__
+static void
+vmx_savectx(void *arg, int vcpu)
+{
+	struct vmx *vmx = arg;
+	struct vmcs *vmcs = &vmx->vmcs[vcpu];
+
+	if (vmx->ctx_loaded[vcpu]) {
+		VERIFY3U(vmclear(vmcs), ==, 0);
+		vmx_msr_guest_exit(vmx, vcpu);
+	}
+}
+
+static void
+vmx_restorectx(void *arg, int vcpu)
+{
+	struct vmx *vmx = arg;
+	struct vmcs *vmcs = &vmx->vmcs[vcpu];
+
+	if (vmx->ctx_loaded[vcpu]) {
+		vmx_msr_guest_enter(vmx, vcpu);
+		VERIFY3U(vmptrld(vmcs), ==, 0);
+	}
+}
+#endif /* __FreeBSD__ */
+
 struct vmm_ops vmm_ops_intel = {
 	vmx_init,
 	vmx_cleanup,
@@ -3722,6 +3758,11 @@ struct vmm_ops vmm_ops_intel = {
 	ept_vmspace_free,
 	vmx_vlapic_init,
 	vmx_vlapic_cleanup,
+
+#ifndef __FreeBSD__
+	vmx_savectx,
+	vmx_restorectx,
+#endif
 };
 
 #ifndef __FreeBSD__
diff --git a/usr/src/uts/i86pc/io/vmm/intel/vmx.h b/usr/src/uts/i86pc/io/vmm/intel/vmx.h
index 04933eef80..a1576a106f 100644
--- a/usr/src/uts/i86pc/io/vmm/intel/vmx.h
+++ b/usr/src/uts/i86pc/io/vmm/intel/vmx.h
@@ -27,7 +27,7 @@
  */
 
 /*
- * Copyright 2017 Joyent, Inc.
+ * Copyright 2018 Joyent, Inc.
  */
 
 #ifndef _VMX_H_
@@ -123,6 +123,7 @@ struct vmx {
 #ifndef	__FreeBSD__
 	uint64_t	host_msrs[VM_MAXCPU][GUEST_MSR_NUM];
 	uint64_t	tsc_offset[VM_MAXCPU];
+	boolean_t	ctx_loaded[VM_MAXCPU];
 #endif
 	struct vmxctx	ctx[VM_MAXCPU];
 	struct vmxcap	cap[VM_MAXCPU];
diff --git a/usr/src/uts/i86pc/io/vmm/vmm.c b/usr/src/uts/i86pc/io/vmm/vmm.c
index 6332a094d4..0a1d26c64f 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm.c
+++ b/usr/src/uts/i86pc/io/vmm/vmm.c
@@ -265,6 +265,16 @@ typedef struct vm_ioport_hook {
 	vmm_rmem_cb_t	vmih_rmem_cb;
 	vmm_wmem_cb_t	vmih_wmem_cb;
 } vm_ioport_hook_t;
+
+/* Flags for vtc_status */
+#define	VTCS_FPU_RESTORED	1 /* guest FPU restored, host FPU saved */
+#define	VTCS_FPU_CTX_CRITCAL	2 /* in ctx where FPU restore cannot be lazy */
+
+typedef struct vm_thread_ctx {
+	struct vm	*vtc_vm;
+	int		vtc_vcpuid;
+	uint_t		vtc_status;
+} vm_thread_ctx_t;
 #endif /* __FreeBSD__ */
 
 #ifdef KTR
@@ -1779,6 +1789,61 @@ vm_localize_resources(struct vm *vm, struct vcpu *vcpu)
 
 	vcpu->lastloccpu = curcpu;
 }
+
+void
+vmm_savectx(void *arg)
+{
+	vm_thread_ctx_t *vtc = arg;
+	struct vm *vm = vtc->vtc_vm;
+	const int vcpuid = vtc->vtc_vcpuid;
+
+	if (ops->vmsavectx != NULL) {
+		ops->vmsavectx(vm->cookie, vcpuid);
+	}
+
+	/*
+	 * If the CPU holds the restored guest FPU state, save it and restore
+	 * the host FPU state before this thread goes off-cpu.
+	 */
+	if ((vtc->vtc_status & VTCS_FPU_RESTORED) != 0) {
+		struct vcpu *vcpu = &vm->vcpu[vcpuid];
+
+		save_guest_fpustate(vcpu);
+		vtc->vtc_status &= ~VTCS_FPU_RESTORED;
+	}
+}
+
+void
+vmm_restorectx(void *arg)
+{
+	vm_thread_ctx_t *vtc = arg;
+	struct vm *vm = vtc->vtc_vm;
+	const int vcpuid = vtc->vtc_vcpuid;
+
+	/*
+	 * When coming back on-cpu, only restore the guest FPU status if the
+	 * thread is in a context marked as requiring it.  This should be rare,
+	 * occurring only when a future logic error results in a voluntary
+	 * sleep during the VMRUN critical section.
+	 *
+	 * The common case will result in ellision of the guest FPU state
+	 * restoration, deferring that action until it is clearly necessary
+	 * during vm_run.
+	 */
+	VERIFY((vtc->vtc_status & VTCS_FPU_RESTORED) == 0);
+	if ((vtc->vtc_status & VTCS_FPU_CTX_CRITCAL) != 0) {
+		struct vcpu *vcpu = &vm->vcpu[vcpuid];
+
+		restore_guest_fpustate(vcpu);
+		vtc->vtc_status |= VTCS_FPU_RESTORED;
+	}
+
+	if (ops->vmrestorectx != NULL) {
+		ops->vmrestorectx(vm->cookie, vcpuid);
+	}
+
+}
+
 #endif /* __FreeBSD */
 
 
@@ -1795,6 +1860,9 @@ vm_run(struct vm *vm, struct vm_run *vmrun)
 	struct vm_exit *vme;
 	bool retu, intr_disabled;
 	pmap_t pmap;
+#ifndef	__FreeBSD__
+	vm_thread_ctx_t vtc;
+#endif
 
 	vcpuid = vmrun->cpuid;
 
@@ -1813,6 +1881,16 @@ vm_run(struct vm *vm, struct vm_run *vmrun)
 	evinfo.rptr = &vm->rendezvous_func;
 	evinfo.sptr = &vm->suspend;
 	evinfo.iptr = &vcpu->reqidle;
+
+#ifndef	__FreeBSD__
+	vtc.vtc_vm = vm;
+	vtc.vtc_vcpuid = vcpuid;
+	vtc.vtc_status = 0;
+
+	installctx(curthread, &vtc, vmm_savectx, vmm_restorectx, NULL, NULL,
+	    NULL, NULL);
+#endif
+
 restart:
 #ifndef	__FreeBSD__
 	thread_affinity_set(curthread, CPU_CURRENT);
@@ -1842,21 +1920,27 @@ restart:
 	ttolwp(curthread)->lwp_pcb.pcb_rupdate = 1;
 #endif
 
-#ifndef	__FreeBSD__
-	installctx(curthread, vcpu, save_guest_fpustate,
-	    restore_guest_fpustate, NULL, NULL, NULL, NULL);
-#endif
+#ifdef	__FreeBSD__
 	restore_guest_fpustate(vcpu);
+#else
+	if ((vtc.vtc_status & VTCS_FPU_RESTORED) == 0) {
+		restore_guest_fpustate(vcpu);
+		vtc.vtc_status |= VTCS_FPU_RESTORED;
+	}
+	vtc.vtc_status |= VTCS_FPU_CTX_CRITCAL;
+#endif
 
 	vcpu_require_state(vm, vcpuid, VCPU_RUNNING);
 	error = VMRUN(vm->cookie, vcpuid, vcpu->nextrip, pmap, &evinfo);
 	vcpu_require_state(vm, vcpuid, VCPU_FROZEN);
 
+#ifdef	__FreeBSD__
 	save_guest_fpustate(vcpu);
-#ifndef	__FreeBSD__
-	removectx(curthread, vcpu, save_guest_fpustate,
-	    restore_guest_fpustate, NULL, NULL, NULL, NULL);
+#else
+	vtc.vtc_status &= ~VTCS_FPU_CTX_CRITCAL;
+#endif
 
+#ifndef	__FreeBSD__
 	/*
 	 * Once clear of the delicate contexts comprising the VM_RUN handler,
 	 * thread CPU affinity can be loosened while other processing occurs.
@@ -1913,6 +1997,23 @@ restart:
 	if (error == 0 && retu == false)
 		goto restart;
 
+#ifndef	__FreeBSD__
+	/*
+	 * Before returning to userspace, explicitly restore the host FPU state
+	 * (saving the guest state).  This is done with kpreempt disabled to
+	 * ensure it is not interrupted, potentially confusing the soon-to-be
+	 * removed savectx/restorectx handlers.
+	 */
+	kpreempt_disable();
+	if ((vtc.vtc_status & VTCS_FPU_RESTORED) != 0) {
+		save_guest_fpustate(vcpu);
+	}
+	kpreempt_enable();
+
+	removectx(curthread, &vtc, vmm_savectx, vmm_restorectx, NULL, NULL,
+	    NULL, NULL);
+#endif
+
 	VCPU_CTR2(vm, vcpuid, "retu %d/%d", error, vme->exitcode);
 
 	/* copy the exit information */
diff --git a/usr/src/uts/i86pc/sys/vmm.h b/usr/src/uts/i86pc/sys/vmm.h
index 757dc7ce7f..4cff614f6a 100644
--- a/usr/src/uts/i86pc/sys/vmm.h
+++ b/usr/src/uts/i86pc/sys/vmm.h
@@ -160,6 +160,10 @@ typedef struct vmspace * (*vmi_vmspace_alloc)(vm_offset_t min, vm_offset_t max);
 typedef void	(*vmi_vmspace_free)(struct vmspace *vmspace);
 typedef struct vlapic * (*vmi_vlapic_init)(void *vmi, int vcpu);
 typedef void	(*vmi_vlapic_cleanup)(void *vmi, struct vlapic *vlapic);
+#ifndef __FreeBSD__
+typedef void	(*vmi_savectx)(void *vmi, int vcpu);
+typedef void	(*vmi_restorectx)(void *vmi, int vcpu);
+#endif
 
 struct vmm_ops {
 	vmm_init_func_t		init;		/* module wide initialization */
@@ -179,6 +183,11 @@ struct vmm_ops {
 	vmi_vmspace_free	vmspace_free;
 	vmi_vlapic_init		vlapic_init;
 	vmi_vlapic_cleanup	vlapic_cleanup;
+
+#ifndef __FreeBSD__
+	vmi_savectx		vmsavectx;
+	vmi_restorectx		vmrestorectx;
+#endif
 };
 
 extern struct vmm_ops vmm_ops_intel;
-- 
2.21.0

