From ee3da23f37c4d1473517fefaa7e7743c18a085ac Mon Sep 17 00:00:00 2001
From: Hans Rosenfeld <rosenfeld@grumpf.hope-2000.org>
Date: Fri, 3 Feb 2017 11:45:59 +0100
Subject: [PATCH] OS-6207 nvme: rework command abortion

---
 usr/src/uts/common/io/nvme/nvme.c     | 361 +++++++++++---------------
 usr/src/uts/common/io/nvme/nvme_var.h |   2 +-
 2 files changed, 159 insertions(+), 204 deletions(-)

diff --git a/usr/src/uts/common/io/nvme/nvme.c b/usr/src/uts/common/io/nvme/nvme.c
index 90c007dba0..3975ccf706 100644
--- a/usr/src/uts/common/io/nvme/nvme.c
+++ b/usr/src/uts/common/io/nvme/nvme.c
@@ -141,13 +141,12 @@
  *
  * Command timeouts are currently detected for all admin commands except
  * asynchronous event requests. If a command times out and the hardware appears
- * to be healthy the driver attempts to abort the command. If this fails the
+ * to be healthy the driver attempts to abort the command. The original command
+ * timeout is also applied to the abort command. If the abort times out too the
  * driver assumes the device to be dead, fences it off, and calls FMA to retire
- * it. In general admin commands are issued at attach time only. No timeout
- * handling of normal I/O commands is presently done.
- *
- * In some cases it may be possible that the ABORT command times out, too. In
- * that case the device is also declared dead and fenced off.
+ * it. In all other cases the aborted command should return immediately with a
+ * status indicating it was aborted, and the driver will wait indefinitely for
+ * that to happen. No timeout handling of normal I/O commands is presently done.
  *
  *
  * Quiesce / Fast Reboot:
@@ -250,10 +249,11 @@ static nvme_cmd_t *nvme_alloc_cmd(nvme_t *, int);
 static void nvme_free_cmd(nvme_cmd_t *);
 static nvme_cmd_t *nvme_create_nvm_cmd(nvme_namespace_t *, uint8_t,
     bd_xfer_t *);
-static int nvme_admin_cmd(nvme_cmd_t *, int);
+static void nvme_admin_cmd(nvme_cmd_t *, int);
 static void nvme_submit_cmd(nvme_qpair_t *, nvme_cmd_t *);
+static nvme_cmd_t *nvme_unqueue_cmd(nvme_t *, nvme_qpair_t *, int);
 static nvme_cmd_t *nvme_retrieve_cmd(nvme_t *, nvme_qpair_t *);
-static boolean_t nvme_wait_cmd(nvme_cmd_t *, uint_t);
+static void nvme_wait_cmd(nvme_cmd_t *, uint_t);
 static void nvme_wakeup_cmd(void *);
 static void nvme_async_event_task(void *);
 
@@ -264,18 +264,18 @@ static int nvme_check_specific_cmd_status(nvme_cmd_t *);
 static int nvme_check_generic_cmd_status(nvme_cmd_t *);
 static inline int nvme_check_cmd_status(nvme_cmd_t *);
 
-static void nvme_abort_cmd(nvme_cmd_t *);
+static int nvme_abort_cmd(nvme_cmd_t *, uint_t);
 static void nvme_async_event(nvme_t *);
 static int nvme_format_nvm(nvme_t *, uint32_t, uint8_t, boolean_t, uint8_t,
     boolean_t, uint8_t);
 static int nvme_get_logpage(nvme_t *, void **, size_t *, uint8_t, ...);
-static void *nvme_identify(nvme_t *, uint32_t);
-static boolean_t nvme_set_features(nvme_t *, uint32_t, uint8_t, uint32_t,
+static int nvme_identify(nvme_t *, uint32_t, void **);
+static int nvme_set_features(nvme_t *, uint32_t, uint8_t, uint32_t,
     uint32_t *);
-static boolean_t nvme_get_features(nvme_t *, uint32_t, uint8_t, uint32_t *,
+static int nvme_get_features(nvme_t *, uint32_t, uint8_t, uint32_t *,
     void **, size_t *);
-static boolean_t nvme_write_cache_set(nvme_t *, boolean_t);
-static int nvme_set_nqueues(nvme_t *, uint16_t);
+static int nvme_write_cache_set(nvme_t *, boolean_t);
+static int nvme_set_nqueues(nvme_t *, uint16_t *);
 
 static void nvme_free_dma(nvme_dma_t *);
 static int nvme_zalloc_dma(nvme_t *, size_t, uint_t, ddi_dma_attr_t *,
@@ -842,6 +842,27 @@ nvme_submit_cmd(nvme_qpair_t *qp, nvme_cmd_t *cmd)
 	mutex_exit(&qp->nq_mutex);
 }
 
+static nvme_cmd_t *
+nvme_unqueue_cmd(nvme_t *nvme, nvme_qpair_t *qp, int cid)
+{
+	nvme_cmd_t *cmd;
+
+	ASSERT(mutex_owned(&qp->nq_mutex));
+	ASSERT(cid < qp->nq_nentry);
+
+	cmd = qp->nq_cmd[cid];
+	qp->nq_cmd[cid] = NULL;
+	ASSERT(qp->nq_active_cmds > 0);
+	qp->nq_active_cmds--;
+	sema_v(&qp->nq_sema);
+
+	ASSERT(cmd != NULL);
+	ASSERT(cmd->nc_nvme == nvme);
+	ASSERT(cmd->nc_sqe.sqe_cid == cid);
+
+	return (cmd);
+}
+
 static nvme_cmd_t *
 nvme_retrieve_cmd(nvme_t *nvme, nvme_qpair_t *qp)
 {
@@ -863,16 +884,10 @@ nvme_retrieve_cmd(nvme_t *nvme, nvme_qpair_t *qp)
 	}
 
 	ASSERT(nvme->n_ioq[cqe->cqe_sqid] == qp);
-	ASSERT(cqe->cqe_cid < qp->nq_nentry);
 
-	cmd = qp->nq_cmd[cqe->cqe_cid];
-	qp->nq_cmd[cqe->cqe_cid] = NULL;
-	qp->nq_active_cmds--;
+	cmd = nvme_unqueue_cmd(nvme, qp, cqe->cqe_cid);
 
-	ASSERT(cmd != NULL);
-	ASSERT(cmd->nc_nvme == nvme);
 	ASSERT(cmd->nc_sqid == cqe->cqe_sqid);
-	ASSERT(cmd->nc_sqe.sqe_cid == cqe->cqe_cid);
 	bcopy(cqe, &cmd->nc_cqe, sizeof (nvme_cqe_t));
 
 	qp->nq_sqhead = cqe->cqe_sqhd;
@@ -885,7 +900,6 @@ nvme_retrieve_cmd(nvme_t *nvme, nvme_qpair_t *qp)
 
 	nvme_put32(cmd->nc_nvme, qp->nq_cqhdbl, head.r);
 	mutex_exit(&qp->nq_mutex);
-	sema_v(&qp->nq_sema);
 
 	return (cmd);
 }
@@ -1170,7 +1184,13 @@ nvme_check_cmd_status(nvme_cmd_t *cmd)
 {
 	nvme_cqe_t *cqe = &cmd->nc_cqe;
 
-	/* take a shortcut if everything is alright */
+	/*
+	 * Take a shortcut if the controller is dead, or if
+	 * command status indicates no error.
+	 */
+	if (cmd->nc_nvme->n_dead)
+		return (EIO);
+
 	if (cqe->cqe_sf.sf_sct == NVME_CQE_SCT_GENERIC &&
 	    cqe->cqe_sf.sf_sc == NVME_CQE_SC_GEN_SUCCESS)
 		return (0);
@@ -1187,46 +1207,19 @@ nvme_check_cmd_status(nvme_cmd_t *cmd)
 	return (nvme_check_unknown_cmd_status(cmd));
 }
 
-/*
- * nvme_abort_cmd_cb -- replaces nc_callback of aborted commands
- *
- * This functions takes care of cleaning up aborted commands. The command
- * status is checked to catch any fatal errors.
- */
-static void
-nvme_abort_cmd_cb(void *arg)
-{
-	nvme_cmd_t *cmd = arg;
-
-	/*
-	 * Grab the command mutex. Once we have it we hold the last reference
-	 * to the command and can safely free it.
-	 */
-	mutex_enter(&cmd->nc_mutex);
-	(void) nvme_check_cmd_status(cmd);
-	mutex_exit(&cmd->nc_mutex);
-
-	nvme_free_cmd(cmd);
-}
-
-static void
-nvme_abort_cmd(nvme_cmd_t *abort_cmd)
+static int
+nvme_abort_cmd(nvme_cmd_t *abort_cmd, uint_t sec)
 {
 	nvme_t *nvme = abort_cmd->nc_nvme;
 	nvme_cmd_t *cmd = nvme_alloc_cmd(nvme, KM_SLEEP);
 	nvme_abort_cmd_t ac = { 0 };
+	int ret = 0;
 
 	sema_p(&nvme->n_abort_sema);
 
 	ac.b.ac_cid = abort_cmd->nc_sqe.sqe_cid;
 	ac.b.ac_sqid = abort_cmd->nc_sqid;
 
-	/*
-	 * Drop the mutex of the aborted command. From this point on
-	 * we must assume that the abort callback has freed the command.
-	 */
-	mutex_exit(&abort_cmd->nc_mutex);
-
 	cmd->nc_sqid = 0;
 	cmd->nc_sqe.sqe_opc = NVME_OPC_ABORT;
 	cmd->nc_callback = nvme_wakeup_cmd;
@@ -1234,47 +1227,44 @@ nvme_abort_cmd(nvme_cmd_t *abort_cmd)
 
 	/*
 	 * Send the ABORT to the hardware. The ABORT command will return _after_
-	 * the aborted command has completed (aborted or otherwise).
+	 * the aborted command has completed (aborted or otherwise), but since
+	 * we still hold the aborted command's mutex its callback hasn't been
+	 * processed yet.
 	 */
-	if (nvme_admin_cmd(cmd, nvme_admin_cmd_timeout) != DDI_SUCCESS) {
-		sema_v(&nvme->n_abort_sema);
-		dev_err(nvme->n_dip, CE_WARN,
-		    "!nvme_admin_cmd failed for ABORT");
-		atomic_inc_32(&nvme->n_abort_failed);
-		return;
-	}
+	nvme_admin_cmd(cmd, sec);
 	sema_v(&nvme->n_abort_sema);
 
-	if (nvme_check_cmd_status(cmd)) {
+	if ((ret = nvme_check_cmd_status(cmd)) != 0) {
 		dev_err(nvme->n_dip, CE_WARN,
 		    "!ABORT failed with sct = %x, sc = %x",
 		    cmd->nc_cqe.cqe_sf.sf_sct, cmd->nc_cqe.cqe_sf.sf_sc);
 		atomic_inc_32(&nvme->n_abort_failed);
 	} else {
-		atomic_inc_32(&nvme->n_cmd_aborted);
+		dev_err(nvme->n_dip, CE_WARN,
+		    "!ABORT of command %d/%d %ssuccessful",
+		    abort_cmd->nc_sqe.sqe_cid, abort_cmd->nc_sqid,
+		    cmd->nc_cqe.cqe_dw0 & 1 ? "un" : "");
+		if ((cmd->nc_cqe.cqe_dw0 & 1) == 0)
+			atomic_inc_32(&nvme->n_cmd_aborted);
 	}
 
 	nvme_free_cmd(cmd);
+	return (ret);
 }
 
 /*
  * nvme_wait_cmd -- wait for command completion or timeout
  *
- * Returns B_TRUE if the command completed normally.
- *
- * Returns B_FALSE if the command timed out and an abort was attempted. The
- * command mutex will be dropped and the command must be considered freed. The
- * freeing of the command is normally done by the abort command callback.
- *
  * In case of a serious error or a timeout of the abort command the hardware
  * will be declared dead and FMA will be notified.
  */
-static boolean_t
+static void
 nvme_wait_cmd(nvme_cmd_t *cmd, uint_t sec)
 {
 	clock_t timeout = ddi_get_lbolt() + drv_usectohz(sec * MICROSEC);
 	nvme_t *nvme = cmd->nc_nvme;
 	nvme_reg_csts_t csts;
+	nvme_qpair_t *qp;
 
 	ASSERT(mutex_owned(&cmd->nc_mutex));
 
@@ -1284,21 +1274,19 @@ nvme_wait_cmd(nvme_cmd_t *cmd, uint_t sec)
 	}
 
 	if (cmd->nc_completed)
-		return (B_TRUE);
-
-	/*
-	 * The command timed out. Change the callback to the cleanup function.
-	 */
-	cmd->nc_callback = nvme_abort_cmd_cb;
+		return;
 
 	/*
+	 * The command timed out.
+	 *
 	 * Check controller for fatal status, any errors associated with the
 	 * register or DMA handle, or for a double timeout (abort command timed
 	 * out). If necessary log a warning and call FMA.
 	 */
 	csts.r = nvme_get32(nvme, NVME_REG_CSTS);
-	dev_err(nvme->n_dip, CE_WARN, "!command timeout, "
-	    "OPC = %x, CFS = %d", cmd->nc_sqe.sqe_opc, csts.b.csts_cfs);
+	dev_err(nvme->n_dip, CE_WARN, "!command %d/%d timeout, "
+	    "OPC = %x, CFS = %d", cmd->nc_sqe.sqe_cid, cmd->nc_sqid,
+	    cmd->nc_sqe.sqe_opc, csts.b.csts_cfs);
 	atomic_inc_32(&nvme->n_cmd_timeout);
 
 	if (csts.b.csts_cfs ||
@@ -1307,20 +1295,22 @@ nvme_wait_cmd(nvme_cmd_t *cmd, uint_t sec)
 	    cmd->nc_sqe.sqe_opc == NVME_OPC_ABORT) {
 		ddi_fm_service_impact(nvme->n_dip, DDI_SERVICE_LOST);
 		nvme->n_dead = B_TRUE;
-		mutex_exit(&cmd->nc_mutex);
-	} else {
+	} else if (nvme_abort_cmd(cmd, sec) == 0) {
 		/*
-		 * Try to abort the command. The command mutex is released by
-		 * nvme_abort_cmd().
-		 * If the abort succeeds it will have freed the aborted command.
-		 * If the abort fails for other reasons we must assume that the
-		 * command may complete at any time, and the callback will free
-		 * it for us.
+		 * If the abort succeeded the command should complete
+		 * immediately with an appropriate status.
 		 */
-		nvme_abort_cmd(cmd);
+		while (!cmd->nc_completed)
+			cv_wait(&cmd->nc_cv, &cmd->nc_mutex);
+
+		return;
 	}
 
-	return (B_FALSE);
+	qp = nvme->n_ioq[cmd->nc_sqid];
+
+	mutex_enter(&qp->nq_mutex);
+	(void) nvme_unqueue_cmd(nvme, qp, cmd->nc_sqe.sqe_cid);
+	mutex_exit(&qp->nq_mutex);
 }
 
 static void
@@ -1329,17 +1319,6 @@ nvme_wakeup_cmd(void *arg)
 	nvme_cmd_t *cmd = arg;
 
 	mutex_enter(&cmd->nc_mutex);
-	/*
-	 * There is a slight chance that this command completed shortly after
-	 * the timeout was hit in nvme_wait_cmd() but before the callback was
-	 * changed. Catch that case here and clean up accordingly.
-	 */
-	if (cmd->nc_callback == nvme_abort_cmd_cb) {
-		mutex_exit(&cmd->nc_mutex);
-		nvme_abort_cmd_cb(cmd);
-		return;
-	}
-
 	cmd->nc_completed = B_TRUE;
 	cv_signal(&cmd->nc_cv);
 	mutex_exit(&cmd->nc_mutex);
@@ -1365,7 +1344,7 @@ nvme_async_event_task(void *arg)
 	 * was aborted, or internal errors in the device. Internal errors are
 	 * reported to FMA, the command aborts need no special handling here.
 	 */
-	if (nvme_check_cmd_status(cmd)) {
+	if (nvme_check_cmd_status(cmd) != 0) {
 		dev_err(cmd->nc_nvme->n_dip, CE_WARN,
 		    "!async event request returned failure, sct = %x, "
 		    "sc = %x, dnr = %d, m = %d", cmd->nc_cqe.cqe_sf.sf_sct,
@@ -1497,22 +1476,13 @@ nvme_async_event_task(void *arg)
 		kmem_free(health_log, logsize);
 }
 
-static int
+static void
 nvme_admin_cmd(nvme_cmd_t *cmd, int sec)
 {
 	mutex_enter(&cmd->nc_mutex);
 	nvme_submit_cmd(cmd->nc_nvme->n_adminq, cmd);
-
-	if (nvme_wait_cmd(cmd, sec) == B_FALSE) {
-		/*
-		 * The command timed out. An abort command was posted that
-		 * will take care of the cleanup.
-		 */
-		return (DDI_FAILURE);
-	}
+	nvme_wait_cmd(cmd, sec);
 	mutex_exit(&cmd->nc_mutex);
-
-	return (DDI_SUCCESS);
 }
 
 static void
@@ -1554,12 +1524,7 @@ nvme_format_nvm(nvme_t *nvme, uint32_t nsid, uint8_t lbaf, boolean_t ms,
 	if (nsid == (uint32_t)-1)
 		cmd->nc_dontpanic = B_TRUE;
 
-	if ((ret = nvme_admin_cmd(cmd, nvme_format_cmd_timeout))
-	    != DDI_SUCCESS) {
-		dev_err(nvme->n_dip, CE_WARN,
-		    "!nvme_admin_cmd failed for FORMAT NVM");
-		return (EIO);
-	}
+	nvme_admin_cmd(cmd, nvme_format_cmd_timeout);
 
 	if ((ret = nvme_check_cmd_status(cmd)) != 0) {
 		dev_err(nvme->n_dip, CE_WARN,
@@ -1578,7 +1543,7 @@ nvme_get_logpage(nvme_t *nvme, void **buf, size_t *bufsize, uint8_t logpage,
 	nvme_cmd_t *cmd = nvme_alloc_cmd(nvme, KM_SLEEP);
 	nvme_getlogpage_t getlogpage = { 0 };
 	va_list ap;
-	int ret = DDI_FAILURE;
+	int ret;
 
 	va_start(ap, logpage);
 
@@ -1613,6 +1578,7 @@ nvme_get_logpage(nvme_t *nvme, void **buf, size_t *bufsize, uint8_t logpage,
 		dev_err(nvme->n_dip, CE_WARN, "!unknown log page requested: %d",
 		    logpage);
 		atomic_inc_32(&nvme->n_unknown_logpage);
+		ret = EINVAL;
 		goto fail;
 	}
 
@@ -1626,6 +1592,7 @@ nvme_get_logpage(nvme_t *nvme, void **buf, size_t *bufsize, uint8_t logpage,
 	    DDI_DMA_READ, &nvme->n_prp_dma_attr, &cmd->nc_dma) != DDI_SUCCESS) {
 		dev_err(nvme->n_dip, CE_WARN,
 		    "!nvme_zalloc_dma failed for GET LOG PAGE");
+		ret = ENOMEM;
 		goto fail;
 	}
 
@@ -1633,6 +1600,7 @@ nvme_get_logpage(nvme_t *nvme, void **buf, size_t *bufsize, uint8_t logpage,
 		dev_err(nvme->n_dip, CE_WARN,
 		    "!too many DMA cookies for GET LOG PAGE");
 		atomic_inc_32(&nvme->n_too_many_cookies);
+		ret = ENOMEM;
 		goto fail;
 	}
 
@@ -1644,13 +1612,9 @@ nvme_get_logpage(nvme_t *nvme, void **buf, size_t *bufsize, uint8_t logpage,
 		    cmd->nc_dma->nd_cookie.dmac_laddress;
 	}
 
-	if (nvme_admin_cmd(cmd, nvme_admin_cmd_timeout) != DDI_SUCCESS) {
-		dev_err(nvme->n_dip, CE_WARN,
-		    "!nvme_admin_cmd failed for GET LOG PAGE");
-		return (ret);
-	}
+	nvme_admin_cmd(cmd, nvme_admin_cmd_timeout);
 
-	if (nvme_check_cmd_status(cmd)) {
+	if ((ret = nvme_check_cmd_status(cmd)) != 0) {
 		dev_err(nvme->n_dip, CE_WARN,
 		    "!GET LOG PAGE failed with sct = %x, sc = %x",
 		    cmd->nc_cqe.cqe_sf.sf_sct, cmd->nc_cqe.cqe_sf.sf_sc);
@@ -1660,19 +1624,20 @@ nvme_get_logpage(nvme_t *nvme, void **buf, size_t *bufsize, uint8_t logpage,
 	*buf = kmem_alloc(*bufsize, KM_SLEEP);
 	bcopy(cmd->nc_dma->nd_memp, *buf, *bufsize);
 
-	ret = DDI_SUCCESS;
-
 fail:
 	nvme_free_cmd(cmd);
 
 	return (ret);
 }
 
-static void *
-nvme_identify(nvme_t *nvme, uint32_t nsid)
+static int
+nvme_identify(nvme_t *nvme, uint32_t nsid, void **buf)
 {
 	nvme_cmd_t *cmd = nvme_alloc_cmd(nvme, KM_SLEEP);
-	void *buf = NULL;
+	int ret;
+
+	if (buf == NULL)
+		return (EINVAL);
 
 	cmd->nc_sqid = 0;
 	cmd->nc_callback = nvme_wakeup_cmd;
@@ -1684,6 +1649,7 @@ nvme_identify(nvme_t *nvme, uint32_t nsid)
 	    &nvme->n_prp_dma_attr, &cmd->nc_dma) != DDI_SUCCESS) {
 		dev_err(nvme->n_dip, CE_WARN,
 		    "!nvme_zalloc_dma failed for IDENTIFY");
+		ret = ENOMEM;
 		goto fail;
 	}
 
@@ -1691,6 +1657,7 @@ nvme_identify(nvme_t *nvme, uint32_t nsid)
 		dev_err(nvme->n_dip, CE_WARN,
 		    "!too many DMA cookies for IDENTIFY");
 		atomic_inc_32(&nvme->n_too_many_cookies);
+		ret = ENOMEM;
 		goto fail;
 	}
 
@@ -1702,35 +1669,31 @@ nvme_identify(nvme_t *nvme, uint32_t nsid)
 		    cmd->nc_dma->nd_cookie.dmac_laddress;
 	}
 
-	if (nvme_admin_cmd(cmd, nvme_admin_cmd_timeout) != DDI_SUCCESS) {
-		dev_err(nvme->n_dip, CE_WARN,
-		    "!nvme_admin_cmd failed for IDENTIFY");
-		return (NULL);
-	}
+	nvme_admin_cmd(cmd, nvme_admin_cmd_timeout);
 
-	if (nvme_check_cmd_status(cmd)) {
+	if ((ret = nvme_check_cmd_status(cmd)) != 0) {
 		dev_err(nvme->n_dip, CE_WARN,
 		    "!IDENTIFY failed with sct = %x, sc = %x",
 		    cmd->nc_cqe.cqe_sf.sf_sct, cmd->nc_cqe.cqe_sf.sf_sc);
 		goto fail;
 	}
 
-	buf = kmem_alloc(NVME_IDENTIFY_BUFSIZE, KM_SLEEP);
-	bcopy(cmd->nc_dma->nd_memp, buf, NVME_IDENTIFY_BUFSIZE);
+	*buf = kmem_alloc(NVME_IDENTIFY_BUFSIZE, KM_SLEEP);
+	bcopy(cmd->nc_dma->nd_memp, *buf, NVME_IDENTIFY_BUFSIZE);
 
 fail:
 	nvme_free_cmd(cmd);
 
-	return (buf);
+	return (ret);
 }
 
-static boolean_t
+static int
 nvme_set_features(nvme_t *nvme, uint32_t nsid, uint8_t feature, uint32_t val,
     uint32_t *res)
 {
 	_NOTE(ARGUNUSED(nsid));
 	nvme_cmd_t *cmd = nvme_alloc_cmd(nvme, KM_SLEEP);
-	boolean_t ret = B_FALSE;
+	int ret = EINVAL;
 
 	ASSERT(res != NULL);
 
@@ -1753,13 +1716,9 @@ nvme_set_features(nvme_t *nvme, uint32_t nsid, uint8_t feature, uint32_t val,
 		goto fail;
 	}
 
-	if (nvme_admin_cmd(cmd, nvme_admin_cmd_timeout) != DDI_SUCCESS) {
-		dev_err(nvme->n_dip, CE_WARN,
-		    "!nvme_admin_cmd failed for SET FEATURES");
-		return (ret);
-	}
+	nvme_admin_cmd(cmd, nvme_admin_cmd_timeout);
 
-	if (nvme_check_cmd_status(cmd)) {
+	if ((ret = nvme_check_cmd_status(cmd)) != 0) {
 		dev_err(nvme->n_dip, CE_WARN,
 		    "!SET FEATURES %d failed with sct = %x, sc = %x",
 		    feature, cmd->nc_cqe.cqe_sf.sf_sct,
@@ -1768,19 +1727,18 @@ nvme_set_features(nvme_t *nvme, uint32_t nsid, uint8_t feature, uint32_t val,
 	}
 
 	*res = cmd->nc_cqe.cqe_dw0;
-	ret = B_TRUE;
 
 fail:
 	nvme_free_cmd(cmd);
 	return (ret);
 }
 
-static boolean_t
+static int
 nvme_get_features(nvme_t *nvme, uint32_t nsid, uint8_t feature, uint32_t *res,
     void **buf, size_t *bufsize)
 {
 	nvme_cmd_t *cmd = nvme_alloc_cmd(nvme, KM_SLEEP);
-	boolean_t ret = B_FALSE;
+	int ret = EINVAL;
 
 	ASSERT(res != NULL);
 
@@ -1846,6 +1804,7 @@ nvme_get_features(nvme_t *nvme, uint32_t nsid, uint8_t feature, uint32_t *res,
 		    &nvme->n_prp_dma_attr, &cmd->nc_dma) != DDI_SUCCESS) {
 			dev_err(nvme->n_dip, CE_WARN,
 			    "!nvme_zalloc_dma failed for GET FEATURES");
+			ret = ENOMEM;
 			goto fail;
 		}
 
@@ -1853,6 +1812,7 @@ nvme_get_features(nvme_t *nvme, uint32_t nsid, uint8_t feature, uint32_t *res,
 			dev_err(nvme->n_dip, CE_WARN,
 			    "!too many DMA cookies for GET FEATURES");
 			atomic_inc_32(&nvme->n_too_many_cookies);
+			ret = ENOMEM;
 			goto fail;
 		}
 
@@ -1866,13 +1826,9 @@ nvme_get_features(nvme_t *nvme, uint32_t nsid, uint8_t feature, uint32_t *res,
 		}
 	}
 
-	if (nvme_admin_cmd(cmd, nvme_admin_cmd_timeout) != DDI_SUCCESS) {
-		dev_err(nvme->n_dip, CE_WARN,
-		    "!nvme_admin_cmd failed for GET FEATURES");
-		return (ret);
-	}
+	nvme_admin_cmd(cmd, nvme_admin_cmd_timeout);
 
-	if (nvme_check_cmd_status(cmd)) {
+	if ((ret = nvme_check_cmd_status(cmd)) != 0) {
 		if (feature == NVME_FEAT_LBA_RANGE &&
 		    cmd->nc_cqe.cqe_sf.sf_sct == NVME_CQE_SCT_GENERIC &&
 		    cmd->nc_cqe.cqe_sf.sf_sc == NVME_CQE_SC_GEN_INV_FLD)
@@ -1892,14 +1848,13 @@ nvme_get_features(nvme_t *nvme, uint32_t nsid, uint8_t feature, uint32_t *res,
 	}
 
 	*res = cmd->nc_cqe.cqe_dw0;
-	ret = B_TRUE;
 
 fail:
 	nvme_free_cmd(cmd);
 	return (ret);
 }
 
-static boolean_t
+static int
 nvme_write_cache_set(nvme_t *nvme, boolean_t enable)
 {
 	nvme_write_cache_t nwc = { 0 };
@@ -1907,28 +1862,26 @@ nvme_write_cache_set(nvme_t *nvme, boolean_t enable)
 	if (enable)
 		nwc.b.wc_wce = 1;
 
-	if (!nvme_set_features(nvme, 0, NVME_FEAT_WRITE_CACHE, nwc.r, &nwc.r))
-		return (B_FALSE);
-
-	return (B_TRUE);
+	return
+	    (nvme_set_features(nvme, 0, NVME_FEAT_WRITE_CACHE, nwc.r, &nwc.r));
 }
 
 static int
-nvme_set_nqueues(nvme_t *nvme, uint16_t nqueues)
+nvme_set_nqueues(nvme_t *nvme, uint16_t *nqueues)
 {
 	nvme_nqueues_t nq = { 0 };
+	int ret;
 
-	nq.b.nq_nsq = nq.b.nq_ncq = nqueues - 1;
+	nq.b.nq_nsq = nq.b.nq_ncq = *nqueues - 1;
 
-	if (!nvme_set_features(nvme, 0, NVME_FEAT_NQUEUES, nq.r, &nq.r)) {
-		return (0);
-	}
+	ret = nvme_set_features(nvme, 0, NVME_FEAT_NQUEUES, nq.r, &nq.r);
 
 	/*
 	 * Always use the same number of submission and completion queues, and
 	 * never use more than the requested number of queues.
 	 */
-	return (MIN(nqueues, MIN(nq.b.nq_nsq, nq.b.nq_ncq) + 1));
+	*nqueues = MIN(*nqueues, MIN(nq.b.nq_nsq, nq.b.nq_ncq) + 1);
+	return (ret);
 }
 
 static int
@@ -1938,6 +1891,7 @@ nvme_create_io_qpair(nvme_t *nvme, nvme_qpair_t *qp, uint16_t idx)
 	nvme_create_queue_dw10_t dw10 = { 0 };
 	nvme_create_cq_dw11_t c_dw11 = { 0 };
 	nvme_create_sq_dw11_t s_dw11 = { 0 };
+	int ret;
 
 	dw10.b.q_qid = idx;
 	dw10.b.q_qsize = qp->nq_nentry - 1;
@@ -1953,18 +1907,13 @@ nvme_create_io_qpair(nvme_t *nvme, nvme_qpair_t *qp, uint16_t idx)
 	cmd->nc_sqe.sqe_cdw11 = c_dw11.r;
 	cmd->nc_sqe.sqe_dptr.d_prp[0] = qp->nq_cqdma->nd_cookie.dmac_laddress;
 
-	if (nvme_admin_cmd(cmd, nvme_admin_cmd_timeout) != DDI_SUCCESS) {
-		dev_err(nvme->n_dip, CE_WARN,
-		    "!nvme_admin_cmd failed for CREATE CQUEUE");
-		return (DDI_FAILURE);
-	}
+	nvme_admin_cmd(cmd, nvme_admin_cmd_timeout);
 
-	if (nvme_check_cmd_status(cmd)) {
+	if ((ret = nvme_check_cmd_status(cmd)) != 0) {
 		dev_err(nvme->n_dip, CE_WARN,
 		    "!CREATE CQUEUE failed with sct = %x, sc = %x",
 		    cmd->nc_cqe.cqe_sf.sf_sct, cmd->nc_cqe.cqe_sf.sf_sc);
-		nvme_free_cmd(cmd);
-		return (DDI_FAILURE);
+		goto fail;
 	}
 
 	nvme_free_cmd(cmd);
@@ -1980,23 +1929,19 @@ nvme_create_io_qpair(nvme_t *nvme, nvme_qpair_t *qp, uint16_t idx)
 	cmd->nc_sqe.sqe_cdw11 = s_dw11.r;
 	cmd->nc_sqe.sqe_dptr.d_prp[0] = qp->nq_sqdma->nd_cookie.dmac_laddress;
 
-	if (nvme_admin_cmd(cmd, nvme_admin_cmd_timeout) != DDI_SUCCESS) {
-		dev_err(nvme->n_dip, CE_WARN,
-		    "!nvme_admin_cmd failed for CREATE SQUEUE");
-		return (DDI_FAILURE);
-	}
+	nvme_admin_cmd(cmd, nvme_admin_cmd_timeout);
 
-	if (nvme_check_cmd_status(cmd)) {
+	if ((ret = nvme_check_cmd_status(cmd)) != 0) {
 		dev_err(nvme->n_dip, CE_WARN,
 		    "!CREATE SQUEUE failed with sct = %x, sc = %x",
 		    cmd->nc_cqe.cqe_sf.sf_sct, cmd->nc_cqe.cqe_sf.sf_sc);
-		nvme_free_cmd(cmd);
-		return (DDI_FAILURE);
+		goto fail;
 	}
 
+fail:
 	nvme_free_cmd(cmd);
 
-	return (DDI_SUCCESS);
+	return (ret);
 }
 
 static boolean_t
@@ -2089,9 +2034,8 @@ nvme_init_ns(nvme_t *nvme, int nsid)
 	int last_rp;
 
 	ns->ns_nvme = nvme;
-	idns = nvme_identify(nvme, nsid);
 
-	if (idns == NULL) {
+	if (nvme_identify(nvme, nsid, (void **)&idns) != 0) {
 		dev_err(nvme->n_dip, CE_WARN,
 		    "!failed to identify namespace %d", nsid);
 		return (DDI_FAILURE);
@@ -2180,7 +2124,7 @@ nvme_init(nvme_t *nvme)
 	nvme_reg_vs_t vs;
 	nvme_reg_csts_t csts;
 	int i = 0;
-	int nqueues;
+	uint16_t nqueues;
 	char model[sizeof (nvme->n_idctl->id_model) + 1];
 	char *vendor, *product;
 
@@ -2345,8 +2289,7 @@ nvme_init(nvme_t *nvme)
 	/*
 	 * Identify Controller
 	 */
-	nvme->n_idctl = nvme_identify(nvme, 0);
-	if (nvme->n_idctl == NULL) {
+	if (nvme_identify(nvme, 0, (void **)&nvme->n_idctl) != 0) {
 		dev_err(nvme->n_dip, CE_WARN,
 		    "!failed to identify controller");
 		goto fail;
@@ -2435,7 +2378,8 @@ nvme_init(nvme_t *nvme)
 
 	if (!nvme->n_write_cache_present) {
 		nvme->n_write_cache_enabled = B_FALSE;
-	} else if (!nvme_write_cache_set(nvme, nvme->n_write_cache_enabled)) {
+	} else if (nvme_write_cache_set(nvme, nvme->n_write_cache_enabled)
+	    != 0) {
 		dev_err(nvme->n_dip, CE_WARN,
 		    "!failed to %sable volatile write cache",
 		    nvme->n_write_cache_enabled ? "en" : "dis");
@@ -2507,10 +2451,11 @@ nvme_init(nvme_t *nvme)
 	/*
 	 * Create I/O queue pairs.
 	 */
-	nvme->n_ioq_count = nvme_set_nqueues(nvme, nqueues);
-	if (nvme->n_ioq_count == 0) {
+
+	if (nvme_set_nqueues(nvme, &nqueues) != 0) {
 		dev_err(nvme->n_dip, CE_WARN,
-		    "!failed to set number of I/O queues to %d", nqueues);
+		    "!failed to set number of I/O queues to %d",
+		    nvme->n_intr_cnt);
 		goto fail;
 	}
 
@@ -2519,14 +2464,16 @@ nvme_init(nvme_t *nvme)
 	 */
 	kmem_free(nvme->n_ioq, sizeof (nvme_qpair_t *));
 	nvme->n_ioq = kmem_zalloc(sizeof (nvme_qpair_t *) *
-	    (nvme->n_ioq_count + 1), KM_SLEEP);
+	    (nqueues + 1), KM_SLEEP);
 	nvme->n_ioq[0] = nvme->n_adminq;
 
+	nvme->n_ioq_count = nqueues;
+
 	/*
 	 * If we got less queues than we asked for we might as well give
 	 * some of the interrupt vectors back to the system.
 	 */
-	if (nvme->n_ioq_count < nqueues) {
+	if (nvme->n_ioq_count < nvme->n_intr_cnt) {
 		nvme_release_interrupts(nvme);
 
 		if (nvme_setup_interrupts(nvme, nvme->n_intr_type,
@@ -2553,8 +2500,7 @@ nvme_init(nvme_t *nvme)
 			goto fail;
 		}
 
-		if (nvme_create_io_qpair(nvme, nvme->n_ioq[i], i)
-		    != DDI_SUCCESS) {
+		if (nvme_create_io_qpair(nvme, nvme->n_ioq[i], i) != 0) {
 			dev_err(nvme->n_dip, CE_WARN,
 			    "!unable to create I/O qpair %d", i);
 			goto fail;
@@ -2588,6 +2534,9 @@ nvme_intr(caddr_t arg1, caddr_t arg2)
 	if (inum >= nvme->n_intr_cnt)
 		return (DDI_INTR_UNCLAIMED);
 
+	if (nvme->n_dead)
+		return (DDI_INTR_CLAIMED);
+
 	/*
 	 * The interrupt vector a queue uses is calculated as queue_idx %
 	 * intr_cnt in nvme_create_io_qpair(). Iterate through the queue array
@@ -3345,6 +3294,9 @@ nvme_open(dev_t *devp, int flag, int otyp, cred_t *cred_p)
 	if (nsid > nvme->n_namespace_count)
 		return (ENXIO);
 
+	if (nvme->n_dead)
+		return (EIO);
+
 	nm = nsid == 0 ? &nvme->n_minor : &nvme->n_ns[nsid - 1].ns_minor;
 
 	mutex_enter(&nm->nm_mutex);
@@ -3417,9 +3369,8 @@ nvme_ioctl_identify(nvme_t *nvme, int nsid, nvme_ioctl_t *nioc, int mode,
 	if (nioc->n_len < NVME_IDENTIFY_BUFSIZE)
 		return (EINVAL);
 
-	idctl = nvme_identify(nvme, nsid);
-	if (idctl == NULL)
-		return (EIO);
+	if ((rv = nvme_identify(nvme, nsid, (void **)&idctl)) != 0)
+		return (rv);
 
 	if (ddi_copyout(idctl, (void *)nioc->n_buf, NVME_IDENTIFY_BUFSIZE, mode)
 	    != 0)
@@ -3586,9 +3537,9 @@ nvme_ioctl_get_features(nvme_t *nvme, int nsid, nvme_ioctl_t *nioc,
 		return (EINVAL);
 	}
 
-	if (nvme_get_features(nvme, nsid, feature, &res, &buf, &bufsize) ==
-	    B_FALSE)
-		return (EIO);
+	rv = nvme_get_features(nvme, nsid, feature, &res, &buf, &bufsize);
+	if (rv != 0)
+		return (rv);
 
 	if (nioc->n_len < bufsize) {
 		kmem_free(buf, bufsize);
@@ -3805,6 +3756,10 @@ nvme_ioctl(dev_t dev, int cmd, intptr_t arg, int mode, cred_t *cred_p,
 	}
 #endif
 
+	if (nvme->n_dead && cmd != NVME_IOC_DETACH)
+		return (EIO);
+
+
 	if (cmd == NVME_IOC_IDENTIFY_CTRL) {
 		/*
 		 * This makes NVME_IOC_IDENTIFY_CTRL work the same on devctl and
diff --git a/usr/src/uts/common/io/nvme/nvme_var.h b/usr/src/uts/common/io/nvme/nvme_var.h
index 5af74cb788..d2c0b5b745 100644
--- a/usr/src/uts/common/io/nvme/nvme_var.h
+++ b/usr/src/uts/common/io/nvme/nvme_var.h
@@ -159,7 +159,7 @@ struct nvme {
 	int n_pagesize;
 
 	int n_namespace_count;
-	int n_ioq_count;
+	uint16_t n_ioq_count;
 
 	nvme_identify_ctrl_t *n_idctl;
 
-- 
2.21.0

