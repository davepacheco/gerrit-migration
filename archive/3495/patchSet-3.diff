commit f07a650d122ed06e6ac66d2739c3b69bd88d875e (refs/changes/95/3495/3)
Author: Trent Mick <trentm@gmail.com>
Date:   2018-02-27T12:30:43-08:00 (1 year, 7 months ago)
    
    TRITON-184 add config-agent <==> SAPI load testing tools
    Reviewed by: Todd Whiteman <todd.whiteman@joyent.com>
    Approved by: Todd Whiteman <todd.whiteman@joyent.com>

diff --git a/agent.js b/agent.js
index 6485fea..98ae5c1 100644
--- a/agent.js
+++ b/agent.js
@@ -119,8 +119,13 @@ async.waterfall([
 
 			zonename = zonename_;
 			if (zonename !== 'global') {
-				config.instances = [ zonename ];
-			} // else TODO AGENT-732
+				// Allow the config file to specify the
+				// instance UUID(s). This is used for load
+				// testing.
+				if (config.instances.length === 0) {
+					config.instances = [ zonename ];
+				}
+			}
 
 			autoMetadata.ZONENAME = zonename;
 
@@ -247,7 +252,7 @@ async.waterfall([
 						+ 'checkAndRefresh failure');
 				} else {
 					log.info('synchronous agent '
-						+ 'checkAndRefresh complete');
+						+ 'checkAndRefresh success');
 				}
 				cb(err);
 			});
diff --git a/bin/agent.sh b/bin/agent.sh
index a1f9ba9..9ec2ee9 100755
--- a/bin/agent.sh
+++ b/bin/agent.sh
@@ -6,71 +6,144 @@
 #
 
 #
-# Copyright (c) 2014, Joyent, Inc.
+# Copyright (c) 2018, Joyent, Inc.
 #
 
-###############################################################################
-# Due to race conditions for dependent services, this script will first run
-# the agent in synchronous mode, then fork an agent process in the background.
-###############################################################################
+#
+# Start the config-agent.
+#
+# This starter script ensures that a first time synchronous run completes,
+# i.e. that the config-agent has at least rendered the `sapi_template`s once.
+# This allows SMF services in this zone to depend on the "config-agent" SMF
+# service to ensure they have their config files before starting.
+#
+# If this is not a first run, then startup will make 3 attempts at a
+# synchronous run before continuing.
+#
 
 set -o xtrace
 
+
+# ---- globals
+
 DIR=$(cd $(dirname $(readlink -f $0))/../ >/dev/null; pwd)
 EXEC="$DIR/build/node/bin/node $DIR/agent.js"
 
-SAPI_URL=
-. /lib/sdc/config.sh
-load_sdc_config
+# Default config path for non-global zone instances.
+DEFAULT_NGZ_CONFIG_FILE=$DIR/etc/config.json
+CONFIG_FILE=
+
+# Ideally this should come from the configured 'pollInterval'. For now we use
+# the same value as the default config-agent pollInterval.
+POLL_INTERVAL_S=120
+
+SMF_INST=${SMF_FMRI##svc:*:}
+RUN_DIR=/var/run/config-agent-$SMF_INST
+FIRST_RUN_FILE=$RUN_DIR/first-run
 
-# compute node
-if [[ -n ${CONFIG_sapi_domain} ]]; then
-   SAPI_URL=http://${CONFIG_sapi_domain}
-elif [[ -n ${CONFIG_datacenter_name} && -n ${CONFIG_dns_domain} ]]; then
-   SAPI_URL=http://sapi.${CONFIG_datacenter_name}.${CONFIG_dns_domain}
+
+# ---- support functions
+
+function fatal
+{
+    echo "$0: fatal error: $*"
+    exit 1
+}
+
+function usage
+{
+    echo "Usage: $0 [-h | -f CONFIG-FILE]"
+}
+
+
+# ---- mainline
+
+while getopts "hf:" opt
+do
+    case "$opt" in
+        h)
+            usage
+            exit 0
+            ;;
+        f)
+            CONFIG_FILE=$OPTARG
+            ;;
+        *)
+            usage
+            exit 1
+            ;;
+    esac
+done
+
+
+# Determine the config details with which to exec config-agent.
+ZONENAME=$(zonename)
+if [[ "$ZONENAME" == "global" ]]; then
+    SAPI_URL=
+    . /lib/sdc/config.sh
+    load_sdc_config
+
+    if [[ -n ${CONFIG_sapi_domain} ]]; then
+        SAPI_URL=http://${CONFIG_sapi_domain}
+    elif [[ -n ${CONFIG_datacenter_name} && -n ${CONFIG_dns_domain} ]]; then
+        SAPI_URL=http://sapi.${CONFIG_datacenter_name}.${CONFIG_dns_domain}
+    else
+        fatal "could not determine SAPI URL from node config"
+    fi
+    EXEC="$EXEC --sapi-url $SAPI_URL"
+
+    if [[ -n "$CONFIG_FILE" ]]; then
+        EXEC="$EXEC -f $CONFIG_FILE"
+    fi
 else
-    # regular zone with mdata-get
+    # Regular zone with mdata-get.
     SAPI_URL=$(/usr/sbin/mdata-get sapi-url)
-fi
+    if [[ -n $SAPI_URL ]]; then
+        EXEC="$EXEC --sapi-url $SAPI_URL"
+    fi
 
-if [[ -n $SAPI_URL ]]; then
-    EXEC="$EXEC --sapi-url $SAPI_URL"
+    if [[ -n "$CONFIG_FILE" ]]; then
+        EXEC="$EXEC -f $CONFIG_FILE"
+    else
+        EXEC="$EXEC -f $DEFAULT_NGZ_CONFIG_FILE"
+    fi
 fi
 
-# default config for zone instances
-if [[ -f $DIR/etc/config.json ]]; then
-    EXEC="$EXEC -f $DIR/etc/config.json"
-fi
 
-RUN_FILE=/var/tmp/.ran_config_agent
-RUN_EXISTS=0
-if [[ -e $RUN_FILE ]]; then
-    RUN_EXISTS=1
+RAN_ONCE=0
+if [[ -e $FIRST_RUN_FILE ]]; then
+    RAN_ONCE=1
 fi
+echo "$0: start sync mode attempts (RAN_ONCE=$RAN_ONCE)"
 
-echo 'Attempting synchronous mode until success.'
 COUNT=0
 SUCCESS=1
 while [[ $SUCCESS != 0 ]]; do
-
-    if [[ $RUN_EXISTS == 1 ]] && [[ $COUNT -gt 2 ]]; then
-        echo 'Exceeded tries.  Agent has successful previous run, continuing...'
+    if [[ $RAN_ONCE == 1 ]] && [[ $COUNT -gt 2 ]]; then
+        echo "$0: failed $COUNT sync attempts, agent has successful" \
+            "previous run, continuing"
         break;
     fi
 
-    $EXEC -s -t 30
+    # Set a timeout of the full poll interval as a balance between (a) actually
+    # retrying if this hangs and (b) not dogpiling in SAPI if it is overloaded.
+    $EXEC -s -t $POLL_INTERVAL_S
     SUCCESS=$?
     if [[ $SUCCESS != 0 ]]; then
-        echo 'Failed to run the agent in synchronous mode.  Sleeping...'
-        sleep 1;
+        DELAY=$(( RANDOM % $POLL_INTERVAL_S ))
+        echo "$0: failed sync attempt (COUNT=$COUNT), retrying in ${DELAY}s"
+        sleep $DELAY
     fi
     let COUNT=COUNT+1
 done
+mkdir -p "$(dirname $FIRST_RUN_FILE)"
+touch $FIRST_RUN_FILE
+
 
-echo 'Starting the agent in daemon mode.'
-touch $RUN_FILE
+echo "$0: starting config-agent in daemon mode"
 $EXEC &
 
+
 # Capture to be able to send refresh signal
 DAEMON_PID=$!
 
diff --git a/tools/loadtest/README.md b/tools/loadtest/README.md
new file mode 100644
index 0000000..5b8aaaa
--- /dev/null
+++ b/tools/loadtest/README.md
@@ -0,0 +1,56 @@
+This dir contains scripts to help setup some load testing of config-agent/SAPI.
+We create a loadtest0 zone in which we'll run N SMF instances of config-agent,
+each polling SAPI as a separate SAPI instance.
+
+
+# Setup
+
+## Setup some tools in SAPI for measuring load
+
+### nhttpsnoop to show GC'ing and requests by SAPI server
+
+    ssh coal
+
+    cd /var/tmp
+    curl -k -O https://raw.githubusercontent.com/joyent/nhttpsnoop/master/nhttpsnoop
+    chmod +x nhttpsnoop
+    cp nhttpsnoop /zones/$(vmadm lookup -1 alias=sapi0)/root/var/tmp
+
+### sapi GetConfig rps
+
+    cd trentops/bin    # a clone of trentops.git
+    scp triton-sapi-getconfig-rps coal:/var/tmp
+    ssh coal
+
+    cp /var/tmp/triton-sapi-getconfig-rps /zones/$(vmadm lookup -1 alias=sapi0)/root/var/tmp/
+
+
+## Setup N config-agents running in a loadtest0 zone
+
+    # on your laptop
+    cd .../config-agent/
+    TRACE=1 ./tools/loadtest/create-loadtest-zone.sh coal
+    ./tools/rsync-to coal loadtest0
+    ssh coal
+
+    # in COAL
+    zlogin $(vmadm lookup -1 alias=loadtest0)
+
+    # in the loadtest0 zone
+    cd /opt/smartdc/config-agent/tools/loadtest
+    ./loadtest.sh
+
+
+# Measure
+
+## nhttpsnoop to show GC'ing and requests by SAPI server
+
+    ssh coal
+    sdc-login -l sapi
+    /var/tmp/nhttpsnoop -slg
+
+## sapi GetConfig rps
+
+    ssh coal
+    sdc-login -l sapi
+    /var/tmp/triton-sapi-getconfig-rps
diff --git a/tools/loadtest/config-agents.xml.head b/tools/loadtest/config-agents.xml.head
new file mode 100644
index 0000000..0296df5
--- /dev/null
+++ b/tools/loadtest/config-agents.xml.head
@@ -0,0 +1,18 @@
+<?xml version="1.0"?>
+<!DOCTYPE service_bundle SYSTEM "/usr/share/lib/xml/dtd/service_bundle.dtd.1">
+
+<service_bundle type="manifest" name="smartdc-config-agent">
+<service name="smartdc/application/config-agent" type="service" version="1">
+
+    <dependency name="network" grouping="require_all" restart_on="error" type="service">
+        <service_fmri value="svc:/network/physical" />
+    </dependency>
+    <dependency name="filesystem" grouping="require_all" restart_on="error" type="service">
+        <service_fmri value="svc:/system/filesystem/local" />
+    </dependency>
+
+    <exec_method type="method" name="start" exec="/opt/smartdc/config-agent/bin/agent.sh -f /opt/smartdc/config-agent/etc/%{config-agent/id}/config.json" timeout_seconds="0" />
+    <exec_method type="method" name="stop" exec=":kill" timeout_seconds="30" />
+    <exec_method type="method" name="refresh" exec=":kill -HUP" timeout_seconds="30" />
+
+    <!-- instances go here -->
diff --git a/tools/loadtest/config-agents.xml.tail b/tools/loadtest/config-agents.xml.tail
new file mode 100644
index 0000000..0ee5ef4
--- /dev/null
+++ b/tools/loadtest/config-agents.xml.tail
@@ -0,0 +1,9 @@
+
+    <stability value='Unstable' />
+
+    <template>
+        <common_name><loctext xml:lang="C">Triton Config Agent load testing</loctext></common_name>
+    </template>
+
+</service>
+</service_bundle>
diff --git a/tools/loadtest/create-loadtest-zone.sh b/tools/loadtest/create-loadtest-zone.sh
new file mode 100755
index 0000000..0bc6a1d
--- /dev/null
+++ b/tools/loadtest/create-loadtest-zone.sh
@@ -0,0 +1,90 @@
+#!/bin/bash
+#
+# Create a loadtest0 zone from which to run config-agent/SAPI load tests.
+#
+# Usage on your laptop:
+#   TRACE=1 ./create-loadtest-zone.sh coal
+#
+
+if [[ -n "$TRACE" ]]; then
+    export PS4='[\D{%FT%TZ}] ${BASH_SOURCE}:${LINENO}: ${FUNCNAME[0]:+${FUNCNAME[0]}(): }'
+    set -o xtrace
+fi
+set -o errexit
+set -o pipefail
+
+
+SSH_OPTIONS="-q -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
+SSH="ssh $SSH_OPTIONS"
+SCP="scp $SSH_OPTIONS"
+
+
+#---- support stuff
+
+function fatal
+{
+    echo "$0: fatal error: $*"
+    exit 1
+}
+
+
+#---- mainline
+
+HEADNODE=$1
+[[ -n "$HEADNODE" ]] || fatal "no HEADNODE arg given"
+
+
+$SSH -T root@$HEADNODE <<SCRIPT
+
+if [[ -n "$TRACE" ]]; then
+    export PS4='\${BASH_SOURCE}:\${LINENO}: \${FUNCNAME[0]:+\${FUNCNAME[0]}(): }'
+    set -o xtrace
+fi
+set -o errexit
+set -o pipefail
+
+PATH=\$PATH:/opt/smartdc/bin
+
+ZONE=\$(vmadm lookup alias=loadtest0)
+
+if [[ -z "\$ZONE" ]]; then
+    sdc-vmapi /vms -X POST -d@- <<EOP | sdc-waitforjob
+{
+    "alias": "loadtest0",
+    "owner_uuid": "\$(bash /lib/sdc/config.sh -json | json ufds_admin_uuid)",
+    "brand": "joyent-minimal",
+    "billing_id": "\$(sdc-papi /packages?name=sdc_2048 | json -H 0.uuid)",
+    "server_uuid": "\$(sysinfo | json UUID)",
+    "networks": [
+        {
+            "uuid": "\$(sdc-napi /networks | json -H -c "this.name=='admin'" 0.uuid)"
+        },
+        {
+            "uuid": "\$(sdc-napi /networks | json -H -c "this.name=='external'" 0.uuid)",
+            "primary": true
+        }
+    ],
+    "image_uuid": "\$(sdc-imgadm list name=triton-origin-multiarch-15.4.1 --latest -H -o uuid)",
+    "customer_metadata": {
+        "sapi-url": "http://sapi.\$(bash /lib/sdc/config.sh -json | json datacenter_name).\$(bash /lib/sdc/config.sh -json | json dns_domain)"
+    },
+    "dns_domain": "\$(bash /lib/sdc/config.sh -json | json dns_domain)"
+}
+EOP
+fi
+
+ZONE=\$(vmadm lookup alias=loadtest0)
+zlogin \$ZONE '
+    latestConfigAgent=\$(curl -s https://updates.joyent.com/images?name=config-agent | json -H -- -1 | json uuid)
+
+    curl -o /var/tmp/config-agent.tar.bz2 https://updates.joyent.com/images/\$latestConfigAgent/file
+    mkdir -p /opt/smartdc
+    cd /opt/smartdc
+    rm -rf config-agent
+    tar xf /var/tmp/config-agent.tar.bz2
+'
+
+echo "Load test zone $ZONE (loadtest0) created successfully"
+
+SCRIPT
+
diff --git a/tools/loadtest/loadtest.sh b/tools/loadtest/loadtest.sh
new file mode 100755
index 0000000..7c8d47a
--- /dev/null
+++ b/tools/loadtest/loadtest.sh
@@ -0,0 +1,183 @@
+#!/bin/bash
+#
+# Load test SAPI/config-agent by creating a bunch of SAPI instances
+# and config-agents. See TRITON-89.
+#
+
+if [[ -n "$TRACE" ]]; then
+    export PS4='[\D{%FT%TZ}] ${BASH_SOURCE}:${LINENO}: ${FUNCNAME[0]:+${FUNCNAME[0]}(): }'
+    set -o xtrace
+fi
+set -o errexit
+set -o pipefail
+
+
+# ---- globals
+
+SRCDIR=$(cd $(dirname $0)/; pwd)
+BASEDIR=/opt/smartdc/config-agent
+
+FMRI=svc:/smartdc/application/config-agent
+SERVICE=loadtest
+DEFAULT_NUM_INSTS=100
+
+
+# ---- internal support functions
+
+function fatal
+{
+    echo "$0: fatal error: $*"
+    exit 1
+}
+
+function adopt_instance
+{
+    local instance_uuid=$1
+    local service_uuid
+    local retry=10
+    local url
+    local data
+
+    if [[ -z "${instance_uuid}" ]]; then
+        fatal 'must pass instance_uuid'
+    fi
+
+    while (( retry-- > 0 )); do
+        #
+        # Fetch the UUID of the SAPI service for this agent.
+        #
+        url="${SAPI_URL}/services?type=agent&name=${SERVICE}"
+        if ! service_uuid="$(curl -sSf -H 'Accept: application/json' "${url}" \
+          | json -Ha uuid)"; then
+            printf 'Could not retrieve SAPI service UUID for "%s"\n' \
+              "${SERVICE}\n" >&2
+            sleep 5
+            continue
+        fi
+
+        #
+        # Attempt to register the SAPI instance for this agent installation.
+        # We need not be overly clever here; SAPI returns success for a
+        # duplicate agent adoption.
+        #
+        url="${SAPI_URL}/instances"
+        data="{
+            \"service_uuid\": \"${service_uuid}\",
+            \"uuid\": \"${instance_uuid}\"
+        }"
+        if ! curl -sSf -X POST -H 'Content-Type: application/json' \
+          -d "${data}" "${url}"; then
+            echo
+            printf 'Could not register SAPI instance with UUID "%s"\n' \
+              "${instance_uuid}\n" >&2
+            sleep 5
+            continue
+        fi
+        echo
+
+        printf 'Agent successfully adopted into SAPI.\n' >&2
+        return 0
+    done
+
+    fatal 'adopt_instance: failing after too many retries'
+}
+
+
+# ---- mainline
+
+NUM_INSTS=$1
+[[ -n "$NUM_INSTS" ]] || NUM_INSTS=$DEFAULT_NUM_INSTS
+
+SAPI_URL=$(mdata-get sapi-url)
+[[ -n "$SAPI_URL" ]] || fatal "no 'sapi-url' metadata"
+
+APP_UUID=$(curl -s $SAPI_URL/applications?name=sdc | json -H 0.uuid)
+[[ -n "$APP_UUID" ]] || fatal "could not determine the 'sdc' SAPI app uuid"
+
+SVC_UUID=$(curl -s -H accept-version:~2 "$SAPI_URL/services?name=$SERVICE&application_uuid=$APP_UUID" | json -H 0.uuid)
+if [[ -z "$SVC_UUID" ]]; then
+    cat <<EOP | curl -sS --fail -H accept-version:~2 -H content-type:application/json $SAPI_URL/services -X POST -d@-
+{
+    "application_uuid": "$APP_UUID",
+    "name": "$SERVICE",
+    "type": "agent",
+    "params": {},
+    "metadata": {},
+    "manifests": {}
+}
+EOP
+    SVC_UUID=$(curl -s -H accept-version:~2 "$SAPI_URL/services?name=$SERVICE&application_uuid=$APP_UUID" | json -H 0.uuid)
+fi
+[[ -n "$SVC_UUID" ]] || fatal "could not determine the '$SERVICE' SAPI svc uuid"
+
+
+SMF_MANIFEST=$BASEDIR/etc/config-agents.xml
+mkdir -p $(dirname $SMF_MANIFEST)
+cat $SRCDIR/config-agents.xml.head >$SMF_MANIFEST
+
+
+i=0
+while (( i < $NUM_INSTS )); do
+    instName=$(printf "beef%04d" $i)
+    instUuid=$instName-$(uuid -v4 | cut -c10-)
+    echo "Creating instance $instName ($instUuid)"
+    adopt_instance $instUuid
+
+    instDir=/opt/smartdc/config-agent/etc/$instName
+    mkdir -p $instDir
+    cat <<EOM >$instDir/config.json
+{
+    "// logLevel": "info",
+    "logLevel": "trace",
+    "pollInterval": 120000,
+    "// pollInterval": 10000,
+    "instances": ["$instUuid"],
+    "localManifestDirs": ["$instDir"]
+}
+EOM
+    mkdir -p $instDir/sapi_manifests/manifest0
+    cat <<EOM >$instDir/sapi_manifests/manifest0/manifest.json
+{
+    "name": "manifest0",
+    "path": "$instDir/manifest0.json",
+    "post_cmd": "echo hi"
+}
+EOM
+    cat <<EOM >$instDir/sapi_manifests/manifest0/template
+{
+    "foo": "{{{ foo }}}"
+}
+EOM
+    cat <<EOM >>$SMF_MANIFEST
+    <instance name="$instName" enabled="true">
+        <property_group name="config-agent" type="application">
+            <propval name="id" type="astring" value="$instName" />
+        </property_group>
+    </instance>
+EOM
+
+    i=$(( i + 1 ));
+done
+
+cat $SRCDIR/config-agents.xml.tail >>$SMF_MANIFEST
+
+# Actually killing these things appears to be difficult sometime. I don't
+# know why.
+if svcs -H $FMRI:* >/dev/null 2>/dev/null; then
+    svcadm disable $FMRI:* || true
+    pgrep -f /opt/smartdc/config-agent/bin/ | xargs -n1 kill || true
+    pgrep -f /opt/smartdc/config-agent/build/node/bin/node | xargs -n1 kill || true
+    svcadm disable -s $FMRI:*
+    svccfg delete -f $FMRI
+    rm -rf /var/run/config-agent-beef*
+fi
+
+echo "Importing new config-agents SMF at: $(date -u)"
+svccfg import $SMF_MANIFEST
+#svcadm enable $FMRI:*
+
+echo
+echo "===="
+echo "Success! There should now be $NUM_INSTS config-agent SMF services"
+echo "running in this loadtest0 zone, putting load on SAPI."
+echo "===="
diff --git a/tools/rsync-to b/tools/rsync-to
index 5ea7f86..c12a962 100755
--- a/tools/rsync-to
+++ b/tools/rsync-to
@@ -66,22 +66,31 @@ if [[ "$ZONEUUID" == "global" ]]; then
     baseDir=/opt/smartdc/agents/lib/node_modules/config-agent
 fi
 
-
 # Clean and rsync.
 if [[ $(uname -s) == "SunOS" ]]; then
     # Clean node_modules everytime because that's preferred and we can
     # use the binary .node files from this plat.
     ssh $NODE rm -rf $baseDir/node_modules
 fi
-for f in agent.js bin cmd lib Makefile node_modules npm package.json smf; do
-    rsync -av ${TOP}/$f $NODE:$baseDir/ $extraOpts
-done
 
+rsync -av ${TOP}/ \
+    $NODE:$baseDir/ \
+    $extraOpts \
+    --exclude .git/ \
+    --exclude /boot/ \
+    --exclude /build/ \
+    --exclude /deps/ \
+    --exclude /etc/ \
+    --exclude /tmp/
 
 # Restart config-agent.
-state=$(ssh ${NODE} svcs -z ${ZONEUUID} -H -o state config-agent)
-if [[ "$state" == "maintenance" ]]; then
-    ssh ${NODE} svcadm -z ${ZONEUUID} clear config-agent
+# This block is written to work with a config-agent service that has multiple
+# instances, as is used for load testing (see tools/loadtest/...).
+FMRI=svc:/smartdc/application/config-agent
+if ssh ${NODE} svcs -z ${ZONEUUID} -H -o state $FMRI | grep maintenance >/dev/null; then
+    echo "Clearing config-agent service"
+    ssh ${NODE} svcadm -z ${ZONEUUID} clear $FMRI
 else
-    ssh ${NODE} svcadm -z ${ZONEUUID} restart config-agent
+    echo "Restarting config-agent service"
+    ssh ${NODE} svcadm -z ${ZONEUUID} restart $FMRI
 fi
