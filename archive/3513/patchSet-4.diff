From fcb810675407fb5309c40f20172c9a7098e695bd Mon Sep 17 00:00:00 2001
From: Patrick Mooney <pmooney@pfmooney.com>
Date: Wed, 3 Jan 2018 21:11:35 +0000
Subject: [PATCH] OS-6681 modernize viona driver

OS-XXXX Overhaul viona
OS-XXXX Wire in viona again and tidy it up some
OS-XXXX clean up viona for easier tracing
OS-XXXX fix viona to work with illumos vioif
OS-XXXX Add initial HCKSUM support to viona
OS-XXXX deliver viona interrupts in-kernel when possible
viona errors should give errno, not -1 (mgerdts)
OS-XXXX viona HCKSUM detection should use correct byte order
OS-XXXX viona should wait patiently for pending xfers during shutdown
Fix debug build (unused variable) (mgerdts)
preliminary sdev plugin for viona (mgerdts)
OS-XXXX viona should pad VLAN-strip-induced runts
OS-XXXX viona can use plain devfsadm plugin
OS-XXXX fix legacy intrs in viona
OS-XXXX clean up BAR cruft from viona

- Harden in-kernel logic to better handle bogus inputs or edge cases
- Use in-kernel ioport hooks for virtqueue notification
- Change viona_desb allocation to static
- Wire up hardware checksum offload for TX/RX.  The edge cases for NICs
  lacking HCKSUM_INET_PARTIAL support have not been explored.
- Add parameter handling to viona userspace for masking out virtio
  feature bits and setting the virtqueue size
- viona sdev fix
---
 usr/src/cmd/bhyve/Makefile.com             |    1 +
 usr/src/cmd/bhyve/pci_emul.c               |    5 +
 usr/src/cmd/bhyve/pci_emul.h               |    7 +
 usr/src/cmd/bhyve/pci_virtio_viona.c       |  472 +++--
 usr/src/cmd/devfsadm/i386/misc_link_i386.c |    6 +
 usr/src/uts/i86pc/Makefile.i86pc           |    1 +
 usr/src/uts/i86pc/io/viona/viona.c         | 2157 +++++++++++++-------
 usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c     |   11 +
 usr/src/uts/i86pc/sys/viona_io.h           |   49 +-
 usr/src/uts/i86pc/sys/vmm_drv.h            |    1 +
 usr/src/uts/i86pc/viona/Makefile           |    5 +-
 11 files changed, 1773 insertions(+), 942 deletions(-)

diff --git a/usr/src/cmd/bhyve/Makefile.com b/usr/src/cmd/bhyve/Makefile.com
index b80bf910de..b9ad3fde70 100644
--- a/usr/src/cmd/bhyve/Makefile.com
+++ b/usr/src/cmd/bhyve/Makefile.com
@@ -42,6 +42,7 @@ SRCS =	acpi.c			\
 	pci_virtio_block.c	\
 	pci_virtio_net.c	\
 	pci_virtio_rnd.c	\
+	pci_virtio_viona.c	\
 	pci_xhci.c		\
 	pm.c			\
 	post.c			\
diff --git a/usr/src/cmd/bhyve/pci_emul.c b/usr/src/cmd/bhyve/pci_emul.c
index 101773b4e5..d48af9521b 100644
--- a/usr/src/cmd/bhyve/pci_emul.c
+++ b/usr/src/cmd/bhyve/pci_emul.c
@@ -1580,6 +1580,11 @@ pci_lintr_update(struct pci_devinst *pi)
 		pci_irq_assert(pi);
 	}
 	pthread_mutex_unlock(&pi->pi_lintr.lock);
+#ifndef __FreeBSD__
+	if (pi->pi_d->pe_lintrupdate != NULL) {
+		pi->pi_d->pe_lintrupdate(pi);
+	}
+#endif /* __FreeBSD__ */
 }
 
 int
diff --git a/usr/src/cmd/bhyve/pci_emul.h b/usr/src/cmd/bhyve/pci_emul.h
index 0fffb19dee..a0aaa425be 100644
--- a/usr/src/cmd/bhyve/pci_emul.h
+++ b/usr/src/cmd/bhyve/pci_emul.h
@@ -25,6 +25,9 @@
  *
  * $FreeBSD$
  */
+/*
+ * Copyright 2018 Joyent, Inc.
+ */
 
 #ifndef _PCI_EMUL_H_
 #define _PCI_EMUL_H_
@@ -69,6 +72,10 @@ struct pci_devemu {
 	uint64_t  (*pe_barread)(struct vmctx *ctx, int vcpu,
 				struct pci_devinst *pi, int baridx,
 				uint64_t offset, int size);
+
+#ifndef __FreeBSD__
+	void	(*pe_lintrupdate)(struct pci_devinst *pi);
+#endif /* __FreeBSD__ */
 };
 #define PCI_EMUL_SET(x)   DATA_SET(pci_devemu_set, x);
 
diff --git a/usr/src/cmd/bhyve/pci_virtio_viona.c b/usr/src/cmd/bhyve/pci_virtio_viona.c
index e5a5cb584f..1c4b739711 100644
--- a/usr/src/cmd/bhyve/pci_virtio_viona.c
+++ b/usr/src/cmd/bhyve/pci_virtio_viona.c
@@ -34,7 +34,7 @@
  * http://www.illumos.org/license/CDDL.
  *
  * Copyright 2015 Pluribus Networks Inc.
- * Copyright 2017 Joyent, Inc.
+ * Copyright 2018 Joyent, Inc.
  */
 
 #include <sys/cdefs.h>
@@ -84,18 +84,6 @@
 
 #define	VIONA_REGSZ	VIONA_R_MAX+1
 
-/*
- * Host capabilities
- */
-#define	VIRTIO_NET_F_MAC	(1 <<  5) /* host supplies MAC */
-#define	VIRTIO_NET_F_MRG_RXBUF	(1 << 15) /* host can merge RX buffers */
-#define	VIRTIO_NET_F_STATUS	(1 << 16) /* config status field available */
-
-#define	VIONA_S_HOSTCAPS		\
-	(VIRTIO_NET_F_MAC |		\
-	VIRTIO_NET_F_MRG_RXBUF |	\
-	VIRTIO_NET_F_STATUS)
-
 /*
  * Queue definitions.
  */
@@ -108,7 +96,7 @@
 /*
  * Debug printf
  */
-static int pci_viona_debug;
+static volatile int pci_viona_debug;
 #define	DPRINTF(params) if (pci_viona_debug) printf params
 #define	WPRINTF(params) printf params
 
@@ -124,26 +112,20 @@ struct pci_viona_softc {
 	int		vsc_isr;
 
 	datalink_id_t	vsc_linkid;
-	char		vsc_linkname[MAXLINKNAMELEN];
 	int		vsc_vnafd;
 
+	/* Configurable parameters */
+	char		vsc_linkname[MAXLINKNAMELEN];
+	uint32_t	vsc_feature_mask;
+	uint16_t	vsc_vq_size;
+
 	uint32_t	vsc_features;
 	uint8_t		vsc_macaddr[6];
 
 	uint64_t	vsc_pfn[VIONA_MAXQ];
 	uint16_t	vsc_msix_table_idx[VIONA_MAXQ];
-	/*
-	 * Flag to see if host is already sending data out.
-	 * If it is, no need to wait for lock and send interrupt to host
-	 * for new data.
-	 */
-	boolean_t	vsc_tx_kick_lock_held;
-
-	pthread_t	tx_tid;
-	pthread_mutex_t	tx_mtx;
-	pthread_cond_t	tx_cond;
+	boolean_t	vsc_msix_active;
 };
-#define	viona_ctx(sc)	((sc)->vsc_pi->pi_vmctx)
 
 /*
  * Return the size of IO BAR that maps virtio header and device specific
@@ -160,15 +142,14 @@ pci_viona_iosize(struct pci_devinst *pi)
 }
 
 static uint16_t
-pci_viona_qsize(int qnum)
+pci_viona_qsize(struct pci_viona_softc *sc, int qnum)
 {
 	/* XXX no ctl queue currently */
 	if (qnum == VIONA_CTLQ) {
 		return (0);
 	}
 
-	/* XXX fixed currently. Maybe different for tx/rx/ctl */
-	return (VIONA_RINGSZ);
+	return (sc->vsc_vq_size);
 }
 
 static void
@@ -180,21 +161,13 @@ pci_viona_ring_reset(struct pci_viona_softc *sc, int ring)
 
 	switch (ring) {
 	case VIONA_RXQ:
-		error = ioctl(sc->vsc_vnafd, VNA_IOC_RX_RING_RESET);
-		if (error != 0) {
-			WPRINTF(("ioctl viona rx ring reset failed %d\n",
-			    error));
-		} else {
-			sc->vsc_pfn[VIONA_RXQ] = 0;
-		}
-		break;
 	case VIONA_TXQ:
-		error = ioctl(sc->vsc_vnafd, VNA_IOC_TX_RING_RESET);
+		error = ioctl(sc->vsc_vnafd, VNA_IOC_RING_RESET, ring);
 		if (error != 0) {
-			WPRINTF(("ioctl viona tx ring reset failed %d\n",
-			    error));
+			WPRINTF(("ioctl viona ring %u reset failed %d\n",
+			    ring, errno));
 		} else {
-			sc->vsc_pfn[VIONA_TXQ] = 0;
+			sc->vsc_pfn[ring] = 0;
 		}
 		break;
 	case VIONA_CTLQ:
@@ -220,11 +193,11 @@ static void *
 pci_viona_poll_thread(void *param)
 {
 	struct pci_viona_softc *sc = param;
-	pollfd_t	pollset;
-	int			error;
+	pollfd_t pollset;
+	const int fd = sc->vsc_vnafd;
 
-	pollset.fd = sc->vsc_vnafd;
-	pollset.events = POLLIN | POLLOUT;
+	pollset.fd = fd;
+	pollset.events = POLLRDBAND;
 
 	for (;;) {
 		if (poll(&pollset, 1, -1) < 0) {
@@ -236,23 +209,35 @@ pci_viona_poll_thread(void *param)
 				break;
 			}
 		}
-		if (pollset.revents & POLLIN) {
-			pci_generate_msix(sc->vsc_pi,
-			    sc->vsc_msix_table_idx[VIONA_RXQ]);
-			error = ioctl(sc->vsc_vnafd, VNA_IOC_RX_INTR_CLR);
-			if (error != 0) {
-				WPRINTF(("ioctl viona rx intr clear failed"
-				    " %d\n", error));
+		if (pollset.revents & POLLRDBAND) {
+			vioc_intr_poll_t vip;
+			uint_t i;
+			int res;
+			boolean_t assert_lintr = B_FALSE;
+			const boolean_t do_msix = pci_msix_enabled(sc->vsc_pi);
+
+			res = ioctl(fd, VNA_IOC_INTR_POLL, &vip);
+			for (i = 0; res > 0 && i < VIONA_VQ_MAX; i++) {
+				if (vip.vip_status[i] == 0) {
+					continue;
+				}
+				if (do_msix) {
+					pci_generate_msix(sc->vsc_pi,
+					    sc->vsc_msix_table_idx[i]);
+				} else {
+					assert_lintr = B_TRUE;
+				}
+				res = ioctl(fd, VNA_IOC_RING_INTR_CLR, i);
+				if (res != 0) {
+					WPRINTF(("ioctl viona vq %d intr "
+					    "clear failed %d\n", i, errno));
+				}
 			}
-		}
-
-		if (pollset.revents & POLLOUT) {
-			pci_generate_msix(sc->vsc_pi,
-			    sc->vsc_msix_table_idx[VIONA_TXQ]);
-			error = ioctl(sc->vsc_vnafd, VNA_IOC_TX_INTR_CLR);
-			if (error != 0) {
-				WPRINTF(("ioctl viona tx intr clear failed"
-				    " %d\n", error));
+			if (assert_lintr) {
+				pthread_mutex_lock(&sc->vsc_mtx);
+				sc->vsc_isr |= VTCFG_ISR_QUEUES;
+				pci_lintr_assert(sc->vsc_pi);
+				pthread_mutex_unlock(&sc->vsc_mtx);
 			}
 		}
 	}
@@ -260,57 +245,6 @@ pci_viona_poll_thread(void *param)
 	pthread_exit(NULL);
 }
 
-static void
-pci_viona_ping_rxq(struct pci_viona_softc *sc)
-{
-	int error;
-
-	error = ioctl(sc->vsc_vnafd, VNA_IOC_RX_RING_KICK);
-	if (error != 0) {
-		WPRINTF(("ioctl viona rx ring kick failed %d\n", error));
-	}
-}
-
-static void *
-pci_viona_tx_thread(void *param)
-{
-	struct pci_viona_softc *sc = (struct pci_viona_softc *)param;
-	int error;
-
-	pthread_mutex_lock(&sc->tx_mtx);
-	for (;;) {
-		error = pthread_cond_wait(&sc->tx_cond, &sc->tx_mtx);
-		assert(error == 0);
-		sc->vsc_tx_kick_lock_held = B_TRUE;
-		error = ioctl(sc->vsc_vnafd, VNA_IOC_TX_RING_KICK);
-		if (error != 0) {
-			WPRINTF(("ioctl viona tx ring kick failed %d\n",
-			    error));
-		}
-		sc->vsc_tx_kick_lock_held = B_FALSE;
-	}
-	pthread_mutex_unlock(&sc->tx_mtx);
-
-	return (NULL);
-}
-
-static void
-pci_viona_ping_txq(struct pci_viona_softc *sc)
-{
-	/* Signal the tx thread for processing */
-	if (sc->vsc_tx_kick_lock_held)
-		return;
-	pthread_mutex_lock(&sc->tx_mtx);
-	pthread_cond_signal(&sc->tx_cond);
-	pthread_mutex_unlock(&sc->tx_mtx);
-}
-
-static void
-pci_viona_ping_ctlq(struct pci_viona_softc *sc)
-{
-	DPRINTF(("viona: control qnotify!\n\r"));
-}
-
 static void
 pci_viona_ring_init(struct pci_viona_softc *sc, uint64_t pfn)
 {
@@ -320,29 +254,19 @@ pci_viona_ring_init(struct pci_viona_softc *sc, uint64_t pfn)
 
 	assert(qnum < VIONA_MAXQ);
 
+	if (qnum == VIONA_CTLQ) {
+		return;
+	}
+
 	sc->vsc_pfn[qnum] = (pfn << VRING_PFN);
 
-	vna_ri.ri_qsize = pci_viona_qsize(qnum);
+	vna_ri.ri_index = qnum;
+	vna_ri.ri_qsize = pci_viona_qsize(sc, qnum);
 	vna_ri.ri_qaddr = (pfn << VRING_PFN);
+	error = ioctl(sc->vsc_vnafd, VNA_IOC_RING_INIT, &vna_ri);
 
-	switch (qnum) {
-	case VIONA_RXQ:
-		error = ioctl(sc->vsc_vnafd, VNA_IOC_RX_RING_INIT, &vna_ri);
-		if (error != 0) {
-			WPRINTF(("ioctl viona rx ring init failed %d\n",
-			    error));
-		}
-		break;
-	case VIONA_TXQ:
-		error = ioctl(sc->vsc_vnafd, VNA_IOC_TX_RING_INIT, &vna_ri);
-		if (error != 0) {
-			WPRINTF(("ioctl viona tx ring init failed %d\n",
-			    error));
-		}
-		break;
-	case VIONA_CTLQ:
-	default:
-		break;
+	if (error != 0) {
+		WPRINTF(("ioctl viona ring %u init failed %d\n", qnum, errno));
 	}
 }
 
@@ -350,36 +274,110 @@ static int
 pci_viona_viona_init(struct vmctx *ctx, struct pci_viona_softc *sc)
 {
 	vioc_create_t		vna_create;
-#if notyet
-	char			devname[MAXNAMELEN];
-	int			ctlfd;
-#endif
 	int			error;
 
-	sc->vsc_vnafd = open("/devices/pseudo/viona@0:ctl", O_RDWR | O_EXCL);
+	sc->vsc_vnafd = open("/dev/viona", O_RDWR | O_EXCL);
 	if (sc->vsc_vnafd == -1) {
-		WPRINTF(("open viona ctl failed\n"));
+		WPRINTF(("open viona ctl failed: %d\n", errno));
 		return (-1);
 	}
 
 	vna_create.c_linkid = sc->vsc_linkid;
-	strlcpy(vna_create.c_vmname, vmname,
-	    sizeof (vna_create.c_vmname));
-#if notyet
-	vm_get_memory_seg(ctx, 1 * (1024 * 1024UL), &vna_create.c_lomem_size,
-	    NULL);
-	vm_get_memory_seg(ctx, 4 * (1024 * 1024 * 1024UL),
-	    &vna_create.c_himem_size, NULL);
-#endif
+	vna_create.c_vmfd = vm_get_device_fd(ctx);
 	error = ioctl(sc->vsc_vnafd, VNA_IOC_CREATE, &vna_create);
 	if (error != 0) {
-		WPRINTF(("ioctl viona create failed %d\n", error));
+		(void) close(sc->vsc_vnafd);
+		WPRINTF(("ioctl viona create failed %d\n", errno));
 		return (-1);
 	}
 
 	return (0);
 }
 
+static int
+pci_viona_parse_opts(struct pci_viona_softc *sc, char *opts)
+{
+	char *next, *cp, *vnic = NULL;
+	int err = 0;
+
+	sc->vsc_vq_size = VIONA_RINGSZ;
+	sc->vsc_feature_mask = 0;
+
+	for (; opts != NULL && *opts != '\0'; opts = next) {
+		char *val;
+
+		if ((cp = strchr(opts, ',')) != NULL) {
+			*cp = '\0';
+			next = cp + 1;
+		} else {
+			next = NULL;
+		}
+
+		if ((cp = strchr(opts, '=')) == NULL) {
+			/* vnic chosen with bare name */
+			if (vnic != NULL) {
+				fprintf(stderr,
+				    "viona: unexpected vnic name '%s'", opts);
+				err = -1;
+			} else {
+				vnic = opts;
+			}
+			continue;
+		}
+
+		/* <param>=<value> handling */
+		val = cp + 1;
+		*cp = '\0';
+		if (strcmp(opts, "feature_mask") == 0) {
+			long num;
+
+			errno = 0;
+			num = strtol(val, NULL, 0);
+			if (errno != 0 || num < 0) {
+				fprintf(stderr,
+				    "viona: invalid mask '%s'", val);
+			} else {
+				sc->vsc_feature_mask = num;
+			}
+		} else if (strcmp(opts, "vqsize") == 0) {
+			long num;
+
+			errno = 0;
+			num = strtol(val, NULL, 0);
+			if (errno != 0) {
+				fprintf(stderr,
+				    "viona: invalid vsqize '%s'", val);
+				err = -1;
+			} else if (num <= 2 || num > 32768) {
+				fprintf(stderr,
+				    "viona: vqsize out of range", num);
+				err = -1;
+			} else if ((1 << (ffs(num) - 1)) != num) {
+				fprintf(stderr,
+				    "viona: vqsize must be power of 2", num);
+				err = -1;
+			} else {
+				sc->vsc_vq_size = num;
+			}
+		} else {
+			fprintf(stderr,
+			    "viona: unrecognized option '%s'", opts);
+			err = -1;
+		}
+	}
+	if (vnic == NULL) {
+		fprintf(stderr, "viona: vnic name required");
+		sc->vsc_linkname[0] = '\0';
+		err = -1;
+	} else {
+		(void) strlcpy(sc->vsc_linkname, vnic, MAXLINKNAMELEN);
+	}
+
+	DPRINTF(("viona=%p dev=%s vqsize=%x feature_mask=%x\n", sc,
+	    sc->vsc_linkname, sc->vsc_vq_size, sc->vsc_feature_mask));
+	return (err);
+}
+
 static int
 pci_viona_init(struct vmctx *ctx, struct pci_devinst *pi, char *opts)
 {
@@ -387,9 +385,9 @@ pci_viona_init(struct vmctx *ctx, struct pci_devinst *pi, char *opts)
 	dladm_status_t		status;
 	dladm_vnic_attr_t	attr;
 	char			errmsg[DLADM_STRSIZE];
-	int error;
+	int error, i;
 	struct pci_viona_softc *sc;
-	int i;
+	uint64_t ioport;
 
 	if (opts == NULL) {
 		printf("virtio-viona: vnic required\n");
@@ -404,7 +402,10 @@ pci_viona_init(struct vmctx *ctx, struct pci_devinst *pi, char *opts)
 
 	pthread_mutex_init(&sc->vsc_mtx, NULL);
 
-	strlcpy(sc->vsc_linkname, opts, MAXLINKNAMELEN);
+	if (pci_viona_parse_opts(sc, opts) != 0) {
+		free(sc);
+		return (1);
+	}
 
 	if ((status = dladm_open(&handle)) != DLADM_STATUS_OK) {
 		WPRINTF(("could not open /dev/dld"));
@@ -430,7 +431,6 @@ pci_viona_init(struct vmctx *ctx, struct pci_devinst *pi, char *opts)
 		return (1);
 	}
 
-	sc->vsc_tx_kick_lock_held = B_FALSE;
 	memcpy(sc->vsc_macaddr, attr.va_mac_addr, ETHERADDRL);
 
 	dladm_close(handle);
@@ -449,42 +449,44 @@ pci_viona_init(struct vmctx *ctx, struct pci_devinst *pi, char *opts)
 	pci_set_cfgdata16(pi, PCIR_VENDOR, VIRTIO_VENDOR);
 	pci_set_cfgdata8(pi, PCIR_CLASS, PCIC_NETWORK);
 	pci_set_cfgdata16(pi, PCIR_SUBDEV_0, VIRTIO_TYPE_NET);
+	pci_set_cfgdata16(pi, PCIR_SUBVEND_0, VIRTIO_VENDOR);
 
 	/* MSI-X support */
 	for (i = 0; i < VIONA_MAXQ; i++)
 		sc->vsc_msix_table_idx[i] = VIRTIO_MSI_NO_VECTOR;
 
-	/*
-	 * BAR 1 used to map MSI-X table and PBA
-	 */
+	/* BAR 1 used to map MSI-X table and PBA */
 	if (pci_emul_add_msixcap(pi, VIONA_MAXQ, 1)) {
 		free(sc);
 		return (1);
 	}
 
-	pci_emul_alloc_bar(pi, 0, PCIBAR_IO, VIONA_REGSZ);
+	/* BAR 0 for legacy-style virtio register access. */
+	error = pci_emul_alloc_bar(pi, 0, PCIBAR_IO, VIONA_REGSZ);
+	if (error != NULL) {
+		WPRINTF(("could not allocate virtio BAR\n"));
+		free(sc);
+		return (1);
+	}
+
+	/* Install ioport hook for virtqueue notification */
+	ioport = pi->pi_bar[0].addr + VTCFG_R_QNOTIFY;
+	error = ioctl(sc->vsc_vnafd, VNA_IOC_SET_NOTIFY_IOP, ioport);
+	if (error != 0) {
+		WPRINTF(("could not install ioport hook at %x\n", ioport));
+		free(sc);
+		return (1);
+	}
 
 	/*
-	 * Initialize tx semaphore & spawn TX processing thread
-	 * As of now, only one thread for TX desc processing is
-	 * spawned.
+	 * Need a legacy interrupt for virtio compliance, even though MSI-X
+	 * operation is _strongly_ suggested for adequate performance.
 	 */
-	pthread_mutex_init(&sc->tx_mtx, NULL);
-	pthread_cond_init(&sc->tx_cond, NULL);
-	pthread_create(&sc->tx_tid, NULL, pci_viona_tx_thread, (void *)sc);
+	pci_lintr_request(pi);
 
 	return (0);
 }
 
-/*
- * Function pointer array to handle queue notifications
- */
-static void (*pci_viona_qnotify[VIONA_MAXQ])(struct pci_viona_softc *) = {
-	pci_viona_ping_rxq,
-	pci_viona_ping_txq,
-	pci_viona_ping_ctlq
-};
-
 static uint64_t
 viona_adjust_offset(struct pci_devinst *pi, uint64_t offset)
 {
@@ -500,6 +502,109 @@ viona_adjust_offset(struct pci_devinst *pi, uint64_t offset)
 	return (offset);
 }
 
+static void
+pci_viona_ring_set_msix(struct pci_devinst *pi, uint_t ring)
+{
+	struct pci_viona_softc *sc = pi->pi_arg;
+	struct msix_table_entry mte;
+	uint16_t tab_index;
+	vioc_ring_msi_t vrm;
+	int res;
+
+	assert(ring <= VIONA_VQ_TX);
+
+	vrm.rm_index = ring;
+	vrm.rm_addr = 0;
+	vrm.rm_msg = 0;
+	tab_index = sc->vsc_msix_table_idx[ring];
+
+	if (tab_index != VIRTIO_MSI_NO_VECTOR && sc->vsc_msix_active) {
+		mte = pi->pi_msix.table[tab_index];
+		if ((mte.vector_control & PCIM_MSIX_VCTRL_MASK) == 0) {
+			vrm.rm_addr = mte.addr;
+			vrm.rm_msg = mte.msg_data;
+		}
+	}
+
+	res = ioctl(sc->vsc_vnafd, VNA_IOC_RING_SET_MSI, &vrm);
+	if (res != 0) {
+		WPRINTF(("ioctl viona set_msi %d failed %d\n", ring, errno));
+	}
+}
+
+static void
+pci_viona_lintrupdate(struct pci_devinst *pi)
+{
+	struct pci_viona_softc *sc = pi->pi_arg;
+	boolean_t msix_on = B_FALSE;
+
+	pthread_mutex_lock(&sc->vsc_mtx);
+	msix_on = pci_msix_enabled(pi) && (pi->pi_msix.function_mask == 0);
+	if ((sc->vsc_msix_active && !msix_on) ||
+	    (msix_on && !sc->vsc_msix_active)) {
+		uint_t i;
+
+		sc->vsc_msix_active = msix_on;
+		/* Update in-kernel ring configs */
+		for (i = 0; i <= VIONA_VQ_TX; i++) {
+			pci_viona_ring_set_msix(pi, i);
+		}
+	}
+	pthread_mutex_unlock(&sc->vsc_mtx);
+}
+
+static void
+pci_viona_msix_update(struct pci_devinst *pi, uint64_t offset)
+{
+	struct pci_viona_softc *sc = pi->pi_arg;
+	uint_t tab_index, i;
+
+	pthread_mutex_lock(&sc->vsc_mtx);
+	if (!sc->vsc_msix_active) {
+		pthread_mutex_unlock(&sc->vsc_mtx);
+		return;
+	}
+
+	/*
+	 * Rather than update every possible MSI-X vector, cheat and use the
+	 * offset to calculate the entry within the table.  Since this should
+	 * only be called when a write to the table succeeds, the index should
+	 * be valid.
+	 */
+	tab_index = offset / MSIX_TABLE_ENTRY_SIZE;
+
+	for (i = 0; i <= VIONA_VQ_TX; i++) {
+		if (sc->vsc_msix_table_idx[i] != tab_index) {
+			continue;
+		}
+		pci_viona_ring_set_msix(pi, i);
+	}
+
+	pthread_mutex_unlock(&sc->vsc_mtx);
+}
+
+static void
+pci_viona_qnotify(struct pci_viona_softc *sc, int ring)
+{
+	int error;
+
+	switch (ring) {
+	case VIONA_TXQ:
+	case VIONA_RXQ:
+		error = ioctl(sc->vsc_vnafd, VNA_IOC_RING_KICK, ring);
+		if (error != 0) {
+			WPRINTF(("ioctl viona ring %d kick failed %d\n",
+			    ring, errno));
+		}
+		break;
+	case VIONA_CTLQ:
+		DPRINTF(("viona: control qnotify!\n"));
+		break;
+	default:
+		break;
+	}
+}
+
 static void
 pci_viona_write(struct vmctx *ctx, int vcpu, struct pci_devinst *pi,
     int baridx, uint64_t offset, int size, uint64_t value)
@@ -510,7 +615,9 @@ pci_viona_write(struct vmctx *ctx, int vcpu, struct pci_devinst *pi,
 
 	if (baridx == pci_msix_table_bar(pi) ||
 	    baridx == pci_msix_pba_bar(pi)) {
-		pci_emul_msix_twrite(pi, offset, size, value);
+		if (pci_emul_msix_twrite(pi, offset, size, value) == 0) {
+			pci_viona_msix_update(pi, offset);
+		}
 		return;
 	}
 
@@ -529,10 +636,14 @@ pci_viona_write(struct vmctx *ctx, int vcpu, struct pci_devinst *pi,
 	switch (offset) {
 	case VTCFG_R_GUESTCAP:
 		assert(size == 4);
+		value &= ~(sc->vsc_feature_mask);
 		err = ioctl(sc->vsc_vnafd, VNA_IOC_SET_FEATURES, &value);
-		if (err != 0)
+		if (err != 0) {
 			WPRINTF(("ioctl feature negotiation returned"
-			    " err = %d\n", err));
+			    " err = %d\n", errno));
+		} else {
+			sc->vsc_features = value;
+		}
 		break;
 	case VTCFG_R_PFN:
 		assert(size == 4);
@@ -546,7 +657,7 @@ pci_viona_write(struct vmctx *ctx, int vcpu, struct pci_devinst *pi,
 	case VTCFG_R_QNOTIFY:
 		assert(size == 2);
 		assert(value < VIONA_MAXQ);
-		(*pci_viona_qnotify[value])(sc);
+		pci_viona_qnotify(sc, value);
 		break;
 	case VTCFG_R_STATUS:
 		assert(size == 1);
@@ -597,7 +708,7 @@ pci_viona_write(struct vmctx *ctx, int vcpu, struct pci_devinst *pi,
 	pthread_mutex_unlock(&sc->vsc_mtx);
 }
 
-uint64_t
+static uint64_t
 pci_viona_read(struct vmctx *ctx, int vcpu, struct pci_devinst *pi,
     int baridx, uint64_t offset, int size)
 {
@@ -627,9 +738,11 @@ pci_viona_read(struct vmctx *ctx, int vcpu, struct pci_devinst *pi,
 	case VTCFG_R_HOSTCAP:
 		assert(size == 4);
 		err = ioctl(sc->vsc_vnafd, VNA_IOC_GET_FEATURES, &value);
-		if (err != 0)
+		if (err != 0) {
 			WPRINTF(("ioctl get host features returned"
-			    " err = %d\n", err));
+			    " err = %d\n", errno));
+		}
+		value &= ~sc->vsc_feature_mask;
 		break;
 	case VTCFG_R_GUESTCAP:
 		assert(size == 4);
@@ -641,7 +754,7 @@ pci_viona_read(struct vmctx *ctx, int vcpu, struct pci_devinst *pi,
 		break;
 	case VTCFG_R_QNUM:
 		assert(size == 2);
-		value = pci_viona_qsize(sc->vsc_curq);
+		value = pci_viona_qsize(sc, sc->vsc_curq);
 		break;
 	case VTCFG_R_QSEL:
 		assert(size == 2);
@@ -705,9 +818,10 @@ pci_viona_read(struct vmctx *ctx, int vcpu, struct pci_devinst *pi,
 }
 
 struct pci_devemu pci_de_viona = {
-	.pe_emu = 	"virtio-net-viona",
+	.pe_emu =	"virtio-net-viona",
 	.pe_init =	pci_viona_init,
 	.pe_barwrite =	pci_viona_write,
-	.pe_barread =	pci_viona_read
+	.pe_barread =	pci_viona_read,
+	.pe_lintrupdate = pci_viona_lintrupdate
 };
 PCI_EMUL_SET(pci_de_viona);
diff --git a/usr/src/cmd/devfsadm/i386/misc_link_i386.c b/usr/src/cmd/devfsadm/i386/misc_link_i386.c
index f1a6ef1172..99085a3d4f 100644
--- a/usr/src/cmd/devfsadm/i386/misc_link_i386.c
+++ b/usr/src/cmd/devfsadm/i386/misc_link_i386.c
@@ -84,6 +84,9 @@ static devfsadm_create_t misc_cbt[] = {
 	{ "pseudo", "ddi_pseudo", "ucode",
 	    TYPE_EXACT | DRV_EXACT, ILEVEL_0, ln_minor_name,
 	},
+	{ "pseudo", "ddi_pseudo", "viona",
+	    TYPE_EXACT | DRV_EXACT, ILEVEL_0, ln_minor_name,
+	}
 };
 
 DEVFSADM_CREATE_INIT_V0(misc_cbt);
@@ -109,6 +112,9 @@ static devfsadm_remove_t misc_remove_cbt[] = {
 	},
 	{ "serial", "^tty[a-z]$", RM_ALWAYS | RM_PRE,
 		ILEVEL_1, devfsadm_rm_all
+	},
+	{ "pseudo", "^viona$", RM_ALWAYS | RM_PRE | RM_HOT,
+		ILEVEL_0, devfsadm_rm_all
 	}
 };
 
diff --git a/usr/src/uts/i86pc/Makefile.i86pc b/usr/src/uts/i86pc/Makefile.i86pc
index 36d5208530..038aa679c7 100644
--- a/usr/src/uts/i86pc/Makefile.i86pc
+++ b/usr/src/uts/i86pc/Makefile.i86pc
@@ -266,6 +266,7 @@ DRV_KMODS	+= dr
 DRV_KMODS	+= ioat
 DRV_KMODS	+= fipe
 DRV_KMODS	+= vmm
+DRV_KMODS	+= viona
 
 DRV_KMODS	+= cpudrv
 
diff --git a/usr/src/uts/i86pc/io/viona/viona.c b/usr/src/uts/i86pc/io/viona/viona.c
index 2371a2f3ae..ee1380ae7d 100644
--- a/usr/src/uts/i86pc/io/viona/viona.c
+++ b/usr/src/uts/i86pc/io/viona/viona.c
@@ -34,13 +34,14 @@
  * http://www.illumos.org/license/CDDL.
  *
  * Copyright 2015 Pluribus Networks Inc.
- * Copyright 2017 Joyent, Inc.
+ * Copyright 2018 Joyent, Inc.
  */
 
 #include <sys/conf.h>
 #include <sys/file.h>
 #include <sys/stat.h>
 #include <sys/ddi.h>
+#include <sys/disp.h>
 #include <sys/sunddi.h>
 #include <sys/sunndi.h>
 #include <sys/sysmacros.h>
@@ -48,34 +49,38 @@
 #include <sys/strsun.h>
 #include <vm/seg_kmem.h>
 
+#include <sys/pattr.h>
 #include <sys/dls.h>
+#include <sys/dlpi.h>
 #include <sys/mac_client.h>
+#include <sys/mac_provider.h>
+#include <sys/mac_client_priv.h>
+#include <sys/vlan.h>
 
+#include <sys/vmm_drv.h>
 #include <sys/viona_io.h>
 
-#define	MB	(1024UL * 1024)
-#define	GB	(1024UL * MB)
-
-/*
- * Min. octets in an ethernet frame minus FCS
- */
-#define	MIN_BUF_SIZE	60
+/* Min. octets in an ethernet frame minus FCS */
+#define	MIN_BUF_SIZE		60
+#define	NEED_VLAN_PAD_SIZE	(MIN_BUF_SIZE - VLAN_TAGSZ)
 
 #define	VIONA_NAME		"Virtio Network Accelerator"
-
 #define	VIONA_CTL_MINOR		0
-#define	VIONA_CTL_NODE_NAME	"ctl"
-
-#define	VIONA_CLI_NAME		"viona"
+#define	VIONA_CLI_NAME		"viona"		/* MAC client name */
 
 #define	VTNET_MAXSEGS		32
 
 #define	VRING_ALIGN		4096
+#define	VRING_MAX_LEN		32768
 
 #define	VRING_DESC_F_NEXT	(1 << 0)
 #define	VRING_DESC_F_WRITE	(1 << 1)
 #define	VRING_DESC_F_INDIRECT	(1 << 2)
 
+#define	VIRTIO_NET_HDR_F_NEEDS_CSUM	(1 << 0)
+#define	VIRTIO_NET_HDR_F_DATA_VALID	(1 << 1)
+
+
 #define	VRING_AVAIL_F_NO_INTERRUPT	1
 
 #define	VRING_USED_F_NO_NOTIFY		1
@@ -84,13 +89,43 @@
 /*
  * Host capabilities
  */
+#define	VIRTIO_NET_F_CSUM	(1 <<  0)
+#define	VIRTIO_NET_F_GUEST_CSUM	(1 <<  1)
 #define	VIRTIO_NET_F_MAC	(1 <<  5) /* host supplies MAC */
 #define	VIRTIO_NET_F_MRG_RXBUF	(1 << 15) /* host can merge RX buffers */
 #define	VIRTIO_NET_F_STATUS	(1 << 16) /* config status field available */
-
-#define	VIONA_S_HOSTCAPS		\
-	(VIRTIO_NET_F_MAC | VIRTIO_NET_F_MRG_RXBUF | \
-	VIRTIO_NET_F_STATUS)
+#define	VIRTIO_F_RING_NOTIFY_ON_EMPTY	(1 << 24)
+#define	VIRTIO_F_RING_INDIRECT_DESC	(1 << 28)
+#define	VIRTIO_F_RING_EVENT_IDX		(1 << 29)
+
+#define	VIONA_S_HOSTCAPS	(	\
+	VIRTIO_NET_F_GUEST_CSUM |	\
+	VIRTIO_NET_F_MAC |		\
+	VIRTIO_NET_F_MRG_RXBUF |	\
+	VIRTIO_NET_F_STATUS |		\
+	VIRTIO_F_RING_NOTIFY_ON_EMPTY |	\
+	VIRTIO_F_RING_INDIRECT_DESC)
+
+/* MAC_CAPAB_HCKSUM specifics of interest */
+#define	VIONA_CAP_HCKSUM_INTEREST	\
+	(HCKSUM_INET_PARTIAL |		\
+	HCKSUM_INET_FULL_V4 |		\
+	HCKSUM_INET_FULL_V6)
+
+
+#define	VIONA_PROBE(name)	DTRACE_PROBE(viona__##name)
+#define	VIONA_PROBE1(name, arg1, arg2)	\
+	DTRACE_PROBE1(viona__##name, arg1, arg2)
+#define	VIONA_PROBE2(name, arg1, arg2, arg3, arg4)	\
+	DTRACE_PROBE2(viona__##name, arg1, arg2, arg3, arg4)
+#define	VIONA_PROBE3(name, arg1, arg2, arg3, arg4, arg5, arg6)	\
+	DTRACE_PROBE3(viona__##name, arg1, arg2, arg3, arg4, arg5, arg6)
+#define	VIONA_PROBE5(name, arg1, arg2, arg3, arg4, arg5, arg6, arg7, arg8, \
+	arg9, arg10) \
+	DTRACE_PROBE5(viona__##name, arg1, arg2, arg3, arg4, arg5, arg6, arg7, \
+	arg8, arg9, arg10)
+#define	VIONA_PROBE_BAD_RING_ADDR(r, a)		\
+	VIONA_PROBE2(bad_ring_addr, viona_vring_t *, r, void *, (void *)(a))
 
 #pragma pack(1)
 struct virtio_desc {
@@ -128,61 +163,93 @@ struct virtio_net_hdr {
 };
 #pragma pack()
 
-typedef struct viona_vring_hqueue {
-	/* Internal state */
-	uint16_t		hq_size;
-	kmutex_t		hq_a_mutex;
-	kmutex_t		hq_u_mutex;
-	uint16_t		hq_cur_aidx;	/* trails behind 'avail_idx' */
+struct viona_link;
+typedef struct viona_link viona_link_t;
+struct viona_desb;
+typedef struct viona_desb viona_desb_t;
 
-	/* Host-context pointers to the queue */
-	caddr_t			hq_baseaddr;
-	uint16_t		*hq_avail_flags;
-	uint16_t		*hq_avail_idx;	/* monotonically increasing */
-	uint16_t		*hq_avail_ring;
+enum viona_ring_state {
+	VRS_RESET	= 0x00,	/* just allocated or reset */
+	VRS_INIT	= 0x01,	/* init-ed with addrs and worker thread */
+	VRS_RUN		= 0x02,	/* running work routine */
 
-	uint16_t		*hq_used_flags;
-	uint16_t		*hq_used_idx;	/* monotonically increasing */
-	struct virtio_used	*hq_used_ring;
-} viona_vring_hqueue_t;
+	/* Additional flag(s): */
+	VRS_SETUP	= 0x04,
+	VRS_REQ_START	= 0x08,
+	VRS_REQ_STOP	= 0x10,
+};
 
+#define	VRS_STATE_MASK (VRS_RESET|VRS_INIT|VRS_RUN)
 
-typedef struct viona_link {
-	datalink_id_t		l_linkid;
+#define	VRING_NEED_BAIL(ring, proc)				\
+		(((ring)->vr_state & VRS_REQ_STOP) != 0 ||	\
+		((proc)->p_flag & SEXITING) != 0)
 
-	struct vm		*l_vm;
-	size_t			l_vm_lomemsize;
-	caddr_t			l_vm_lomemaddr;
-	size_t			l_vm_himemsize;
-	caddr_t			l_vm_himemaddr;
+typedef struct viona_vring {
+	viona_link_t	*vr_link;
 
-	mac_handle_t		l_mh;
-	mac_client_handle_t	l_mch;
+	kmutex_t	vr_lock;
+	kcondvar_t	vr_cv;
+	uint_t		vr_state;
+	uint_t		vr_xfer_outstanding;
+	kthread_t	*vr_worker_thread;
+	viona_desb_t	*vr_desb;
 
-	kmem_cache_t		*l_desb_kmc;
+	uint_t		vr_intr_enabled;
+	uint64_t	vr_msi_addr;
+	uint64_t	vr_msi_msg;
 
-	pollhead_t		l_pollhead;
+	/* Internal ring-related state */
+	kmutex_t	vr_a_mutex;	/* sync consumers of 'avail' */
+	kmutex_t	vr_u_mutex;	/* sync consumers of 'used' */
+	uint16_t	vr_size;
+	uint16_t	vr_mask;	/* cached from vr_size */
+	uint16_t	vr_cur_aidx;	/* trails behind 'avail_idx' */
+
+	/* Host-context pointers to the queue */
+	volatile struct virtio_desc	*vr_descr;
 
-	viona_vring_hqueue_t	l_rx_vring;
-	uint_t			l_rx_intr;
+	volatile uint16_t		*vr_avail_flags;
+	volatile uint16_t		*vr_avail_idx;
+	volatile uint16_t		*vr_avail_ring;
+	volatile uint16_t		*vr_avail_used_event;
+
+	volatile uint16_t		*vr_used_flags;
+	volatile uint16_t		*vr_used_idx;
+	volatile struct virtio_used	*vr_used_ring;
+	volatile uint16_t		*vr_used_avail_event;
+} viona_vring_t;
+
+struct viona_link {
+	vmm_hold_t		*l_vm_hold;
+	boolean_t		l_destroyed;
+
+	viona_vring_t		l_vrings[VIONA_VQ_MAX];
 
-	viona_vring_hqueue_t	l_tx_vring;
-	kcondvar_t		l_tx_cv;
-	uint_t			l_tx_intr;
-	kmutex_t		l_tx_mutex;
-	int			l_tx_outstanding;
 	uint32_t		l_features;
-} viona_link_t;
+	uint32_t		l_features_hw;
+	uint32_t		l_cap_csum;
+
+	uintptr_t		l_notify_ioport;
+	void			*l_notify_cookie;
+
+	datalink_id_t		l_linkid;
+	mac_handle_t		l_mh;
+	mac_client_handle_t	l_mch;
 
-typedef struct {
+	pollhead_t		l_pollhead;
+};
+
+struct viona_desb {
 	frtn_t			d_frtn;
-	viona_link_t		*d_link;
+	viona_vring_t		*d_ring;
 	uint_t			d_ref;
+	uint32_t		d_len;
 	uint16_t		d_cookie;
-	int			d_len;
-} viona_desb_t;
+};
 
 typedef struct viona_soft_state {
+	kmutex_t		ss_lock;
 	viona_link_t		*ss_link;
 } viona_soft_state_t;
 
@@ -193,16 +260,17 @@ typedef struct used_elem {
 
 static void			*viona_state;
 static dev_info_t		*viona_dip;
-static id_space_t		*viona_minor_ids;
+static id_space_t		*viona_minors;
+static mblk_t			*viona_vlan_pad_mp;
+
 /*
  * copy tx mbufs from virtio ring to avoid necessitating a wait for packet
  * transmission to free resources.
  */
-static boolean_t		copy_tx_mblks = B_TRUE;
-
-extern struct vm *vm_lookup_by_name(char *name);
-extern uint64_t vm_gpa2hpa(struct vm *vm, uint64_t gpa, size_t len);
+static boolean_t		viona_force_copy_tx_mblks = B_FALSE;
 
+static int viona_info(dev_info_t *dip, ddi_info_cmd_t cmd, void *arg,
+    void **result);
 static int viona_attach(dev_info_t *dip, ddi_attach_cmd_t cmd);
 static int viona_detach(dev_info_t *dip, ddi_detach_cmd_t cmd);
 static int viona_open(dev_t *devp, int flag, int otype, cred_t *credp);
@@ -212,27 +280,28 @@ static int viona_ioctl(dev_t dev, int cmd, intptr_t data, int mode,
 static int viona_chpoll(dev_t dev, short events, int anyyet, short *reventsp,
     struct pollhead **phpp);
 
-static int viona_ioc_create(viona_soft_state_t *ss, vioc_create_t *u_create);
+static int viona_ioc_create(viona_soft_state_t *, void *, int, cred_t *);
 static int viona_ioc_delete(viona_soft_state_t *ss);
 
-static int viona_vm_map(viona_link_t *link);
-static caddr_t viona_gpa2kva(viona_link_t *link, uint64_t gpa);
-static void viona_vm_unmap(viona_link_t *link);
-
-static int viona_ioc_rx_ring_init(viona_link_t *link,
-    vioc_ring_init_t *u_ri);
-static int viona_ioc_tx_ring_init(viona_link_t *link,
-    vioc_ring_init_t *u_ri);
-static int viona_ioc_rx_ring_reset(viona_link_t *link);
-static int viona_ioc_tx_ring_reset(viona_link_t *link);
-static void viona_ioc_rx_ring_kick(viona_link_t *link);
-static void viona_ioc_tx_ring_kick(viona_link_t *link);
-static int viona_ioc_rx_intr_clear(viona_link_t *link);
-static int viona_ioc_tx_intr_clear(viona_link_t *link);
-
-static void viona_rx(void *arg, mac_resource_handle_t mrh, mblk_t *mp,
-    boolean_t loopback);
-static void viona_tx(viona_link_t *link, viona_vring_hqueue_t *hq);
+static void *viona_gpa2kva(viona_link_t *link, uint64_t gpa, size_t len);
+
+static void viona_ring_alloc(viona_link_t *, viona_vring_t *);
+static void viona_ring_free(viona_vring_t *);
+static kthread_t *viona_create_worker(viona_vring_t *);
+
+static int viona_ioc_set_notify_ioport(viona_link_t *, uint_t);
+static int viona_ioc_ring_init(viona_link_t *, void *, int);
+static int viona_ioc_ring_reset(viona_link_t *, uint_t);
+static int viona_ioc_ring_kick(viona_link_t *, uint_t);
+static int viona_ioc_ring_set_msi(viona_link_t *, void *, int);
+static int viona_ioc_ring_intr_clear(viona_link_t *, uint_t);
+static int viona_ioc_intr_poll(viona_link_t *, void *, int, int *);
+
+static void viona_intr_ring(viona_vring_t *);
+
+static void viona_desb_release(viona_desb_t *);
+static void viona_rx(void *, mac_resource_handle_t, mblk_t *, boolean_t);
+static void viona_tx(viona_link_t *, viona_vring_t *);
 
 static struct cb_ops viona_cb_ops = {
 	viona_open,
@@ -258,7 +327,7 @@ static struct cb_ops viona_cb_ops = {
 static struct dev_ops viona_ops = {
 	DEVO_REV,
 	0,
-	nodev,
+	viona_info,
 	nulldev,
 	nulldev,
 	viona_attach,
@@ -285,14 +354,14 @@ _init(void)
 {
 	int	ret;
 
-	ret = ddi_soft_state_init(&viona_state,
-	    sizeof (viona_soft_state_t), 0);
-	if (ret == 0) {
-		ret = mod_install(&modlinkage);
-		if (ret != 0) {
-			ddi_soft_state_fini(&viona_state);
-			return (ret);
-		}
+	ret = ddi_soft_state_init(&viona_state, sizeof (viona_soft_state_t), 0);
+	if (ret != 0)
+		return (ret);
+
+	ret = mod_install(&modlinkage);
+	if (ret != 0) {
+		ddi_soft_state_fini(&viona_state);
+		return (ret);
 	}
 
 	return (ret);
@@ -321,33 +390,65 @@ static void
 set_viona_tx_mode()
 {
 	major_t bcm_nic_major;
+
 	if ((bcm_nic_major = ddi_name_to_major(BCM_NIC_DRIVER))
 	    != DDI_MAJOR_T_NONE) {
 		if (ddi_hold_installed_driver(bcm_nic_major) != NULL) {
-			copy_tx_mblks = B_FALSE;
+			viona_force_copy_tx_mblks = B_TRUE;
 			ddi_rele_driver(bcm_nic_major);
+			return;
 		}
 	}
+	viona_force_copy_tx_mblks = B_FALSE;
+}
+
+/* ARGSUSED */
+static int
+viona_info(dev_info_t *dip, ddi_info_cmd_t cmd, void *arg, void **result)
+{
+	int error;
+
+	switch (cmd) {
+	case DDI_INFO_DEVT2DEVINFO:
+		*result = (void *)viona_dip;
+		error = DDI_SUCCESS;
+		break;
+	case DDI_INFO_DEVT2INSTANCE:
+		*result = (void *)0;
+		error = DDI_SUCCESS;
+		break;
+	default:
+		error = DDI_FAILURE;
+		break;
+	}
+	return (error);
 }
 
 static int
 viona_attach(dev_info_t *dip, ddi_attach_cmd_t cmd)
 {
+	mblk_t *mp;
+
 	if (cmd != DDI_ATTACH) {
 		return (DDI_FAILURE);
 	}
 
-	viona_minor_ids = id_space_create("viona_minor_id",
-	    VIONA_CTL_MINOR + 1, UINT16_MAX);
-
-	if (ddi_create_minor_node(dip, VIONA_CTL_NODE_NAME,
-	    S_IFCHR, VIONA_CTL_MINOR, DDI_PSEUDO, 0) != DDI_SUCCESS) {
+	if (ddi_create_minor_node(dip, "viona", S_IFCHR, VIONA_CTL_MINOR,
+	    DDI_PSEUDO, 0) != DDI_SUCCESS) {
 		return (DDI_FAILURE);
 	}
 
-	viona_dip = dip;
+	viona_minors = id_space_create("viona_minors",
+	    VIONA_CTL_MINOR + 1, UINT16_MAX);
+
+	/* Create mblk for padding when VLAN tags are stripped */
+	mp = allocb_wait(VLAN_TAGSZ, BPRI_HI, STR_NOSIG, NULL);
+	bzero(mp->b_rptr, VLAN_TAGSZ);
+	mp->b_wptr += VLAN_TAGSZ;
+	viona_vlan_pad_mp = mp;
 
 	set_viona_tx_mode();
+	viona_dip = dip;
 	ddi_report_dev(viona_dip);
 
 	return (DDI_SUCCESS);
@@ -356,14 +457,20 @@ viona_attach(dev_info_t *dip, ddi_attach_cmd_t cmd)
 static int
 viona_detach(dev_info_t *dip, ddi_detach_cmd_t cmd)
 {
+	mblk_t *mp;
+
 	if (cmd != DDI_DETACH) {
 		return (DDI_FAILURE);
 	}
 
-	id_space_destroy(viona_minor_ids);
+	/* Clean up the VLAN padding mblk */
+	mp = viona_vlan_pad_mp;
+	viona_vlan_pad_mp = NULL;
+	VERIFY(mp != NULL && mp->b_cont == NULL);
+	freemsg(mp);
 
+	id_space_destroy(viona_minors);
 	ddi_remove_minor_node(viona_dip, NULL);
-
 	viona_dip = NULL;
 
 	return (DDI_SUCCESS);
@@ -373,29 +480,36 @@ static int
 viona_open(dev_t *devp, int flag, int otype, cred_t *credp)
 {
 	int	minor;
+	viona_soft_state_t *ss;
 
 	if (otype != OTYP_CHR) {
 		return (EINVAL);
 	}
-
+#if 0
+	/*
+	 * XXX-mg: drv_priv() is wrong, but I'm not sure what is right.
+	 * Should the check be at open() or ioctl()?
+	 */
 	if (drv_priv(credp) != 0) {
 		return (EPERM);
 	}
-
+#endif
 	if (getminor(*devp) != VIONA_CTL_MINOR) {
 		return (ENXIO);
 	}
 
-	minor = id_alloc(viona_minor_ids);
+	minor = id_alloc_nosleep(viona_minors);
 	if (minor == 0) {
 		/* All minors are busy */
 		return (EBUSY);
 	}
-
 	if (ddi_soft_state_zalloc(viona_state, minor) != DDI_SUCCESS) {
-		id_free(viona_minor_ids, minor);
+		id_free(viona_minors, minor);
+		return (ENOMEM);
 	}
 
+	ss = ddi_get_soft_state(viona_state, minor);
+	mutex_init(&ss->ss_lock, NULL, MUTEX_DEFAULT, NULL);
 	*devp = makedevice(getmajor(*devp), minor);
 
 	return (0);
@@ -411,10 +525,6 @@ viona_close(dev_t dev, int flag, int otype, cred_t *credp)
 		return (EINVAL);
 	}
 
-	if (drv_priv(credp) != 0) {
-		return (EPERM);
-	}
-
 	minor = getminor(dev);
 
 	ss = ddi_get_soft_state(viona_state, minor);
@@ -423,20 +533,19 @@ viona_close(dev_t dev, int flag, int otype, cred_t *credp)
 	}
 
 	viona_ioc_delete(ss);
-
 	ddi_soft_state_free(viona_state, minor);
-
-	id_free(viona_minor_ids, minor);
+	id_free(viona_minors, minor);
 
 	return (0);
 }
 
 static int
-viona_ioctl(dev_t dev, int cmd, intptr_t data, int mode,
-    cred_t *credp, int *rval)
+viona_ioctl(dev_t dev, int cmd, intptr_t data, int md, cred_t *cr, int *rv)
 {
-	viona_soft_state_t	*ss;
-	int			err = 0;
+	viona_soft_state_t *ss;
+	void *dptr = (void *)data;
+	int err = 0, val;
+	viona_link_t *link;
 
 	ss = ddi_get_soft_state(viona_state, getminor(dev));
 	if (ss == NULL) {
@@ -445,80 +554,62 @@ viona_ioctl(dev_t dev, int cmd, intptr_t data, int mode,
 
 	switch (cmd) {
 	case VNA_IOC_CREATE:
-		err = viona_ioc_create(ss, (vioc_create_t *)data);
-		break;
+		return (viona_ioc_create(ss, dptr, md, cr));
 	case VNA_IOC_DELETE:
-		err = viona_ioc_delete(ss);
-		break;
-	case VNA_IOC_SET_FEATURES:
-		if (ss->ss_link == NULL) {
-			return (ENOSYS);
-		}
-		ss->ss_link->l_features = *(int *)data & VIONA_S_HOSTCAPS;
+		return (viona_ioc_delete(ss));
+	default:
 		break;
+	}
+
+	mutex_enter(&ss->ss_lock);
+	if ((link = ss->ss_link) == NULL || link->l_destroyed ||
+	    vmm_drv_expired(link->l_vm_hold)) {
+		mutex_exit(&ss->ss_lock);
+		return (ENXIO);
+	}
+
+	switch (cmd) {
 	case VNA_IOC_GET_FEATURES:
-		if (ss->ss_link == NULL) {
-			return (ENOSYS);
+		val = VIONA_S_HOSTCAPS | link->l_features_hw;
+		if (ddi_copyout(&val, dptr, sizeof (val), md) != 0) {
+			err = EFAULT;
 		}
-		*(int *)data = VIONA_S_HOSTCAPS;
 		break;
-	case VNA_IOC_RX_RING_INIT:
-		if (ss->ss_link == NULL) {
-			return (ENOSYS);
+	case VNA_IOC_SET_FEATURES:
+		if (ddi_copyin(dptr, &val, sizeof (val), md) != 0) {
+			err = EFAULT;
+			break;
 		}
-		err = viona_ioc_rx_ring_init(ss->ss_link,
-		    (vioc_ring_init_t *)data);
+		val &= (VIONA_S_HOSTCAPS | link->l_features_hw);
+		link->l_features = val;
 		break;
-	case VNA_IOC_RX_RING_RESET:
-		if (ss->ss_link == NULL) {
-			return (ENOSYS);
-		}
-		err = viona_ioc_rx_ring_reset(ss->ss_link);
+	case VNA_IOC_RING_INIT:
+		err = viona_ioc_ring_init(link, dptr, md);
 		break;
-	case VNA_IOC_RX_RING_KICK:
-		if (ss->ss_link == NULL) {
-			return (ENOSYS);
-		}
-		viona_ioc_rx_ring_kick(ss->ss_link);
-		err = 0;
+	case VNA_IOC_RING_RESET:
+		err = viona_ioc_ring_reset(link, (uint_t)data);
 		break;
-	case VNA_IOC_TX_RING_INIT:
-		if (ss->ss_link == NULL) {
-			return (ENOSYS);
-		}
-		err = viona_ioc_tx_ring_init(ss->ss_link,
-		    (vioc_ring_init_t *)data);
+	case VNA_IOC_RING_KICK:
+		err = viona_ioc_ring_kick(link, (uint_t)data);
 		break;
-	case VNA_IOC_TX_RING_RESET:
-		if (ss->ss_link == NULL) {
-			return (ENOSYS);
-		}
-		err = viona_ioc_tx_ring_reset(ss->ss_link);
+	case VNA_IOC_RING_SET_MSI:
+		err = viona_ioc_ring_set_msi(link, dptr, md);
 		break;
-	case VNA_IOC_TX_RING_KICK:
-		if (ss->ss_link == NULL) {
-			return (ENOSYS);
-		}
-		viona_ioc_tx_ring_kick(ss->ss_link);
-		err = 0;
+	case VNA_IOC_RING_INTR_CLR:
+		err = viona_ioc_ring_intr_clear(link, (uint_t)data);
 		break;
-	case VNA_IOC_RX_INTR_CLR:
-		if (ss->ss_link == NULL) {
-			return (ENOSYS);
-		}
-		err = viona_ioc_rx_intr_clear(ss->ss_link);
+	case VNA_IOC_INTR_POLL:
+		err = viona_ioc_intr_poll(link, dptr, md, rv);
 		break;
-	case VNA_IOC_TX_INTR_CLR:
-		if (ss->ss_link == NULL) {
-			return (ENOSYS);
-		}
-		err = viona_ioc_tx_intr_clear(ss->ss_link);
+	case VNA_IOC_SET_NOTIFY_IOP:
+		err = viona_ioc_set_notify_ioport(link, (uint_t)data);
 		break;
 	default:
 		err = ENOTTY;
 		break;
 	}
 
+	mutex_exit(&ss->ss_lock);
 	return (err);
 }
 
@@ -526,867 +617,1385 @@ static int
 viona_chpoll(dev_t dev, short events, int anyyet, short *reventsp,
     struct pollhead **phpp)
 {
-	viona_soft_state_t	*ss;
+	viona_soft_state_t *ss;
+	viona_link_t *link;
 
 	ss = ddi_get_soft_state(viona_state, getminor(dev));
-	if (ss == NULL || ss->ss_link == NULL) {
+	if (ss == NULL) {
 		return (ENXIO);
 	}
 
-	*reventsp = 0;
-
-	if (ss->ss_link->l_rx_intr && (events & POLLIN)) {
-		*reventsp |= POLLIN;
+	mutex_enter(&ss->ss_lock);
+	if ((link = ss->ss_link) == NULL || link->l_destroyed) {
+		mutex_exit(&ss->ss_lock);
+		return (ENXIO);
 	}
 
-	if (ss->ss_link->l_tx_intr && (events & POLLOUT)) {
-		*reventsp |= POLLOUT;
+	*reventsp = 0;
+	if ((events & POLLRDBAND) != 0) {
+		for (uint_t i = 0; i < VIONA_VQ_MAX; i++) {
+			if (link->l_vrings[i].vr_intr_enabled != 0) {
+				*reventsp |= POLLRDBAND;
+				break;
+			}
+		}
 	}
-
-	if (*reventsp == 0 && !anyyet) {
-		*phpp = &ss->ss_link->l_pollhead;
+	if ((*reventsp == 0 && !anyyet) || (events & POLLET)) {
+		*phpp = &link->l_pollhead;
 	}
+	mutex_exit(&ss->ss_lock);
 
 	return (0);
 }
 
-static int
-viona_ioc_create(viona_soft_state_t *ss, vioc_create_t *u_create)
+static void
+viona_get_mac_capab(viona_link_t *link)
 {
-	vioc_create_t		k_create;
-	viona_link_t		*link;
-	char			cli_name[MAXNAMELEN];
-	int			err;
+	mac_handle_t mh = link->l_mh;
+	uint32_t cap = 0;
 
-	if (ss->ss_link != NULL) {
-		return (ENOSYS);
+	link->l_features_hw = 0;
+	if (mac_capab_get(mh, MAC_CAPAB_HCKSUM, &cap)) {
+		/*
+		 * Only report HW checksum ability if the underlying MAC
+		 * resource is capable of populating the L4 header.
+		 */
+		if ((cap & VIONA_CAP_HCKSUM_INTEREST) != 0) {
+			link->l_features_hw |= VIRTIO_NET_F_CSUM;
+		}
+		link->l_cap_csum = cap;
 	}
-	if (copyin(u_create, &k_create, sizeof (k_create)) != 0) {
+}
+
+static int
+viona_ioc_create(viona_soft_state_t *ss, void *dptr, int md, cred_t *cr)
+{
+	vioc_create_t	kvc;
+	viona_link_t	*link = NULL;
+	char		cli_name[MAXNAMELEN];
+	int		err = 0;
+	file_t		*fp;
+	vmm_hold_t	*hold = NULL;
+
+	ASSERT(MUTEX_NOT_HELD(&ss->ss_lock));
+
+	if (ddi_copyin(dptr, &kvc, sizeof (kvc), md) != 0) {
 		return (EFAULT);
 	}
 
-	link = kmem_zalloc(sizeof (viona_link_t), KM_SLEEP);
+	mutex_enter(&ss->ss_lock);
+	if (ss->ss_link != NULL) {
+		mutex_exit(&ss->ss_lock);
+		return (EEXIST);
+	}
 
-	link->l_linkid = k_create.c_linkid;
-	link->l_vm = vm_lookup_by_name(k_create.c_vmname);
-	if (link->l_vm == NULL) {
-		err = ENXIO;
+	if ((fp = getf(kvc.c_vmfd)) == NULL) {
+		err = EBADF;
 		goto bail;
 	}
-
-	link->l_vm_lomemsize = k_create.c_lomem_size;
-	link->l_vm_himemsize = k_create.c_himem_size;
-	err = viona_vm_map(link);
+	err = vmm_drv_hold(fp, cr, &hold);
+	releasef(kvc.c_vmfd);
 	if (err != 0) {
 		goto bail;
 	}
 
+	link = kmem_zalloc(sizeof (viona_link_t), KM_SLEEP);
+	link->l_linkid = kvc.c_linkid;
+	link->l_vm_hold = hold;
+
 	err = mac_open_by_linkid(link->l_linkid, &link->l_mh);
 	if (err != 0) {
-		cmn_err(CE_WARN, "viona create mac_open_by_linkid"
-		    " returned %d\n", err);
 		goto bail;
 	}
 
-	snprintf(cli_name, sizeof (cli_name), "%s-%d",
-	    VIONA_CLI_NAME, link->l_linkid);
+	viona_get_mac_capab(link);
+
+	(void) snprintf(cli_name, sizeof (cli_name), "%s-%d", VIONA_CLI_NAME,
+	    link->l_linkid);
 	err = mac_client_open(link->l_mh, &link->l_mch, cli_name, 0);
 	if (err != 0) {
-		cmn_err(CE_WARN, "viona create mac_client_open"
-		    " returned %d\n", err);
 		goto bail;
 	}
 
-	link->l_features = VIONA_S_HOSTCAPS;
-	link->l_desb_kmc = kmem_cache_create(cli_name,
-	    sizeof (viona_desb_t), 0, NULL, NULL, NULL, NULL, NULL, 0);
-
-	mutex_init(&link->l_rx_vring.hq_a_mutex, NULL, MUTEX_DRIVER, NULL);
-	mutex_init(&link->l_rx_vring.hq_u_mutex, NULL, MUTEX_DRIVER, NULL);
-	mutex_init(&link->l_rx_vring.hq_a_mutex, NULL, MUTEX_DRIVER, NULL);
-	mutex_init(&link->l_tx_vring.hq_u_mutex, NULL, MUTEX_DRIVER, NULL);
-	if (copy_tx_mblks) {
-		mutex_init(&link->l_tx_mutex, NULL, MUTEX_DRIVER, NULL);
-		cv_init(&link->l_tx_cv, NULL, CV_DRIVER, NULL);
-	}
+	viona_ring_alloc(link, &link->l_vrings[VIONA_VQ_RX]);
+	viona_ring_alloc(link, &link->l_vrings[VIONA_VQ_TX]);
 	ss->ss_link = link;
 
+	mutex_exit(&ss->ss_lock);
 	return (0);
 
 bail:
-	if (link->l_mch != NULL) {
-		mac_client_close(link->l_mch, 0);
+	if (link != NULL) {
+		if (link->l_mch != NULL) {
+			mac_client_close(link->l_mch, 0);
+		}
+		if (link->l_mh != NULL) {
+			mac_close(link->l_mh);
+		}
+		kmem_free(link, sizeof (viona_link_t));
 	}
-	if (link->l_mh != NULL) {
-		mac_close(link->l_mh);
+	if (hold != NULL) {
+		vmm_drv_rele(hold);
 	}
 
-	kmem_free(link, sizeof (viona_link_t));
-
+	mutex_exit(&ss->ss_lock);
 	return (err);
 }
 
 static int
 viona_ioc_delete(viona_soft_state_t *ss)
 {
-	viona_link_t	*link;
+	viona_link_t *link;
 
-	link = ss->ss_link;
-	if (link == NULL) {
-		return (ENOSYS);
+	mutex_enter(&ss->ss_lock);
+	if ((link = ss->ss_link) == NULL) {
+		mutex_exit(&ss->ss_lock);
+		return (0);
 	}
-	if (copy_tx_mblks) {
-		mutex_enter(&link->l_tx_mutex);
-		while (link->l_tx_outstanding != 0) {
-			cv_wait(&link->l_tx_cv, &link->l_tx_mutex);
-		}
-		mutex_exit(&link->l_tx_mutex);
+
+	if (link->l_destroyed) {
+		/* Another thread made it here first. */
+		mutex_exit(&ss->ss_lock);
+		return (EAGAIN);
 	}
+	link->l_destroyed = B_TRUE;
+	mutex_exit(&ss->ss_lock);
+
+	VERIFY0(viona_ioc_ring_reset(link, VIONA_VQ_RX));
+	VERIFY0(viona_ioc_ring_reset(link, VIONA_VQ_TX));
+
+	mutex_enter(&ss->ss_lock);
+	VERIFY0(viona_ioc_set_notify_ioport(link, 0));
 	if (link->l_mch != NULL) {
-		mac_rx_clear(link->l_mch);
+		/*
+		 * The RX ring will have cleared its receive function from the
+		 * mac client handle, so all that is left to do is close it.
+		 */
 		mac_client_close(link->l_mch, 0);
 	}
 	if (link->l_mh != NULL) {
 		mac_close(link->l_mh);
 	}
-
-	viona_vm_unmap(link);
-	mutex_destroy(&link->l_tx_vring.hq_a_mutex);
-	mutex_destroy(&link->l_tx_vring.hq_u_mutex);
-	mutex_destroy(&link->l_rx_vring.hq_a_mutex);
-	mutex_destroy(&link->l_rx_vring.hq_u_mutex);
-	if (copy_tx_mblks) {
-		mutex_destroy(&link->l_tx_mutex);
-		cv_destroy(&link->l_tx_cv);
+	if (link->l_vm_hold != NULL) {
+		vmm_drv_rele(link->l_vm_hold);
+		link->l_vm_hold = NULL;
 	}
 
-	kmem_cache_destroy(link->l_desb_kmc);
-
-	kmem_free(link, sizeof (viona_link_t));
-
+	viona_ring_free(&link->l_vrings[VIONA_VQ_RX]);
+	viona_ring_free(&link->l_vrings[VIONA_VQ_TX]);
+	pollhead_clean(&link->l_pollhead);
 	ss->ss_link = NULL;
+	mutex_exit(&ss->ss_lock);
 
+	kmem_free(link, sizeof (viona_link_t));
 	return (0);
 }
 
-static caddr_t
-viona_mapin_vm_chunk(viona_link_t *link, uint64_t gpa, size_t len)
-{
-	caddr_t		addr;
-	size_t		offset;
-	pfn_t		pfnum;
-
-	if (len == 0)
-		return (NULL);
-
-	addr = vmem_alloc(heap_arena, len, VM_SLEEP);
-	if (addr == NULL)
-		return (NULL);
-
-	for (offset = 0; offset < len; offset += PAGESIZE) {
-		pfnum = btop(vm_gpa2hpa(link->l_vm, gpa + offset, PAGESIZE));
-		ASSERT(pfnum);
-		hat_devload(kas.a_hat, addr + offset, PAGESIZE, pfnum,
-		    PROT_READ | PROT_WRITE, HAT_LOAD_LOCK);
-	}
-
-	return (addr);
-}
-
 /*
- * Map the guest physical address space into the kernel virtual address space.
+ * Translate a guest physical address into a kernel virtual address.
  */
-static int
-viona_vm_map(viona_link_t *link)
+static void *
+viona_gpa2kva(viona_link_t *link, uint64_t gpa, size_t len)
 {
-	link->l_vm_lomemaddr = viona_mapin_vm_chunk(link,
-	    0, link->l_vm_lomemsize);
-	if (link->l_vm_lomemaddr == NULL)
-		return (-1);
-	link->l_vm_himemaddr = viona_mapin_vm_chunk(link,
-	    4 * (1024 * 1024 * 1024UL), link->l_vm_himemsize);
-	if (link->l_vm_himemsize && link->l_vm_himemaddr == NULL)
-		return (-1);
-
-	return (0);
+	return (vmm_drv_gpa2kva(link->l_vm_hold, gpa, len));
 }
 
-/*
- * Translate a guest physical address into a kernel virtual address.
- */
-static caddr_t
-viona_gpa2kva(viona_link_t *link, uint64_t gpa)
+static void
+viona_ring_alloc(viona_link_t *link, viona_vring_t *ring)
 {
-	if (gpa < link->l_vm_lomemsize)
-		return (link->l_vm_lomemaddr + gpa);
-
-	gpa -= (4 * GB);
-	if (gpa < link->l_vm_himemsize)
-		return (link->l_vm_himemaddr + gpa);
-
-	return (NULL);
+	ring->vr_link = link;
+	mutex_init(&ring->vr_lock, NULL, MUTEX_DRIVER, NULL);
+	cv_init(&ring->vr_cv, NULL, CV_DRIVER, NULL);
+	mutex_init(&ring->vr_a_mutex, NULL, MUTEX_DRIVER, NULL);
+	mutex_init(&ring->vr_u_mutex, NULL, MUTEX_DRIVER, NULL);
 }
 
 static void
-viona_vm_unmap(viona_link_t *link)
+viona_ring_free(viona_vring_t *ring)
 {
-	if (link->l_vm_lomemaddr) {
-		hat_unload(kas.a_hat, link->l_vm_lomemaddr,
-		    link->l_vm_lomemsize, HAT_UNLOAD_UNLOCK);
-		vmem_free(heap_arena, link->l_vm_lomemaddr,
-		    link->l_vm_lomemsize);
-	}
-	if (link->l_vm_himemaddr) {
-		hat_unload(kas.a_hat, link->l_vm_himemaddr,
-		    link->l_vm_himemsize, HAT_UNLOAD_UNLOCK);
-		vmem_free(heap_arena, link->l_vm_himemaddr,
-		    link->l_vm_himemsize);
-	}
+	mutex_destroy(&ring->vr_lock);
+	cv_destroy(&ring->vr_cv);
+	mutex_destroy(&ring->vr_a_mutex);
+	mutex_destroy(&ring->vr_u_mutex);
+	ring->vr_link = NULL;
 }
 
 static int
-viona_ioc_ring_init_common(viona_link_t *link, viona_vring_hqueue_t *hq,
-    vioc_ring_init_t *u_ri)
+viona_ioc_ring_init(viona_link_t *link, void *udata, int md)
 {
-	vioc_ring_init_t	k_ri;
-
-	if (copyin(u_ri, &k_ri, sizeof (k_ri)) != 0) {
+	vioc_ring_init_t kri;
+	viona_vring_t *ring;
+	kthread_t *t;
+	uintptr_t pos;
+	size_t desc_sz, avail_sz, used_sz;
+	uint16_t cnt;
+	int err = 0;
+
+	if (ddi_copyin(udata, &kri, sizeof (kri), md) != 0) {
 		return (EFAULT);
 	}
 
-	hq->hq_size = k_ri.ri_qsize;
-	hq->hq_baseaddr = viona_gpa2kva(link, k_ri.ri_qaddr);
-	if (hq->hq_baseaddr == NULL)
+	if (kri.ri_index >= VIONA_VQ_MAX) {
 		return (EINVAL);
-
-	hq->hq_avail_flags = (uint16_t *)(viona_gpa2kva(link,
-	    k_ri.ri_qaddr + hq->hq_size * sizeof (struct virtio_desc)));
-	if (hq->hq_avail_flags == NULL)
-		return (EINVAL);
-	hq->hq_avail_idx = hq->hq_avail_flags + 1;
-	hq->hq_avail_ring = hq->hq_avail_flags + 2;
-
-	hq->hq_used_flags = (uint16_t *)(viona_gpa2kva(link,
-	    P2ROUNDUP(k_ri.ri_qaddr +
-	    hq->hq_size * sizeof (struct virtio_desc) + 2, VRING_ALIGN)));
-	if (hq->hq_used_flags == NULL)
+	}
+	cnt = kri.ri_qsize;
+	if (cnt == 0 || cnt > VRING_MAX_LEN || (1 << (ffs(cnt) - 1)) != cnt) {
 		return (EINVAL);
-	hq->hq_used_idx = hq->hq_used_flags + 1;
-	hq->hq_used_ring = (struct virtio_used *)(hq->hq_used_flags + 2);
+	}
 
-	/*
-	 * Initialize queue indexes
-	 */
-	hq->hq_cur_aidx = 0;
+	ring = &link->l_vrings[kri.ri_index];
+	mutex_enter(&ring->vr_lock);
+	if (ring->vr_state != VRS_RESET) {
+		mutex_exit(&ring->vr_lock);
+		return (EBUSY);
+	}
 
-	return (0);
-}
+	pos = kri.ri_qaddr;
+	desc_sz = cnt * sizeof (struct virtio_desc);
+	avail_sz = (cnt + 3) * sizeof (uint16_t);
+	used_sz = (cnt * sizeof (struct virtio_used)) + (sizeof (uint16_t) * 3);
+
+	ring->vr_size = kri.ri_qsize;
+	ring->vr_mask = (ring->vr_size - 1);
+	ring->vr_descr = viona_gpa2kva(link, pos, desc_sz);
+	if (ring->vr_descr == NULL) {
+		err = EINVAL;
+		goto fail;
+	}
+	pos += desc_sz;
 
-static int
-viona_ioc_rx_ring_init(viona_link_t *link, vioc_ring_init_t *u_ri)
-{
-	viona_vring_hqueue_t	*hq;
-	int			rval;
+	ring->vr_avail_flags = viona_gpa2kva(link, pos, avail_sz);
+	if (ring->vr_avail_flags == NULL) {
+		err = EINVAL;
+		goto fail;
+	}
+	ring->vr_avail_idx = ring->vr_avail_flags + 1;
+	ring->vr_avail_ring = ring->vr_avail_flags + 2;
+	ring->vr_avail_used_event = ring->vr_avail_ring + cnt;
+	pos += avail_sz;
+
+	pos = P2ROUNDUP(pos, VRING_ALIGN);
+	ring->vr_used_flags = viona_gpa2kva(link, pos, used_sz);
+	if (ring->vr_used_flags == NULL) {
+		err = EINVAL;
+		goto fail;
+	}
+	ring->vr_used_idx = ring->vr_used_flags + 1;
+	ring->vr_used_ring = (struct virtio_used *)(ring->vr_used_flags + 2);
+	ring->vr_used_avail_event = (uint16_t *)(ring->vr_used_ring + cnt);
+
+	/* Initialize queue indexes */
+	ring->vr_cur_aidx = 0;
+
+	/* Allocate desb handles for TX ring if packet copying not disabled */
+	if (kri.ri_index == VIONA_VQ_TX && !viona_force_copy_tx_mblks) {
+		viona_desb_t *desb, *dp;
+
+		desb = kmem_zalloc(sizeof (viona_desb_t) * cnt, KM_SLEEP);
+		dp = desb;
+		for (uint_t i = 0; i < cnt; i++, dp++) {
+			dp->d_frtn.free_func = viona_desb_release;
+			dp->d_frtn.free_arg = (void *)dp;
+			dp->d_ring = ring;
+		}
+		ring->vr_desb = desb;
+	}
 
-	hq = &link->l_rx_vring;
+	/* Zero out MSI-X configuration */
+	ring->vr_msi_addr = 0;
+	ring->vr_msi_msg = 0;
 
-	rval = viona_ioc_ring_init_common(link, hq, u_ri);
-	if (rval != 0) {
-		return (rval);
+	t = viona_create_worker(ring);
+	if (t == NULL) {
+		err = ENOMEM;
+		goto fail;
 	}
-
+	ring->vr_worker_thread = t;
+	ring->vr_state = VRS_RESET|VRS_SETUP;
+	cv_broadcast(&ring->vr_cv);
+	mutex_exit(&ring->vr_lock);
 	return (0);
+
+fail:
+	if (ring->vr_desb != NULL) {
+		kmem_free(ring->vr_desb, sizeof (viona_desb_t) * cnt);
+	}
+	ring->vr_size = 0;
+	ring->vr_mask = 0;
+	ring->vr_descr = NULL;
+	ring->vr_avail_flags = NULL;
+	ring->vr_avail_idx = NULL;
+	ring->vr_avail_ring = NULL;
+	ring->vr_avail_used_event = NULL;
+	ring->vr_used_flags = NULL;
+	ring->vr_used_idx = NULL;
+	ring->vr_used_ring = NULL;
+	ring->vr_used_avail_event = NULL;
+	ring->vr_state = VRS_RESET;
+	mutex_exit(&ring->vr_lock);
+	return (err);
 }
 
 static int
-viona_ioc_tx_ring_init(viona_link_t *link, vioc_ring_init_t *u_ri)
+viona_ioc_ring_reset(viona_link_t *link, uint_t idx)
 {
-	viona_vring_hqueue_t	*hq;
+	viona_vring_t *ring;
 
-	hq = &link->l_tx_vring;
+	if (idx >= VIONA_VQ_MAX) {
+		return (EINVAL);
+	}
+	ring = &link->l_vrings[idx];
+
+	mutex_enter(&ring->vr_lock);
+	if (ring->vr_state != VRS_RESET) {
+		ring->vr_state |= VRS_REQ_STOP;
+		cv_broadcast(&ring->vr_cv);
+		while (ring->vr_state != VRS_RESET) {
+			cv_wait_sig(&ring->vr_cv, &ring->vr_lock);
+		}
+	}
+	if (ring->vr_desb != NULL) {
+		VERIFY(ring->vr_xfer_outstanding == 0);
+		kmem_free(ring->vr_desb, sizeof (viona_desb_t) * ring->vr_size);
+		ring->vr_desb = NULL;
+	}
+	mutex_exit(&ring->vr_lock);
 
-	return (viona_ioc_ring_init_common(link, hq, u_ri));
+	return (0);
 }
 
 static int
-viona_ioc_ring_reset_common(viona_vring_hqueue_t *hq)
+viona_ioc_ring_kick(viona_link_t *link, uint_t idx)
 {
-	/*
-	 * Reset all soft state
-	 */
-	hq->hq_cur_aidx = 0;
+	viona_vring_t *ring;
+	uint_t state;
+	int err;
 
-	return (0);
+	if (idx >= VIONA_VQ_MAX) {
+		return (EINVAL);
+	}
+	ring = &link->l_vrings[idx];
+
+	mutex_enter(&ring->vr_lock);
+	state = ring->vr_state & VRS_STATE_MASK;
+	switch (state) {
+	case VRS_INIT:
+		ring->vr_state |= VRS_REQ_START;
+		/* FALLTHROUGH */
+	case VRS_RUN:
+		cv_broadcast(&ring->vr_cv);
+		err = 0;
+		break;
+	default:
+		err = EBUSY;
+		break;
+	}
+	mutex_exit(&ring->vr_lock);
+
+	return (err);
 }
 
 static int
-viona_ioc_rx_ring_reset(viona_link_t *link)
+viona_ioc_ring_set_msi(viona_link_t *link, void *data, int md)
 {
-	viona_vring_hqueue_t	*hq;
+	vioc_ring_msi_t vrm;
+	viona_vring_t *ring;
 
-	mac_rx_clear(link->l_mch);
+	if (ddi_copyin(data, &vrm, sizeof (vrm), md) != 0) {
+		return (EFAULT);
+	}
+	if (vrm.rm_index >= VIONA_VQ_MAX) {
+		return (EINVAL);
+	}
 
-	hq = &link->l_rx_vring;
+	ring = &link->l_vrings[vrm.rm_index];
+	mutex_enter(&ring->vr_lock);
+	ring->vr_msi_addr = vrm.rm_addr;
+	ring->vr_msi_msg = vrm.rm_msg;
+	mutex_exit(&ring->vr_lock);
 
-	return (viona_ioc_ring_reset_common(hq));
+	return (0);
 }
 
 static int
-viona_ioc_tx_ring_reset(viona_link_t *link)
+viona_notify_wcb(void *arg, uintptr_t ioport, uint_t sz, uint64_t val)
 {
-	viona_vring_hqueue_t	*hq;
-
-	hq = &link->l_tx_vring;
+	viona_link_t *link = (viona_link_t *)arg;
+	uint16_t vq = (uint16_t)val;
 
-	return (viona_ioc_ring_reset_common(hq));
+	if (ioport != link->l_notify_ioport || sz != sizeof (uint16_t)) {
+		return (EINVAL);
+	}
+	return (viona_ioc_ring_kick(link, vq));
 }
 
-static void
-viona_ioc_rx_ring_kick(viona_link_t *link)
+static int
+viona_ioc_set_notify_ioport(viona_link_t *link, uint_t ioport)
 {
-	viona_vring_hqueue_t	*hq = &link->l_rx_vring;
+	int err = 0;
 
-	atomic_or_16(hq->hq_used_flags, VRING_USED_F_NO_NOTIFY);
+	if (link->l_notify_ioport != 0) {
+		vmm_drv_ioport_unhook(link->l_vm_hold, &link->l_notify_cookie);
+		link->l_notify_ioport = 0;
+	}
 
-	mac_rx_set(link->l_mch, viona_rx, link);
+	if (ioport != 0) {
+		err = vmm_drv_ioport_hook(link->l_vm_hold, ioport, NULL,
+		    viona_notify_wcb, (void *)link, &link->l_notify_cookie);
+		if (err == 0) {
+			link->l_notify_ioport = ioport;
+		}
+	}
+	return (err);
 }
 
 /*
- * Return the number of available descriptors in the vring taking care
- * of the 16-bit index wraparound.
+ * Return the number of available descriptors in the vring taking care of the
+ * 16-bit index wraparound.
  */
 static inline int
-viona_hq_num_avail(viona_vring_hqueue_t *hq)
+viona_vr_num_avail(viona_vring_t *ring)
 {
 	uint16_t ndesc;
 
 	/*
 	 * We're just computing (a-b) in GF(216).
 	 *
-	 * The only glitch here is that in standard C,
-	 * uint16_t promotes to (signed) int when int has
-	 * more than 16 bits (pretty much always now), so
-	 * we have to force it back to unsigned.
+	 * The only glitch here is that in standard C, uint16_t promotes to
+	 * (signed) int when int has more than 16 bits (almost always now).
+	 * A cast back to unsigned is necessary for proper operation.
 	 */
-	ndesc = (unsigned)*hq->hq_avail_idx - (unsigned)hq->hq_cur_aidx;
+	ndesc = (unsigned)*ring->vr_avail_idx - (unsigned)ring->vr_cur_aidx;
 
-	ASSERT(ndesc <= hq->hq_size);
+	ASSERT(ndesc <= ring->vr_size);
 
 	return (ndesc);
 }
 
 static void
-viona_ioc_tx_ring_kick(viona_link_t *link)
+viona_worker_rx(viona_vring_t *ring, viona_link_t *link)
 {
-	viona_vring_hqueue_t	*hq = &link->l_tx_vring;
+	proc_t *p = ttoproc(curthread);
+
+	ASSERT(MUTEX_HELD(&ring->vr_lock));
+	ASSERT(ring->vr_state == (VRS_INIT|VRS_REQ_START));
+
+	atomic_or_16(ring->vr_used_flags, VRING_USED_F_NO_NOTIFY);
+	mac_rx_set(link->l_mch, viona_rx, link);
+	ring->vr_state = VRS_RUN;
 
 	do {
-		atomic_or_16(hq->hq_used_flags, VRING_USED_F_NO_NOTIFY);
-		while (viona_hq_num_avail(hq)) {
-			viona_tx(link, hq);
+		/*
+		 * For now, there is little to do in the RX worker as inbound
+		 * data is delivered by MAC via the viona_rx callback.
+		 * If tap-like functionality is added later, this would be a
+		 * convenient place to inject frames into the guest.
+		 */
+		(void) cv_wait_sig(&ring->vr_cv, &ring->vr_lock);
+	} while (!VRING_NEED_BAIL(ring, p));
+
+	mac_rx_clear(link->l_mch);
+}
+
+static void
+viona_worker_tx(viona_vring_t *ring, viona_link_t *link)
+{
+	proc_t *p = ttoproc(curthread);
+	size_t ntx = 0;
+
+	ASSERT(MUTEX_HELD(&ring->vr_lock));
+	ASSERT(ring->vr_state == (VRS_INIT|VRS_REQ_START));
+
+	ring->vr_state = VRS_RUN;
+	mutex_exit(&ring->vr_lock);
+
+	for (;;) {
+		boolean_t bail = B_FALSE;
+
+		atomic_or_16(ring->vr_used_flags, VRING_USED_F_NO_NOTIFY);
+		while (viona_vr_num_avail(ring)) {
+			viona_tx(link, ring);
+			ntx++;
 		}
-		if (copy_tx_mblks) {
-			mutex_enter(&link->l_tx_mutex);
-			if (link->l_tx_outstanding != 0) {
-				cv_wait_sig(&link->l_tx_cv, &link->l_tx_mutex);
-			}
-			mutex_exit(&link->l_tx_mutex);
+		atomic_and_16(ring->vr_used_flags, ~VRING_USED_F_NO_NOTIFY);
+
+		/*
+		 * Check for available descriptors on the ring once more in
+		 * case a late addition raced with the NO_NOTIFY flag toggle.
+		 */
+		bail = VRING_NEED_BAIL(ring, p);
+		if (!bail && viona_vr_num_avail(ring)) {
+			continue;
+		}
+
+		VIONA_PROBE2(tx, viona_link_t *, link, uint_t, ntx);
+		ntx = 0;
+		if ((link->l_features & VIRTIO_F_RING_NOTIFY_ON_EMPTY) != 0) {
+			viona_intr_ring(ring);
+		}
+
+		mutex_enter(&ring->vr_lock);
+		while (!bail && !viona_vr_num_avail(ring)) {
+			(void) cv_wait_sig(&ring->vr_cv, &ring->vr_lock);
+			bail = VRING_NEED_BAIL(ring, p);
+		}
+		if (bail) {
+			break;
 		}
-		atomic_and_16(hq->hq_used_flags, ~VRING_USED_F_NO_NOTIFY);
-	} while (viona_hq_num_avail(hq));
+		mutex_exit(&ring->vr_lock);
+	}
+
+	ASSERT(MUTEX_HELD(&ring->vr_lock));
+
+	while (ring->vr_xfer_outstanding != 0) {
+		/*
+		 * Paying heed to signals is counterproductive here.  This is a
+		 * very tight loop if pending transfers take an extended amount
+		 * of time to be reclaimed while the host process is exiting.
+		 */
+		cv_wait(&ring->vr_cv, &ring->vr_lock);
+	}
+}
+
+static void
+viona_worker(void *arg)
+{
+	viona_vring_t *ring = (viona_vring_t *)arg;
+	viona_link_t *link = ring->vr_link;
+	proc_t *p = ttoproc(curthread);
+
+	mutex_enter(&ring->vr_lock);
+	VERIFY(ring->vr_state == (VRS_RESET|VRS_SETUP));
+
+	/* Report worker thread as alive and notify creator */
+	ring->vr_state = VRS_INIT;
+	cv_broadcast(&ring->vr_cv);
+
+	while (ring->vr_state == VRS_INIT) {
+		(void) cv_wait_sig(&ring->vr_cv, &ring->vr_lock);
+
+		if (VRING_NEED_BAIL(ring, p)) {
+			goto cleanup;
+		}
+	}
+	ASSERT(ring->vr_state & VRS_REQ_START);
+
+	/* Process actual work */
+	if (ring == &link->l_vrings[VIONA_VQ_RX]) {
+		viona_worker_rx(ring, link);
+	} else if (ring == &link->l_vrings[VIONA_VQ_TX]) {
+		viona_worker_tx(ring, link);
+	} else {
+		panic("unexpected ring: %p", (void *)ring);
+	}
+
+cleanup:
+	ring->vr_cur_aidx = 0;
+	ring->vr_state = VRS_RESET;
+	ring->vr_worker_thread = NULL;
+	cv_broadcast(&ring->vr_cv);
+	mutex_exit(&ring->vr_lock);
+
+	mutex_enter(&ttoproc(curthread)->p_lock);
+	lwp_exit();
+}
+
+static kthread_t *
+viona_create_worker(viona_vring_t *ring)
+{
+	k_sigset_t hold_set;
+	proc_t *p = curproc;
+	kthread_t *t;
+	klwp_t *lwp;
+
+	ASSERT(MUTEX_HELD(&ring->vr_lock));
+	ASSERT(ring->vr_state == VRS_RESET);
+
+	sigfillset(&hold_set);
+	lwp = lwp_create(viona_worker, (void *)ring, 0, p, TS_STOPPED,
+	    minclsyspri - 1, &hold_set, curthread->t_cid, 0);
+	if (lwp == NULL) {
+		return (NULL);
+	}
+
+	t = lwptot(lwp);
+	mutex_enter(&p->p_lock);
+	t->t_proc_flag = (t->t_proc_flag & ~TP_HOLDLWP) | TP_KTHREAD;
+	lwp_create_done(t);
+	mutex_exit(&p->p_lock);
+
+	return (t);
 }
 
 static int
-viona_ioc_rx_intr_clear(viona_link_t *link)
+viona_ioc_ring_intr_clear(viona_link_t *link, uint_t idx)
 {
-	link->l_rx_intr = 0;
+	if (idx >= VIONA_VQ_MAX) {
+		return (EINVAL);
+	}
 
+	link->l_vrings[idx].vr_intr_enabled = 0;
 	return (0);
 }
 
 static int
-viona_ioc_tx_intr_clear(viona_link_t *link)
+viona_ioc_intr_poll(viona_link_t *link, void *udata, int md, int *rv)
 {
-	link->l_tx_intr = 0;
+	uint_t cnt = 0;
+	vioc_intr_poll_t vip;
+
+	for (uint_t i = 0; i < VIONA_VQ_MAX; i++) {
+		uint_t val = link->l_vrings[i].vr_intr_enabled;
+
+		vip.vip_status[i] = val;
+		if (val != 0) {
+			cnt++;
+		}
+	}
 
+	if (ddi_copyout(&vip, udata, sizeof (vip), md) != 0) {
+		return (EFAULT);
+	}
+	*rv = (int)cnt;
 	return (0);
 }
-#define	VQ_MAX_DESCRIPTORS	512
 
 static int
-vq_popchain(viona_link_t *link, viona_vring_hqueue_t *hq, struct iovec *iov,
-    int n_iov, uint16_t *cookie)
+vq_popchain(viona_vring_t *ring, struct iovec *iov, int niov, uint16_t *cookie)
 {
-	int			i;
-	int			ndesc, nindir;
-	int			idx, head, next;
-	struct virtio_desc	*vdir, *vindir, *vp;
+	viona_link_t *link = ring->vr_link;
+	uint_t i, ndesc, idx, head, next;
+	struct virtio_desc vdir;
+	void *buf;
 
-	idx = hq->hq_cur_aidx;
-	ndesc = (uint16_t)((unsigned)*hq->hq_avail_idx - (unsigned)idx);
+	ASSERT(iov != NULL);
+	ASSERT(niov > 0);
 
-	if (ndesc == 0)
+	mutex_enter(&ring->vr_a_mutex);
+	idx = ring->vr_cur_aidx;
+	ndesc = (uint16_t)((unsigned)*ring->vr_avail_idx - (unsigned)idx);
+
+	if (ndesc == 0) {
+		mutex_exit(&ring->vr_a_mutex);
 		return (0);
-	if (ndesc > hq->hq_size) {
-		cmn_err(CE_NOTE, "ndesc (%d) out of range\n", ndesc);
+	}
+	if (ndesc > ring->vr_size) {
+		VIONA_PROBE2(ndesc_too_high, viona_vring_t *, ring,
+		    uint16_t, ndesc);
+		mutex_exit(&ring->vr_a_mutex);
 		return (-1);
 	}
 
-	head = hq->hq_avail_ring[idx & (hq->hq_size - 1)];
+	head = ring->vr_avail_ring[idx & ring->vr_mask];
 	next = head;
 
-	for (i = 0; i < VQ_MAX_DESCRIPTORS; next = vdir->vd_next) {
-		if (next >= hq->hq_size) {
-			cmn_err(CE_NOTE, "descriptor index (%d)"
-			    "out of range\n", next);
-			return (-1);
+	for (i = 0; i < niov; next = vdir.vd_next) {
+		if (next >= ring->vr_size) {
+			VIONA_PROBE2(bad_idx, viona_vring_t *, ring,
+			    uint16_t, next);
+			goto bail;
 		}
 
-		vdir = (struct virtio_desc *)(hq->hq_baseaddr +
-		    next * sizeof (struct virtio_desc));
-		if ((vdir->vd_flags & VRING_DESC_F_INDIRECT) == 0) {
-			if (i > n_iov)
-				return (-1);
-			iov[i].iov_base = viona_gpa2kva(link, vdir->vd_addr);
-			if (iov[i].iov_base == NULL) {
-				cmn_err(CE_NOTE, "invalid guest physical"
-				    " address 0x%"PRIx64"\n", vdir->vd_addr);
-				return (-1);
+		vdir = ring->vr_descr[next];
+		if ((vdir.vd_flags & VRING_DESC_F_INDIRECT) == 0) {
+			buf = viona_gpa2kva(link, vdir.vd_addr, vdir.vd_len);
+			if (buf == NULL) {
+				VIONA_PROBE_BAD_RING_ADDR(ring, vdir.vd_addr);
+				goto bail;
 			}
-			iov[i++].iov_len = vdir->vd_len;
+			iov[i].iov_base = buf;
+			iov[i].iov_len = vdir.vd_len;
+			i++;
 		} else {
-			nindir = vdir->vd_len / 16;
-			if ((vdir->vd_len & 0xf) || nindir == 0) {
-				cmn_err(CE_NOTE, "invalid indir len 0x%x\n",
-				    vdir->vd_len);
-				return (-1);
+			const uint_t nindir = vdir.vd_len / 16;
+			volatile struct virtio_desc *vindir;
+
+			if ((vdir.vd_len & 0xf) || nindir == 0) {
+				VIONA_PROBE2(indir_bad_len,
+				    viona_vring_t *, ring,
+				    uint32_t, vdir.vd_len);
+				goto bail;
 			}
-			vindir = (struct virtio_desc *)
-			    viona_gpa2kva(link, vdir->vd_addr);
+			vindir = viona_gpa2kva(link, vdir.vd_addr, vdir.vd_len);
 			if (vindir == NULL) {
-				cmn_err(CE_NOTE, "invalid guest physical"
-				    " address 0x%"PRIx64"\n", vdir->vd_addr);
-				return (-1);
+				VIONA_PROBE_BAD_RING_ADDR(ring, vdir.vd_addr);
+				goto bail;
 			}
 			next = 0;
 			for (;;) {
-				vp = &vindir[next];
-				if (vp->vd_flags & VRING_DESC_F_INDIRECT) {
-					cmn_err(CE_NOTE, "indirect desc"
-					    " has INDIR flag\n");
-					return (-1);
+				struct virtio_desc vp;
+
+				/*
+				 * A copy of the indirect descriptor is made
+				 * here, rather than simply using a reference
+				 * pointer.  This prevents malicious or
+				 * erroneous guest writes to the descriptor
+				 * from fooling the flags/bounds verification
+				 * through a race.
+				 */
+				vp = vindir[next];
+				if (vp.vd_flags & VRING_DESC_F_INDIRECT) {
+					VIONA_PROBE1(indir_bad_nest,
+					    viona_vring_t *, ring);
+					goto bail;
 				}
-				if (i > n_iov)
-					return (-1);
-				iov[i].iov_base =
-				    viona_gpa2kva(link, vp->vd_addr);
-				if (iov[i].iov_base == NULL) {
-					cmn_err(CE_NOTE, "invalid guest"
-					    " physical address 0x%"PRIx64"\n",
-					    vp->vd_addr);
-					return (-1);
+				buf = viona_gpa2kva(link, vp.vd_addr,
+				    vp.vd_len);
+				if (buf == NULL) {
+					VIONA_PROBE_BAD_RING_ADDR(ring,
+					    vp.vd_addr);
+					goto bail;
 				}
-				iov[i++].iov_len = vp->vd_len;
+				iov[i].iov_base = buf;
+				iov[i].iov_len = vp.vd_len;
+				i++;
 
-				if (i > VQ_MAX_DESCRIPTORS)
-					goto loopy;
-				if ((vp->vd_flags & VRING_DESC_F_NEXT) == 0)
+				if ((vp.vd_flags & VRING_DESC_F_NEXT) == 0)
 					break;
+				if (i >= niov) {
+					goto loopy;
+				}
 
-				next = vp->vd_next;
+				next = vp.vd_next;
 				if (next >= nindir) {
-					cmn_err(CE_NOTE, "invalid next"
-					    " %d > %d\n", next, nindir);
-					return (-1);
+					VIONA_PROBE3(indir_bad_next,
+					    viona_vring_t *, ring,
+					    uint16_t, next,
+					    uint_t, nindir);
+					goto bail;
 				}
 			}
 		}
-		if ((vdir->vd_flags & VRING_DESC_F_NEXT) == 0) {
+		if ((vdir.vd_flags & VRING_DESC_F_NEXT) == 0) {
 			*cookie = head;
-			hq->hq_cur_aidx++;
+			ring->vr_cur_aidx++;
+			mutex_exit(&ring->vr_a_mutex);
 			return (i);
 		}
 	}
 
 loopy:
-	cmn_err(CE_NOTE, "%d > descriptor loop count\n", i);
-
+	VIONA_PROBE1(too_many_desc, viona_vring_t *, ring);
+bail:
+	mutex_exit(&ring->vr_a_mutex);
 	return (-1);
 }
 
 static void
-vq_pushchain(viona_vring_hqueue_t *hq, uint32_t len, uint16_t cookie)
+vq_pushchain(viona_vring_t *ring, uint32_t len, uint16_t cookie)
 {
-	struct virtio_used	*vu;
-	int			uidx;
+	volatile struct virtio_used *vu;
+	uint_t uidx;
 
-	uidx = *hq->hq_used_idx;
-	vu = &hq->hq_used_ring[uidx++ & (hq->hq_size - 1)];
+	mutex_enter(&ring->vr_u_mutex);
+
+	uidx = *ring->vr_used_idx;
+	vu = &ring->vr_used_ring[uidx++ & ring->vr_mask];
 	vu->vu_idx = cookie;
 	vu->vu_tlen = len;
 	membar_producer();
-	*hq->hq_used_idx = uidx;
+	*ring->vr_used_idx = uidx;
+
+	mutex_exit(&ring->vr_u_mutex);
 }
 
 static void
-vq_pushchain_mrgrx(viona_vring_hqueue_t *hq, int num_bufs, used_elem_t *elem)
+vq_pushchain_mrgrx(viona_vring_t *ring, int num_bufs, used_elem_t *elem)
 {
-	struct virtio_used	*vu;
-	int			uidx;
-	int			i;
+	volatile struct virtio_used *vu;
+	uint_t uidx, i;
 
-	uidx = *hq->hq_used_idx;
+	mutex_enter(&ring->vr_u_mutex);
+
+	uidx = *ring->vr_used_idx;
 	if (num_bufs == 1) {
-		vu = &hq->hq_used_ring[uidx++ & (hq->hq_size - 1)];
+		vu = &ring->vr_used_ring[uidx++ & ring->vr_mask];
 		vu->vu_idx = elem[0].id;
 		vu->vu_tlen = elem[0].len;
 	} else {
 		for (i = 0; i < num_bufs; i++) {
-			vu = &hq->hq_used_ring[(uidx + i) & (hq->hq_size - 1)];
+			vu = &ring->vr_used_ring[(uidx + i) & ring->vr_mask];
 			vu->vu_idx = elem[i].id;
 			vu->vu_tlen = elem[i].len;
 		}
 		uidx = uidx + num_bufs;
 	}
 	membar_producer();
-	*hq->hq_used_idx = uidx;
+	*ring->vr_used_idx = uidx;
+
+	mutex_exit(&ring->vr_u_mutex);
+}
+
+static void
+viona_intr_ring(viona_vring_t *ring)
+{
+	uint64_t addr;
+
+	mutex_enter(&ring->vr_lock);
+	/* Deliver the interrupt directly, if so configured. */
+	if ((addr = ring->vr_msi_addr) != 0) {
+		uint64_t msg = ring->vr_msi_msg;
+
+		mutex_exit(&ring->vr_lock);
+		(void) vmm_drv_msi(ring->vr_link->l_vm_hold, addr, msg);
+		return;
+	}
+	mutex_exit(&ring->vr_lock);
+
+	if (atomic_cas_uint(&ring->vr_intr_enabled, 0, 1) == 0) {
+		pollwakeup(&ring->vr_link->l_pollhead, POLLRDBAND);
+	}
+}
+
+static size_t
+viona_copy_mblk(const mblk_t *mp, size_t seek, caddr_t buf, size_t len,
+    boolean_t *end)
+{
+	size_t copied = 0;
+	size_t off = 0;
+
+	/* Seek past already-consumed data */
+	while (seek > 0 && mp != NULL) {
+		size_t chunk = MBLKL(mp);
+
+		if (chunk > seek) {
+			off = seek;
+			break;
+		}
+		mp = mp->b_cont;
+		seek -= chunk;
+	}
+
+	while (mp != NULL) {
+		const size_t chunk = MBLKL(mp) - off;
+		const size_t to_copy = MIN(chunk, len);
+
+		bcopy(mp->b_rptr + off, buf, to_copy);
+		copied += to_copy;
+		buf += to_copy;
+		len -= to_copy;
+
+		/* Go no further if the buffer has been filled */
+		if (len == 0) {
+			break;
+		}
+
+		/*
+		 * Any offset into the initially chosen mblk_t buffer is
+		 * consumed on the first copy operation.
+		 */
+		off = 0;
+		mp = mp->b_cont;
+	}
+	*end = (mp == NULL);
+	return (copied);
 }
 
-/*
- * Copy bytes from mp to iov.
- * copied_buf: Total num_bytes copied from mblk to iov array.
- * buf: pointer to iov_base.
- * i: index of iov array. Mainly used to identify if we are
- *    dealing with first iov array element.
- * rxhdr_size: Virtio header size. Two possibilities in case
- *    of MRGRX buf, header has 2 additional bytes.
- *    In case of mrgrx, virtio header should be part of iov[0].
- *    In case of non-mrgrx, virtio header may or may not be part
- *    of iov[0].
- */
 static int
-copy_in_mblk(mblk_t *mp, int copied_buf, caddr_t buf, struct iovec *iov,
-    int i, int rxhdr_size)
+viona_recv_plain(viona_vring_t *ring, const mblk_t *mp, size_t msz)
 {
-	int copied_chunk = 0;
-	mblk_t *ml;
-	int total_buf_len = iov->iov_len;
-	/*
-	 * iov[0] might have header, adjust
-	 * total_buf_len accordingly
-	 */
-	if (i == 0) {
-		total_buf_len = iov->iov_len - rxhdr_size;
+	struct iovec iov[VTNET_MAXSEGS];
+	uint16_t cookie;
+	int n;
+	const size_t hdr_sz = sizeof (struct virtio_net_hdr);
+	struct virtio_net_hdr *hdr;
+	size_t len, copied = 0;
+	caddr_t buf = NULL;
+	boolean_t end = B_FALSE;
+
+	n = vq_popchain(ring, iov, VTNET_MAXSEGS, &cookie);
+	if (n <= 0) {
+		/* Without available buffers, the frame must be dropped. */
+		return (ENOSPC);
 	}
-	for (ml = mp; ml != NULL; ml = ml->b_cont) {
-		size_t	chunk = MBLKL(ml);
+	if (iov[0].iov_len < hdr_sz) {
 		/*
-		 * If chunk is less than
-		 * copied_buf we should move
-		 * to correct msgblk
+		 * There is little to do if there is not even space available
+		 * for the sole header.  Zero the buffer and bail out as a last
+		 * act of desperation.
 		 */
-		if (copied_buf != 0) {
-			if (copied_buf < chunk) {
-				chunk -= copied_buf;
-			} else {
-				copied_buf -= chunk;
-				continue;
-			}
+		bzero(iov[0].iov_base, iov[0].iov_len);
+		goto bad_frame;
+	}
+
+	/* Grab the address of the header before anything else */
+	hdr = (struct virtio_net_hdr *)iov[0].iov_base;
+
+	/*
+	 * If there is any space remaining in the first buffer after writing
+	 * the header, fill it with frame data.
+	 */
+	if (iov[0].iov_len > hdr_sz) {
+		buf = (caddr_t)iov[0].iov_base + hdr_sz;
+		len = iov[0].iov_len - hdr_sz;
+
+		copied += viona_copy_mblk(mp, copied, buf, len, &end);
+	}
+
+	/* Copy any remaining data into subsequent buffers, if present */
+	for (int i = 1; i < n && !end; i++) {
+		buf = (caddr_t)iov[i].iov_base;
+		len = iov[i].iov_len;
+
+		copied += viona_copy_mblk(mp, copied, buf, len, &end);
+	}
+
+	/*
+	 * Is the copied data long enough to be considered an ethernet frame of
+	 * the minimum length?  Does it match the total length of the mblk?
+	 */
+	if (copied < MIN_BUF_SIZE || copied != msz) {
+		VIONA_PROBE5(too_short, viona_vring_t *, ring,
+		    uint16_t, cookie, mblk_t *, mp, size_t, copied,
+		    size_t, msz);
+		goto bad_frame;
+	}
+
+	/* Populate (read: zero) the header and account for it in the size */
+	bzero(hdr, hdr_sz);
+	copied += hdr_sz;
+
+	/* Add chksum bits, if needed */
+	if ((ring->vr_link->l_features & VIRTIO_NET_F_GUEST_CSUM) != 0) {
+		uint32_t cksum_flags;
+
+		mac_hcksum_get((mblk_t *)mp, NULL, NULL, NULL, NULL,
+		    &cksum_flags);
+		if ((cksum_flags & HCK_FULLCKSUM_OK) != 0) {
+			hdr->vrh_flags |= VIRTIO_NET_HDR_F_DATA_VALID;
 		}
+	}
+
+	/* Release this chain */
+	vq_pushchain(ring, copied, cookie);
+	return (0);
+
+bad_frame:
+	VIONA_PROBE3(bad_rx_frame, viona_vring_t *, ring, uint16_t, cookie,
+	    mblk_t *, mp);
+	vq_pushchain(ring, MAX(copied, MIN_BUF_SIZE + hdr_sz), cookie);
+	return (EINVAL);
+}
+
+static int
+viona_recv_merged(viona_vring_t *ring, const mblk_t *mp, size_t msz)
+{
+	struct iovec iov[VTNET_MAXSEGS];
+	used_elem_t uelem[VTNET_MAXSEGS];
+	int n, i = 0, buf_idx = 0, err = 0;
+	uint16_t cookie;
+	caddr_t buf;
+	size_t len, copied = 0, chunk = 0;
+	struct virtio_net_mrgrxhdr *hdr = NULL;
+	const size_t hdr_sz = sizeof (struct virtio_net_mrgrxhdr);
+	boolean_t end = B_FALSE;
+
+	n = vq_popchain(ring, iov, VTNET_MAXSEGS, &cookie);
+	if (n <= 0) {
+		/* Without available buffers, the frame must be dropped. */
+		VIONA_PROBE2(no_space, viona_vring_t *, ring, mblk_t *, mp);
+		return (ENOSPC);
+	}
+	if (iov[0].iov_len < hdr_sz) {
 		/*
-		 * iov[0] already has virtio header.
-		 * and if copied chunk is length of iov_len break
+		 * There is little to do if there is not even space available
+		 * for the sole header.  Zero the buffer and bail out as a last
+		 * act of desperation.
 		 */
-		if (copied_chunk == total_buf_len) {
-			break;
+		bzero(iov[0].iov_base, iov[0].iov_len);
+		uelem[0].id = cookie;
+		uelem[0].len = iov[0].iov_len;
+		err = EINVAL;
+		goto done;
+	}
+
+	/* Grab the address of the header and do initial population */
+	hdr = (struct virtio_net_mrgrxhdr *)iov[0].iov_base;
+	bzero(hdr, hdr_sz);
+	hdr->vrh_bufs = 1;
+
+	/*
+	 * If there is any space remaining in the first buffer after writing
+	 * the header, fill it with frame data.
+	 */
+	if (iov[0].iov_len > hdr_sz) {
+		buf = iov[0].iov_base + hdr_sz;
+		len = iov[0].iov_len - hdr_sz;
+
+		chunk += viona_copy_mblk(mp, copied, buf, len, &end);
+		copied += chunk;
+	}
+	i = 1;
+
+	do {
+		while (i < n && !end) {
+			buf = iov[i].iov_base;
+			len = iov[i].iov_len;
+
+			chunk += viona_copy_mblk(mp, copied, buf, len, &end);
+			copied += chunk;
+			i++;
 		}
+
+		uelem[buf_idx].id = cookie;
+		uelem[buf_idx].len = chunk;
+
 		/*
-		 * Sometimes chunk is total mblk len, sometimes mblk is
-		 * divided into multiple chunks.
+		 * Try to grab another buffer from the ring if the mblk has not
+		 * yet been entirely copied out.
 		 */
-		if (chunk > copied_buf) {
-			if (chunk > copied_chunk) {
-				if ((chunk + copied_chunk) > total_buf_len)
-					chunk = (size_t)total_buf_len
-					    - copied_chunk;
-			} else {
-				if (chunk > (total_buf_len - copied_chunk))
-					chunk = (size_t)((total_buf_len
-					    - copied_chunk) - chunk);
+		if (!end) {
+			if (buf_idx == (VTNET_MAXSEGS - 1)) {
+				/*
+				 * Our arbitrary limit on the number of buffers
+				 * to offer for merge has already been reached.
+				 */
+				err = EOVERFLOW;
+				break;
 			}
-			bcopy(ml->b_rptr + copied_buf, buf, chunk);
-		} else {
-			if (chunk > (total_buf_len - copied_chunk)) {
-				chunk = (size_t)(total_buf_len - copied_chunk);
+			n = vq_popchain(ring, iov, VTNET_MAXSEGS, &cookie);
+			if (n <= 0) {
+				/*
+				 * Without more immediate space to perform the
+				 * copying, there is little choice left but to
+				 * drop the packet.
+				 */
+				err = EMSGSIZE;
 			}
-			bcopy(ml->b_rptr + copied_buf, buf, chunk);
+			chunk = 0;
+			i = 0;
+			buf_idx++;
+			/*
+			 * Keep the header up-to-date with the number of
+			 * buffers, but never reference its value since the
+			 * guest could meddle with it.
+			 */
+			hdr->vrh_bufs++;
+		}
+	} while (!end && copied < msz);
+
+	/* Account for the header size in the first buffer */
+	uelem[0].len += hdr_sz;
+
+	/*
+	 * Is the copied data long enough to be considered an ethernet frame of
+	 * the minimum length?  Does it match the total length of the mblk?
+	 */
+	if (copied < MIN_BUF_SIZE || copied != msz) {
+		/* Do not override an existing error */
+		VIONA_PROBE5(too_short, viona_vring_t *, ring,
+		    uint16_t, cookie, mblk_t *, mp, size_t, copied,
+		    size_t, msz);
+		err = (err == 0) ? EINVAL : err;
+	}
+
+	/* Add chksum bits, if needed */
+	if ((ring->vr_link->l_features & VIRTIO_NET_F_GUEST_CSUM) != 0) {
+		uint32_t cksum_flags;
+
+		mac_hcksum_get((mblk_t *)mp, NULL, NULL, NULL, NULL,
+		    &cksum_flags);
+		if ((cksum_flags & HCK_FULLCKSUM_OK) != 0) {
+			hdr->vrh_flags |= VIRTIO_NET_HDR_F_DATA_VALID;
 		}
-		buf += chunk;
-		copied_chunk += chunk;
 	}
-	return (copied_chunk);
+
+done:
+	switch (err) {
+	case 0:
+		/* Success can fall right through to ring delivery */
+		break;
+
+	case EMSGSIZE:
+		VIONA_PROBE3(rx_merge_underrun, viona_vring_t *, ring,
+		    uint16_t, cookie, mblk_t *, mp);
+		break;
+
+	case EOVERFLOW:
+		VIONA_PROBE3(rx_merge_overrun, viona_vring_t *, ring,
+		    uint16_t, cookie, mblk_t *, mp);
+		break;
+
+	default:
+		VIONA_PROBE3(bad_rx_frame, viona_vring_t *, ring,
+		    uint16_t, cookie, mblk_t *, mp);
+	}
+	vq_pushchain_mrgrx(ring, buf_idx + 1, uelem);
+	return (err);
 }
 
 static void
-viona_rx(void *arg, mac_resource_handle_t mrh, mblk_t *mp,
-    boolean_t loopback)
+viona_rx(void *arg, mac_resource_handle_t mrh, mblk_t *mp, boolean_t loopback)
 {
-	viona_link_t		*link = arg;
-	viona_vring_hqueue_t	*hq = &link->l_rx_vring;
-	mblk_t			*mp0 = mp;
-
-	while (viona_hq_num_avail(hq)) {
-		struct iovec		iov[VTNET_MAXSEGS];
-		size_t			mblklen;
-		int			n, i = 0;
-		uint16_t		cookie;
-		struct virtio_net_hdr	*vrx = NULL;
-		struct virtio_net_mrgrxhdr *vmrgrx = NULL;
-#if notyet
-		mblk_t			*ml;
-#endif
-		caddr_t			buf = NULL;
-		int			total_len = 0;
-		int			copied_buf = 0;
-		int			num_bufs = 0;
-		int			num_pops = 0;
-		used_elem_t		uelem[VTNET_MAXSEGS];
-
-		if (mp == NULL) {
-			break;
-		}
-		mblklen = msgsize(mp);
-		if (mblklen == 0) {
-			break;
-		}
+	viona_link_t *link = (viona_link_t *)arg;
+	viona_vring_t *ring = &link->l_vrings[VIONA_VQ_RX];
+	mblk_t *mprx = NULL, **mprx_prevp = &mprx;
+	mblk_t *mpdrop = NULL, **mpdrop_prevp = &mpdrop;
+	const boolean_t do_merge =
+	    ((link->l_features & VIRTIO_NET_F_MRG_RXBUF) != 0);
+	size_t nrx = 0, ndrop = 0;
+
+	while (mp != NULL) {
+		mblk_t *next, *pad = NULL;
+		size_t size;
+		int err = 0;
+
+		next = mp->b_next;
+		mp->b_next = NULL;
+		size = msgsize(mp);
 
-		mutex_enter(&hq->hq_a_mutex);
-		n = vq_popchain(link, hq, iov, VTNET_MAXSEGS, &cookie);
-		mutex_exit(&hq->hq_a_mutex);
-		if (n <= 0) {
-			break;
+		/*
+		 * Stripping the VLAN tag off already-small frames can cause
+		 * them to fall below the minimum size.  If this happens, pad
+		 * them out as they would have been if they lacked the tag in
+		 * the first place.
+		 */
+		if (size == NEED_VLAN_PAD_SIZE) {
+			ASSERT(MBLKL(viona_vlan_pad_mp) == VLAN_TAGSZ);
+			ASSERT(viona_vlan_pad_mp->b_cont == NULL);
+
+			for (pad = mp; pad->b_cont != NULL; pad = pad->b_cont)
+				;
+
+			pad->b_cont = viona_vlan_pad_mp;
+			size += VLAN_TAGSZ;
 		}
-		num_pops++;
-		if (link->l_features & VIRTIO_NET_F_MRG_RXBUF) {
-			int total_n = n;
-			int mrgrxhdr_size = sizeof (struct virtio_net_mrgrxhdr);
-			/*
-			 * Get a pointer to the rx header, and use the
-			 * data immediately following it for the packet buffer.
-			 */
-			vmrgrx = (struct virtio_net_mrgrxhdr *)iov[0].iov_base;
-			if (n == 1) {
-				buf = iov[0].iov_base + mrgrxhdr_size;
-			}
-			while (mblklen > copied_buf) {
-				if (total_n == i) {
-					mutex_enter(&hq->hq_a_mutex);
-					n = vq_popchain(link, hq, &iov[i],
-					    VTNET_MAXSEGS, &cookie);
-					mutex_exit(&hq->hq_a_mutex);
-					if (n <= 0) {
-						freemsgchain(mp0);
-						return;
-					}
-					num_pops++;
-					total_n += n;
-				}
-				if (total_n > i) {
-					int copied_chunk = 0;
-					if (i != 0) {
-						buf = iov[i].iov_base;
-					}
-					copied_chunk = copy_in_mblk(mp,
-					    copied_buf, buf, &iov[i], i,
-					    mrgrxhdr_size);
-					copied_buf += copied_chunk;
-					uelem[i].id = cookie;
-					uelem[i].len = copied_chunk;
-					if (i == 0) {
-						uelem[i].len += mrgrxhdr_size;
-					}
-				}
-				num_bufs++;
-				i++;
-			}
+
+		if (do_merge) {
+			err = viona_recv_merged(ring, mp, size);
 		} else {
-			boolean_t virt_hdr_incl_iov = B_FALSE;
-			int rxhdr_size = sizeof (struct virtio_net_hdr);
-			/* First element is header */
-			vrx = (struct virtio_net_hdr *)iov[0].iov_base;
-			if (n == 1 || iov[0].iov_len > rxhdr_size) {
-				buf = iov[0].iov_base + rxhdr_size;
-				virt_hdr_incl_iov = B_TRUE;
-				total_len += rxhdr_size;
-				if (iov[0].iov_len < rxhdr_size) {
-					// Buff too small to fit pkt. Drop it.
-					freemsgchain(mp0);
-					return;
-				}
-			} else {
-				total_len = iov[0].iov_len;
-			}
-			if (iov[0].iov_len == rxhdr_size)
-				i++;
-			while (mblklen > copied_buf) {
-				if (n > i) {
-					int copied_chunk = 0;
-					if (i != 0) {
-						buf = iov[i].iov_base;
-					}
-					/*
-					 * In case of non-mrgrx buf, first
-					 * descriptor always has header and
-					 * rest of the descriptors have data.
-					 * But it is not guaranteed that first
-					 * descriptor will only have virtio
-					 * header. It might also have data.
-					 */
-					if (virt_hdr_incl_iov) {
-						copied_chunk = copy_in_mblk(mp,
-						    copied_buf, buf, &iov[i],
-						    i, rxhdr_size);
-					} else {
-						copied_chunk = copy_in_mblk(mp,
-						    copied_buf, buf, &iov[i],
-						    i, 0);
-					}
-					copied_buf += copied_chunk;
-					total_len += copied_chunk;
-				} else {
-					/*
-					 * Drop packet as it cant fit
-					 * in buf provided by guest.
-					 */
-					freemsgchain(mp0);
-					return;
-				}
-				i++;
-			}
+			err = viona_recv_plain(ring, mp, size);
 		}
+
 		/*
-		 * The only valid field in the rx packet header is the
-		 * number of buffers, which is always 1 without TSO
-		 * support.
+		 * The VLAN padding mblk is meant for continual reuse, so
+		 * remove it from the chain to prevent it from being freed
 		 */
-		if (link->l_features & VIRTIO_NET_F_MRG_RXBUF) {
-			memset(vmrgrx, 0, sizeof (struct virtio_net_mrgrxhdr));
-			vmrgrx->vrh_bufs = num_bufs;
+		if (pad != NULL) {
+			pad->b_cont = NULL;
+		}
+
+		if (err != 0) {
+			*mpdrop_prevp = mp;
+			mpdrop_prevp = &mp->b_next;
+
 			/*
-			 * Make sure iov[0].iov_len >= MIN_BUF_SIZE
-			 * otherwise guest will consider it as invalid frame.
+			 * If the available ring is empty, do not bother
+			 * attempting to deliver any more frames.  Count the
+			 * rest as dropped too.
 			 */
-			if (num_bufs == 1 && uelem[0].len < MIN_BUF_SIZE) {
-				uelem[0].len = MIN_BUF_SIZE;
+			if (err == ENOSPC) {
+				mp->b_next = next;
+				break;
 			}
-			/*
-			 * Release this chain and handle more chains.
-			 */
-			mutex_enter(&hq->hq_u_mutex);
-			vq_pushchain_mrgrx(hq, num_pops, uelem);
-			mutex_exit(&hq->hq_u_mutex);
 		} else {
-			memset(vrx, 0, sizeof (struct virtio_net_hdr));
-			if (total_len < MIN_BUF_SIZE) {
-				total_len = MIN_BUF_SIZE;
-			}
-			/*
-			 * Release this chain and handle more chains.
-			 */
-			mutex_enter(&hq->hq_u_mutex);
-			vq_pushchain(hq, total_len, cookie);
-			mutex_exit(&hq->hq_u_mutex);
+			/* Chain successful mblks to be freed later */
+			*mprx_prevp = mp;
+			mprx_prevp = &mp->b_next;
+			nrx++;
 		}
+		mp = next;
+	}
 
-		mp = mp->b_next;
+	if ((*ring->vr_avail_flags & VRING_AVAIL_F_NO_INTERRUPT) == 0) {
+		viona_intr_ring(ring);
 	}
 
-	if ((*hq->hq_avail_flags & VRING_AVAIL_F_NO_INTERRUPT) == 0) {
-		if (atomic_cas_uint(&link->l_rx_intr, 0, 1) == 0) {
-			pollwakeup(&link->l_pollhead, POLLIN);
-		}
+	/* Free successfully received frames */
+	if (mprx != NULL) {
+		freemsgchain(mprx);
 	}
 
-	freemsgchain(mp0);
+	/* Free dropped frames, also tallying them */
+	mp = mpdrop;
+	while (mp != NULL) {
+		mblk_t *next = mp->b_next;
+
+		mp->b_next = NULL;
+		freemsg(mp);
+		mp = next;
+		ndrop++;
+	}
+	VIONA_PROBE3(rx, viona_link_t *, link, size_t, nrx, size_t, ndrop);
 }
 
 static void
-viona_desb_free(viona_desb_t *dp)
+viona_tx_done(viona_vring_t *ring, uint32_t len, uint16_t cookie)
 {
-	viona_link_t		*link;
-	viona_vring_hqueue_t	*hq;
-#if notyet
-	struct virtio_used	*vu;
-	int			uidx;
-#endif
-	uint_t			ref;
+	vq_pushchain(ring, len, cookie);
+
+	if ((*ring->vr_avail_flags & VRING_AVAIL_F_NO_INTERRUPT) == 0) {
+		viona_intr_ring(ring);
+	}
+}
+
+static void
+viona_desb_release(viona_desb_t *dp)
+{
+	viona_vring_t *ring = dp->d_ring;
+	uint_t ref;
+	uint32_t len;
+	uint16_t cookie;
 
 	ref = atomic_dec_uint_nv(&dp->d_ref);
-	if (ref != 0)
+	if (ref > 1) {
 		return;
+	}
 
-	link = dp->d_link;
-	hq = &link->l_tx_vring;
+	/*
+	 * The desb corresponding to this index must be ready for reuse before
+	 * the descriptor is returned to the guest via the 'used' ring.
+	 */
+	len = dp->d_len;
+	cookie = dp->d_cookie;
+	dp->d_len = 0;
+	dp->d_cookie = 0;
+	dp->d_ref = 0;
 
-	mutex_enter(&hq->hq_u_mutex);
-	vq_pushchain(hq, dp->d_len, dp->d_cookie);
-	mutex_exit(&hq->hq_u_mutex);
+	viona_tx_done(ring, len, cookie);
 
-	kmem_cache_free(link->l_desb_kmc, dp);
+	mutex_enter(&ring->vr_lock);
+	if ((--ring->vr_xfer_outstanding) == 0) {
+		cv_broadcast(&ring->vr_cv);
+	}
+	mutex_exit(&ring->vr_lock);
+}
 
-	if ((*hq->hq_avail_flags & VRING_AVAIL_F_NO_INTERRUPT) == 0) {
-		if (atomic_cas_uint(&link->l_tx_intr, 0, 1) == 0) {
-			pollwakeup(&link->l_pollhead, POLLOUT);
-		}
+static boolean_t
+viona_tx_csum(viona_link_t *link, const struct virtio_net_hdr *hdr,
+    mblk_t *mp, uint32_t len)
+{
+	const struct ether_header *eth;
+	uint_t eth_len = sizeof (struct ether_header);
+	ushort_t ftype;
+
+	eth = (const struct ether_header *)mp->b_rptr;
+	if (MBLKL(mp) < sizeof (*eth)) {
+		/* Buffers shorter than an ethernet header are hopeless */
+		return (B_FALSE);
+	}
+
+	ftype = ntohs(eth->ether_type);
+	if (ftype == ETHERTYPE_VLAN) {
+		const struct ether_vlan_header *veth;
+
+		/* punt on QinQ for now */
+		eth_len = sizeof (struct ether_vlan_header);
+		veth = (const struct ether_vlan_header *)eth;
+		ftype = ntohs(veth->ether_type);
 	}
-	if (copy_tx_mblks) {
-		mutex_enter(&link->l_tx_mutex);
-		if (--link->l_tx_outstanding == 0) {
-			cv_broadcast(&link->l_tx_cv);
+
+	/*
+	 * Partial checksum support from the NIC is ideal, since it most
+	 * closely maps to the interface defined by virtio.
+	 */
+	if ((link->l_cap_csum & HCKSUM_INET_PARTIAL) != 0) {
+		uint_t start, stuff, end;
+
+		/*
+		 * The lower-level driver is expecting these offsets to be
+		 * relative to the start of the L3 header rather than the
+		 * ethernet frame.
+		 */
+		start = hdr->vrh_csum_start - eth_len;
+		stuff = start + hdr->vrh_csum_offset;
+		end = len - eth_len;
+		mac_hcksum_set(mp, start, stuff, end, 0, HCK_PARTIALCKSUM);
+		return (B_TRUE);
+	}
+
+	/*
+	 * Without partial checksum support, look to the L3/L4 protocol
+	 * information to see if the NIC can handle it.  If not, the
+	 * checksum will need to calculated inline.
+	 */
+	if (ftype == ETHERTYPE_IP) {
+		if ((link->l_cap_csum & HCKSUM_INET_FULL_V4) != 0) {
+			mac_hcksum_set(mp, 0, 0, 0, 0, HCK_FULLCKSUM);
+			return (B_TRUE);
+		}
+
+		/* XXX: Implement manual fallback checksumming? */
+		VIONA_PROBE2(fail_hcksum, viona_link_t *, link, mblk_t *, mp);
+		return (B_FALSE);
+	} else if (ftype == ETHERTYPE_IPV6) {
+		if ((link->l_cap_csum & HCKSUM_INET_FULL_V6) != 0) {
+			mac_hcksum_set(mp, 0, 0, 0, 0, HCK_FULLCKSUM);
+			return (B_TRUE);
 		}
-		mutex_exit(&link->l_tx_mutex);
+
+		/* XXX: Implement manual fallback checksumming? */
+		VIONA_PROBE2(fail_hcksum6, viona_link_t *, link, mblk_t *, mp);
+		return (B_FALSE);
 	}
+
+	/* Cannot even emulate hcksum for unrecognized protocols */
+	VIONA_PROBE2(fail_hcksum_proto, viona_link_t *, link, mblk_t *, mp);
+	return (B_FALSE);
 }
 
 static void
-viona_tx(viona_link_t *link, viona_vring_hqueue_t *hq)
+viona_tx(viona_link_t *link, viona_vring_t *ring)
 {
 	struct iovec		iov[VTNET_MAXSEGS];
 	uint16_t		cookie;
-	int			i, n;
+	int			n;
+	uint32_t		len;
 	mblk_t			*mp_head, *mp_tail, *mp;
-	viona_desb_t		*dp;
+	viona_desb_t		*dp = NULL;
 	mac_client_handle_t	link_mch = link->l_mch;
+	const struct virtio_net_hdr *hdr;
 
 	mp_head = mp_tail = NULL;
 
-	mutex_enter(&hq->hq_a_mutex);
-	n = vq_popchain(link, hq, iov, VTNET_MAXSEGS, &cookie);
-	mutex_exit(&hq->hq_a_mutex);
-	ASSERT(n != 0);
+	n = vq_popchain(ring, iov, VTNET_MAXSEGS, &cookie);
+	if (n <= 0) {
+		VIONA_PROBE1(tx_absent, viona_vring_t *, ring);
+		return;
+	}
 
-	dp = kmem_cache_alloc(link->l_desb_kmc, KM_SLEEP);
-	dp->d_frtn.free_func = viona_desb_free;
-	dp->d_frtn.free_arg = (void *)dp;
-	dp->d_link = link;
-	dp->d_cookie = cookie;
+	if (ring->vr_desb != NULL) {
+		dp = &ring->vr_desb[cookie];
 
-	dp->d_ref = 0;
-	dp->d_len = iov[0].iov_len;
+		/*
+		 * If the guest driver is operating properly, each desb slot
+		 * should be available for use when processing a TX descriptor
+		 * from the 'avail' ring.  In the case of drivers that reuse a
+		 * descriptor before it has been posted to the 'used' ring, the
+		 * data is simply dropped.
+		 */
+		if (atomic_cas_uint(&dp->d_ref, 0, 1) != 0) {
+			dp = NULL;
+			goto drop_fail;
+		}
+		dp->d_cookie = cookie;
+	}
 
-	for (i = 1; i < n; i++) {
-		dp->d_ref++;
-		dp->d_len += iov[i].iov_len;
-		if (copy_tx_mblks) {
+	/* Grab the header and ensure it is of adequate length */
+	hdr = (const struct virtio_net_hdr *)iov[0].iov_base;
+	len = iov[0].iov_len;
+	if (len < sizeof (struct virtio_net_hdr)) {
+		goto drop_fail;
+	}
+
+	for (uint_t i = 1; i < n; i++) {
+		if (dp != NULL) {
 			mp = desballoc((uchar_t *)iov[i].iov_base,
 			    iov[i].iov_len, BPRI_MED, &dp->d_frtn);
-			ASSERT(mp);
+			if (mp == NULL) {
+				goto drop_fail;
+			}
+			dp->d_ref++;
 		} else {
 			mp = allocb(iov[i].iov_len, BPRI_MED);
-			ASSERT(mp);
+			if (mp == NULL) {
+				goto drop_fail;
+			}
 			bcopy((uchar_t *)iov[i].iov_base, mp->b_wptr,
 			    iov[i].iov_len);
 		}
+
+		len += iov[i].iov_len;
 		mp->b_wptr += iov[i].iov_len;
 		if (mp_head == NULL) {
 			ASSERT(mp_tail == NULL);
@@ -1397,13 +2006,69 @@ viona_tx(viona_link_t *link, viona_vring_hqueue_t *hq)
 		}
 		mp_tail = mp;
 	}
-	if (copy_tx_mblks == B_FALSE) {
-		viona_desb_free(dp);
+
+	/* Request hardware checksumming, if necessary */
+	if ((link->l_features & VIRTIO_NET_F_CSUM) != 0 &&
+	    (hdr->vrh_flags & VIRTIO_NET_HDR_F_NEEDS_CSUM) != 0) {
+		if (!viona_tx_csum(link, hdr, mp_head, len - iov[0].iov_len)) {
+			goto drop_fail;
+		}
 	}
-	if (copy_tx_mblks) {
-		mutex_enter(&link->l_tx_mutex);
-		link->l_tx_outstanding++;
-		mutex_exit(&link->l_tx_mutex);
+
+	if (dp != NULL) {
+		dp->d_len = len;
+		mutex_enter(&ring->vr_lock);
+		ring->vr_xfer_outstanding++;
+		mutex_exit(&ring->vr_lock);
+	} else {
+		/*
+		 * If the data was cloned out of the ring, the descriptors can
+		 * be marked as 'used' now, rather than deferring that action
+		 * until after successful packet transmission.
+		 */
+		viona_tx_done(ring, len, cookie);
 	}
+
 	mac_tx(link_mch, mp_head, 0, MAC_DROP_ON_NO_DESC, NULL);
+	return;
+
+drop_fail:
+	/*
+	 * On the off chance that memory is not available via the desballoc or
+	 * allocb calls, there are few options left besides to fail and drop
+	 * the frame on the floor.
+	 */
+
+	if (dp != NULL) {
+		/*
+		 * Take an additional reference on the desb handle (if present)
+		 * so any desballoc-sourced mblks can release their hold on it
+		 * without the handle reaching its final state and executing
+		 * its clean-up logic.
+		 */
+		dp->d_ref++;
+	}
+
+	/*
+	 * Free any already-allocated blocks and sum up the total length of the
+	 * dropped data to be released to the used ring.
+	 */
+	freemsgchain(mp_head);
+	len = 0;
+	for (uint_t i = 0; i < n; i++) {
+		len += iov[i].iov_len;
+	}
+
+	if (dp != NULL) {
+		VERIFY(dp->d_ref == 2);
+
+		/* Clean up the desb handle, releasing the extra hold. */
+		dp->d_len = 0;
+		dp->d_cookie = 0;
+		dp->d_ref = 0;
+	}
+
+	VIONA_PROBE3(tx_drop, viona_vring_t *, ring, uint_t, len,
+	    uint16_t, cookie);
+	viona_tx_done(ring, len, cookie);
 }
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c b/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c
index 39f1a260c8..ce0e8ba607 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c
+++ b/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c
@@ -1409,6 +1409,17 @@ vmm_drv_ioport_unhook(vmm_hold_t *hold, void **cookie)
 	mutex_exit(&vmmdev_mtx);
 }
 
+int
+vmm_drv_msi(vmm_hold_t *hold, uint64_t addr, uint64_t msg)
+{
+	struct vm *vm;
+
+	ASSERT(hold != NULL);
+
+	vm = hold->vmh_sc->vmm_vm;
+	return (lapic_intr_msi(vm, addr, msg));
+}
+
 static int
 vmm_drv_purge(vmm_softc_t *sc)
 {
diff --git a/usr/src/uts/i86pc/sys/viona_io.h b/usr/src/uts/i86pc/sys/viona_io.h
index a26cc00a55..46cc72eb06 100644
--- a/usr/src/uts/i86pc/sys/viona_io.h
+++ b/usr/src/uts/i86pc/sys/viona_io.h
@@ -11,36 +11,53 @@
 
 /*
  * Copyright 2013 Pluribus Networks Inc.
- * Copyright 2017 Joyent, Inc.
+ * Copyright 2018 Joyent, Inc.
  */
 
 #ifndef	_VIONA_IO_H_
 #define	_VIONA_IO_H_
 
 #define	VNA_IOC			(('V' << 16)|('C' << 8))
-#define	VNA_IOC_CREATE		(VNA_IOC | 1)
-#define	VNA_IOC_DELETE		(VNA_IOC | 2)
-#define	VNA_IOC_RX_RING_INIT	(VNA_IOC | 3)
-#define	VNA_IOC_TX_RING_INIT	(VNA_IOC | 4)
-#define	VNA_IOC_RX_RING_RESET	(VNA_IOC | 5)
-#define	VNA_IOC_TX_RING_RESET	(VNA_IOC | 6)
-#define	VNA_IOC_RX_RING_KICK	(VNA_IOC | 7)
-#define	VNA_IOC_TX_RING_KICK	(VNA_IOC | 8)
-#define	VNA_IOC_RX_INTR_CLR	(VNA_IOC | 9)
-#define	VNA_IOC_TX_INTR_CLR	(VNA_IOC | 10)
-#define	VNA_IOC_SET_FEATURES	(VNA_IOC | 11)
-#define	VNA_IOC_GET_FEATURES	(VNA_IOC | 12)
+#define	VNA_IOC_CREATE		(VNA_IOC | 0x01)
+#define	VNA_IOC_DELETE		(VNA_IOC | 0x02)
+
+#define	VNA_IOC_RING_INIT	(VNA_IOC | 0x10)
+#define	VNA_IOC_RING_RESET	(VNA_IOC | 0x11)
+#define	VNA_IOC_RING_KICK	(VNA_IOC | 0x12)
+#define	VNA_IOC_RING_SET_MSI	(VNA_IOC | 0x13)
+#define	VNA_IOC_RING_INTR_CLR	(VNA_IOC | 0x14)
+
+#define	VNA_IOC_INTR_POLL	(VNA_IOC | 0x20)
+#define	VNA_IOC_SET_FEATURES	(VNA_IOC | 0x21)
+#define	VNA_IOC_GET_FEATURES	(VNA_IOC | 0x22)
+#define	VNA_IOC_SET_NOTIFY_IOP	(VNA_IOC | 0x23)
 
 typedef struct vioc_create {
 	datalink_id_t	c_linkid;
-	char		c_vmname[64];
-	size_t		c_lomem_size;
-	size_t		c_himem_size;
+	int		c_vmfd;
 } vioc_create_t;
 
 typedef struct vioc_ring_init {
+	uint16_t	ri_index;
 	uint16_t	ri_qsize;
 	uint64_t	ri_qaddr;
 } vioc_ring_init_t;
 
+typedef struct vioc_ring_msi {
+	uint16_t	rm_index;
+	uint64_t	rm_addr;
+	uint64_t	rm_msg;
+} vioc_ring_msi_t;
+
+enum viona_vq_id {
+	VIONA_VQ_RX = 0,
+	VIONA_VQ_TX = 1,
+	VIONA_VQ_MAX = 2
+};
+
+typedef struct vioc_intr_poll {
+	uint32_t	vip_status[VIONA_VQ_MAX];
+} vioc_intr_poll_t;
+
+
 #endif	/* _VIONA_IO_H_ */
diff --git a/usr/src/uts/i86pc/sys/vmm_drv.h b/usr/src/uts/i86pc/sys/vmm_drv.h
index 4437f06f3a..b883070abf 100644
--- a/usr/src/uts/i86pc/sys/vmm_drv.h
+++ b/usr/src/uts/i86pc/sys/vmm_drv.h
@@ -34,6 +34,7 @@ extern void *vmm_drv_gpa2kva(vmm_hold_t *, uintptr_t, size_t);
 extern int vmm_drv_ioport_hook(vmm_hold_t *, uint_t, vmm_drv_rmem_cb_t,
     vmm_drv_wmem_cb_t, void *, void **);
 extern void vmm_drv_ioport_unhook(vmm_hold_t *, void **);
+extern int vmm_drv_msi(vmm_hold_t *, uint64_t, uint64_t);
 #endif /* _KERNEL */
 
 #endif /* _VMM_DRV_H_ */
diff --git a/usr/src/uts/i86pc/viona/Makefile b/usr/src/uts/i86pc/viona/Makefile
index 4ede5bbd84..cbf0be9539 100644
--- a/usr/src/uts/i86pc/viona/Makefile
+++ b/usr/src/uts/i86pc/viona/Makefile
@@ -11,7 +11,7 @@
 
 #
 # Copyright 2013 Pluribus Networks Inc.
-# Copyright 2017 Joyent, Inc.
+# Copyright 2018 Joyent, Inc.
 #
 
 #
@@ -49,6 +49,9 @@ LINTTAGS	+= -erroff=E_BAD_PTR_CAST_ALIGN
 LINTTAGS	+= -erroff=E_FUNC_RET_MAYBE_IGNORED2
 LINTTAGS	+= -erroff=E_FUNC_RET_ALWAYS_IGNOR2
 
+ALL_BUILDS	= $(ALL_BUILDSONLY64)
+DEF_BUILDS	= $(DEF_BUILDSONLY64)
+
 CFLAGS		+= $(CCVERBOSE)
 LDFLAGS		+= -dy -Ndrv/dld -Nmisc/mac -Nmisc/dls -Ndrv/vmm
 
-- 
2.21.0

