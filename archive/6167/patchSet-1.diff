From 7a0ab66831e462dad715d125985bcc190c439395 Mon Sep 17 00:00:00 2001
From: Patrick Mooney <pmooney@pfmooney.com>
Date: Wed, 24 Apr 2019 14:45:41 +0000
Subject: [PATCH] OS-7753: XXX first try

---
 usr/src/uts/common/brand/lx/os/lx_pid.c       |   1 -
 .../uts/common/brand/lx/procfs/lx_prsubr.c    |   1 -
 usr/src/uts/common/disp/fss.c                 | 233 ++++-----------
 usr/src/uts/common/disp/sysdc.c               |  26 --
 usr/src/uts/common/disp/ts.c                  | 275 ++++++------------
 usr/src/uts/common/fs/lxproc/lxpr_subr.c      |   2 -
 usr/src/uts/common/fs/proc/prsubr.c           |   2 -
 usr/src/uts/common/fs/ufs/ufs_directio.c      |  18 --
 usr/src/uts/common/os/bio.c                   |   1 -
 usr/src/uts/common/os/condvar.c               |   1 -
 usr/src/uts/common/os/pid.c                   |   4 -
 usr/src/uts/common/os/rwlock.c                |  17 +-
 usr/src/uts/common/sys/fss.h                  |   1 -
 usr/src/uts/common/sys/ia.h                   |   1 -
 usr/src/uts/common/sys/thread.h               |  16 +-
 usr/src/uts/common/sys/ts.h                   |   1 -
 usr/src/uts/common/vm/page_lock.c             |   9 -
 usr/src/uts/i86pc/ml/offsets.in               |   1 -
 usr/src/uts/intel/ia32/ml/lock_prim.s         |  10 +-
 usr/src/uts/intel/ia32/os/syscall.c           |   2 -
 usr/src/uts/sfmmu/vm/hat_sfmmu.c              |   2 -
 usr/src/uts/sparc/os/syscall.c                |   2 -
 usr/src/uts/sparc/v9/ml/lock_prim.s           |   8 +-
 usr/src/uts/sun4/ml/offsets.in                |   1 -
 24 files changed, 160 insertions(+), 475 deletions(-)

diff --git a/usr/src/uts/common/brand/lx/os/lx_pid.c b/usr/src/uts/common/brand/lx/os/lx_pid.c
index 8439a23e58..b4b87764fc 100644
--- a/usr/src/uts/common/brand/lx/os/lx_pid.c
+++ b/usr/src/uts/common/brand/lx/os/lx_pid.c
@@ -366,7 +366,6 @@ retry:
 			goto retry;
 		} else {
 			p->p_proc_flag |= P_PR_LOCK;
-			THREAD_KPRI_REQUEST();
 		}
 	}
 
diff --git a/usr/src/uts/common/brand/lx/procfs/lx_prsubr.c b/usr/src/uts/common/brand/lx/procfs/lx_prsubr.c
index 07dc432329..4fee98b257 100644
--- a/usr/src/uts/common/brand/lx/procfs/lx_prsubr.c
+++ b/usr/src/uts/common/brand/lx/procfs/lx_prsubr.c
@@ -350,7 +350,6 @@ lxpr_unlock(proc_t *p)
 	cv_signal(&pr_pid_cv[p->p_slot]);
 	p->p_proc_flag &= ~P_PR_LOCK;
 	mutex_exit(&p->p_lock);
-	THREAD_KPRI_RELEASE();
 }
 
 void
diff --git a/usr/src/uts/common/disp/fss.c b/usr/src/uts/common/disp/fss.c
index 15aeda6d00..7907ed63c4 100644
--- a/usr/src/uts/common/disp/fss.c
+++ b/usr/src/uts/common/disp/fss.c
@@ -1373,8 +1373,6 @@ fss_update_list(int i)
 		 */
 		if (t->t_cid != fss_cid)
 			goto next;
-		if ((fssproc->fss_flags & FSSKPRI) != 0)
-			goto next;
 
 		fssproj = FSSPROC2FSSPROJ(fssproc);
 		if (fssproj == NULL)
@@ -1889,7 +1887,7 @@ fss_fork(kthread_t *pt, kthread_t *ct, void *bufp)
 	cpucaps_sc_init(&cfssproc->fss_caps);
 
 	cfssproc->fss_flags =
-	    pfssproc->fss_flags & ~(FSSKPRI | FSSBACKQ | FSSRESTORE);
+	    pfssproc->fss_flags & ~(FSSBACKQ | FSSRESTORE);
 	ct->t_cldata = (void *)cfssproc;
 	ct->t_schedflag |= TS_RUNQMATCH;
 	thread_unlock(pt);
@@ -1940,7 +1938,6 @@ fss_forkret(kthread_t *t, kthread_t *ct)
 	fssproc->fss_timeleft = fss_quantum;
 	t->t_pri = fssproc->fss_umdpri;
 	ASSERT(t->t_pri >= 0 && t->t_pri <= fss_maxglobpri);
-	fssproc->fss_flags &= ~FSSKPRI;
 	THREAD_TRANSITION(t);
 
 	/*
@@ -2039,11 +2036,6 @@ fss_parmsset(kthread_t *t, void *parmsp, id_t reqpcid, cred_t *reqpcredp)
 	fssproc->fss_nice = nice;
 	fss_newpri(fssproc, B_FALSE);
 
-	if ((fssproc->fss_flags & FSSKPRI) != 0) {
-		thread_unlock(t);
-		return (0);
-	}
-
 	fss_change_priority(t, fssproc);
 	thread_unlock(t);
 	return (0);
@@ -2158,7 +2150,7 @@ fss_swapin(kthread_t *t, int flags)
 		time_t swapout_time;
 
 		swapout_time = (ddi_get_lbolt() - t->t_stime) / hz;
-		if (INHERITED(t) || (fssproc->fss_flags & FSSKPRI)) {
+		if (INHERITED(t)) {
 			epri = (long)DISP_PRIO(t) + swapout_time;
 		} else {
 			/*
@@ -2198,7 +2190,6 @@ fss_swapout(kthread_t *t, int flags)
 	ASSERT(THREAD_LOCK_HELD(t));
 
 	if (INHERITED(t) ||
-	    (fssproc->fss_flags & FSSKPRI) ||
 	    (t->t_proc_flag & TP_LWPEXIT) ||
 	    (t->t_state & (TS_ZOMB|TS_FREE|TS_STOPPED|TS_ONPROC|TS_WAIT)) ||
 	    !(t->t_schedflag & TS_LOAD) ||
@@ -2241,11 +2232,7 @@ fss_swapout(kthread_t *t, int flags)
 }
 
 /*
- * If thread is currently at a kernel mode priority (has slept) and is
- * returning to the userland we assign it the appropriate user mode priority
- * and time quantum here.  If we're lowering the thread's priority below that
- * of other runnable threads then we will set runrun via cpu_surrender() to
- * cause preemption.
+ * Run swap-out checks when returning to userspace.
  */
 static void
 fss_trapret(kthread_t *t)
@@ -2258,20 +2245,6 @@ fss_trapret(kthread_t *t)
 	ASSERT(cp->cpu_dispthread == t);
 	ASSERT(t->t_state == TS_ONPROC);
 
-	t->t_kpri_req = 0;
-	if (fssproc->fss_flags & FSSKPRI) {
-		/*
-		 * If thread has blocked in the kernel
-		 */
-		THREAD_CHANGE_PRI(t, fssproc->fss_umdpri);
-		cp->cpu_dispatch_pri = DISP_PRIO(t);
-		ASSERT(t->t_pri >= 0 && t->t_pri <= fss_maxglobpri);
-		fssproc->fss_flags &= ~FSSKPRI;
-
-		if (DISP_MUST_SURRENDER(t))
-			cpu_surrender(t);
-	}
-
 	/*
 	 * Swapout lwp if the swapper is waiting for this thread to reach
 	 * a safe point.
@@ -2298,19 +2271,6 @@ fss_preempt(kthread_t *t)
 	ASSERT(THREAD_LOCK_HELD(curthread));
 	ASSERT(t->t_state == TS_ONPROC);
 
-	/*
-	 * If preempted in the kernel, make sure the thread has a kernel
-	 * priority if needed.
-	 */
-	lwp = curthread->t_lwp;
-	if (!(fssproc->fss_flags & FSSKPRI) && lwp != NULL && t->t_kpri_req) {
-		fssproc->fss_flags |= FSSKPRI;
-		THREAD_CHANGE_PRI(t, minclsyspri);
-		ASSERT(t->t_pri >= 0 && t->t_pri <= fss_maxglobpri);
-		t->t_trapret = 1;	/* so that fss_trapret will run */
-		aston(t);
-	}
-
 	/*
 	 * This thread may be placed on wait queue by CPU Caps. In this case we
 	 * do not need to do anything until it is removed from the wait queue.
@@ -2320,7 +2280,7 @@ fss_preempt(kthread_t *t)
 		(void) cpucaps_charge(t, &fssproc->fss_caps,
 		    CPUCAPS_CHARGE_ENFORCE);
 
-		if (!(fssproc->fss_flags & FSSKPRI) && CPUCAPS_ENFORCE(t))
+		if (CPUCAPS_ENFORCE(t))
 			return;
 	}
 
@@ -2329,6 +2289,7 @@ fss_preempt(kthread_t *t)
 	 * cannot be holding any kernel locks.
 	 */
 	ASSERT(t->t_schedflag & TS_DONT_SWAP);
+	lwp = ttolwp(t);
 	if (lwp != NULL && lwp->lwp_state == LWP_USER)
 		t->t_schedflag &= ~TS_DONT_SWAP;
 
@@ -2346,18 +2307,16 @@ fss_preempt(kthread_t *t)
 	if (t->t_schedctl && schedctl_get_nopreempt(t)) {
 		if (fssproc->fss_timeleft > -SC_MAX_TICKS) {
 			DTRACE_SCHED1(schedctl__nopreempt, kthread_t *, t);
-			if (!(fssproc->fss_flags & FSSKPRI)) {
-				/*
-				 * If not already remembered, remember current
-				 * priority for restoration in fss_yield().
-				 */
-				if (!(fssproc->fss_flags & FSSRESTORE)) {
-					fssproc->fss_scpri = t->t_pri;
-					fssproc->fss_flags |= FSSRESTORE;
-				}
-				THREAD_CHANGE_PRI(t, fss_maxumdpri);
-				t->t_schedflag |= TS_DONT_SWAP;
+			/*
+			 * If not already remembered, remember current
+			 * priority for restoration in fss_yield().
+			 */
+			if (!(fssproc->fss_flags & FSSRESTORE)) {
+				fssproc->fss_scpri = t->t_pri;
+				fssproc->fss_flags |= FSSRESTORE;
 			}
+			THREAD_CHANGE_PRI(t, fss_maxumdpri);
+			t->t_schedflag |= TS_DONT_SWAP;
 			schedctl_set_yield(t, 1);
 			setfrontdq(t);
 			return;
@@ -2374,15 +2333,12 @@ fss_preempt(kthread_t *t)
 		}
 	}
 
-	flags = fssproc->fss_flags & (FSSBACKQ | FSSKPRI);
+	flags = fssproc->fss_flags & FSSBACKQ;
 
 	if (flags == FSSBACKQ) {
 		fssproc->fss_timeleft = fss_quantum;
 		fssproc->fss_flags &= ~FSSBACKQ;
 		setbackdq(t);
-	} else if (flags == (FSSBACKQ | FSSKPRI)) {
-		fssproc->fss_flags &= ~FSSBACKQ;
-		setbackdq(t);
 	} else {
 		setfrontdq(t);
 	}
@@ -2404,12 +2360,7 @@ fss_setrun(kthread_t *t)
 	fssproc->fss_timeleft = fss_quantum;
 
 	fssproc->fss_flags &= ~FSSBACKQ;
-	/*
-	 * If previously were running at the kernel priority then keep that
-	 * priority and the fss_timeleft doesn't matter.
-	 */
-	if ((fssproc->fss_flags & FSSKPRI) == 0)
-		THREAD_CHANGE_PRI(t, fssproc->fss_umdpri);
+	THREAD_CHANGE_PRI(t, fssproc->fss_umdpri);
 
 	if (t->t_disp_time != ddi_get_lbolt())
 		setbackdq(t);
@@ -2418,8 +2369,7 @@ fss_setrun(kthread_t *t)
 }
 
 /*
- * Prepare thread for sleep. We reset the thread priority so it will run at the
- * kernel priority level when it wakes up.
+ * Prepare thread for sleep.
  */
 static void
 fss_sleep(kthread_t *t)
@@ -2437,31 +2387,6 @@ fss_sleep(kthread_t *t)
 	(void) CPUCAPS_CHARGE(t, &fssproc->fss_caps, CPUCAPS_CHARGE_ENFORCE);
 
 	fss_inactive(t);
-
-	/*
-	 * Assign a system priority to the thread and arrange for it to be
-	 * retained when the thread is next placed on the run queue (i.e.,
-	 * when it wakes up) instead of being given a new pri.  Also arrange
-	 * for trapret processing as the thread leaves the system call so it
-	 * will drop back to normal priority range.
-	 */
-	if (t->t_kpri_req) {
-		THREAD_CHANGE_PRI(t, minclsyspri);
-		fssproc->fss_flags |= FSSKPRI;
-		t->t_trapret = 1;	/* so that fss_trapret will run */
-		aston(t);
-	} else if (fssproc->fss_flags & FSSKPRI) {
-		/*
-		 * The thread has done a THREAD_KPRI_REQUEST(), slept, then
-		 * done THREAD_KPRI_RELEASE() (so no t_kpri_req is 0 again),
-		 * then slept again all without finishing the current system
-		 * call so trapret won't have cleared FSSKPRI
-		 */
-		fssproc->fss_flags &= ~FSSKPRI;
-		THREAD_CHANGE_PRI(t, fssproc->fss_umdpri);
-		if (DISP_MUST_SURRENDER(curthread))
-			cpu_surrender(t);
-	}
 	t->t_stime = ddi_get_lbolt();	/* time stamp for the swapper */
 }
 
@@ -2503,67 +2428,56 @@ fss_tick(kthread_t *t)
 	 * Do not surrender CPU if running in the SYS class.
 	 */
 	if (CPUCAPS_ON()) {
-		cpucaps_enforce = cpucaps_charge(t,
-		    &fssproc->fss_caps, CPUCAPS_CHARGE_ENFORCE) &&
-		    !(fssproc->fss_flags & FSSKPRI);
+		cpucaps_enforce = cpucaps_charge(t, &fssproc->fss_caps,
+		    CPUCAPS_CHARGE_ENFORCE);
 	}
 
-	/*
-	 * A thread's execution time for threads running in the SYS class
-	 * is not tracked.
-	 */
-	if ((fssproc->fss_flags & FSSKPRI) == 0) {
+	if (--fssproc->fss_timeleft <= 0) {
+		pri_t new_pri;
+
 		/*
-		 * If thread is not in kernel mode, decrement its fss_timeleft
+		 * If we're doing preemption control and trying to avoid
+		 * preempting this thread, just note that the thread should
+		 * yield soon and let it keep running (unless it's been a
+		 * while).
 		 */
-		if (--fssproc->fss_timeleft <= 0) {
-			pri_t new_pri;
-
-			/*
-			 * If we're doing preemption control and trying to
-			 * avoid preempting this thread, just note that the
-			 * thread should yield soon and let it keep running
-			 * (unless it's been a while).
-			 */
-			if (t->t_schedctl && schedctl_get_nopreempt(t)) {
-				if (fssproc->fss_timeleft > -SC_MAX_TICKS) {
-					DTRACE_SCHED1(schedctl__nopreempt,
-					    kthread_t *, t);
-					schedctl_set_yield(t, 1);
-					thread_unlock_nopreempt(t);
-					return;
-				}
+		if (t->t_schedctl && schedctl_get_nopreempt(t)) {
+			if (fssproc->fss_timeleft > -SC_MAX_TICKS) {
+				DTRACE_SCHED1(schedctl__nopreempt,
+				    kthread_t *, t);
+				schedctl_set_yield(t, 1);
+				thread_unlock_nopreempt(t);
+				return;
 			}
-			fssproc->fss_flags &= ~FSSRESTORE;
+		}
+		fssproc->fss_flags &= ~FSSRESTORE;
 
-			fss_newpri(fssproc, B_TRUE);
-			new_pri = fssproc->fss_umdpri;
-			ASSERT(new_pri >= 0 && new_pri <= fss_maxglobpri);
+		fss_newpri(fssproc, B_TRUE);
+		new_pri = fssproc->fss_umdpri;
+		ASSERT(new_pri >= 0 && new_pri <= fss_maxglobpri);
 
-			/*
-			 * When the priority of a thread is changed, it may
-			 * be necessary to adjust its position on a sleep queue
-			 * or dispatch queue. The function thread_change_pri
-			 * accomplishes this.
-			 */
-			if (thread_change_pri(t, new_pri, 0)) {
-				if ((t->t_schedflag & TS_LOAD) &&
-				    (lwp = t->t_lwp) &&
-				    lwp->lwp_state == LWP_USER)
-					t->t_schedflag &= ~TS_DONT_SWAP;
-				fssproc->fss_timeleft = fss_quantum;
-			} else {
-				call_cpu_surrender = B_TRUE;
-			}
-		} else if (t->t_state == TS_ONPROC &&
-		    t->t_pri < t->t_disp_queue->disp_maxrunpri) {
-			/*
-			 * If there is a higher-priority thread which is
-			 * waiting for a processor, then thread surrenders
-			 * the processor.
-			 */
+		/*
+		 * When the priority of a thread is changed, it may be
+		 * necessary to adjust its position on a sleep queue or
+		 * dispatch queue. The function thread_change_pri accomplishes
+		 * this.
+		 */
+		if (thread_change_pri(t, new_pri, 0)) {
+			if ((t->t_schedflag & TS_LOAD) &&
+			    (lwp = t->t_lwp) &&
+			    lwp->lwp_state == LWP_USER)
+				t->t_schedflag &= ~TS_DONT_SWAP;
+			fssproc->fss_timeleft = fss_quantum;
+		} else {
 			call_cpu_surrender = B_TRUE;
 		}
+	} else if (t->t_state == TS_ONPROC &&
+	    t->t_pri < t->t_disp_queue->disp_maxrunpri) {
+		/*
+		 * If there is a higher-priority thread which is waiting for a
+		 * processor, then thread surrenders the processor.
+		 */
+		call_cpu_surrender = B_TRUE;
 	}
 
 	if (cpucaps_enforce && 2 * fssproc->fss_timeleft > fss_quantum) {
@@ -2618,32 +2532,13 @@ fss_wakeup(kthread_t *t)
 	fssproc = FSSPROC(t);
 	fssproc->fss_flags &= ~FSSBACKQ;
 
-	if (fssproc->fss_flags & FSSKPRI) {
-		/*
-		 * If we already have a kernel priority assigned, then we
-		 * just use it.
-		 */
-		setbackdq(t);
-	} else if (t->t_kpri_req) {
-		/*
-		 * Give thread a priority boost if we were asked.
-		 */
-		fssproc->fss_flags |= FSSKPRI;
-		THREAD_CHANGE_PRI(t, minclsyspri);
-		setbackdq(t);
-		t->t_trapret = 1;	/* so that fss_trapret will run */
-		aston(t);
+	/* Recalculate the priority. */
+	if (t->t_disp_time == ddi_get_lbolt()) {
+		setfrontdq(t);
 	} else {
-		/*
-		 * Otherwise, we recalculate the priority.
-		 */
-		if (t->t_disp_time == ddi_get_lbolt()) {
-			setfrontdq(t);
-		} else {
-			fssproc->fss_timeleft = fss_quantum;
-			THREAD_CHANGE_PRI(t, fssproc->fss_umdpri);
-			setbackdq(t);
-		}
+		fssproc->fss_timeleft = fss_quantum;
+		THREAD_CHANGE_PRI(t, fssproc->fss_umdpri);
+		setbackdq(t);
 	}
 }
 
diff --git a/usr/src/uts/common/disp/sysdc.c b/usr/src/uts/common/disp/sysdc.c
index 40cde57856..1f50788ceb 100644
--- a/usr/src/uts/common/disp/sysdc.c
+++ b/usr/src/uts/common/disp/sysdc.c
@@ -193,32 +193,6 @@
  *	flag.  This flag currently has no effect, but marks threads which
  *	do bulk processing.
  *
- * - t_kpri_req
- *
- *	The TS and FSS scheduling classes pay attention to t_kpri_req,
- *	which provides a simple form of priority inheritance for
- *	synchronization primitives (such as rwlocks held as READER) which
- *	cannot be traced to a unique thread.  The SDC class does not honor
- *	t_kpri_req, for a few reasons:
- *
- *	1.  t_kpri_req is notoriously inaccurate.  A measure of its
- *	    inaccuracy is that it needs to be cleared every time a thread
- *	    returns to user mode, because it is frequently non-zero at that
- *	    point.  This can happen because "ownership" of synchronization
- *	    primitives that use t_kpri_req can be silently handed off,
- *	    leaving no opportunity to will the t_kpri_req inheritance.
- *
- *	2.  Unlike in TS and FSS, threads in SDC *will* eventually run at
- *	    kernel priority.  This means that even if an SDC thread
- *	    is holding a synchronization primitive and running at low
- *	    priority, its priority will eventually be raised above 60,
- *	    allowing it to drive on and release the resource.
- *
- *	3.  The first consumer of SDC uses the taskq subsystem, which holds
- *	    a reader lock for the duration of the task's execution.  This
- *	    would mean that SDC threads would never drop below kernel
- *	    priority in practice, which defeats one of the purposes of SDC.
- *
  * - Why not FSS?
  *
  *	It might seem that the existing FSS scheduling class could solve
diff --git a/usr/src/uts/common/disp/ts.c b/usr/src/uts/common/disp/ts.c
index bf65c3c42d..233ddfac60 100644
--- a/usr/src/uts/common/disp/ts.c
+++ b/usr/src/uts/common/disp/ts.c
@@ -229,7 +229,6 @@ static void	ia_set_process_group(pid_t, pid_t, pid_t);
 
 static void	ts_change_priority(kthread_t *, tsproc_t *);
 
-extern pri_t	ts_maxkmdpri;	/* maximum kernel mode ts priority */
 static pri_t	ts_maxglobpri;	/* maximum global priority used by ts class */
 static kmutex_t	ts_dptblock;	/* protects time sharing dispatch table */
 static kmutex_t	ts_list_lock[TS_LISTS];	/* protects tsproc lists */
@@ -703,7 +702,7 @@ ts_fork(kthread_t *t, kthread_t *ct, void *bufp)
 	TS_NEWUMDPRI(ctspp);
 	ctspp->ts_nice = ptspp->ts_nice;
 	ctspp->ts_dispwait = 0;
-	ctspp->ts_flags = ptspp->ts_flags & ~(TSKPRI | TSBACKQ | TSRESTORE);
+	ctspp->ts_flags = ptspp->ts_flags & ~(TSBACKQ | TSRESTORE);
 	ctspp->ts_tp = ct;
 	cpucaps_sc_init(&ctspp->ts_caps);
 	thread_unlock(t);
@@ -754,7 +753,6 @@ ts_forkret(kthread_t *t, kthread_t *ct)
 	tspp->ts_dispwait = 0;
 	t->t_pri = ts_dptbl[tspp->ts_umdpri].ts_globpri;
 	ASSERT(t->t_pri >= 0 && t->t_pri <= ts_maxglobpri);
-	tspp->ts_flags &= ~TSKPRI;
 	THREAD_TRANSITION(t);
 	ts_setrun(t);
 	thread_unlock(t);
@@ -1217,11 +1215,6 @@ ts_parmsset(kthread_t *tx, void *parmsp, id_t reqpcid, cred_t *reqpcredp)
 	TS_NEWUMDPRI(tspp);
 	tspp->ts_nice = nice;
 
-	if ((tspp->ts_flags & TSKPRI) != 0) {
-		thread_unlock(tx);
-		return (0);
-	}
-
 	tspp->ts_dispwait = 0;
 	ts_change_priority(tx, tspp);
 	thread_unlock(tx);
@@ -1373,33 +1366,20 @@ static void
 ts_preempt(kthread_t *t)
 {
 	tsproc_t	*tspp = (tsproc_t *)(t->t_cldata);
-	klwp_t		*lwp = curthread->t_lwp;
+	klwp_t		*lwp = ttolwp(t);
 	pri_t		oldpri = t->t_pri;
 
 	ASSERT(t == curthread);
 	ASSERT(THREAD_LOCK_HELD(curthread));
 
-	/*
-	 * If preempted in the kernel, make sure the thread has
-	 * a kernel priority if needed.
-	 */
-	if (!(tspp->ts_flags & TSKPRI) && lwp != NULL && t->t_kpri_req) {
-		tspp->ts_flags |= TSKPRI;
-		THREAD_CHANGE_PRI(t, ts_kmdpris[0]);
-		ASSERT(t->t_pri >= 0 && t->t_pri <= ts_maxglobpri);
-		t->t_trapret = 1;		/* so ts_trapret will run */
-		aston(t);
-	}
-
 	/*
 	 * This thread may be placed on wait queue by CPU Caps. In this case we
 	 * do not need to do anything until it is removed from the wait queue.
-	 * Do not enforce CPU caps on threads running at a kernel priority
 	 */
 	if (CPUCAPS_ON()) {
 		(void) cpucaps_charge(t, &tspp->ts_caps,
 		    CPUCAPS_CHARGE_ENFORCE);
-		if (!(tspp->ts_flags & TSKPRI) && CPUCAPS_ENFORCE(t))
+		if (CPUCAPS_ENFORCE(t))
 			return;
 	}
 
@@ -1425,18 +1405,16 @@ ts_preempt(kthread_t *t)
 	if (t->t_schedctl && schedctl_get_nopreempt(t)) {
 		if (tspp->ts_timeleft > -SC_MAX_TICKS) {
 			DTRACE_SCHED1(schedctl__nopreempt, kthread_t *, t);
-			if (!(tspp->ts_flags & TSKPRI)) {
-				/*
-				 * If not already remembered, remember current
-				 * priority for restoration in ts_yield().
-				 */
-				if (!(tspp->ts_flags & TSRESTORE)) {
-					tspp->ts_scpri = t->t_pri;
-					tspp->ts_flags |= TSRESTORE;
-				}
-				THREAD_CHANGE_PRI(t, ts_maxumdpri);
-				t->t_schedflag |= TS_DONT_SWAP;
+			/*
+			 * If not already remembered, remember current
+			 * priority for restoration in ts_yield().
+			 */
+			if (!(tspp->ts_flags & TSRESTORE)) {
+				tspp->ts_scpri = t->t_pri;
+				tspp->ts_flags |= TSRESTORE;
 			}
+			THREAD_CHANGE_PRI(t, ts_maxumdpri);
+			t->t_schedflag |= TS_DONT_SWAP;
 			schedctl_set_yield(t, 1);
 			setfrontdq(t);
 			goto done;
@@ -1456,14 +1434,11 @@ ts_preempt(kthread_t *t)
 		}
 	}
 
-	if ((tspp->ts_flags & (TSBACKQ|TSKPRI)) == TSBACKQ) {
+	if ((tspp->ts_flags & TSBACKQ) != 0) {
 		tspp->ts_timeleft = ts_dptbl[tspp->ts_cpupri].ts_quantum;
 		tspp->ts_dispwait = 0;
 		tspp->ts_flags &= ~TSBACKQ;
 		setbackdq(t);
-	} else if ((tspp->ts_flags & (TSBACKQ|TSKPRI)) == (TSBACKQ|TSKPRI)) {
-		tspp->ts_flags &= ~TSBACKQ;
-		setbackdq(t);
 	} else {
 		setfrontdq(t);
 	}
@@ -1485,11 +1460,8 @@ ts_setrun(kthread_t *t)
 		TS_NEWUMDPRI(tspp);
 		tspp->ts_timeleft = ts_dptbl[tspp->ts_cpupri].ts_quantum;
 		tspp->ts_dispwait = 0;
-		if ((tspp->ts_flags & TSKPRI) == 0) {
-			THREAD_CHANGE_PRI(t,
-			    ts_dptbl[tspp->ts_umdpri].ts_globpri);
-			ASSERT(t->t_pri >= 0 && t->t_pri <= ts_maxglobpri);
-		}
+		THREAD_CHANGE_PRI(t, ts_dptbl[tspp->ts_umdpri].ts_globpri);
+		ASSERT(t->t_pri >= 0 && t->t_pri <= ts_maxglobpri);
 	}
 
 	tspp->ts_flags &= ~TSBACKQ;
@@ -1509,8 +1481,7 @@ ts_setrun(kthread_t *t)
 
 
 /*
- * Prepare thread for sleep. We reset the thread priority so it will
- * run at the kernel priority level when it wakes up.
+ * Prepare thread for sleep.
  */
 static void
 ts_sleep(kthread_t *t)
@@ -1528,13 +1499,7 @@ ts_sleep(kthread_t *t)
 	(void) CPUCAPS_CHARGE(t, &tspp->ts_caps, CPUCAPS_CHARGE_ENFORCE);
 
 	flags = tspp->ts_flags;
-	if (t->t_kpri_req) {
-		tspp->ts_flags = flags | TSKPRI;
-		THREAD_CHANGE_PRI(t, ts_kmdpris[0]);
-		ASSERT(t->t_pri >= 0 && t->t_pri <= ts_maxglobpri);
-		t->t_trapret = 1;		/* so ts_trapret will run */
-		aston(t);
-	} else if (tspp->ts_dispwait > ts_dptbl[tspp->ts_umdpri].ts_maxwait) {
+	if (tspp->ts_dispwait > ts_dptbl[tspp->ts_umdpri].ts_maxwait) {
 		/*
 		 * If thread has blocked in the kernel (as opposed to
 		 * being merely preempted), recompute the user mode priority.
@@ -1548,16 +1513,6 @@ ts_sleep(kthread_t *t)
 		    ts_dptbl[tspp->ts_umdpri].ts_globpri);
 		ASSERT(curthread->t_pri >= 0 &&
 		    curthread->t_pri <= ts_maxglobpri);
-		tspp->ts_flags = flags & ~TSKPRI;
-
-		if (DISP_MUST_SURRENDER(curthread))
-			cpu_surrender(curthread);
-	} else if (flags & TSKPRI) {
-		THREAD_CHANGE_PRI(curthread,
-		    ts_dptbl[tspp->ts_umdpri].ts_globpri);
-		ASSERT(curthread->t_pri >= 0 &&
-		    curthread->t_pri <= ts_maxglobpri);
-		tspp->ts_flags = flags & ~TSKPRI;
 
 		if (DISP_MUST_SURRENDER(curthread))
 			cpu_surrender(curthread);
@@ -1594,9 +1549,9 @@ ts_swapin(kthread_t *t, int flags)
 		time_t swapout_time;
 
 		swapout_time = (ddi_get_lbolt() - t->t_stime) / hz;
-		if (INHERITED(t) || (tspp->ts_flags & (TSKPRI | TSIASET)))
+		if (INHERITED(t) || (tspp->ts_flags & TSIASET)) {
 			epri = (long)DISP_PRIO(t) + swapout_time;
-		else {
+		} else {
 			/*
 			 * Threads which have been out for a long time,
 			 * have high user mode priority and are associated
@@ -1648,7 +1603,7 @@ ts_swapout(kthread_t *t, int flags)
 
 	ASSERT(THREAD_LOCK_HELD(t));
 
-	if (INHERITED(t) || (tspp->ts_flags & (TSKPRI | TSIASET)) ||
+	if (INHERITED(t) || (tspp->ts_flags & TSIASET) ||
 	    (t->t_proc_flag & TP_LWPEXIT) ||
 	    (t->t_state & (TS_ZOMB | TS_FREE | TS_STOPPED |
 	    TS_ONPROC | TS_WAIT)) ||
@@ -1717,62 +1672,59 @@ ts_tick(kthread_t *t)
 	 */
 	if (CPUCAPS_ON()) {
 		call_cpu_surrender = cpucaps_charge(t, &tspp->ts_caps,
-		    CPUCAPS_CHARGE_ENFORCE) && !(tspp->ts_flags & TSKPRI);
+		    CPUCAPS_CHARGE_ENFORCE);
 	}
 
-	if ((tspp->ts_flags & TSKPRI) == 0) {
-		if (--tspp->ts_timeleft <= 0) {
-			pri_t	new_pri;
+	if (--tspp->ts_timeleft <= 0) {
+		pri_t	new_pri;
 
-			/*
-			 * If we're doing preemption control and trying to
-			 * avoid preempting this thread, just note that
-			 * the thread should yield soon and let it keep
-			 * running (unless it's been a while).
-			 */
-			if (t->t_schedctl && schedctl_get_nopreempt(t)) {
-				if (tspp->ts_timeleft > -SC_MAX_TICKS) {
-					DTRACE_SCHED1(schedctl__nopreempt,
-					    kthread_t *, t);
-					schedctl_set_yield(t, 1);
-					thread_unlock_nopreempt(t);
-					return;
-				}
-
-				TNF_PROBE_2(schedctl_failsafe,
-				    "schedctl TS ts_tick", /* CSTYLED */,
-				    tnf_pid, pid, ttoproc(t)->p_pid,
-				    tnf_lwpid, lwpid, t->t_tid);
-			}
-			tspp->ts_flags &= ~TSRESTORE;
-			tspp->ts_cpupri = ts_dptbl[tspp->ts_cpupri].ts_tqexp;
-			TS_NEWUMDPRI(tspp);
-			tspp->ts_dispwait = 0;
-			new_pri = ts_dptbl[tspp->ts_umdpri].ts_globpri;
-			ASSERT(new_pri >= 0 && new_pri <= ts_maxglobpri);
-			/*
-			 * When the priority of a thread is changed,
-			 * it may be necessary to adjust its position
-			 * on a sleep queue or dispatch queue.
-			 * The function thread_change_pri accomplishes
-			 * this.
-			 */
-			if (thread_change_pri(t, new_pri, 0)) {
-				if ((t->t_schedflag & TS_LOAD) &&
-				    (lwp = t->t_lwp) &&
-				    lwp->lwp_state == LWP_USER)
-					t->t_schedflag &= ~TS_DONT_SWAP;
-				tspp->ts_timeleft =
-				    ts_dptbl[tspp->ts_cpupri].ts_quantum;
-			} else {
-				call_cpu_surrender = B_TRUE;
+		/*
+		 * If we're doing preemption control and trying to avoid
+		 * preempting this thread, just note that the thread should
+		 * yield soon and let it keep running (unless it's been a
+		 * while).
+		 */
+		if (t->t_schedctl && schedctl_get_nopreempt(t)) {
+			if (tspp->ts_timeleft > -SC_MAX_TICKS) {
+				DTRACE_SCHED1(schedctl__nopreempt,
+				    kthread_t *, t);
+				schedctl_set_yield(t, 1);
+				thread_unlock_nopreempt(t);
+				return;
 			}
-			TRACE_2(TR_FAC_DISP, TR_TICK,
-			    "tick:tid %p old pri %d", t, oldpri);
-		} else if (t->t_state == TS_ONPROC &&
-		    t->t_pri < t->t_disp_queue->disp_maxrunpri) {
+
+			TNF_PROBE_2(schedctl_failsafe,
+			    "schedctl TS ts_tick", /* CSTYLED */,
+			    tnf_pid, pid, ttoproc(t)->p_pid,
+			    tnf_lwpid, lwpid, t->t_tid);
+		}
+		tspp->ts_flags &= ~TSRESTORE;
+		tspp->ts_cpupri = ts_dptbl[tspp->ts_cpupri].ts_tqexp;
+		TS_NEWUMDPRI(tspp);
+		tspp->ts_dispwait = 0;
+		new_pri = ts_dptbl[tspp->ts_umdpri].ts_globpri;
+		ASSERT(new_pri >= 0 && new_pri <= ts_maxglobpri);
+		/*
+		 * When the priority of a thread is changed, it may be
+		 * necessary to adjust its position on a sleep queue or
+		 * dispatch queue.  The function thread_change_pri accomplishes
+		 * this.
+		 */
+		if (thread_change_pri(t, new_pri, 0)) {
+			if ((t->t_schedflag & TS_LOAD) &&
+			    (lwp = t->t_lwp) &&
+			    lwp->lwp_state == LWP_USER)
+				t->t_schedflag &= ~TS_DONT_SWAP;
+			tspp->ts_timeleft =
+			    ts_dptbl[tspp->ts_cpupri].ts_quantum;
+		} else {
 			call_cpu_surrender = B_TRUE;
 		}
+		TRACE_2(TR_FAC_DISP, TR_TICK,
+		    "tick:tid %p old pri %d", t, oldpri);
+	} else if (t->t_state == TS_ONPROC &&
+	    t->t_pri < t->t_disp_queue->disp_maxrunpri) {
+		call_cpu_surrender = B_TRUE;
 	}
 
 	if (call_cpu_surrender) {
@@ -1785,11 +1737,8 @@ ts_tick(kthread_t *t)
 
 
 /*
- * If thread is currently at a kernel mode priority (has slept)
- * we assign it the appropriate user mode priority and time quantum
- * here.  If we are lowering the thread's priority below that of
- * other runnable threads we will normally set runrun via cpu_surrender() to
- * cause preemption.
+ * If we are lowering the thread's priority below that of other runnable
+ * threads we will normally set runrun via cpu_surrender() to cause preemption.
  */
 static void
 ts_trapret(kthread_t *t)
@@ -1803,7 +1752,6 @@ ts_trapret(kthread_t *t)
 	ASSERT(cp->cpu_dispthread == t);
 	ASSERT(t->t_state == TS_ONPROC);
 
-	t->t_kpri_req = 0;
 	if (tspp->ts_dispwait > ts_dptbl[tspp->ts_umdpri].ts_maxwait) {
 		tspp->ts_cpupri = ts_dptbl[tspp->ts_cpupri].ts_slpret;
 		TS_NEWUMDPRI(tspp);
@@ -1817,27 +1765,14 @@ ts_trapret(kthread_t *t)
 		THREAD_CHANGE_PRI(t, ts_dptbl[tspp->ts_umdpri].ts_globpri);
 		cp->cpu_dispatch_pri = DISP_PRIO(t);
 		ASSERT(t->t_pri >= 0 && t->t_pri <= ts_maxglobpri);
-		tspp->ts_flags &= ~TSKPRI;
-
-		if (DISP_MUST_SURRENDER(t))
-			cpu_surrender(t);
-	} else if (tspp->ts_flags & TSKPRI) {
-		/*
-		 * If thread has blocked in the kernel (as opposed to
-		 * being merely preempted), recompute the user mode priority.
-		 */
-		THREAD_CHANGE_PRI(t, ts_dptbl[tspp->ts_umdpri].ts_globpri);
-		cp->cpu_dispatch_pri = DISP_PRIO(t);
-		ASSERT(t->t_pri >= 0 && t->t_pri <= ts_maxglobpri);
-		tspp->ts_flags &= ~TSKPRI;
 
 		if (DISP_MUST_SURRENDER(t))
 			cpu_surrender(t);
 	}
 
 	/*
-	 * Swapout lwp if the swapper is waiting for this thread to
-	 * reach a safe point.
+	 * Swapout lwp if the swapper is waiting for this thread to reach a
+	 * safe point.
 	 */
 	if ((t->t_schedflag & TS_SWAPENQ) && !(tspp->ts_flags & TSIASET)) {
 		thread_unlock(t);
@@ -1931,8 +1866,6 @@ ts_update_list(int i)
 		    tx->t_clfuncs != &ia_classfuncs.thread)
 			goto next;
 		tspp->ts_dispwait++;
-		if ((tspp->ts_flags & TSKPRI) != 0)
-			goto next;
 		if (tspp->ts_dispwait <= ts_dptbl[tspp->ts_umdpri].ts_maxwait)
 			goto next;
 		if (tx->t_schedctl && schedctl_get_nopreempt(tx))
@@ -1968,12 +1901,7 @@ next:
 }
 
 /*
- * Processes waking up go to the back of their queue.  We don't
- * need to assign a time quantum here because thread is still
- * at a kernel mode priority and the time slicing is not done
- * for threads running in the kernel after sleeping.  The proper
- * time quantum will be assigned by ts_trapret before the thread
- * returns to user mode.
+ * Processes waking up go to the back of their queue.
  */
 static void
 ts_wakeup(kthread_t *t)
@@ -1984,46 +1912,27 @@ ts_wakeup(kthread_t *t)
 
 	t->t_stime = ddi_get_lbolt();		/* time stamp for the swapper */
 
-	if (tspp->ts_flags & TSKPRI) {
-		tspp->ts_flags &= ~TSBACKQ;
+	if (tspp->ts_dispwait > ts_dptbl[tspp->ts_umdpri].ts_maxwait) {
+		tspp->ts_cpupri = ts_dptbl[tspp->ts_cpupri].ts_slpret;
+		TS_NEWUMDPRI(tspp);
+		tspp->ts_timeleft = ts_dptbl[tspp->ts_cpupri].ts_quantum;
+		tspp->ts_dispwait = 0;
+		THREAD_CHANGE_PRI(t, ts_dptbl[tspp->ts_umdpri].ts_globpri);
+		ASSERT(t->t_pri >= 0 && t->t_pri <= ts_maxglobpri);
+	}
+
+	tspp->ts_flags &= ~TSBACKQ;
+
+	if (tspp->ts_flags & TSIA) {
 		if (tspp->ts_flags & TSIASET)
 			setfrontdq(t);
 		else
 			setbackdq(t);
-	} else if (t->t_kpri_req) {
-		/*
-		 * Give thread a priority boost if we were asked.
-		 */
-		tspp->ts_flags |= TSKPRI;
-		THREAD_CHANGE_PRI(t, ts_kmdpris[0]);
-		setbackdq(t);
-		t->t_trapret = 1;	/* so that ts_trapret will run */
-		aston(t);
 	} else {
-		if (tspp->ts_dispwait > ts_dptbl[tspp->ts_umdpri].ts_maxwait) {
-			tspp->ts_cpupri = ts_dptbl[tspp->ts_cpupri].ts_slpret;
-			TS_NEWUMDPRI(tspp);
-			tspp->ts_timeleft =
-			    ts_dptbl[tspp->ts_cpupri].ts_quantum;
-			tspp->ts_dispwait = 0;
-			THREAD_CHANGE_PRI(t,
-			    ts_dptbl[tspp->ts_umdpri].ts_globpri);
-			ASSERT(t->t_pri >= 0 && t->t_pri <= ts_maxglobpri);
-		}
-
-		tspp->ts_flags &= ~TSBACKQ;
-
-		if (tspp->ts_flags & TSIA) {
-			if (tspp->ts_flags & TSIASET)
-				setfrontdq(t);
-			else
-				setbackdq(t);
-		} else {
-			if (t->t_disp_time != ddi_get_lbolt())
-				setbackdq(t);
-			else
-				setfrontdq(t);
-		}
+		if (t->t_disp_time != ddi_get_lbolt())
+			setbackdq(t);
+		else
+			setfrontdq(t);
 	}
 }
 
@@ -2291,10 +2200,6 @@ ia_set_process_group(pid_t sid, pid_t bg_pgid, pid_t fg_pgid)
 			tspp->ts_flags |= TSIASET;
 			tspp->ts_boost = ia_boost;
 			TS_NEWUMDPRI(tspp);
-			if ((tspp->ts_flags & TSKPRI) != 0) {
-				thread_unlock(tx);
-				continue;
-			}
 			tspp->ts_dispwait = 0;
 			ts_change_priority(tx, tspp);
 			thread_unlock(tx);
@@ -2344,10 +2249,6 @@ skip:
 			tspp->ts_flags &= ~TSIASET;
 			tspp->ts_boost = -ia_boost;
 			TS_NEWUMDPRI(tspp);
-			if ((tspp->ts_flags & TSKPRI) != 0) {
-				thread_unlock(tx);
-				continue;
-			}
 
 			tspp->ts_dispwait = 0;
 			ts_change_priority(tx, tspp);
diff --git a/usr/src/uts/common/fs/lxproc/lxpr_subr.c b/usr/src/uts/common/fs/lxproc/lxpr_subr.c
index 24c010a463..265b6e605a 100644
--- a/usr/src/uts/common/fs/lxproc/lxpr_subr.c
+++ b/usr/src/uts/common/fs/lxproc/lxpr_subr.c
@@ -232,7 +232,6 @@ lxpr_lock(pid_t pid)
 	}
 
 	p->p_proc_flag |= P_PR_LOCK;
-	THREAD_KPRI_REQUEST();
 	return (p);
 }
 
@@ -251,7 +250,6 @@ lxpr_unlock(proc_t *p)
 	cv_signal(&pr_pid_cv[p->p_slot]);
 	p->p_proc_flag &= ~P_PR_LOCK;
 	mutex_exit(&p->p_lock);
-	THREAD_KPRI_RELEASE();
 }
 
 void
diff --git a/usr/src/uts/common/fs/proc/prsubr.c b/usr/src/uts/common/fs/proc/prsubr.c
index 3b4a7f36d0..eff63aaefc 100644
--- a/usr/src/uts/common/fs/proc/prsubr.c
+++ b/usr/src/uts/common/fs/proc/prsubr.c
@@ -715,7 +715,6 @@ pr_p_lock(prnode_t *pnp)
 		mutex_enter(&p->p_lock);
 	}
 	p->p_proc_flag |= P_PR_LOCK;
-	THREAD_KPRI_REQUEST();
 	return (p);
 }
 
@@ -822,7 +821,6 @@ prunmark(proc_t *p)
 
 	cv_signal(&pr_pid_cv[p->p_slot]);
 	p->p_proc_flag &= ~P_PR_LOCK;
-	THREAD_KPRI_RELEASE();
 }
 
 void
diff --git a/usr/src/uts/common/fs/ufs/ufs_directio.c b/usr/src/uts/common/fs/ufs/ufs_directio.c
index 940bd964f4..eff1b98436 100644
--- a/usr/src/uts/common/fs/ufs/ufs_directio.c
+++ b/usr/src/uts/common/fs/ufs/ufs_directio.c
@@ -221,40 +221,22 @@ directio_wait_one(struct directio_buf *dbp, long *bytes_iop)
  * Wait for all of the direct IO operations to finish
  */
 
-uint32_t	ufs_directio_drop_kpri = 0;	/* enable kpri hack */
-
 static int
 directio_wait(struct directio_buf *tail, long *bytes_iop)
 {
 	int	error = 0, newerror;
 	struct directio_buf	*dbp;
-	uint_t	kpri_req_save;
 
 	/*
 	 * The linked list of directio buf structures is maintained
 	 * in reverse order (tail->last request->penultimate request->...)
 	 */
-	/*
-	 * This is the k_pri_req hack. Large numbers of threads
-	 * sleeping with kernel priority will cause scheduler thrashing
-	 * on an MP machine. This can be seen running Oracle using
-	 * directio to ufs files. Sleep at normal priority here to
-	 * more closely mimic physio to a device partition. This
-	 * workaround is disabled by default as a niced thread could
-	 * be starved from running while holding i_rwlock and i_contents.
-	 */
-	if (ufs_directio_drop_kpri) {
-		kpri_req_save = curthread->t_kpri_req;
-		curthread->t_kpri_req = 0;
-	}
 	while ((dbp = tail) != NULL) {
 		tail = dbp->next;
 		newerror = directio_wait_one(dbp, bytes_iop);
 		if (error == 0)
 			error = newerror;
 	}
-	if (ufs_directio_drop_kpri)
-		curthread->t_kpri_req = kpri_req_save;
 	return (error);
 }
 /*
diff --git a/usr/src/uts/common/os/bio.c b/usr/src/uts/common/os/bio.c
index b5b2bee298..4853a82ed0 100644
--- a/usr/src/uts/common/os/bio.c
+++ b/usr/src/uts/common/os/bio.c
@@ -1380,7 +1380,6 @@ pageio_setup(struct page *pp, size_t len, struct vnode *vp, int flags)
 
 	VN_HOLD(vp);
 	bp->b_vp = vp;
-	THREAD_KPRI_RELEASE_N(btopr(len)); /* release kpri from page_locks */
 
 	/*
 	 * Caller sets dev & blkno and can adjust
diff --git a/usr/src/uts/common/os/condvar.c b/usr/src/uts/common/os/condvar.c
index e9c418ffbd..acb1b01e21 100644
--- a/usr/src/uts/common/os/condvar.c
+++ b/usr/src/uts/common/os/condvar.c
@@ -552,7 +552,6 @@ cv_wait_sig_swap_core(kcondvar_t *cvp, kmutex_t *mp, int *sigret)
 	lwp->lwp_asleep = 1;
 	lwp->lwp_sysabort = 0;
 	thread_lock(t);
-	t->t_kpri_req = 0;	/* don't need kernel priority */
 	cv_block_sig(t, (condvar_impl_t *)cvp);
 	/* I can be swapped now */
 	curthread->t_schedflag &= ~TS_DONT_SWAP;
diff --git a/usr/src/uts/common/os/pid.c b/usr/src/uts/common/os/pid.c
index eba6147fab..3a95cbbd3e 100644
--- a/usr/src/uts/common/os/pid.c
+++ b/usr/src/uts/common/os/pid.c
@@ -430,7 +430,6 @@ sprtrylock_proc(proc_t *p)
 		return (1);
 
 	p->p_proc_flag |= P_PR_LOCK;
-	THREAD_KPRI_REQUEST();
 
 	return (0);
 }
@@ -515,7 +514,6 @@ sprlock_proc(proc_t *p)
 	}
 
 	p->p_proc_flag |= P_PR_LOCK;
-	THREAD_KPRI_REQUEST();
 }
 
 void
@@ -532,7 +530,6 @@ sprunlock(proc_t *p)
 	cv_signal(&pr_pid_cv[p->p_slot]);
 	p->p_proc_flag &= ~P_PR_LOCK;
 	mutex_exit(&p->p_lock);
-	THREAD_KPRI_RELEASE();
 }
 
 /*
@@ -546,7 +543,6 @@ sprunprlock(proc_t *p)
 
 	cv_signal(&pr_pid_cv[p->p_slot]);
 	p->p_proc_flag &= ~P_PR_LOCK;
-	THREAD_KPRI_RELEASE();
 }
 
 void
diff --git a/usr/src/uts/common/os/rwlock.c b/usr/src/uts/common/os/rwlock.c
index f851686ce0..8e6858b8ae 100644
--- a/usr/src/uts/common/os/rwlock.c
+++ b/usr/src/uts/common/os/rwlock.c
@@ -269,9 +269,6 @@ void (*rw_lock_delay)(uint_t) = NULL;
 /*
  * Full-service implementation of rw_enter() to handle all the hard cases.
  * Called from the assembly version if anything complicated is going on.
- * The only semantic difference between calling rw_enter() and calling
- * rw_enter_sleep() directly is that we assume the caller has already done
- * a THREAD_KPRI_REQUEST() in the RW_READER cases.
  */
 void
 rw_enter_sleep(rwlock_impl_t *lp, krw_t rw)
@@ -342,15 +339,13 @@ rw_enter_sleep(rwlock_impl_t *lp, krw_t rw)
 		}
 
 		/*
-		 * We really are going to block.  Bump the stats, and drop
-		 * kpri if we're a reader.
+		 * We really are going to block, so bump the stats.
 		 */
 		ASSERT(lp->rw_wwwh & lock_wait);
 		ASSERT(lp->rw_wwwh & RW_LOCKED);
 
 		sleep_time = -gethrtime();
 		if (rw != RW_WRITER) {
-			THREAD_KPRI_RELEASE();
 			CPU_STATS_ADDQ(CPU, sys, rw_rdfails, 1);
 			(void) turnstile_block(ts, TS_READER_Q, lp,
 			    &rw_sobj_ops, NULL, NULL);
@@ -366,8 +361,8 @@ rw_enter_sleep(rwlock_impl_t *lp, krw_t rw)
 		    old >> RW_HOLD_COUNT_SHIFT);
 
 		/*
-		 * We wake up holding the lock (and having kpri if we're
-		 * a reader) via direct handoff from the previous owner.
+		 * We wake up holding the lock via direct handoff from the
+		 * previous owner.
 		 */
 		break;
 	}
@@ -394,7 +389,6 @@ rw_readers_to_wake(turnstile_t *ts)
 	while (next_reader != NULL) {
 		if (DISP_PRIO(next_reader) < wpri)
 			break;
-		next_reader->t_kpri_req++;
 		next_reader = next_reader->t_link;
 		count++;
 	}
@@ -523,7 +517,6 @@ rw_exit_wakeup(rwlock_impl_t *lp)
 	}
 
 	if (lock_value == RW_READ_LOCK) {
-		THREAD_KPRI_RELEASE();
 		LOCKSTAT_RECORD(LS_RW_EXIT_RELEASE, lp, RW_READER);
 	} else {
 		LOCKSTAT_RECORD(LS_RW_EXIT_RELEASE, lp, RW_WRITER);
@@ -539,11 +532,9 @@ rw_tryenter(krwlock_t *rwlp, krw_t rw)
 	if (rw != RW_WRITER) {
 		uint_t backoff = 0;
 		int loop_count = 0;
-		THREAD_KPRI_REQUEST();
 		for (;;) {
 			if ((old = lp->rw_wwwh) & (rw == RW_READER ?
 			    RW_WRITE_CLAIMED : RW_WRITE_LOCKED)) {
-				THREAD_KPRI_RELEASE();
 				return (0);
 			}
 			if (casip(&lp->rw_wwwh, old, old + RW_READ_LOCK) == old)
@@ -573,7 +564,6 @@ rw_downgrade(krwlock_t *rwlp)
 {
 	rwlock_impl_t *lp = (rwlock_impl_t *)rwlp;
 
-	THREAD_KPRI_REQUEST();
 	membar_exit();
 
 	if ((lp->rw_wwwh & RW_OWNER) != (uintptr_t)curthread) {
@@ -612,7 +602,6 @@ rw_tryupgrade(krwlock_t *rwlp)
 	} while (casip(&lp->rw_wwwh, old, new) != old);
 
 	membar_enter();
-	THREAD_KPRI_RELEASE();
 	LOCKSTAT_RECORD0(LS_RW_TRYUPGRADE_UPGRADE, lp);
 	ASSERT(rw_locked(lp, RW_WRITER));
 	return (1);
diff --git a/usr/src/uts/common/sys/fss.h b/usr/src/uts/common/sys/fss.h
index e73dd5c0e8..40bff5d7d0 100644
--- a/usr/src/uts/common/sys/fss.h
+++ b/usr/src/uts/common/sys/fss.h
@@ -160,7 +160,6 @@ typedef struct fsszone {
 /*
  * fss_flags
  */
-#define	FSSKPRI		0x01	/* the thread is in kernel mode	*/
 #define	FSSBACKQ	0x02	/* thread should be placed at the back of */
 				/* the dispatch queue if preempted */
 #define	FSSRESTORE	0x04	/* thread was not preempted, due to schedctl */
diff --git a/usr/src/uts/common/sys/ia.h b/usr/src/uts/common/sys/ia.h
index 26c1002134..f263b5e816 100644
--- a/usr/src/uts/common/sys/ia.h
+++ b/usr/src/uts/common/sys/ia.h
@@ -85,7 +85,6 @@ typedef struct iaproc {
 
 
 /* flags */
-#define	IAKPRI	0x01	/* thread at kernel mode priority */
 #define	IABACKQ	0x02	/* thread goes to back of disp q when preempted */
 #define	IASLEPT	0x04	/* thread had long-term suspend - give new slice */
 
diff --git a/usr/src/uts/common/sys/thread.h b/usr/src/uts/common/sys/thread.h
index 6cc474f864..4fc206a37c 100644
--- a/usr/src/uts/common/sys/thread.h
+++ b/usr/src/uts/common/sys/thread.h
@@ -206,7 +206,7 @@ typedef struct _kthread {
 	lock_t		t_lock_flush;	/* for lock_mutex_flush() impl */
 	struct _disp	*t_disp_queue;	/* run queue for chosen CPU */
 	clock_t		t_disp_time;	/* last time this thread was running */
-	uint_t		t_kpri_req;	/* kernel priority required */
+	uint_t		_t_kpri_req;	/* kernel priority required */
 
 	/*
 	 * Post-syscall / post-trap flags.
@@ -617,20 +617,6 @@ extern int default_stksize;
 
 #define	THREAD_NAME_MAX	32	/* includes terminating NUL */
 
-/*
- * Macros to indicate that the thread holds resources that could be critical
- * to other kernel threads, so this thread needs to have kernel priority
- * if it blocks or is preempted.  Note that this is not necessary if the
- * resource is a mutex or a writer lock because of priority inheritance.
- *
- * The only way one thread may legally manipulate another thread's t_kpri_req
- * is to hold the target thread's thread lock while that thread is asleep.
- * (The rwlock code does this to implement direct handoff to waiting readers.)
- */
-#define	THREAD_KPRI_REQUEST()	(curthread->t_kpri_req++)
-#define	THREAD_KPRI_RELEASE()	(curthread->t_kpri_req--)
-#define	THREAD_KPRI_RELEASE_N(n) (curthread->t_kpri_req -= (n))
-
 /*
  * Macro to change a thread's priority.
  */
diff --git a/usr/src/uts/common/sys/ts.h b/usr/src/uts/common/sys/ts.h
index 266d63a3ea..50388b94c4 100644
--- a/usr/src/uts/common/sys/ts.h
+++ b/usr/src/uts/common/sys/ts.h
@@ -78,7 +78,6 @@ typedef struct tsproc {
 } tsproc_t;
 
 /* flags */
-#define	TSKPRI		0x01	/* thread at kernel mode priority	*/
 #define	TSBACKQ		0x02	/* thread goes to back of dispq if preempted */
 #define	TSIA		0x04	/* thread is interactive		*/
 #define	TSIASET		0x08	/* interactive thread is "on"		*/
diff --git a/usr/src/uts/common/vm/page_lock.c b/usr/src/uts/common/vm/page_lock.c
index 7305c9c85a..166b995c99 100644
--- a/usr/src/uts/common/vm/page_lock.c
+++ b/usr/src/uts/common/vm/page_lock.c
@@ -364,7 +364,6 @@ page_lock_es(page_t *pp, se_t se, kmutex_t *lock, reclaim_t reclaim, int es)
 			retval = 0;
 		} else if ((pp->p_selock & ~SE_EWANTED) == 0) {
 			/* no reader/writer lock held */
-			THREAD_KPRI_REQUEST();
 			/* this clears our setting of the SE_EWANTED bit */
 			pp->p_selock = SE_WRITER;
 			retval = 1;
@@ -551,7 +550,6 @@ page_try_reclaim_lock(page_t *pp, se_t se, int es)
 	if (!(old & SE_EWANTED) || (es & SE_EXCL_WANTED)) {
 		if ((old & ~SE_EWANTED) == 0) {
 			/* no reader/writer lock held */
-			THREAD_KPRI_REQUEST();
 			/* this clears out our setting of the SE_EWANTED bit */
 			pp->p_selock = SE_WRITER;
 			mutex_exit(pse);
@@ -590,7 +588,6 @@ page_trylock(page_t *pp, se_t se)
 
 	if (se == SE_EXCL) {
 		if (pp->p_selock == 0) {
-			THREAD_KPRI_REQUEST();
 			pp->p_selock = SE_WRITER;
 			mutex_exit(pse);
 			return (1);
@@ -628,7 +625,6 @@ page_unlock_nocapture(page_t *pp)
 	} else if ((old & ~SE_EWANTED) == SE_DELETED) {
 		panic("page_unlock_nocapture: page %p is deleted", (void *)pp);
 	} else if (old < 0) {
-		THREAD_KPRI_RELEASE();
 		pp->p_selock &= SE_EWANTED;
 		if (CV_HAS_WAITERS(&pp->p_cv))
 			cv_broadcast(&pp->p_cv);
@@ -662,7 +658,6 @@ page_unlock(page_t *pp)
 	} else if ((old & ~SE_EWANTED) == SE_DELETED) {
 		panic("page_unlock: page %p is deleted", (void *)pp);
 	} else if (old < 0) {
-		THREAD_KPRI_RELEASE();
 		pp->p_selock &= SE_EWANTED;
 		if (CV_HAS_WAITERS(&pp->p_cv))
 			cv_broadcast(&pp->p_cv);
@@ -682,7 +677,6 @@ page_unlock(page_t *pp)
 		if ((pp->p_toxic & PR_CAPTURE) &&
 		    !(curthread->t_flag & T_CAPTURING) &&
 		    !PP_RETIRED(pp)) {
-			THREAD_KPRI_REQUEST();
 			pp->p_selock = SE_WRITER;
 			mutex_exit(pse);
 			page_unlock_capture(pp);
@@ -712,7 +706,6 @@ page_tryupgrade(page_t *pp)
 	if (!(pp->p_selock & SE_EWANTED)) {
 		/* no threads want exclusive access, try upgrade */
 		if (pp->p_selock == SE_READER) {
-			THREAD_KPRI_REQUEST();
 			/* convert to exclusive lock */
 			pp->p_selock = SE_WRITER;
 			mutex_exit(pse);
@@ -738,7 +731,6 @@ page_downgrade(page_t *pp)
 
 	mutex_enter(pse);
 	excl_waiting =  pp->p_selock & SE_EWANTED;
-	THREAD_KPRI_RELEASE();
 	pp->p_selock = SE_READER | excl_waiting;
 	if (CV_HAS_WAITERS(&pp->p_cv))
 		cv_broadcast(&pp->p_cv);
@@ -756,7 +748,6 @@ page_lock_delete(page_t *pp)
 	ASSERT(!PP_ISFREE(pp));
 
 	mutex_enter(pse);
-	THREAD_KPRI_RELEASE();
 	pp->p_selock = SE_DELETED;
 	if (CV_HAS_WAITERS(&pp->p_cv))
 		cv_broadcast(&pp->p_cv);
diff --git a/usr/src/uts/i86pc/ml/offsets.in b/usr/src/uts/i86pc/ml/offsets.in
index 8dc6f0f572..ed8f21a167 100644
--- a/usr/src/uts/i86pc/ml/offsets.in
+++ b/usr/src/uts/i86pc/ml/offsets.in
@@ -88,7 +88,6 @@ _kthread	THREAD_SIZE
 	t_lockstat
 	t_lockp
 	t_lock_flush
-	t_kpri_req
 	t_oldspl
 	t_pri
 	t_pil
diff --git a/usr/src/uts/intel/ia32/ml/lock_prim.s b/usr/src/uts/intel/ia32/ml/lock_prim.s
index 884ca02de8..7d9ee80be7 100644
--- a/usr/src/uts/intel/ia32/ml/lock_prim.s
+++ b/usr/src/uts/intel/ia32/ml/lock_prim.s
@@ -916,10 +916,8 @@ rw_exit(krwlock_t *lp)
 #if defined(__amd64)
 
 	ENTRY(rw_enter)
-	movq	%gs:CPU_THREAD, %rdx		/* rdx = thread ptr */
 	cmpl	$RW_WRITER, %esi
 	je	.rw_write_enter
-	incl	T_KPRI_REQ(%rdx)		/* THREAD_KPRI_REQUEST() */
 	movq	(%rdi), %rax			/* rax = old rw_wwwh value */
 	testl	$RW_WRITE_LOCKED|RW_WRITE_WANTED, %eax
 	jnz	rw_enter_sleep
@@ -935,6 +933,7 @@ rw_exit(krwlock_t *lp)
 	movl	$RW_READER, %edx
 	jmp	lockstat_wrapper_arg
 .rw_write_enter:
+	movq	%gs:CPU_THREAD, %rdx
 	orq	$RW_WRITE_LOCKED, %rdx		/* rdx = write-locked value */
 	xorl	%eax, %eax			/* rax = unheld value */
 	lock
@@ -970,8 +969,6 @@ rw_exit(krwlock_t *lp)
 	lock
 	cmpxchgq %rdx, (%rdi)			/* try to drop read lock */
 	jnz	rw_exit_wakeup
-	movq	%gs:CPU_THREAD, %rcx		/* rcx = thread ptr */
-	decl	T_KPRI_REQ(%rcx)		/* THREAD_KPRI_RELEASE() */
 .rw_read_exit_lockstat_patch_point:
 	ret
 	movq	%rdi, %rsi			/* rsi = lock ptr */
@@ -1004,11 +1001,9 @@ rw_exit(krwlock_t *lp)
 #else
 
 	ENTRY(rw_enter)
-	movl	%gs:CPU_THREAD, %edx		/* edx = thread ptr */
 	movl	4(%esp), %ecx			/* ecx = lock ptr */
 	cmpl	$RW_WRITER, 8(%esp)
 	je	.rw_write_enter
-	incl	T_KPRI_REQ(%edx)		/* THREAD_KPRI_REQUEST() */
 	movl	(%ecx), %eax			/* eax = old rw_wwwh value */
 	testl	$RW_WRITE_LOCKED|RW_WRITE_WANTED, %eax
 	jnz	rw_enter_sleep
@@ -1023,6 +1018,7 @@ rw_exit(krwlock_t *lp)
 	pushl	$RW_READER
 	jmp	lockstat_wrapper_arg
 .rw_write_enter:
+	movl	%gs:CPU_THREAD, %edx
 	orl	$RW_WRITE_LOCKED, %edx		/* edx = write-locked value */
 	xorl	%eax, %eax			/* eax = unheld value */
 	lock
@@ -1058,8 +1054,6 @@ rw_exit(krwlock_t *lp)
 	lock
 	cmpxchgl %edx, (%ecx)			/* try to drop read lock */
 	jnz	rw_exit_wakeup
-	movl	%gs:CPU_THREAD, %edx		/* edx = thread ptr */
-	decl	T_KPRI_REQ(%edx)		/* THREAD_KPRI_RELEASE() */
 .rw_read_exit_lockstat_patch_point:
 	ret
 	movl	$LS_RW_EXIT_RELEASE, %eax
diff --git a/usr/src/uts/intel/ia32/os/syscall.c b/usr/src/uts/intel/ia32/os/syscall.c
index 3aa32ad95b..4d4c14f9e7 100644
--- a/usr/src/uts/intel/ia32/os/syscall.c
+++ b/usr/src/uts/intel/ia32/os/syscall.c
@@ -1202,7 +1202,6 @@ loadable_syscall(
 	 * Try to autoload the system call if necessary
 	 */
 	module_lock = lock_syscall(se, code);
-	THREAD_KPRI_RELEASE();	/* drop priority given by rw_enter */
 
 	/*
 	 * we've locked either the loaded syscall or nosys
@@ -1234,7 +1233,6 @@ loadable_syscall(
 		}
 	}
 
-	THREAD_KPRI_REQUEST();	/* regain priority from read lock */
 	rw_exit(module_lock);
 	return (rval);
 }
diff --git a/usr/src/uts/sfmmu/vm/hat_sfmmu.c b/usr/src/uts/sfmmu/vm/hat_sfmmu.c
index 2ef3ea20e8..3dae2f6396 100644
--- a/usr/src/uts/sfmmu/vm/hat_sfmmu.c
+++ b/usr/src/uts/sfmmu/vm/hat_sfmmu.c
@@ -10882,7 +10882,6 @@ sfmmu_ismhat_enter(sfmmu_t *sfmmup, int hatlock_held)
 {
 	hatlock_t *hatlockp;
 
-	THREAD_KPRI_REQUEST();
 	if (!hatlock_held)
 		hatlockp = sfmmu_hat_enter(sfmmup);
 	while (SFMMU_FLAGS_ISSET(sfmmup, HAT_ISMBUSY))
@@ -10904,7 +10903,6 @@ sfmmu_ismhat_exit(sfmmu_t *sfmmup, int hatlock_held)
 	cv_broadcast(&sfmmup->sfmmu_tsb_cv);
 	if (!hatlock_held)
 		sfmmu_hat_exit(hatlockp);
-	THREAD_KPRI_RELEASE();
 }
 
 /*
diff --git a/usr/src/uts/sparc/os/syscall.c b/usr/src/uts/sparc/os/syscall.c
index 06e14ea513..7807eccfed 100644
--- a/usr/src/uts/sparc/os/syscall.c
+++ b/usr/src/uts/sparc/os/syscall.c
@@ -1026,7 +1026,6 @@ loadable_syscall(
 	 * Try to autoload the system call if necessary.
 	 */
 	module_lock = lock_syscall(se, code);
-	THREAD_KPRI_RELEASE();	/* drop priority given by rw_enter */
 
 	/*
 	 * we've locked either the loaded syscall or nosys
@@ -1040,7 +1039,6 @@ loadable_syscall(
 		rval = syscall_ap();
 	}
 
-	THREAD_KPRI_REQUEST();	/* regain priority from read lock */
 	rw_exit(module_lock);
 	return (rval);
 }
diff --git a/usr/src/uts/sparc/v9/ml/lock_prim.s b/usr/src/uts/sparc/v9/ml/lock_prim.s
index d1e78869c2..ff0ec210a2 100644
--- a/usr/src/uts/sparc/v9/ml/lock_prim.s
+++ b/usr/src/uts/sparc/v9/ml/lock_prim.s
@@ -502,10 +502,7 @@ rw_exit(krwlock_t *lp)
 	cmp	%o1, RW_WRITER			! entering as writer?
 	be,a,pn	%icc, 2f			! if so, go do it ...
 	or	THREAD_REG, RW_WRITE_LOCKED, %o5 ! delay: %o5 = owner
-	ld	[THREAD_REG + T_KPRI_REQ], %o3	! begin THREAD_KPRI_REQUEST()
 	ldn	[%o0], %o4			! %o4 = old lock value
-	inc	%o3				! bump kpri
-	st	%o3, [THREAD_REG + T_KPRI_REQ]	! store new kpri
 1:
 	andcc	%o4, RW_WRITE_CLAIMED, %g0	! write-locked or write-wanted?
 	bz,pt	%xcc, 3f	 		! if so, prepare to block
@@ -552,15 +549,14 @@ rw_exit(krwlock_t *lp)
 	bnz,pn	%xcc, 2f			! single reader, no waiters?
 	clr	%o1
 1:
-	ld	[THREAD_REG + T_KPRI_REQ], %g1	! begin THREAD_KPRI_RELEASE()
 	srl	%o4, RW_HOLD_COUNT_SHIFT, %o3	! %o3 = hold count (lockstat)
 	casx	[%o0], %o4, %o5			! try to drop lock
 	cmp	%o4, %o5			! did we succeed?
 	bne,pn	%xcc, rw_exit_wakeup		! if not, go to C
-	dec	%g1				! delay: drop kpri
+	nop
 .rw_read_exit_lockstat_patch_point:
 	retl
-	st	%g1, [THREAD_REG + T_KPRI_REQ]	! delay: store new kpri
+	nop
 2:
 	andcc	%o4, RW_WRITE_LOCKED, %g0	! are we a writer?
 	bnz,a,pt %xcc, 3f
diff --git a/usr/src/uts/sun4/ml/offsets.in b/usr/src/uts/sun4/ml/offsets.in
index 4f6d19ba01..3af535dd6f 100644
--- a/usr/src/uts/sun4/ml/offsets.in
+++ b/usr/src/uts/sun4/ml/offsets.in
@@ -165,7 +165,6 @@ _kthread	THREAD_SIZE
 	_tu._ts._t_post_sys	T_POST_SYS
 	_tu._ts._t_trapret	T_TRAPRET
 	t_preempt_lk
-	t_kpri_req
 	t_lockstat
 	t_pil
 	t_intr_start
-- 
2.21.0

