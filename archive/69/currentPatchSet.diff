From 69c8386d8a3d14f0993278cf60ae4a151468877f Mon Sep 17 00:00:00 2001
From: Dave Pacheco <dap@joyent.com>
Date: Wed, 13 Jul 2016 09:41:47 -0700
Subject: [PATCH] TOOLS-1478 cr.joyent.us operator guide needs work

---
 docs/operator/README.md | 314 +++++++++++++++++++++++-----------------
 1 file changed, 179 insertions(+), 135 deletions(-)

diff --git a/docs/operator/README.md b/docs/operator/README.md
index 02bf676..23ffa76 100644
--- a/docs/operator/README.md
+++ b/docs/operator/README.md
@@ -12,27 +12,82 @@
 
 This document needs a lot of work!
 
-## Setting up new users
+## Site policy and configuration
+
+### Access Control (Groups)
+
+Gerrit supports access control through Groups.
+
+Most privileges in Gerrit are configured at the Project level, and on
+cr.joyent.us, nearly all of those privileges are inherited from the top-level
+"All-Projects" project.  Some projects add some additional restrictions or
+privileges.
+
+Groups can be members of other groups.  To help manage access control, in many
+cases we've separated "roles" (groups to which privileges are assigned) from
+specific lists of people.
+
+
+#### Groups used as roles
+
+"Change Approver Role" is a group that gets the privilege of approving changes
+(voting +2).  It currently contains the "Joyent Team" group.  If we decide to
+entrust community members with this privilege, we should add another group for
+them, and then add that group to this role.
+
+
+#### Groups with people in them
+
+"Joyent Team" are members of the Joyent engineering team.  We do not grant
+privileges to this group directly, but rather we make this group a member of
+other groups that get privileges (e.g., "Change Approver Role").
+
+"illumos mergers" are people who merge upstream changes from illumos into
+illumos-joyent.  They need privileges to push merge commits to illumos-joyent.
+
+"Administrators" are essentially super-users, both in the UI and via git.  This
+overrides most of the checks we have put in place (and would like to put in
+place) to avoid things like accidentally pushing to master.  People should not
+need to be administrators in order to do day-to-day work.  We should only add
+people to this group in order to help maintain cr.joyent.us, not to work around
+some other access control issue.
+
+#### Groups used by the infrastructure
+
+"GitHub (joyent) Replication Group": this group is created specifically to
+manage which repositories are replicated to GitHub.  The replication process
+runs as this group and replicates all repositories starting with "joyent/" that
+it can read from.  All of our projects inherit from the GitHub-Joyent project,
+which enables read access to this group.  This generally should not need
+maintenance.
+
+#### Note on project creation
+
+Only administrators in Gerrit are allowed to edit projects, and editing is
+essentially required during initial project setup.  As a result, we require
+administrators to create or import projects.  There's a deprecated "Project
+Creator Role" where we experimented with delegating this privilege.
+
+
+## Administrative tasks
+
+### Setting up new users
 
 Because cr.joyent.us is on the internet and uses GitHub for authentication,
 anybody can register an account and start using it.  That's by design, so that
 we can code review and accept community contributions. However, only authorized
 users will be able to approve changes.
 
-New users that are employees of Joyent should be added to the Gerrit group
-called ["Joyent Team"](https://cr.joyent.us/#/admin/groups/6).
+New users that are employees of Joyent should be added to the ["Joyent
+Team"](https://cr.joyent.us/#/admin/groups/6) group.
 
 New users that are not employees of Joyent, but which we will trust for
 approvals should be put into a new group called "External Approvers".  (This
 hasn't happened yet.)  That group should in turn become a member of "Change
-Approver Role".
+Approver Role".  See "Access Control" below.
 
-**Background:** we're keeping Joyent Team and External Approvers as separate
-groups, but both ultimately need to be part of "Change Approver Role", so that's
-why there are separate groups.
 
-
-## Importing repositories that are on GitHub
+### Importing repositories that are on GitHub
 
 **Please do not create repositories by hand in the Gerrit web UI.**  It's hard
 to get the settings right, and we likely won't discover if they're wrong until
@@ -55,7 +110,7 @@ on GitHub**.  They have a bunch of important notes about how things change once
 a repository is imported.
 
 
-## Creating new repositories
+### Creating new repositories
 
 See the user instructions "Creating a new repository".  (They basically say to
 create the repository on GitHub and then treat it as an import.)
@@ -67,137 +122,143 @@ something bad has already happened.)
 
 ## Deployment notes
 
-This section needs work.
-
-### Environment
+cr.joyent.us is deployed in us-west-1 (behind TLS), using CNS for internal and
+external service discovery.
 
-us-west-1 (behind TLS), using CNS for internal and external service discovery.
-
-### Topology
-
-Gerrit uses both a PostgreSQL database and the local filesystem.  In order to
-ensure that we can redeploy PostgreSQL and Gerrit itself without losing data, we
-use separate data volumes for the PostgreSQL database and for the local
-filesystem.
+### Container topology
 
+Gerrit uses both a PostgreSQL database and the local filesystem.  We use
+separate data volumes for the PostgreSQL database and for the local filesystem.
 There are five total containers:
 
 - data container for the PostgreSQL database
-- data container for Gerrits local filesystem data
+- data container for Gerrit's local filesystem data
 - PostgreSQL database
 - Gerrit itself
 - nginx proxy (the only thing that's public)
 
-The data containers never need to be modified or redeployed.  The PostgreSQL,
-Gerrit, and nginx containers can be redeployed as desired to update
-configuration or upgrade software, subject to the same constraints around
-compatibility of on-disk formats as PostgreSQL and Gerrit normally have with
-local filesystem storage.
+Originally, the data containers would never be modified or redeployed, while the
+other containers could be redeployed at will (subject to the usual constraints
+around on-disk formats with new versions of Gerrit or PostgreSQL).  Now, the
+current deployment tools always deploy an entire new stack from a backup, so
+it's less necessary to use separate data containers, but it's still convenient
+for doing backup/restore and for inspecting the contents of these datasets.
 
 ### Service discovery
 
 We've created a CNAME in public DNS for `cr.joyent.us`.  This points to a
-CNS-provided DNS name for the gerrit service.  This way, if we want to redeploy
+CNS-provided DNS name for the nginx container.  This way, if we want to redeploy
 the Gerrit instance, the instance IP may change, but the CNS name will be
 automatically updated with the new IP address, so users of the public CNAME will
 also get the new IP.  (It's not clear that Gerrit supports multiple instances
 running on the same data, so rolling upgrades are likely not possible, but this
 approach still makes it easy to upgrade instances.)
 
-### Build steps
+### GitHub authentication
+
+To use GitHub auth, we've set up a [GitHub
+application](https://github.com/organizations/joyent/settings/applications/371013).
+If the public hostname changes, this needs to be updated.
+
 
-We have custom images for the Gerrit application server and for the nginx proxy.
-These will be moved into this repository.
+### Image builds and configuration
 
-### Backup/restore
+We use two custom images:
 
-There are some janky backup/restore scripts that will be moved into this
-repository.  These need some work.
+* `joyentunsupported/joyent-gerrit:dev`: Gerrit application server
+* `arekinath/gerrit-nginx`: nginx container
 
-Backup effectively works by shutting down the appserver container, using
-`pg_dump` to backup the database, and using `docker cp` to create a tarball of
-the Gerrit data volume.  Restore builds up the entire stack from scratch,
-restoring these two chunks of data using `pg_restore` and `docker cp`.
+The appserver image is built from this repository.  See images/appserver.
+There's a README.md in there that describes the configuration model in some
+detail.  In short, non-deployment-specific, non-secret configuration is built
+into the image.  If we want to change that, we build a new image.  This
+encourages testing those kinds of configuration changes, and also would
+facilitate blue-green deploys, if Gerrit supported that.
 
-### GitHub auth
 
-To use GitHub auth, we've set up a [GitHub
-application](https://github.com/organizations/joyent/settings/applications/371013).
-If the public hostname changes, this needs to be updated.
+### Deployment using backup and restore
+
+This entire stack (PostgreSQL, Gerrit, nginx, and data) can be backed up and
+restored from backup using tools in the "bin" directory inside this repo.  You
+can backup the production cr.joyent.us and restore it to a test environment to
+test new images or configuration changes.  These tools use docker(1) and
+triton(1) to locate and deploy containers.
+
+You can have multiple deployments of the stack alongside each other in the same
+datacenter.  These are distinguished by the _prefix_.  The default prefix for
+backup is "gerrit" (which is the current production prefix).  You can deploy a
+second copy with a different name like "gerrit-staging".  This prefix shows up
+in each of the containers' names as well as their CNS service names.
 
-### Manual deployment steps
-
-**Note: the best way to deploy a new stand-up is using the gerritrestore script
-on an existing backup.  These instructions may grow out of date!**
-
-These instructions won't work as-is for non-Joyent\_Dev deployments.
-
-PostgreSQL data container:
-
-    # docker run --name=gerrit-volume-db -v /gerrit-postgres-db-data ubuntu echo "Database volume container created."
-
-Gerrit data container:
-
-    # docker run --name=gerrit-volume-gerrit -v /var/gerrit/review_site ubuntu echo "Gerrit volume container created."
-
-PostgreSQL runtime container:
-
-    # docker run \
-        --name gerrit-postgres \
-	--label triton.cns.services=gerritdb \
-        -e POSTGRES_USER=gerrit2 \
-        -e POSTGRES_PASSWORD=gerrit \
-        -e POSTGRES_DB=reviewdb \
-        -e PGDATA=/gerrit-postgres-db-data \
-        --volumes-from gerrit-volume-db \
-        --restart=always \
-        -d postgres:9.5.3
-
-Note: this doesn't use -p to expose the port because it will be on a Triton
-fabric network.  In an environment without fabrics set up, you may need "-p
-5432:5432".
-
-Gerrit app container:
-
-    # docker run \
-        --name gerrit-appserver \
-        --label triton.cns.services=gerrit \
-        --volumes-from=gerrit-volume-gerrit \
-        --restart=always \
-        -e AUTH_TYPE=OAUTH \
-        -e OAUTH_GITHUB_CLIENT_ID=... \
-        -e OAUTH_GITHUB_CLIENT_SECRET=... \
-        -e DATABASE_TYPE=postgresql \
-        -e DB_PORT_5432_TCP_ADDR=gerritdb.svc.ddb63097-4093-4a74-b8e8-56b23eb253e0.us-west-1.cns.joyent.com \
-        -e WEBURL=http://cr.joyent.us \
-        -e SMTP_SERVER=relay.joyent.com \
-        -e SMTP_CONNECT_TIMEOUT=60sec \
-        -e USER_EMAIL=no-reply@cr.joyent.us \
-        -d joyentunsupported/joyent-gerrit:dev
-
-Note: this also assumes fabrics with Docker containers.  Without it, you'll want
-to add `--link gerrit-postgres:db -p 8080:8080 -p 29418:29418` and remove the
-database flags other than DATABASE\_TYPE.
-
-Front door container:
-
-    docker run -d \
-        --name=gerrit-frontdoor \
-        --label triton.cns.services=gerrit \
-        -e MY_NAME=cr.joyent.us \
-        -e GERRIT_HOST=gerrit-backend.svc.JOYENT_DEV_ACCOUNT_ID.us-west-1.cns.joyent.com \
-        -e SSH_PORT=29418 \
-        -e HTTP_PORT=8080 \
-        -p 22 \
-        -p 80 \
-        -p 443 \
-        -p 29418 \
-        arekinath/gerrit-nginx
-
-## Setup notes
+The `crbackup` tool backs up the data contained in a running stack.  The
+PostgreSQL database is backed up with pg\_dump and the Gerrit filesystem is
+backed up as a tarball with "docker cp".  Both of these are downloaded into a
+local directory that can subsequently be given to `crrestore`.
+
+`crrestore` takes the backup directory saved by `crbackup` and redeploys the
+entire stack from the data contained in the tarball.  There are basically two
+modes: "production" mode and "dev" mode.  In "production" mode, the server
+advertises itself as "cr.joyent.us" and has replication to GitHub enabled.  In
+"dev" mode, the server advertises itself as "localhost".  To use it, you would
+typically set up port forwarding on your local system.
+
+To back up the stack:
+
+    # Stop the application container to get a consistent backup.
+    docker stop gerrit-appserver
+
+    # Back up to local directory "./2016-06-12-0".  This looks for containers
+    # with the prefix "gerrit" by default (e.g., "gerrit-appserver").  You can
+    # override this with the "-n" option.
+    crbackup ./2016-06-12-0
+
+The containers representing the service are automatically located by name (e.g.,
+"gerrit-appserver") using the "docker" tool.
+
+To create a new _test_ production deployment from a backup, you could run:
+
+    #
+    # Create a production-like deployment called "gerrit-staging" from the
+    # backup in "./2016-06-12-0".
+    #
+    crrestore -p -n gerrit-staging ./2016-06-12-0
+
+To replace the real production deployment, you could specify "-n gerrit"
+instead, which will cause the nginx image to have the right CNS name that
+cr.joyent.us points to.  A better approach (that doesn't require clobbering the
+existing deployment) would be to let "crrestore" pick its own prefix and
+manually tag the nginx image with the appropriate CNS service.  Obviously, we
+need some tooling for this.
+
+**Note:** the SSL certificate for the nginx container is not currently part of
+the backup/restore process, so it will always try to generate a new certificate
+when you redeploy it.  This generally won't work unless you're deploying a
+production version, and even then it won't work until after "cr.joyent.us"
+points to the new nginx container.
+
+To create a new test environment from a backup, you would typically run:
+
+    crrestore -c CLIENT_ID -s CLIENT_SECRET ./2016-06-12-0
+
+where CLIENT\_ID and CLIENT\_SECRET come from a GitHub oauth application that
+you've set up for testing.
+
+
+## References
+
+* https://github.com/openfrontier/docker-gerrit
+* https://gerrit.googlesource.com/plugins/github/+/master/README.md
+* https://www.packtpub.com/books/content/using-gerrit-github
+* https://hub.docker.com/\_/postgres/
+* Replication: https://gist.github.com/Aricg/56f1a769cbdcbb93b459
+
+
+## Historical notes
+
+From the stock openfrontier image, we got Gerrit set up by modifying:
 
 * Mail notifications: see SMTP and USER\_EMAIL params in deployment
-* GitHub auth: see AUTH/OATH params in deployment
+* GitHub auth: see AUTH/OAUTH params in deployment
 * Auto-links in commit comments to issue trackers
   * add two sections to /var/gerrit/review\_site/etc/gerrit.config
   * had originally created a separate Docker image with a customization script
@@ -223,15 +284,10 @@ Front door container:
 * Enable pushing merge commits for administrators to All-Projects so that we can
   import projects from GitHub that have merge commits already.
 
-## References
-
-* https://github.com/openfrontier/docker-gerrit
-* https://gerrit.googlesource.com/plugins/github/+/master/README.md
-* https://www.packtpub.com/books/content/using-gerrit-github
-* https://hub.docker.com/\_/postgres/
-* Replication: https://gist.github.com/Aricg/56f1a769cbdcbb93b459
+Subsequent config changes should be documented either in this repository's
+history (when the image changes) or in Git (when project configuration changes).
 
-## Known issues
+### Known issues
 
 Importing platform: could not push existing platform repo to Gerrit ssh server
 because of invalid commit messages.  Instead, used "docker exec", cloned repo
@@ -241,18 +297,6 @@ Attempted to create node-cueball project, but it actually created a new change
 for each commit.  I haven't seen this again, so I don't know what caused it.
 
 By attempting to remove an old version of the node-cueball project, that project
-wound up in a busted state.  I can't access changes for it, and I can't delete
-it.  Reports a ConcurrentModificationException.  This was in a previous
+wound up in a busted state.  I couldn't access changes for it, and I couldn't
+delete it.  Reports a ConcurrentModificationException.  This was in a previous
 deployment; it's no longer present in the current deployment.
-
-Issues from migration from staging to west1:
-
-* Official Triton docs don't work for setting up my Docker client cert.
-  I used my existing one, but that ended up creating stuff under the wrong
-  user (dap instead of Joyent\_Dev).
-* PostgreSQL docs are confusing/wrong about how to restore a DB.  Need to use
-  the command documented separately.
-* openfrontier image changed to a version with no bash, which broke my customize
-  script.
-* Networking params and CNS names are different in JPC than staging.
-* Some provision failures in west-1.
-- 
2.21.0

