commit c192fa7c51037b3394939e3551ed65c222999fe5 (refs/changes/60/5060/3)
Author: Robert Bogart <Robert.bogart@joyent.com>
Date:   2018-11-15T17:50:23+00:00 (11 months ago)
    
    MANTA-3664 Research Simple Minnow Based File Count / Size Check

diff --git a/README.md b/README.md
index 2092a0c..732842e 100644
--- a/README.md
+++ b/README.md
@@ -1,2 +1,172 @@
 # mako-regional-report
-Mechanism for aggregating object sizes and counts across all makos in a region.
+
+This repository is part of the Joyent Manta project.  For contribution
+guidelines, issues, and general documentation, visit the main
+[Manta](http://github.com/joyent/manta) project page.
+
+This is a Mechanism for aggregating object sizes and counts across all
+storage nodes (makos) in a region.  Currently, it is comprised of a single
+script which consumes Manta public APIs in order to fetch the entire manifest
+generated by a mako describing its storage contents located in
+`/poseidon/stor/mako/<storage id>`.
+
+Initially, a summary of each manifet mako is generated, derived from the full
+manifest.  It contains a per-account summarization of the number of objects
+stored on the mako, along with their average size and cumulative size.  Size
+units are in kilobytes.  The last line in the summary will always be a global
+calculation of the stats mentioned above across all accounts on the storage
+node.
+
+Next, the totals of each summary are included in a region-wide report as part
+of a JSON array, where each object is a storage node.  An example of such a
+report might look like this:
+
+```
+[
+  {
+    "datacenter": "robertdc",
+    "storage_id": "1.stor.west.example.com",
+    "kb": 30271952,
+    "objects": 163525,
+    "avg": 185.121248,
+    "tombstone": [
+      {
+        "date": "2018-10-11",
+        "kb": 2231,
+        "objects": 24
+      },
+      {
+        "date": "2018-10-12",
+        "kb": 249,
+        "objects": 5
+      },
+      {
+        "date": "2018-10-13",
+        "kb": 109,
+        "objects": 3
+      }
+    ]
+  },
+  {
+    "datacenter": "robertdc",
+    "storage_id": "2.stor.west.example.com",
+    "kb": 31533450,
+    "objects": 162932,
+    "avg": 193.537488,
+    "tombstone": []
+  },
+  {
+    "datacenter": "robertdc",
+    "storage_id": "3.stor.west.example.com",
+    "kb": 29700783,
+    "objects": 163327,
+    "avg": 181.84858,
+    "tombstone": []
+  }
+]
+```
+
+Note that each object in the array represents a single storage node.  Each
+member of the object is described below:
+* `datacenter`: The name of the datacenter that the storage node is a part of.
+* `storage_id`: The name of the storage node.
+* `kb`: The total amount of physical storage (in kilobytes) currently consumed
+by the storage node.
+* `objects`: Total number of objects on the storage node.
+* `avg`: Average object size on the storage node.
+* `tombstone`: This is a JSON array containing information about objects which
+are scheduled for deletion.  The array can be empty, or have a varying number of
+records depending on what ojects have been marked for deletion (and when).  Each
+element in the array represents a subdirectory within `/manta/tombstone` and is
+named after the date at which it was created, this is part of a larger process
+referred to as Garbage Collection.  For more information on that process, refer
+to the [Garbage Collection](http://github.com/joyent/manta-garbage-collector) 
+project page.
+
+Since generating the summary can be quite expensive, as a first recourse, we
+will always check `/poseidon/stor/mako/summary` first to see if one already
+exists.  If one is not present, then we will download the full mako manifest
+and derive the summary from it.  It's worth mentioning that in the event that
+we download the full summary, we do not actually save it to disk, rather, we
+perform the calculations on the stream, saving only the resulting summary to
+disk.
+
+After all mako summaries in a region have been obtained (whether downloaded or
+derived from the full mako manifests) and the regional report `region.json` has
+been completed, it is then uploaded to `/poseidon/mako/summary`.
+
+# Dependencies
+The automation in `bin` requires that the following are installed:
+* GNU awk 4.1.3 or later: (earlier versions do not allow you to specify the
+level of precision which would limit the user to 53 bits).
+* Manta Client Tools: The automation consumed Manta public APIs only and
+uses several utilies to access mako manifests as well as upload regional
+results.  This has been tested with Manta Client Tools 5.1.1.
+
+**Note:**  Please note the environment variables that must be set in order to
+access Manta via the client tools.  More information regarding configuring the
+Manta Client Tools can be found on the
+[Manta Client Tools and SDK](https://github.com/joyent/node-manta) project page.
+
+# Configuration
+In the event that the process of aggregating a region does not complete in a
+timely fasion due to having to derive summaries for all mako manifests, it is
+possible to offload that responsibility by enabling post-processing of the mako
+manifest on the storage node itself.  The net result being that the storage node
+will generate its own summary based on the contents of its full manifest and
+upload that summary to `/poseidon/stor/mako/summary`.
+
+### To enable post-processing of the manifest on the mako:
+
+```
+MANTA_APP=$(sdc-sapi /applications?name=manta | json -Ha uuid)
+echo '{ "metadata": {"MAKO_PROCESS_MANIFEST": true } }' | sapiadm update $MANTA_APP
+```
+
+### To disable post-processing of the manifest on the mako:
+
+```
+MANTA_APP=$(sdc-sapi /applications?name=manta | json -Ha uuid)
+echo '{ "metadata": {"MAKO_PROCESS_MANIFEST": false } }' | sapiadm update $MANTA_APP
+```
+
+**Note:** Setting this paramter will likely take effect within seconds or
+minutes, however the value of this paramter will not be evaluated on the mako
+until the next time /opt/smartdc/mako/bin/upload_mako_ls.sh is run on the
+storage node, so if a summary is needed sooner than that, then it might be
+necessary to login to the storage node and run the script buy hand rather than
+waiting for the daily scheduled cron job to run it.
+
+### To Force Processing
+
+Normally, the upload of manifests from the makos are handled via a cron job on 
+each storage node.
+
+```
+1 8 * * * /opt/smartdc/mako/bin/upload_mako_ls.sh >>/var/log/mako-ls-upload.log 2>&1
+```
+
+**Note:** This time may have been adjusted in the environment you are working
+in.
+
+If you need to force the mako to run an upload (i.e. an out of band upload for
+testing) you can do this by running the `upload_mako_ls.sh` by hand.
+
+### Checking Results
+
+The pre-processed summary for the makos will be placed in
+`/posiedon/stor/makos/summary`.  For example, if you are testing your canary
+deployment with `1.stor.west.example.com` you should see a file created in that
+directory after processing is finished:
+
+```
+[root@bf4d027b-a0df-e8bf-9b84-9487f9a5eab4 ~]# mls -l /poseidon/stor/mako/summary/1.stor.west.example.com
+-rwxr-xr-x 1 poseidon           473 Nov 14 05:53 1.stor.west.example.com
+```
+
+Looking into the file, you should see a `Totals` line:
+
+```
+[root@6890ac1b (storage) ~]$ mget -q /poseidon/stor/mako/summary/1.stor.west.example.com | egrep "^totals"
+totals  262755955915.000000     163891.000000   184.999487      30319751.000000
+```
diff --git a/bin/report.sh b/bin/report.sh
new file mode 100755
index 0000000..b9c6977
--- /dev/null
+++ b/bin/report.sh
@@ -0,0 +1,298 @@
+#!/bin/bash
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2018, Joyent, Inc.
+#
+
+PID=$$
+PID_FILE=/tmp/report.pid
+REGION_LISTING="region"
+REMOTE_PATH="/poseidon/stor/mako"
+SUMMARY_DIR="/tmp/summary"
+STATUS=0
+sns=
+
+function warn()
+{
+	local LNOW=`date`
+	echo "$LNOW: $(basename $0): warning: $*" >&2
+	STATUS=1
+}
+
+function log
+{
+	local LNOW=`date`
+	echo "$LNOW: $(basename $0): info: $*" >&2
+}
+
+function fatal
+{
+	local LNOW=`date`
+	echo "$LNOW: $(basename $0): fatal error: $*" >&2
+	rm "$PID_FILE"
+	exit 1
+}
+
+function check_env_var()
+{
+	var=$1
+	name=$2
+
+	if [[ -z "$var" ]]; then
+		fatal "$name is not set"
+	fi
+}
+
+function download_summary()
+{
+	file=$1
+
+	mget "${REMOTE_PATH}/summary/${file}" > "$SUMMARY_DIR/$file"
+
+	if [[ $? -ne 0 ]]; then
+		log "Unable to find summary for $file."
+		return 1
+	fi
+
+	return 0
+}
+
+function generate_summary()
+{
+	log "Downloading mako manifest $file"
+
+	#
+	# This will download the contents of a full manifest from
+	# /poseidon/stor/mako and attempt to generate the summary. Because
+	# this is the most expensive part of the process, we have the option
+	# of asking the mako itself to perform this for us by changing the
+	# value of the SAPI tunable `process_manifest' to true.  Then, the next
+	# time a mako generates a manifest, it will follow up by deriving its
+	# own summary for it and storing it in /poseidon/stor/mako/summary.
+	# Once the number of storage nodes in a given Manta installation goes
+	# beyond a certain number (whatever that might be), this may become
+	# necessary in order to complete a full aggregation of a region in a
+	# reasonable amount of time.
+	#
+	mget "${REMOTE_PATH}/${file}" | gawk -M -v PREC="quad" '{
+		split($1, x, "/")
+		acct=x[3]
+		bytes[acct] += $2
+		objects[acct]++
+		kilobytes[acct] += $4
+		total_bytes += $2
+		total_objects++
+		total_kilobytes += $4
+
+		#
+		# If the Manta directory happens to be "tombstone" then x[4]
+		# contains the name of the subdirectory which will always be
+		# a date.  We want to organize the objects in this part of the
+		# tree by their subdirectory name (i.e. its date of creation)
+		# so that when analyzing a summary, a determination can be made
+		# not only about how much storage we stand to reclaim in overall
+		# but also _when_ we stand to reclaim each fraction of the
+		# tombstone directory tree.
+		#
+		if (x[3] == "tombstone") {
+			date=x[4]
+			tombstone_bytes[date] += $2
+			tombstone_kilobytes[date] += $4
+			tombstone_objects[date]++
+		}
+	} END {
+		printf("%s\t%s\t%s\t%s\t%s\n", "account", "bytes",
+		    "objects", "average size kb", "kilobytes");
+
+		for (date in tombstone_bytes) {
+			printf("tombstone_%s\t%f\t%f\t%f\t%f\n", date,
+			    tombstone_bytes[date], tombstone_objects[date],
+			    tombstone_kilobytes[date] / tombstone_objects[date],
+			    tombstone_kilobytes[date]);
+		}
+
+		for (acct in bytes) {
+			printf("%s\t%f\t%f\t%f\t%f\n",
+			    acct, bytes[acct], objects[acct],
+			    kilobytes[acct] / objects[acct], kilobytes[acct]);
+		}
+
+		if (total_objects == 0) {
+			total_avg = 0;
+		} else {
+			total_avg = total_kilobytes / total_objects;
+		}
+
+		printf("%s\t%f\t%f\t%f\t%f\n", "totals", total_bytes,
+		    total_objects, total_avg, total_kilobytes);
+	}' > "$SUMMARY_DIR/$file"
+
+	#
+	# It is imporant to check the value of PIPESTATUS[0] because the
+	# call to gawk can succeed even if the mget failed, thus masking a
+	# possible failure.
+	#
+	if [[ ${PIPESTATUS[0]} -ne 0 || $? -ne 0 ]]; then
+		rm "$SUMMARY_DIR/$file"
+		return 1
+	fi
+	return 0
+}
+
+process_summary()
+{
+	sn="$1"
+	dc="$2"
+	region=$3
+
+	download_summary "$sn"
+
+	#
+	# It would be great if someone did the heavy lifting for us already
+	# but if there is no summary which has already been generated, then
+	# we must download the manifest ourselves and generate it.
+	#
+	if [[ $? -ne 0 ]]; then
+		generate_summary "$sn"
+		if [[ $? -ne 0 ]]; then
+			warn "Unable to generate summary for $sn"
+			return
+		fi
+	fi
+
+	echo "{" >> "$SUMMARY_DIR/$region"
+
+	#
+	# Grab the global totals from this mako summary.
+	#
+	tail -1 "$SUMMARY_DIR/$sn" | \
+	    gawk -M -v PREC="quad" -v dc="$dc" -v sn="$sn" '{
+		printf("\"datacenter\":\"%s\", \"storage_id\":\"%s\",\
+		    \"kb\":%f, \"objects\":%f," "\"avg\":%f",
+		    dc, sn, $5, $3, $4);
+	}' >> "$SUMMARY_DIR/$region"
+
+	#
+	# Include any tombstone information which may (or may not) be part of
+	# the summary.
+	#
+	echo ",\"tombstone\": [" >> "$SUMMARY_DIR/$region"
+
+	#
+	# We want to capture all of the subdirectories within tombstone and
+	# give their information individually as they probably will not all be
+	# reclaimed at the same time.
+	#
+	cat "$SUMMARY_DIR/$sn" | grep "tombstone_" | gawk -M -v PREC="quad" '{
+		if ($1 == "tombstone") {
+			label = "total"
+		} else {
+			split($1, y, "_");
+			label = y[2];
+		}
+		printf("{\"date\":\"%s\", \"kb\":%f, \"objects\":%f}", label,\
+		     $5, $3)
+	}' >> "$SUMMARY_DIR/$region"
+
+	#
+	# Complete the json object for this mako.
+	#
+	echo "]}" >> "$SUMMARY_DIR/$region"
+
+	#
+	# If we are unable to update the aggregation file, there is no reason
+	# to believe that trying again would yield a different result.  There is
+	# probably no point in continuing further.
+	#
+	if [[ $? -ne 0 ]]; then
+		fatal "Unable to update aggregation file $SUMMARY_DIR/$region"
+	fi
+}
+
+get_makos()
+{
+	sns=$(mls --type=o /poseidon/stor/mako)
+	return $?
+}
+
+get_datacenter()
+{
+	file="$1"
+
+	dc=$(minfo $file |grep m-datacenter | gawk '{print $2}')
+	if [[ ${PIPESTATUS[0]} -ne 0 ]]; then
+		warn "Unable to obtain datacenter for $file"
+		return 1
+	fi
+
+	echo $dc
+	return 0
+}
+
+process_group()
+{
+	get_makos
+
+	for i in ${sns[@]}
+	do
+		dc=$(get_datacenter "$REMOTE_PATH/$i")
+		process_summary  "$i" "$dc" "$REGION_LISTING"
+	done
+}
+
+#
+# If an instance of this script is already running, then exit.
+#
+LAST_PID=$(cat $PID_FILE 2>/dev/null)
+
+if [[ -n "$LAST_PID" ]]; then
+    ps -p $LAST_PID >/dev/null
+    if [[ $? -eq 0 ]]; then
+        echo "$0 process still running.  Exiting."
+        exit 1
+    fi
+fi
+
+#
+# No other instances of the script are currently running.  Record our pid.
+#
+echo -n $PID > $PID_FILE
+
+#
+# To even have a chance at succeeding, these environment variables must be set.
+#
+check_env_var "$MANTA_USER" "MANTA_USER"
+check_env_var "$MANTA_KEY_ID" "MANTA_KEY_ID"
+check_env_var "$MANTA_URL" "MANTA_URL"
+
+if [ -d "$SUMMARY_DIR" ]; then
+	rm -rf "$SUMMARY_DIR"
+fi
+
+mkdir "$SUMMARY_DIR"
+
+process_group
+
+#
+# Convert the aggregated summary to JSON
+#
+cat "$SUMMARY_DIR/$REGION_LISTING" |\
+    json -g > "$SUMMARY_DIR/${REGION_LISTING}.json"
+
+mmkdir "$REMOTE_PATH/summary"
+if [[ $? -ne 0 ]]; then
+	fatal "Unable to create summary directory in manta"
+fi
+
+mput -f $SUMMARY_DIR/${REGION_LISTING}.json "$REMOTE_PATH/summary"
+if [[ $? -ne 0 ]]; then
+	fatal "Unable to upload $SUMMARY_DIR/${REGION_LISTING}.json to manta"
+fi
+
+rm "$PID_FILE"
+exit $STATUS
