From 46db5705cea4be4517d93c7ec83b3c702f15d97d Mon Sep 17 00:00:00 2001
From: David Pacheco <dap@joyent.com>
Date: Thu, 10 Nov 2016 07:57:04 -0800
Subject: [PATCH] MANTA-3004 Excise references to "worker" in manta-marlin
 documentation

---
 Makefile                            |   6 +-
 README.md                           |  10 +-
 agent/lib/agent/agent.js            |  23 ++---
 agent/sbin/mrdeploycompute          |   8 +-
 client/sbin/mrjob                   |   9 +-
 common/lib/bus.js                   |   6 +-
 common/lib/marlin.js                |  13 ++-
 common/lib/schema.js                |  67 ++++++-------
 dev/test/live/common.js             |   6 +-
 docs/index.md                       | 116 +++++++++++-----------
 jobsupervisor/lib/worker/locator.js |   6 +-
 jobsupervisor/lib/worker/schema.js  |   4 +-
 jobsupervisor/lib/worker/server.js  |  26 ++---
 jobsupervisor/lib/worker/worker.js  | 147 ++++++++++++++--------------
 14 files changed, 229 insertions(+), 218 deletions(-)

diff --git a/Makefile b/Makefile
index 8111b38..6d39a70 100644
--- a/Makefile
+++ b/Makefile
@@ -5,7 +5,7 @@
 #
 
 #
-# Copyright (c) 2014, Joyent, Inc.
+# Copyright (c) 2016, Joyent, Inc.
 #
 
 #
@@ -366,8 +366,8 @@ CLEAN_FILES		+= $(MG_PROTO) \
 
 #
 # "release" target creates two tarballs: the first to be used as an input for
-# the jobworker zone, the second to be installed in each system's global zone
-# via apm.
+# the jobsupervisor zone, the second to be installed in each system's global
+# zone via apm.
 #
 .PHONY: release
 release: $(PROTO_TARBALL) $(AGENT_TARBALL) $(AGENT_MANIFEST)
diff --git a/README.md b/README.md
index 3db3368..c3e7047 100644
--- a/README.md
+++ b/README.md
@@ -5,7 +5,7 @@
 -->
 
 <!--
-    Copyright (c) 2014, Joyent, Inc.
+    Copyright (c) 2016, Joyent, Inc.
 -->
 
 # manta-marlin
@@ -154,8 +154,8 @@ the Manta tools (mls, mput, etc.)
 
 ## Running the code
 
-There are two main components in Marlin: the job supervisor (formerly called the "worker")
- and the agent.
+There are two main components in Marlin: the job supervisor (formerly called the
+"worker") and the agent.
 
 The supervisor can be run directly out of the repo.  The easiest way to get started
 is to copy the config.json file from an existing Marlin supervisor deployed on the
@@ -166,7 +166,7 @@ same system, modify the port number (since the default is a privileged port),
     $ . dev/env.sh
     $ make
     $ node build/proto/root/opt/smartdc/marlin/lib/worker/server.js \
-          ../config.json | tee ../worker.out | bunyan -o short
+          ../config.json | tee ../supervisor.out | bunyan -o short
 
 With this approach, you can apply changes by stopping the supervisor, running "make
 proto", and starting the supervisor again.
@@ -262,7 +262,7 @@ In one shell, start the supervisor in a way that will be automatically restarted
 
     $ while :; do LOG_LEVEL=debug node \
         build/proto/root/opt/smartdc/marlin/lib/worker/server.js \
-        ../config.json | tee -a ../worker.out | bunyan -linfo; done
+        ../config.json | tee -a ../supervisor.out | bunyan -linfo; done
 
 In another, start a loop that will *kill* the supervisor a random interval
 between 5 and 30 seconds apart.  There's a script in /dev/tools/ for this:
diff --git a/agent/lib/agent/agent.js b/agent/lib/agent/agent.js
index b68722f..274a935 100644
--- a/agent/lib/agent/agent.js
+++ b/agent/lib/agent/agent.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright (c) 2015, Joyent, Inc.
+ * Copyright (c) 2016, Joyent, Inc.
  */
 
 /*
@@ -18,7 +18,8 @@
  * This agent runs in the global zone of participating compute and storage nodes
  * and manages tasks run on that node.  It's responsible for setting up compute
  * zones for user jobs, executing the jobs, monitoring the user code, tearing
- * down the zones, and emitting progress updates to the appropriate job worker.
+ * down the zones, and emitting progress updates to the appropriate job
+ * supervisor.
  *
  * This agent is tightly-coupled with the lackey that runs in each compute zone
  * and manages tasks execution in that zone.
@@ -1212,12 +1213,12 @@ mAgent.prototype.onRecordTaskInput = function (record, barrier)
 	/*
 	 * If we don't know anything about the corresponding task, we ignore
 	 * this record.  This is unlikely because task records are usually
-	 * written when the job is picked up by the worker, while taskinput
-	 * records cannot be written out until the worker has resolved the input
-	 * object's user and location, so it's tough for the latter to beat the
-	 * former.  But if this does happen, we just assume that we will read
-	 * the task record shortly and will process this taskinput record on one
-	 * of the next go-arounds.
+	 * written when the job is picked up by the supervisor, while taskinput
+	 * records cannot be written out until the supervisor has resolved the
+	 * input object's user and location, so it's tough for the latter to
+	 * beat the former.  But if this does happen, we just assume that we
+	 * will read the task record shortly and will process this taskinput
+	 * record on one of the next go-arounds.
 	 */
 	taskid = record['value']['taskId'];
 
@@ -2919,7 +2920,7 @@ mAgent.prototype.taskStreamAdvance = function (stream, callback)
 	if (abbrpath instanceof Error) {
 		/*
 		 * This should be impossible because it would have been caught
-		 * by the worker, but we handle it defensively.
+		 * by the supervisor, but we handle it defensively.
 		 */
 		this.taskMarkFailed(task, now, {
 		    'code': EM_INVALIDARGUMENT,
@@ -4242,7 +4243,7 @@ function maTask(record, agent)
 	this.t_cancelled = false;		/* task is cancelled */
 	this.t_nread = 0;			/* number of inputs read */
 	this.t_ninputs = record['value']['nInputs'];	/* nr of inputs */
-	this.t_abandoned = false;		/* task stolen by worker */
+	this.t_abandoned = false;		/* task stolen by supervisor */
 	this.t_nout_pending = 0;		/* nr of pending taskoutputs */
 	this.t_done = false;			/* task ended */
 	this.t_async_error = undefined;		/* for tasks done early */
@@ -4274,7 +4275,7 @@ function maTaskGroup(groupid, job, pi, log)
 	this.g_intermediate = undefined; /* outputs are intermediate */
 	this.g_checkpoint = false;	/* checkpoint frequently */
 	this.g_poolid = undefined;	/* zone pool to pick from (image) */
-	this.g_domainid = undefined;	/* worker domain id */
+	this.g_domainid = undefined;	/* supervisor domain id */
 
 	/* dynamic state */
 	this.g_state = maTaskGroup.TASKGROUP_S_INIT;
diff --git a/agent/sbin/mrdeploycompute b/agent/sbin/mrdeploycompute
index 7ad482c..c87347f 100755
--- a/agent/sbin/mrdeploycompute
+++ b/agent/sbin/mrdeploycompute
@@ -6,13 +6,13 @@
 #
 
 #
-# Copyright (c) 2014, Joyent, Inc.
+# Copyright (c) 2016, Joyent, Inc.
 #
 
 #
-# mrdeploycompute: combines the steps of provisioning a new worker zone for
-#   Marlin, adding it to the agent, and saving the name in a file for pick up
-#   when the agent restarts.
+# mrdeploycompute: combines the steps of provisioning a new compute zone for
+# Marlin, adding it to the agent, and saving the name in a file for pick up when
+# the agent restarts.
 #
 
 set -o pipefail
diff --git a/client/sbin/mrjob b/client/sbin/mrjob
index 1fb6cf9..5424858 100755
--- a/client/sbin/mrjob
+++ b/client/sbin/mrjob
@@ -7,7 +7,7 @@
  */
 
 /*
- * Copyright (c) 2015, Joyent, Inc.
+ * Copyright (c) 2016, Joyent, Inc.
  */
 
 /*
@@ -69,7 +69,7 @@ var msUsage = [
     'BASIC JOB INSPECTION',
     '',
     '       mrjob list     [-H] [-o field1[,field2...]] [-c] [-s state]',
-    '                      [-t sec] [-u user] [-w worker]',
+    '                      [-t sec] [-u user] [-w supervisor]',
     '       mrjob get      [-ipt] [-l limit] jobid',
     '       mrjob where    jobid',
     '',
@@ -110,7 +110,7 @@ var msHelp = [
     'SUBCOMMANDS',
     '',
     '   list [-H] [-o field1[,field2...]] [-c] [-s state] [-t sec] [-u user]',
-    '        [-T] [-w worker]',
+    '        [-T] [-w supervisor]',
     '',
     '       Lists jobs.  The default output mode is a human-readable, unstable',
     '       output format.  With -H, the output is a simple table with no ',
@@ -125,7 +125,8 @@ var msHelp = [
     '                           "all" may be used to select all jobs.',
     '           -t sec          only jobs completed in last "sec" seconds',
     '           -u user         only jobs owned by "user"',
-    '           -w worker       only jobs currently run by worker "worker"',
+    '           -w supervisor   only jobs currently run by supervisor ' +
+	'"supervisor"',
     '           -T              only non-transient jobs',
     '',
     '   get [-ipt] [-l limit] jobid',
diff --git a/common/lib/bus.js b/common/lib/bus.js
index 843fe16..52bceb4 100644
--- a/common/lib/bus.js
+++ b/common/lib/bus.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright (c) 2014, Joyent, Inc.
+ * Copyright (c) 2016, Joyent, Inc.
  */
 
 /*
@@ -201,9 +201,9 @@ MorayBus.prototype.oneshot = function (bucket, query, options, onrecord, ondone)
  */
 MorayBus.prototype.convertOneshot = function (id, ondone)
 {
-	var worker = this;
+	var bus = this;
 	this.fence(id, function () {
-		delete (worker.mb_subscriptions[id]);
+		delete (bus.mb_subscriptions[id]);
 
 		if (ondone !== undefined)
 			ondone();
diff --git a/common/lib/marlin.js b/common/lib/marlin.js
index ffff647..693246e 100644
--- a/common/lib/marlin.js
+++ b/common/lib/marlin.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright (c) 2015, Joyent, Inc.
+ * Copyright (c) 2016, Joyent, Inc.
  */
 
 /*
@@ -194,11 +194,14 @@ function createFinish(conf, moray, callback, filename, err, contents)
 /*
  * Private constructor for Marlin client handle.  Arguments include:
  *
- *    conf.moray		Moray storage configuration (see worker config)
+ *    conf.moray		Moray storage configuration
+ *                              (see supervisor config)
  *
- *    conf.buckets		Bucket names (see worker config)
+ *    conf.buckets		Bucket names
+ *                              (see supervisor config)
  *
- *    conf.images		Supported image versions (see worker config)
+ *    conf.images		Supported image versions
+ *                              (see supervisor config)
  *
  *    log			Bunyan logger
  *
@@ -1160,7 +1163,7 @@ function jobArchiveReset(api, jobid, options, callback)
  *
  *    jobId		List only job with specified jobId.
  *
- *    worker		List only jobs with specified worker.
+ *    worker		List only jobs with specified worker (jobsupervisor).
  *
  *    doneSince		List only jobs completed in last specified number of
  *			seconds.
diff --git a/common/lib/schema.js b/common/lib/schema.js
index 6249b81..ac15430 100644
--- a/common/lib/schema.js
+++ b/common/lib/schema.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright (c) 2014, Joyent, Inc.
+ * Copyright (c) 2016, Joyent, Inc.
  */
 
 /*
@@ -208,16 +208,16 @@ var sMorayDomain = {
 		'domainId': sStringRequiredNonEmpty,
 
 		/*
-		 * Domains represent a layer of indirection over workers.
+		 * Domains represent a layer of indirection over supervisors.
 		 * Running jobs are partitioned into domains, and the domain for
 		 * a job doesn't change as long as the job runs.  At most one
-		 * worker instance operates a domain at a time, and there's one
-		 * domain per worker under normal conditions, but a worker may
-		 * operate multiple domains when the worker normally responsible
-		 * for it has failed.
+		 * supervisor instance operates a domain at a time, and there's
+		 * one domain per supervisor under normal conditions, but a
+		 * supervisor may operate multiple domains when the supervisor
+		 * normally responsible for it has failed.
 		 */
-		'operatedBy': sString,	/* worker currently operating domain */
-		'wantTransfer': sString	/* worker that wants to take domain */
+		'operatedBy': sString,	/* supervisor currently operating it */
+		'wantTransfer': sString	/* supervisor that wants to take it */
 	}
 };
 
@@ -449,8 +449,8 @@ var sMorayTask = {
 		 *
 		 * Note that not all tasks go through all of these steps.
 		 *
-		 * All tasks are dispatched: this is the time when the worker
-		 * initially wrote out the record.
+		 * All tasks are dispatched: this is the time when the
+		 * supervisor initially wrote out the record.
 		 */
 		'timeDispatched': sDateTimeRequired,	/* time created */
 
@@ -480,13 +480,14 @@ var sMorayTask = {
 		'timeDone': sDateTime,		/* time execution ended */
 
 		/*
-		 * After the task becomes "done", the worker "commits" it,
+		 * After the task becomes "done", the supervisor "commits" it,
 		 * meaning that it has either processed the results of the task
 		 * already or at least marked them to be processed.  This
 		 * process itself involves several state changes on each
-		 * "taskoutput" record.  It's described in lib/worker/worker.js.
+		 * "taskoutput" record.  It's described in
+		 * jobsupervisor/lib/worker/worker.js.
 		 */
-		'timeCommitted': sDateTime,	/* time worker committed */
+		'timeCommitted': sDateTime,	/* time supervisor committed */
 
 		/*
 		 * Tasks can have very large numbers of outputs, so we mark
@@ -524,29 +525,29 @@ var sMorayTask = {
 		/*
 		 * There are two cases where task processing may stop without
 		 * going through the above path: tasks may be taken over by the
-		 * worker with an error, or they may be cancelled.
+		 * supervisor with an error, or they may be cancelled.
 		 *
-		 * The worker takes over a task when it gives up on the agent
-		 * responsible for executing the task.  This happens if the
-		 * agent fails to heartbeat for too long, and also happens today
-		 * if the agent restarts because we don't support resuming
+		 * The supervisor takes over a task when it gives up on the
+		 * agent responsible for executing the task.  This happens if
+		 * the agent fails to heartbeat for too long, and also happens
+		 * today if the agent restarts because we don't support resuming
 		 * accepted tasks after an agent restart.  When taking over a
-		 * task, the worker updates all the fields of the task that the
-		 * agent would except those pertaining to the actual execution
-		 * of the task.  It also sets timeAbandoned.
+		 * task, the supervisor updates all the fields of the task that
+		 * the agent would except those pertaining to the actual
+		 * execution of the task.  It also sets timeAbandoned.
 		 */
-		'timeAbandoned': sDateTime,	/* time worker abandoned */
+		'timeAbandoned': sDateTime,	/* time supervisor abandoned */
 
 		/*
 		 * Cancellation occurs when the job itself is cancelled.  The
-		 * worker cancels all non-done tasks.  Agents operating on these
-		 * tasks stop execution immediately.
+		 * supervisor cancels all non-done tasks.  Agents operating on
+		 * these tasks stop execution immediately.
 		 */
 		'timeCancelled': sDateTime,	/* time task was cancelled */
 
 		/*
-		 * Tasks may be retried by the worker.  wantRetry is set when
-		 * the task is committed, and a separate lap will read such
+		 * Tasks may be retried by the supervisor.  wantRetry is set
+		 * when the task is committed, and a separate lap will read such
 		 * records and set timeRetried when the retry task is
 		 * dispatched.
 		 */
@@ -617,9 +618,9 @@ var sMorayTaskInput = {
 		/*
 		 * To handle cleaning up intermediate data, once a reduce task
 		 * is completed, wantInputRemoved is set on the input objects.
-		 * The worker scans for these, and if the input identifies an
-		 * intermediate object created by this job, we remove it and set
-		 * timeInputRemoved.
+		 * The supervisor scans for these, and if the input identifies
+		 * an intermediate object created by this job, we remove it and
+		 * set timeInputRemoved.
 		 */
 		'wantInputRemoved': sBooleanWorkaround,
 		'timeInputRemoved': sDateTime
@@ -677,10 +678,10 @@ var sBktJsonSchemas =  {
  * Objects in these buckets must conform to the more constrained schemas above.
  *
  * Any changes to these schemas, even backwards-compatible ones, require
- * updating the version number.  On startup, each component (workers and muskie)
- * tries to create or update schemas for the buckets that it uses, so the
- * version number is required to ensure that components with older versions of
- * the schema don't inadvertently replace newer versions (installed by newer
+ * updating the version number.  On startup, each component (supervisors and
+ * muskie) tries to create or update schemas for the buckets that it uses, so
+ * the version number is required to ensure that components with older versions
+ * of the schema don't inadvertently replace newer versions (installed by newer
  * components) when they restart.
  *
  * Backwards-incompatible changes are more complicated: the impact on existing
diff --git a/dev/test/live/common.js b/dev/test/live/common.js
index 7cb9990..d19f9dd 100644
--- a/dev/test/live/common.js
+++ b/dev/test/live/common.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright (c) 2015, Joyent, Inc.
+ * Copyright (c) 2016, Joyent, Inc.
  */
 
 /*
@@ -206,8 +206,8 @@ function jobTestRunner(testcases, argv, concurrency)
  *       including output contents).
  *
  *     o The job completed without any job reassignments (which would indicate a
- *       worker crash) or retries (which would indicate an agent crash), unless
- *       being run in "stress" mode.
+ *       supervisor crash) or retries (which would indicate an agent crash),
+ *       unless being run in "stress" mode.
  *
  *     o The various job counters (inputs, tasks, errors, and outputs) match
  *       what we expect.
diff --git a/docs/index.md b/docs/index.md
index be70279..3ad0ac1 100644
--- a/docs/index.md
+++ b/docs/index.md
@@ -10,7 +10,7 @@ apisections: Task Control API
 -->
 
 <!--
-    Copyright (c) 2014, Joyent, Inc.
+    Copyright (c) 2016, Joyent, Inc.
 -->
 
 # Marlin
@@ -106,10 +106,10 @@ doubt, check the source.
 End users manage compute jobs through the "jobs" entry points under the main
 Manta API.  See the Manta API documentation for details.
 
-Internally, Marlin worker processes (which are responsible for managing the
+Internally, Marlin supervisor processes (which are responsible for managing the
 distributed execution of jobs) communicate with agents (which actually execute
 user commands) through small JSON records stored in Moray, the Manta metadata
-tier.  This mechanism is documented in the worker source code.
+tier.  This mechanism is documented in the supervisor source code.
 
 On each physical server where jobs are executed, the Marlin agent communicates
 with a lackey running inside each compute zone using a private HTTP API which is
@@ -132,13 +132,14 @@ Marlin makes use of two generic Manta components:
 
 On top of these, Marlin adds two components:
 
-* Job worker tier, which locates new and abandoned jobs in Moray, assigns task
-  groups to individual storage and compute nodes, and monitors job progress.
+* Job supervisor tier, which locates new and abandoned jobs in Moray, assigns
+  task groups to individual storage and compute nodes, and monitors job
+  progress.
 * Compute node agents, which execute and monitor tasks and report progress and
   completion.
 
-There's a cluster of web tier workers and job workers in each datacenter, but
-there's no real DC-awareness among these components.
+There's a cluster of web tier servers and job supervisors in each datacenter,
+but there's no real DC-awareness among these components.
 
 **Job state**
 
@@ -156,8 +157,8 @@ implies that Moray becoming unavailable makes Marlin unavailable, and that
 Moray must be able to keep up with the Marlin workload.  See "Scalability"
 below.
 
-Job state is stored across seeral Moray buckets with indexed fields that allow
-the web tier, job workers, and agents to poll on the subset of jobs and job
+Job state is stored across several Moray buckets with indexed fields that allow
+the web tier, job supervisors, and agents to poll on the subset of jobs and job
 state that they care about.  For details on these buckets and the records
 contained therein, see the documentation in lib/schema.js.  A design constraint
 is that the size of every JSON record in the system must be bounded (and should
@@ -172,32 +173,32 @@ given jobId.  When a new job is submitted, muskie drops the job definition into
 the jobs bucket.
 
 
-**Job workers**
+**Job supervisors**
 
-To synchronize changes to job state, each job is assigned to one worker at a
+To synchronize changes to job state, each job is assigned to one supervisor at a
 time.  This assignment is stored in the job object in Moray.  An assignment is
 valid as long as the job's modification time (mtime) is within the last
 WORKER\_TIMEOUT seconds.  Workers save their jobs periodically to indicate
 liveness.  They also poll periodically for unassigned jobs (to pick them up) and
-jobs that have not been updated within the timeout period (to handle worker
+jobs that have not been updated within the timeout period (to handle supervisor
 failure).
 
 Managing the streaming, distributed execution of jobs is a complex process.  The
-basic idea is that workers query Moray for state changes needing attention (new
-jobs, new job inputs, and completed tasks) and process them by issuing new
+basic idea is that supervisors query Moray for state changes needing attention
+(new jobs, new job inputs, and completed tasks) and process them by issuing new
 tasks, updating job state, and so on.  Each input is processed by locating the
 corresponding object and writing out either a task assignment (for map tasks) or
-a taskinput record (for reduce tasks).  For details, see the worker source.
+a taskinput record (for reduce tasks).  For details, see the supervisor source.
 
 **Compute node agents**
 
-Like workers, the per-storage-node agents periodically poll Moray for state
+Like supervisors, the per-storage-node agents periodically poll Moray for state
 changes needing attention (newly assigned tasks, cancelled tasks, and so on) and
 process them by scheduling execution.  Agents are responsible for assigning
 tasks to zones, managing the lifecycle of those zones, keeping track of output,
 and proxying front-door requests between zones and Manta.  As tasks complete
 (successfully or otherwise), the agent writes new updates to Moray that will be
-processed by the worker.
+processed by the supervisor.
 
 
 **Analysis of distributed system failure modes**
@@ -205,7 +206,7 @@ processed by the worker.
 Invariants:
 
 * All state is stored in Moray, so all components are stateless.
-* At most one worker can ever "own" a job.  Only the owner of a job can write
+* At most one supervisor can ever "own" a job.  Only the owner of a job can write
   task assignments or update the job record in "marlinJobs".
 * To a first approximation, a component is partitioned iff it cannot reach
   Moray.  This is an oversimplification because Moray itself is sharded, so it's
@@ -220,24 +221,25 @@ become visible in the final job state.  It's the user's problem at that point.
 
 If a compute node agent fails transiently (e.g., dumps core and restarts), fails
 for an extended period of time, or becomes partitioned from its Moray shard, any
-running tasks are aborted by the worker.  The worker will retry aborted tasks up
-to once.  This retry should be invisible to the user's code and the rest of the
-system, modulo increased task completion time.  Agents ignore tasks dispatched
-before they began running, which covers the case where the agent actually
-crashes.  To cover the case where the agent simply becomes partitioned, the
-worker explicitly cancels tasks that it has retried elsewhere.
-
-If a job worker fails transiently, it can pick up exactly where it left off,
+running tasks are aborted by the supervisor.  The supervisor will retry aborted
+tasks up to once.  This retry should be invisible to the user's code and the
+rest of the system, modulo increased task completion time.  Agents ignore tasks
+dispatched before they began running, which covers the case where the agent
+actually crashes.  To cover the case where the agent simply becomes partitioned,
+the supervisor explicitly cancels tasks that it has retried elsewhere.
+
+If a job supervisor fails transiently, it can pick up exactly where it left off,
 since all state is stored in Moray.  This should be invisible to the rest of the
-system, modulo slightly increased job completion time.  If a job worker fails
-for an extended period, another job worker will pick up its jobs.  This becomes
-exactly like the case where a worker fails transiently: the state in Moray is
-sufficient for the new worker to pick up where the old one left off.
+system, modulo slightly increased job completion time.  If a job supervisor
+fails for an extended period, another job supervisor will pick up its jobs.
+This becomes exactly like the case where a supervisor fails transiently: the
+state in Moray is sufficient for the new supervisor to pick up where the old one
+left off.
 
-If a job worker becomes partitioned from one or more Moray shards, it will stop
-processing updates (since it won't see them).  After the job timeout elapses,
-the worker will stop trying to save the record and another worker will pick up
-the job.  This looks just like a worker failure.
+If a job supervisor becomes partitioned from one or more Moray shards, it will
+stop processing updates (since it won't see them).  After the job timeout
+elapses, the supervisor will stop trying to save the record and another
+supervisor will pick up the job.  This looks just like a supervisor failure.
 
 Web tier nodes are completely stateless.  Any failure results in failing any
 in-flight requests.  Persistent failures have no additional impact as long as
@@ -245,13 +247,13 @@ enough nodes are in service to handle load.  Partitions between the web node and
 the Moray shard storing job records would result in failed requests.
 
 If an entire datacenter becomes partitioned from the rest, then its web tier
-nodes certainly won't be able to service any requests.  All of its job workers
-become partitioned (see above -- the work should eventually be picked up by
-workers in other datacenters).  All of its compute node agents effectively also
-become partitioned, and their work *may* be duplicated in other datacenters.
-Importantly, jobs submitted to the majority partition should always be able to
-complete successfully in this state, since there's always enough copies of the
-data to process the job.
+nodes certainly won't be able to service any requests.  All of its job
+supervisors become partitioned (see above -- the work should eventually be
+picked up by supervisors in other datacenters).  All of its compute node agents
+effectively also become partitioned, and their work *may* be duplicated in other
+datacenters.  Importantly, jobs submitted to the majority partition should
+always be able to complete successfully in this state, since there's always
+enough copies of the data to process the job.
 
 **Scalability analysis**
 
@@ -259,22 +261,22 @@ Being stateless, web tier nodes can be scaled arbitrarily horizontally.  If load
 exceeds capacity, requests will queue up and potentially eventually fail, but
 alarms should ring long before that happens.
 
-Similarly, job workers can be scaled arbitrarily horizontally, and capacity
+Similarly, job supervisors can be scaled arbitrarily horizontally, and capacity
 monitoring should allow administrators to ensure that sufficient capacity is
 always available.  The failure mode for excess load is increased job completion
-time.  Task updates may accumulate in Moray faster than the worker can process
-them, but the total space used in Moray is the same regardless of how fast the
-job workers process them, so there's no cascading failure.  If job workers
-cannot update their locks in Moray, the system may thrash as workers try to take
-each others' jobs, but this would be very extreme.
+time.  Task updates may accumulate in Moray faster than the supervisors can
+process them, but the total space used in Moray is the same regardless of how
+fast the job supervisors process them, so there's no cascading failure.  If job
+supervisors cannot update their locks in Moray, the system may thrash as
+supervisors try to take each others' jobs, but this would be very extreme.
 
 Compute nodes rely on the OS to properly balance the workload within that box.
 They may queue tasks after some very large number of concurrent tasks, but it's
 not anticipated that we would ever reach that point in practice.  If this
-becomes a problem, job workers could conceivably redispatch the task to another
-node with the same data available.  If this truly became a serious problem, we
-could support compute agents that download data, rather than operating locally
-(just as reducers already do).
+becomes a problem, job supervisors could conceivably redispatch the task to
+another node with the same data available.  If this truly became a serious
+problem, we could support compute agents that download data, rather than
+operating locally (just as reducers already do).
 
 Given that there's enough capacity in each of the Marlin tiers, the remaining
 question is Moray.  Moray will be sharded, and we will say that each compute
@@ -286,10 +288,10 @@ relatively minimal.  We should be able to reshard to scale this out.
 
 Each compute node can batch its Moray updates from all tasks for up to N
 seconds, sharply reducing the number of requests required.  Similarly, job
-workers can batch job state updates for many jobs up to N seconds.  Batching of
-"location" GET requests is discussed above, under "job workers".  Thus, the
+supervisors can batch job state updates for many jobs up to N seconds.  Batching of
+"location" GET requests is discussed above, under "job supervisors".  Thus, the
 total number of Moray-wide requests should be boundable to something like (# of
-workers \* # of compute zone lackeys) / (N seconds), where N is how frequently
+supervisors \* # of compute zone lackeys) / (N seconds), where N is how frequently
 each agent updates Moray.  These requests should mostly be distributed across
 all shards.
 
@@ -300,7 +302,7 @@ the source.
 Batching requests over multiple records helps network throughput, but not the
 total number of database operations, since each record must be operated on
 separately.  The total number of database operations could still be around (# of
-active tasks \* # of agents \* # of active jobs \* # of job workers).  Again,
+active tasks \* # of agents \* # of active jobs \* # of job supervisors).  Again,
 these are distributed across many Moray shards, which can be scaled
 horizontally.
 
@@ -308,7 +310,7 @@ horizontally.
 ## Cancellation
 
 To cancel a job, the web tier writes an update to the job record.
-Asynchronously, the job worker will need to be polling on the job state
+Asynchronously, the job supervisor will need to be polling on the job state
 occasionally to notice the cancelled flag.  It propagates this to individual
 task assignments, where the compute node agents will notice that the job has
 been cancelled and abort them.  Eventually, all work on the job will complete,
@@ -584,7 +586,7 @@ that this approach is prohibitively painful.
 
 * flesh out maggr
 * apache2json?
-* Crazy idea #1: eliminate the worker by writing a library to lookup input
+* Crazy idea #1: eliminate the supervisor by writing a library to lookup input
   objects and dispatch them to the next phase.  Have muskie do this for phase 0,
   and agents do this for subsequent phases.  (Should this be a service instead
   of a library?)
diff --git a/jobsupervisor/lib/worker/locator.js b/jobsupervisor/lib/worker/locator.js
index 66ea489..1ebe9eb 100644
--- a/jobsupervisor/lib/worker/locator.js
+++ b/jobsupervisor/lib/worker/locator.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright (c) 2014, Joyent, Inc.
+ * Copyright (c) 2016, Joyent, Inc.
  */
 
 /*
@@ -74,11 +74,11 @@ exports.createLocator = createLocator;
  * deployments.  This locator takes a ring of Moray shards as input and uses
  * them to locate objects.  For future extension, createLocator is used to
  * create an appropriate locator for a given configuration, which is a subset of
- * the standard job worker configuration with the following properties:
+ * the standard job supervisor configuration with the following properties:
  *
  *    locator	The default value is "manta".  See above.
  *
- *    moray	Moray configuration (see generic worker configuration)
+ *    moray	Moray configuration (see generic supervisor configuration)
  *
  *	moray.index: electric moray configuration
  *
diff --git a/jobsupervisor/lib/worker/schema.js b/jobsupervisor/lib/worker/schema.js
index 94f9f3e..37d44c5 100644
--- a/jobsupervisor/lib/worker/schema.js
+++ b/jobsupervisor/lib/worker/schema.js
@@ -5,11 +5,11 @@
  */
 
 /*
- * Copyright (c) 2014, Joyent, Inc.
+ * Copyright (c) 2016, Joyent, Inc.
  */
 
 /*
- * lib/worker/schema.js: worker configuration schema
+ * lib/worker/schema.js: supervisor configuration schema
  */
 
 var mod_schema = require('../schema');
diff --git a/jobsupervisor/lib/worker/server.js b/jobsupervisor/lib/worker/server.js
index c920c62..e43b20a 100644
--- a/jobsupervisor/lib/worker/server.js
+++ b/jobsupervisor/lib/worker/server.js
@@ -5,11 +5,11 @@
  */
 
 /*
- * Copyright (c) 2014, Joyent, Inc.
+ * Copyright (c) 2016, Joyent, Inc.
  */
 
 /*
- * lib/worker/server.js: worker executable
+ * lib/worker/server.js: supervisor (worker) executable
  */
 
 var mod_bunyan = require('bunyan');
@@ -21,11 +21,11 @@ var mod_mautil = require('../util');
 var mod_worker = require('./worker');
 
 var mwLogLevel = process.env['LOG_LEVEL'] || 'info';
-var mwWorkerInstance;
+var mwSupervisorInstance;
 
 function main()
 {
-	var log, conf, worker, server;
+	var log, conf, supervisor, server;
 
 	if (process.argv.length < 3) {
 		console.error('usage: %s %s config.json', process.argv[0],
@@ -44,7 +44,7 @@ function main()
 	conf = mod_mautil.readConf(log, mod_worker.mwConfSchema,
 	    process.argv[2]);
 
-	worker = mwWorkerInstance = new mod_worker.mwWorker({
+	supervisor = mwSupervisorInstance = new mod_worker.mwSupervisor({
 	    'conf': conf,
 	    'log': log
 	});
@@ -59,15 +59,15 @@ function main()
 	    'component': 'jobworker',
 	    'version': '0.0.1',
 	    'ident': conf['instanceUuid'],
-	    'list_types': worker.kangListTypes.bind(worker),
-	    'list_objects': worker.kangListObjects.bind(worker),
-	    'get': worker.kangGetObject.bind(worker),
-	    'schema': worker.kangSchema.bind(worker),
-	    'stats': worker.kangStats.bind(worker)
+	    'list_types': supervisor.kangListTypes.bind(supervisor),
+	    'list_objects': supervisor.kangListObjects.bind(supervisor),
+	    'get': supervisor.kangGetObject.bind(supervisor),
+	    'schema': supervisor.kangSchema.bind(supervisor),
+	    'stats': supervisor.kangStats.bind(supervisor)
 	}));
 
 	server.post('/quiesce', function (request, response, next) {
-		worker.quiesce(true, function (err) {
+		supervisor.quiesce(true, function (err) {
 			if (!err)
 				response.send(204);
 			next(err);
@@ -75,7 +75,7 @@ function main()
 	});
 
 	server.post('/unquiesce', function (request, response, next) {
-		worker.quiesce(false, function (err) {
+		supervisor.quiesce(false, function (err) {
 			if (!err)
 				response.send(204);
 			next(err);
@@ -86,7 +86,7 @@ function main()
 		var addr = server.address();
 		log.info('server listening at http://%s:%d',
 		    addr['address'], addr['port']);
-		worker.start();
+		supervisor.start();
 	});
 }
 
diff --git a/jobsupervisor/lib/worker/worker.js b/jobsupervisor/lib/worker/worker.js
index 723f2f4..9485fd6 100644
--- a/jobsupervisor/lib/worker/worker.js
+++ b/jobsupervisor/lib/worker/worker.js
@@ -5,35 +5,37 @@
  */
 
 /*
- * Copyright (c) 2014, Joyent, Inc.
+ * Copyright (c) 2016, Joyent, Inc.
  */
 
 /*
- * lib/worker/worker.js: job worker implementation
+ * lib/worker/worker.js: job supervisor implementation
  */
 
 /*
- * Each Marlin deployment includes a fleet of job workers that are responsible
+ * Each Marlin deployment includes a fleet of job supervisors (formerly called
+ * "workers", and still referred to as such in the code) that are responsible
  * for managing the distributed execution of Marlin jobs.  The core of each
- * worker is a loop that looks for new and abandoned jobs, divides each job into
- * chunks called tasks, assigns these tasks to individual compute nodes,
+ * supervisor is a loop that looks for new and abandoned jobs, divides each job
+ * into chunks called tasks, assigns these tasks to individual compute nodes,
  * monitors each node's progress, and collects the results.  While individual
- * workers are not resource-intensive, a fleet is used to support very large
+ * supervisors are not resource-intensive, a fleet is used to support very large
  * numbers of jobs concurrently and to provide increased availability in the
  * face of failures and partitions.
  *
  * Jobs and tasks are represented as records within Moray instances, which are
  * themselves highly available.  At any given time, a job is assigned to at most
- * one worker, and this assignment is stored in the job's record in Moray.
- * Workers do not maintain any state which cannot be reconstructed from the
- * state stored in Moray.  This makes it possible for workers to pick up jobs
- * abandoned by other workers which have failed or become partitioned from the
- * Moray ring.  In order to detect such failures, workers must update job
- * records on a regular basis (even if there's no substantial state change) so
- * that failure to update the job record indicates that a worker has failed.
+ * one supervisor, and this assignment is stored in the job's record in Moray.
+ * Supervisors do not maintain any state which cannot be reconstructed from the
+ * state stored in Moray.  This makes it possible for supervisors to pick up
+ * jobs abandoned by other supervisors which have failed or become partitioned
+ * from the Moray ring.  In order to detect such failures, supervisors must
+ * update job records on a regular basis (even if there's no substantial state
+ * change) so that failure to update the job record indicates that a supervisor
+ * has failed.
  *
- * All communication among the workers, compute nodes, and the web tier (through
- * which jobs are submitted and monitored) goes through Moray.
+ * All communication among the supervisors, compute nodes, and the web tier
+ * (through which jobs are submitted and monitored) goes through Moray.
  *
  * Jobs run through the following states:
  *
@@ -67,14 +69,14 @@
  *     login, and authentication token), and various metadata about the job
  *     (time created and the like).
  *
- * (2) Job assignment: some time very shortly later, one of several job workers
- *     finds the job and does an etag-conditional PUT to set "worker" to the
- *     worker's uuid.  Many workers may attempt this, but only one may succeed.
- *     The others forget about the job.
+ * (2) Job assignment: some time very shortly later, one of several job
+ *     supervisors finds the job and does an etag-conditional PUT to set
+ *     "worker" to the supervisor's uuid.  Many supervisors may attempt this,
+ *     but only one may succeed.  The others forget about the job.
  *
  * (3) Job input submission: the user submits inputs for the job via muskie.
  *     For each input, muskie creates a "jobinput" record that includes the
- *     object's name.  The worker runs through the pipeline described under
+ *     object's name.  The supervisor runs through the pipeline described under
  *     "Dispatch pipeline" below, which results in either an error or a task
  *     assigned to a particular agent.
  *
@@ -90,26 +92,26 @@
  *     Otherwise, a separate "error" object is included in the same transaction
  *     as the task update, and the task's result = "fail".
  *
- * (6) Task commit: the worker sees the task has been updated with state =
- *     "done".  If the task failed, then the worker updates the task *and* all
- *     taskoutput records with "committed" and "propagated" to true, but "valid"
- *     = false.  If the task completed successfully, the worker writes a
- *     transaction that sets "committed" and "valid" to true on the task *and*
- *     all the taskoutput records.  Note that in both cases, the taskoutput
- *     updates are a server-side mass update, since there could be an
+ * (6) Task commit: the supervisor sees the task has been updated with state =
+ *     "done".  If the task failed, then the supervisor updates the task *and*
+ *     all taskoutput records with "committed" and "propagated" to true, but
+ *     "valid" = false.  If the task completed successfully, the supervisor
+ *     writes a transaction that sets "committed" and "valid" to true on the
+ *     task *and* all the taskoutput records.  Note that in both cases, the
+ *     taskoutput updates are a server-side mass update, since there could be an
  *     arbitrarily large number of them.
  *
  * (7) Task propagation: for each "taskoutput" record committed but not marked
  *     propagated in (4f), all of step (4) is repeated, except that in (4b), the
- *     worker sets propagated = true on the "taskoutput" record (instead of a
- *     "jobinput" record).
+ *     supervisor sets propagated = true on the "taskoutput" record (instead of
+ *     a "jobinput" record).
  *
  * (8) Ending job input: some time later, the user indicates to muskie that the
  *     job's input stream has ended.  When this happens, muskie updates the
  *     "job" record to indicate that inputDone = true.
  *
- * (9) Job completion: When the job completes, the worker updates the job record
- *     to set state = "done".  The job is complete only when:
+ * (9) Job completion: When the job completes, the supervisor updates the job
+ *     record to set state = "done".  The job is complete only when:
  *
  *         (a) The "job" record indicates that the input stream has ended.
  *
@@ -137,9 +139,9 @@
  * TASK CANCELLATION
  *
  * Individual tasks may be cancelled, either because of a job cancellation or
- * because the worker decides to time out an agent.  In that case, the task's
- * record is updated to set cancelled = true, and the agent immediately forgets
- * about the task.
+ * because the supervisor decides to time out an agent.  In that case, the
+ * task's record is updated to set cancelled = true, and the agent immediately
+ * forgets about the task.
  *
  *
  * REDUCE TASKS
@@ -159,14 +161,14 @@
  *     execution will continue (blocking if necessary) until the task's
  *     inputDone is true and all inputs have been read.
  *
- * (2) Instead of dispatching a "task" for each input object, the worker
+ * (2) Instead of dispatching a "task" for each input object, the supervisor
  *     dispatches a "taskinput" record.  When the agent sees it, instead of
  *     setting state = "accepted", it sets read = true.
  *
- * (3) The worker and agent must coordinate to keep track of when the input for
- *     each reducer is complete.  This condition is exactly equivalent to the
- *     "job done" condition described in (9) above, except it must only be true
- *     for phases 0 ... i - 1, where i = the reducer's phase.
+ * (3) The supervisor and agent must coordinate to keep track of when the input
+ *     for each reducer is complete.  This condition is exactly equivalent to
+ *     the "job done" condition described in (9) above, except it must only be
+ *     true for phases 0 ... i - 1, where i = the reducer's phase.
  *
  * The detailed list of queries used for polling can be found in
  * agent/lib/agent/queries.js and jobsupervisor/lib/worker/queries.js.
@@ -208,13 +210,13 @@ require('../errors');
 
 /* Public interface */
 exports.mwConfSchema = mwConfSchema;
-exports.mwWorker = Worker;
+exports.mwSupervisor = Worker;
 
 /*
  * Maintains state for a single job.  The logic for this class lives in the
  * worker class.  Arguments include:
  *
- *    conf	worker configuration
+ *    conf	supervisor (worker) configuration
  *
  *    log	bunyan-style logger
  *
@@ -252,10 +254,11 @@ function JobState(args)
 	this.j_last_input = undefined;
 
 	/*
-	 * We only poll for job inputs with our own workerid.  Once the workerid
-	 * has been assigned, muskie will fill it in for new job inputs, but we
-	 * must periodically mark it for existing inputs in case muskie wrote
-	 * (or is writing) some inputs before the workerid was assigned.
+	 * We only poll for job inputs with our own jobsupervisor id.  Once the
+	 * jobsupervisor id has been assigned, muskie will fill it in for new
+	 * job inputs, but we must periodically mark it for existing inputs in
+	 * case muskie wrote (or is writing) some inputs before the
+	 * jobsupervisor id was assigned.
 	 */
 	this.j_mark_inputs = new Throttler(tunables['timeMarkInputs']);
 	this.j_mark_pending = false;
@@ -357,15 +360,15 @@ function AgentState(record)
 }
 
 /*
- * Worker health management: workers are also responsible for monitoring the
- * health of other workers and taking over for them when they disappear.
+ * Supervisor health management: supervisors are also responsible for monitoring
+ * the health of other supervisors and taking over for them when they disappear.
  */
-function WorkerState(record)
+function SupervisorState(record)
 {
-	this.ws_last = Date.now();
-	this.ws_record = record;
-	this.ws_operating = [];
-	this.ws_timedout = false;
+	this.ss_last = Date.now();
+	this.ss_record = record;
+	this.ss_operating = [];
+	this.ss_timedout = false;
 }
 
 function JobCacheState()
@@ -2259,7 +2262,7 @@ Worker.prototype.taskRecordCleanupFini = function (record, barrier, job,
 
 Worker.prototype.onRecordDomain = function (record, barrier)
 {
-	var domainid, oldrecord, oldoperator, newoperator, ws, i;
+	var domainid, oldrecord, oldoperator, newoperator, ss, i;
 
 	/*
 	 * Update the reverse mapping of worker -> domains operated based on the
@@ -2273,17 +2276,17 @@ Worker.prototype.onRecordDomain = function (record, barrier)
 		oldoperator = oldrecord['value']['operatedBy'];
 		if (oldoperator !== newoperator &&
 		    this.w_allworkers.hasOwnProperty(oldoperator)) {
-			ws = this.w_allworkers[oldoperator];
-			i = ws.ws_operating.indexOf(domainid);
+			ss = this.w_allworkers[oldoperator];
+			i = ss.ss_operating.indexOf(domainid);
 			if (i != -1)
-				ws.ws_operating.splice(i, 1);
+				ss.ss_operating.splice(i, 1);
 		}
 	}
 
 	if (this.w_allworkers.hasOwnProperty(newoperator)) {
-		ws = this.w_allworkers[newoperator];
-		if (ws.ws_operating.indexOf(domainid) == -1)
-			ws.ws_operating.push(domainid);
+		ss = this.w_allworkers[newoperator];
+		if (ss.ss_operating.indexOf(domainid) == -1)
+			ss.ss_operating.push(domainid);
 	}
 
 	this.w_alldomains[domainid] = record;
@@ -2326,7 +2329,7 @@ Worker.prototype.onRecordHealth = function (record, barrier)
 
 Worker.prototype.onWorkerHealth = function (record, barrier)
 {
-	var instance, ws, oldrec, now;
+	var instance, ss, oldrec, now;
 
 	instance = record['value']['instance'];
 	if (instance == this.w_uuid)
@@ -2334,22 +2337,22 @@ Worker.prototype.onWorkerHealth = function (record, barrier)
 
 	if (!this.w_allworkers.hasOwnProperty(instance)) {
 		this.w_log.info('worker "%s": discovered', instance);
-		this.w_allworkers[instance] = new WorkerState(record);
+		this.w_allworkers[instance] = new SupervisorState(record);
 		return;
 	}
 
-	ws = this.w_allworkers[instance];
-	oldrec = ws.ws_record;
+	ss = this.w_allworkers[instance];
+	oldrec = ss.ss_record;
 	now = Date.now();
 	if (oldrec['_mtime'] != record['_mtime']) {
 		/* This worker is alive, since the health record was updated. */
-		if (ws.ws_timedout) {
+		if (ss.ss_timedout) {
 			this.w_log.info('worker "%s": came back', instance);
-			ws.ws_timedout = false;
+			ss.ws_timedout = false;
 		}
-		ws.ws_last = now;
-		ws.ws_record = record;
-	} else if (now - ws.ws_last >
+		ss.ss_last = now;
+		ss.ss_record = record;
+	} else if (now - ss.ss_last >
 	    this.w_conf['tunables']['timeWorkerAbandon']) {
 		/*
 		 * It's been too long since this health record has been updated.
@@ -2359,15 +2362,15 @@ Worker.prototype.onWorkerHealth = function (record, barrier)
 		 * load.  We use the barrier to avoid queueing multiple attempts
 		 * to takeover the same domain.
 		 */
-		if (!ws.ws_timedout) {
+		if (!ss.ws_timedout) {
 			this.w_log.info('worker "%s": timed out', instance);
 			this.w_dtrace.fire('worker-timeout',
 			    function () { return ([ instance ]); });
-			ws.ws_timedout = true;
+			ss.ws_timedout = true;
 		}
 
-		if (ws.ws_operating.length > 0)
-			this.domainTakeover(mod_jsprim.randElt(ws.ws_operating),
+		if (ss.ss_operating.length > 0)
+			this.domainTakeover(mod_jsprim.randElt(ss.ss_operating),
 			    barrier, instance);
 	}
 };
-- 
2.21.0

