commit dd0f7e05dcf38521c6b9af723e77a43d8e476628 (refs/changes/49/1749/1)
Author: David Pacheco <dap@joyent.com>
Date:   2017-04-04T16:18:14-07:00 (2 years, 6 months ago)
    
    MANTA-3195 manta amon configuration could use work
    MANTA-3191 nameservice zones get duplicate "logs not uploaded" probes
    MANTA-3194 postgres zones get duplicate "registrar-logscan" probes

diff --git a/Makefile b/Makefile
index 316845a..adcbb64 100644
--- a/Makefile
+++ b/Makefile
@@ -5,7 +5,7 @@
 #
 
 #
-# Copyright (c) 2016, Joyent, Inc.
+# Copyright (c) 2017, Joyent, Inc.
 #
 
 #
@@ -24,6 +24,7 @@
 # Programs
 #
 CATEST		 = deps/catest/catest
+PROBECHK	 = node ./tools/probecfgchk.js
 
 #
 # Options and overrides
@@ -50,6 +51,7 @@ JSON_FILES	 = package.json \
 		   $(shell find config \
 				manifests \
 				sapi_manifests -name '*.json*')
+PROBE_FILES	 = $(wildcard alarm_metadata/probe_templates/*.yaml)
 
 include ./tools/mk/Makefile.defs
 include ./tools/mk/Makefile.node_deps.defs
@@ -88,7 +90,10 @@ all: $(SMF_MANIFESTS) deps sdc-scripts
 .PHONY: manpages
 manpages: $(MAN_OUTPUTS)
 
-check:: $(NODE_EXEC)
+check:: $(NODE_EXEC) check-probe-files
+
+check-probe-files:
+	$(PROBECHK) $(PROBE_FILES)
 
 .PHONY: test
 test: | $(CATEST)
diff --git a/alarm_metadata/probe_templates/authcache.yaml b/alarm_metadata/probe_templates/authcache.yaml
new file mode 100644
index 0000000..a022c58
--- /dev/null
+++ b/alarm_metadata/probe_templates/authcache.yaml
@@ -0,0 +1,181 @@
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2017, Joyent, Inc.
+#
+
+#
+# amon probes for the "authcache" service
+#
+# For background information, see lib/alarms/index.js.  The format of this file
+# is described in lib/alarms/metadata.js.
+#
+
+#
+# Log scanners
+#
+
+-
+    # log scan: "mahi-replicator" SMF service
+    event: upset.manta.authcache.mahi_replicator.log_error
+    legacyName: mahi-replicator-logscan-error
+    scope:
+        service: authcache
+    checks:
+        -
+            type: bunyan-log-scan
+            config:
+                smfServiceName: mahi-replicator
+                fields:
+                    level: ERROR
+                threshold: 1
+                period: 60
+    ka:
+        title: '"mahi-replicator" logged an error'
+        description: The "mahi-replicator" service has logged an error.
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            If the problem was transient, there may be no impact.  Otherwise,
+            replication may be falling behind.  In that case, recent changes to
+            user accounts, sub-users, roles, and ssh keys may not be reflected
+            in Manta.  Newly-created accounts, users, roles, and keys may not be
+            available, and those that have recently been deleted may still be
+            working.  Service is unaffected for accounts, users, roles, and keys
+            that have not changed recently.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+-
+    # log scan: "mahi-server" SMF service
+    event: upset.manta.authcache.mahi_server.log_error
+    legacyName: mahi-server-logscan-error
+    scope:
+        service: authcache
+    checks:
+        -
+            type: bunyan-log-scan
+            config:
+                smfServiceName: mahi-server
+                fields:
+                    level: ERROR
+                threshold: 1
+                period: 60
+    ka:
+        title: '"mahi-server" logged an error'
+        description: The "mahi-server" service has logged an error.
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            Some end user requests may have failed due to authentiation
+            failures.  The problem may be ongoing.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+#
+# Periodic checks
+#
+
+-
+    # Checks whether Redis is at least responding to its own pings.
+    event: upset.manta.authcache.redis.ping_fail
+    legacyName: redis-ok
+    scope: 
+        service: authcache
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "/opt/local/bin/redis-cli PING"
+                stdoutMatch: 
+                    pattern: PONG
+                    invert: true
+                threshold: 1
+                period: 60
+    ka: 
+        title: Authcache Redis ping failed
+        description: >-
+            A periodic ping of an authentication cache's redis instance has
+            failed.
+        severity: critical
+        response: No automated response will be taken.
+        impact: >-
+            Some requests may be failing due to authentication failures.  The
+            impact is likely to be proportional to the number of "authcache"
+            instances that have experienced this problem.  Note that
+            authentication information is cached for a few minutes, so requests
+            may not start failing immediately.
+        action: >-
+            Root-cause the failure and resolve the underlying issue.  You may
+            need to restart the Redis service.
+
+-
+    # Checks for v1 are under the "v1" namespace.
+    event: upset.manta.authcache.v1.behind
+    legacyName: mahi v1 falling behind by more than 5000 changenumbers
+    scope:
+        service: authcache
+    checks:
+        -
+            type: cmd
+            config:
+                cmd: "if [ $(/opt/smartdc/mahi/bin/cn_delta) -gt 5000 ]; then exit 1; else exit 0; fi"
+                interval: 30
+                threshold: 10
+                period: 1800
+                timeout: 29
+    ka:
+        title: Authcache v1 replication has fallen behind
+        description: >-
+            Replication for the v1 authentication cache has fallen behind.
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            Recent changes to user accounts, sub-users, roles, and ssh keys may
+            not be reflected in Manta.  Newly-created accounts, users, roles,
+            and keys may not be available, and those that have recently been
+            deleted may still be working.  Service is unaffected for accounts,
+            users, roles, and keys that have not changed recently.
+        action: >-
+            Check the log for the "mahi" SMF service inside the affected
+            authcache zone.  If necessary, restart the service.
+
+-
+    #
+    # Checks for the current authcache version ("v2") are not under
+    # version-specific namespace.
+    #
+    event: upset.manta.authcache.replicator.behind
+    legacyName: mahi v2 falling behind by more than 5000 changenumbers
+    scope:
+        service: authcache
+    checks:
+        - 
+            type: cmd
+            config:
+                cmd: "if [ $(/opt/smartdc/mahi/bin/cn_delta2) -gt 5000 ]; then exit 1; else exit 0; fi"
+                interval: 30
+                threshold: 10
+                period: 1800
+                timeout: 29
+    ka:
+        title: Authcache replication has fallen behind
+        description: Replication for the authentication cache has fallen behind
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            Recent changes to user accounts, sub-users, roles, and ssh keys may
+            not be reflected in Manta.  Newly-created accounts, users, roles,
+            and keys may not be available, and those that have recently been
+            deleted may still be working.  Service is unaffected for accounts,
+            users, roles, and keys that have not changed recently.
+        action: >-
+            Check the log for the "mahi-replicator" SMF service inside the
+            affected authcache zone.  If necessary, restart the service.
+
diff --git a/alarm_metadata/probe_templates/common.yaml b/alarm_metadata/probe_templates/common.yaml
new file mode 100644
index 0000000..7d2e234
--- /dev/null
+++ b/alarm_metadata/probe_templates/common.yaml
@@ -0,0 +1,182 @@
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2017, Joyent, Inc.
+#
+
+#
+# amon probes common to all services deployed to non-global zones
+#
+# For background information, see lib/alarms/index.js.  The format of this file
+# is described in lib/alarms/metadata.js.
+#
+
+-
+    #
+    # The CPU utilization probe notifies operators when any server is at high
+    # overall CPU utilization for an extended period of time (currently 23
+    # per-minute samples within 30 minutes).  This might be indicative of
+    # degraded service, though it doesn't react quickly enough to usefully
+    # identify that.  It's conceivably useful for capacity planning, but that
+    # would likely be better accomplished using a metric collection system that
+    # keeps historical data.
+    #
+    # Also of note: this probe is deliberately per-zone rather than per-CN.
+    # That helps identify which components are affected by heavy CPU
+    # utilization.
+    #
+    event: upset.manta.zone.cpu_utilized
+    legacyName: cpu utilization
+    scope: 
+        service: all
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "test ! $(mpstat -a 1 2 | tail -n 1 | nawk '{total=$13+$14} END {print total}') -gt 80"
+                interval: 60
+                threshold: 23
+                period: 1800
+                timeout: 30
+    ka: 
+        title: Excessive CPU utilization
+        description: Aggregate CPU usage has been high for several minutes
+        severity: minor
+        response: No automated response will be taken.
+        impact: Service provided by affected components may be degraded.
+        action: >-
+            Identify the source of excessive CPU utilization and resolve the
+            issue.  If components are behaving normally, consider adding
+            additional capacity using new servers.
+
+-
+    event: upset.manta.zone.filesystem_almost_full
+    legacyName: "free space on / below 20%"
+    scope: 
+        service: all
+    checks: 
+        - 
+            type: disk-usage
+            config: 
+                path: "/"
+                threshold: "20%"
+                interval: 3600
+    ka: 
+        title: Filesystem almost full
+        description: A zone filesystem is running low on free space
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            There is no immediate impact, but if the filesystem fills up,
+            service may become severely degraded.  End user requests may
+            experience high error rates or increased latency.
+        action: >-
+            Identify the cause of excessive disk usage and resolve the
+            underlying issue.
+
+-
+    event: upset.manta.registrar.log_error
+    legacyName: registrar-logscan
+    scope: 
+        service: all
+    checks: 
+        - 
+            type: bunyan-log-scan
+            config: 
+                smfServiceName: registrar
+                fields: 
+                    level: FATAL
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"registrar" logged an error'
+        description: The "registrar" service has logged an error.
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            If the problem was transient, there may be no impact.  Otherwise, a
+            component may no longer be registered for internal service
+            discovery.  Capacity may be affected, resulting in increased latency
+            or error rates for end user requests.  If enough instances
+            experience this issue, a major service disruption could result.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+-
+    event: upset.manta.zone.logs_lingering
+    legacyName: logs not uploaded
+    scope: 
+        service: all
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "test ! $(find /var/log/manta/upload -type f | wc -l) -gt 0"
+                interval: 300
+                threshold: 5
+                period: 1800
+    ka: 
+        title: Log files not uploaded
+        description: Some log files have not been uploaded
+        severity: minor
+        response: >-
+            The system automatically retries hourly to upload any internal log
+            files that have not yet been uploaded.
+        impact: >-
+            There is no impact to end-user service.  However, failure to upload
+            files is often indicative of problems affecting end user requests.
+            
+            If the affected logs are used for metering, then metering reports
+            and access logs for end users may be unavailable or incomplete until
+            the affected logs are uploaded and the relevant metering jobs re-run
+            by an operator.
+        action: >-
+            Identify the reason for the failure and resolve the underlying
+            issue.  If logs used for metering were affected, you may need to
+            re-run the relevant metering jobs once all logs are available.
+            
+            In most components, the log "/var/log/mbackup.log" has a record of
+            recent upload attempts and results.  Another common cause of log
+            upload failure is when a component or service was offline during the
+            scheduled log upload time.
+
+-
+    event: upset.manta.$service.smf_maintenance
+    legacyName: "svcs: SMF maintenance"
+    #
+    # We use the scope "each" here (rather than "all") to make sure that we get
+    # different probe groups (and therefore different alarms) when SMF services
+    # from different SAPI services go into maintenance.
+    #
+    scope: 
+        service: each
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "/usr/bin/svcs -x"
+                stdoutMatch: 
+                    pattern: maintenance
+                    matchWord: true
+                threshold: 1
+                period: 60
+                timeout: 30
+    ka: 
+        title: SMF service in maintenance
+        description: One or more SMF services are in maintenance
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            The impact depends on which services are in maintenance.  In some
+            cases, overall request handling capacity may be reduced.  If enough
+            instances are in maintenance, end users could experience errors.
+        action: >-
+            In the affected zones, use "svcs -xv" to identify the services in
+            maintenance and see basic instructions for tracking down the
+            problem.
+
diff --git a/alarm_metadata/probe_templates/electric-moray.yaml b/alarm_metadata/probe_templates/electric-moray.yaml
new file mode 100644
index 0000000..b1e255d
--- /dev/null
+++ b/alarm_metadata/probe_templates/electric-moray.yaml
@@ -0,0 +1,43 @@
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2017, Joyent, Inc.
+#
+
+#
+# amon probes for the "electric-moray" service
+#
+# For background information, see lib/alarms/index.js.  The format of this file
+# is described in lib/alarms/metadata.js.
+#
+
+-
+    event: upset.manta.electric_moray.log_error
+    legacyName: electric-moray-logscan
+    scope: 
+        service: electric-moray
+    checks: 
+        - 
+            type: bunyan-log-scan
+            config: 
+                path: "/var/log/electric-moray.log"
+                fields: 
+                    level: ERROR
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"electric-moray" logged an error'
+        description: The "electric-moray" service has logged an error.
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            A small number of end-user requests may have failed.  Some job
+            tasks may experience increased latency to dispatch.  If the problem
+            persists, many requests and tasks may be affected.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
diff --git a/alarm_metadata/probe_templates/jobsupervisor.yaml b/alarm_metadata/probe_templates/jobsupervisor.yaml
new file mode 100644
index 0000000..2eb05d8
--- /dev/null
+++ b/alarm_metadata/probe_templates/jobsupervisor.yaml
@@ -0,0 +1,59 @@
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2017, Joyent, Inc.
+#
+
+#
+# amon probes for the "jobsupervisor" service
+#
+# For background information, see lib/alarms/index.js.  The format of this file
+# is described in lib/alarms/metadata.js.
+#
+
+-
+    event: upset.manta.jobsupervisor.log_error
+    legacyName: jobsupervisor-logscan-error, jobsupervisor-logscan-fatal, jobsupervisor-logscan-core
+    scope: 
+        service: jobsupervisor
+    checks: 
+        - 
+            type: bunyan-log-scan
+            config: 
+                smfServiceName: jobsupervisor
+                fields: 
+                    level: ERROR
+                threshold: 1
+                period: 60
+        - 
+            type: bunyan-log-scan
+            config: 
+                smfServiceName: jobsupervisor
+                fields: 
+                    level: FATAL
+                threshold: 1
+                period: 60
+        - 
+            type: log-scan
+            config: 
+                smfServiceName: jobsupervisor
+                match: 
+                    pattern: Stopping because process dumped core.
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"jobsupervisor" logged an error'
+        description: The "jobsupervisor" service has logged an error.
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            If the problem was transient, there may be no impact.  Otherwise,
+            some jobs may have experienced errors or additional latency.  It is
+            possible that some jobs are stuck.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
diff --git a/alarm_metadata/probe_templates/loadbalancer.yaml b/alarm_metadata/probe_templates/loadbalancer.yaml
new file mode 100644
index 0000000..eddb9fe
--- /dev/null
+++ b/alarm_metadata/probe_templates/loadbalancer.yaml
@@ -0,0 +1,148 @@
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2017, Joyent, Inc.
+#
+
+#
+# amon probes for the "loadbalancer" service
+#
+# For background information, see lib/alarms/index.js.  The format of this file
+# is described in lib/alarms/metadata.js.
+#
+
+-
+    event: upset.manta.loadbalancer.haproxy.memory
+    legacyName: "haproxy memory size (1G)"
+    scope: 
+        service: loadbalancer
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "ps -o rss= -p \"$(pgrep -c \"$(svcs -H -o ctid haproxy)\")\" | awk '$1 > 1048576{ printf(\"haproxy rss too large\\n\"); }'"
+                stdoutMatch: 
+                    pattern: haproxy rss too large
+                interval: 120
+                threshold: 2
+                period: 360
+    ka: 
+        title: Loadbalancer "haproxy" using too much memory
+        description: >-
+            Loadbalancer "haproxy" processes are using more memory than
+            expected.
+        severity: minor
+        response: No automated response will be taken.
+        impact: >-
+            There is no immediate impact.  However, if processes are leaking
+            memory, then performance may degrade and errors may be induced in
+            end-user requests.
+        action:
+            Check the "haproxy" process for memory leaks.
+
+-
+    event: upset.manta.loadbalancer.stud.memory
+    legacyName: "stud memory size (1G)"
+    scope: 
+        service: loadbalancer
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "test ! $(ps -orss -p \"`pgrep stud`\" | grep -v RSS | nawk '{t+=$1}END{print t}') -gt 1048576"
+                interval: 120
+                threshold: 2
+                period: 360
+    ka: 
+        title: Loadbalancer "stud" using too much memory
+        description: >-
+            Loadbalancer "stud" processes are using more memory than expected.
+        severity: minor
+        response: No automated response will be taken.
+        impact: >-
+            There is no immediate impact.  However, if processes are leaking
+            memory, then performance may degrade and errors may be induced in
+            end-user requests.
+        action:
+            Check the "stud" process for memory leaks.
+
+-
+    event: upset.manta.loadbalancer.muppet.memory
+    legacyName: "muppet memory size (512M)"
+    scope: 
+        service: loadbalancer
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "test ! $(ps -orss -p $(svcs -Hoctid -p muppet | tail -n 1 | awk '{print $2}') | tail -n 1) -gt 524288"
+                interval: 120
+                threshold: 2
+                period: 360
+    ka: 
+        title: Loadbalancer "muppet" using too much memory
+        description: >-
+            Loadbalancer "muppet" processes are using more memory than expected.
+        severity: minor
+        response: No automated response will be taken.
+        impact: There is no immediate impact.
+        action: Check the "muppet" process for memory leaks.
+
+-
+    event: upset.manta.loadbalancer.muppet.log_error
+    legacyName: muppet-logscan
+    scope: 
+        service: loadbalancer
+    checks: 
+        - 
+            type: bunyan-log-scan
+            config: 
+                smfServiceName: muppet
+                fields: 
+                    level: ERROR
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"muppet" logged an error'
+        description: The "muppet" service has logged an error.
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            If the problem was transient, there may be no impact.  Otherwise,
+            loadbalancers may not be correctly identifying when "webapi"
+            instances have come and gone.  They may be continuing to use old
+            webapi instances or ignoring new instances.
+        action:
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+-
+    event: upset.manta.loadbalancer.no_backends
+    legacyName: no backend servers
+    scope: 
+        service: loadbalancer
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "test -n $(/opt/smartdc/muppet/build/node/bin/node /opt/smartdc/muppet/node_modules/.bin/haproxystat showStat /tmp/haproxy  | /opt/smartdc/muppet/build/node/bin/node -e 's=\"\"; process.stdin.resume(); process.stdin.on(\"data\",function(c){s+=c}); process.stdin.on(\"end\",function(){o=eval(\"(\"+s+\")\");console.log(JSON.stringify(o)); });' | /usr/bin/json  -c 'this.type == \"backend\" && !/stats/.test(this.pxname) && act == 0' -a pxname)"
+                interval: 30
+                threshold: 2
+                period: 120
+    ka: 
+        title: Loadbalancer has no backends
+        description: A loadbalancer has no working backends
+        severity: critical
+        response: No automated response will be taken.
+        impact:
+            End user requests may be experiencing very high error rates.
+        action:
+            Determine why the loadbalancer has no backends and resolve the
+            underlying issue.  First, ensure that there are working "webapi"
+            instances.  If so, identify whether they're registered in ZooKeeper.
+            If so, see if the Muppet instance in this loadbalancer zone has
+            found them.
diff --git a/alarm_metadata/probe_templates/moray.yaml b/alarm_metadata/probe_templates/moray.yaml
new file mode 100644
index 0000000..70b63f2
--- /dev/null
+++ b/alarm_metadata/probe_templates/moray.yaml
@@ -0,0 +1,44 @@
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2017, Joyent, Inc.
+#
+
+#
+# amon probes for the "moray" service
+#
+# For background information, see lib/alarms/index.js.  The format of this file
+# is described in lib/alarms/metadata.js.
+#
+
+-
+    event: upset.manta.moray.log_error
+    legacyName: moray-logscan
+    scope: 
+        service: moray
+    checks: 
+        - 
+            type: bunyan-log-scan
+            config: 
+                path: "/var/log/moray.log"
+                fields: 
+                    level: ERROR
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"moray" logged an error'
+        description: The "moray" service has logged an error.
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            If the problem was transient, there may be no impact.  Otherwise,
+            some end user requests may be experiencing errors or some jobs may
+            be experiencing errors or additional latency.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
diff --git a/alarm_metadata/probe_templates/nameservice.yaml b/alarm_metadata/probe_templates/nameservice.yaml
new file mode 100644
index 0000000..2cfe4fb
--- /dev/null
+++ b/alarm_metadata/probe_templates/nameservice.yaml
@@ -0,0 +1,129 @@
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2017, Joyent, Inc.
+#
+
+#
+# amon probes for the "nameservice" service
+#
+# For background information, see lib/alarms/index.js.  The format of this file
+# is described in lib/alarms/metadata.js.
+#
+
+-
+    event: upset.manta.nameservice.zookeeper.log_error
+    legacyName: "ZK: logscan 'ERROR'"
+    scope: 
+        service: nameservice
+    checks: 
+        - 
+            type: log-scan
+            config: 
+                path: "/var/log/zookeeper/zookeeper.out"
+                match: 
+                    pattern: ERROR
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"zookeeper" logged an error'
+        description: The "zookeeper" service has logged an error.
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            If the problem was transient, there may be no impact.  Otherwise,
+            Manatee shard fault tolerance may be affected and service discovery
+            may be offline.  Services should continue to function, but with
+            significantly reduced ability to respond to other failures.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+-
+    event: upset.manta.nameservice.zookeeper.log_connrefused
+    legacyName: "ZK: logscan 'Connection refused'"
+    scope: 
+        service: nameservice
+    checks: 
+        - 
+            type: log-scan
+            config: 
+                path: "/var/log/zookeeper/zookeeper.out"
+                match: 
+                    pattern: "java.net.ConnectException: Connection refused"
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"zookeeper" logged a connection failure'
+        description: The "zookeeper" service logged a connection failure.
+        severity: minor
+        response: No automated response will be taken.
+        impact: >-
+            If the problem was transient, there may be no impact.  Otherwise,
+            Manatee shard fault tolerance may be affected and service discovery
+            may be offline.  Services should continue to function, but with
+            significantly reduced ability to respond to other failures.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+-
+    event: upset.manta.nameservice.binder.log_error
+    legacyName: "binder: logscan"
+    scope: 
+        service: nameservice
+    checks: 
+        - 
+            type: bunyan-log-scan
+            config: 
+                smfServiceName: binder
+                fields: 
+                    level: ERROR
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"binder" logged an error'
+        description: The "binder" service has logged an error.
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            If the problem was transient, there may be no impact.  Otherwise,
+            service discovery may be offline.  Services should continue to
+            function, but with significantly reduced ability to respond to other
+            failures.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+-
+    event: upset.manta.nameservice.zookeeper.notok
+    legacyName: "ZK: ruok"
+    scope: 
+        service: nameservice
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "echo 'ruok' | nc localhost 2181"
+                stdoutMatch: 
+                    pattern: imok
+                    invert: true
+                threshold: 1
+                period: 60
+    ka: 
+        title: ZooKeeper not okay
+        description: ZooKeeper reports that it is not okay
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            If the problem was transient, there may be no impact.  Otherwise,
+            Manatee shard fault tolerance may be affected and service discovery
+            may be offline.  Services should continue to function, but with
+            significantly reduced ability to respond to other failures.
+        action: >-
+            Determine the scope of the problem and resolve the underlying issue.
+
diff --git a/alarm_metadata/probe_templates/ops.yaml b/alarm_metadata/probe_templates/ops.yaml
new file mode 100644
index 0000000..2a104c2
--- /dev/null
+++ b/alarm_metadata/probe_templates/ops.yaml
@@ -0,0 +1,503 @@
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2017, Joyent, Inc.
+#
+
+#
+# amon probes for the "ops" service
+#
+# For background information, see lib/alarms/index.js.  The format of this file
+# is described in lib/alarms/metadata.js.
+#
+
+-
+    event: upset.manta.ops.log_error
+    legacyName: mackerel-logscan
+    scope: 
+        service: ops
+    checks: 
+        - 
+            type: bunyan-log-scan
+            config: 
+                path: "/var/log/mackerel.log"
+                fields: 
+                    level: FATAL
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"mackerel" logged an error'
+        description: The "mackerel" subsystem has logged an error.
+        severity: minor
+        response: No automated response will be taken.
+        impact: >-
+            One or more metering reports may be missing or incomplete.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+-
+    event: upset.manta.ops.backup_unpack.log_error
+    legacyName: mola-pg-transform-logscan-error,mola-pg-transform-logscan-fatal
+    scope: 
+        service: ops
+    checks: 
+        - 
+            type: bunyan-log-scan
+            config: 
+                path: "/var/log/mola-pg-transform.log"
+                fields: 
+                    level: ERROR
+                threshold: 1
+                period: 60
+        - 
+            type: bunyan-log-scan
+            config: 
+                path: "/var/log/mola-pg-transform.log"
+                fields: 
+                    level: FATAL
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"mola-pg-transform" logged an error'
+        description: The "mola-pg-transform" subsystem has logged an error.
+        severity: minor
+        response: No automated response will be taken.
+        impact: >-
+            The daily metadata backups may not have been unpacked.  As a result,
+            regularly scheduled garbage collection, audit, and metering jobs may
+            not be running or their results may be incomplete.   Disk usage may
+            accumulate on metadata and storage nodes until the problem is
+            resolved.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+-
+    event: upset.manta.ops.gc.log_error
+    legacyName: mola-logscan-error,mola-logscan-fatal
+    scope: 
+        service: ops
+    checks: 
+        - 
+            type: bunyan-log-scan
+            config: 
+                path: "/var/log/mola.log"
+                fields: 
+                    level: ERROR
+                threshold: 1
+                period: 60
+        - 
+            type: bunyan-log-scan
+            config: 
+                path: "/var/log/mola.log"
+                fields: 
+                    level: FATAL
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"mola" logged an error'
+        description: The garbage collection subsystem has logged an error.
+        severity: minor
+        response: No automated response will be taken.
+        impact: >-
+            Garbage collection may not be running.  Disk space used may
+            accumulate on metadata and storage nodes until the problem is
+            repaired.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+-
+    event: upset.manta.ops.gc.create_links_log_error
+    legacyName: mola-gc-create-links-logscan-error,mola-gc-create-links-logscan-fatal
+    scope: 
+        service: ops
+    checks: 
+        - 
+            type: bunyan-log-scan
+            config: 
+                path: "/var/log/mola-gc-create-links.log"
+                fields: 
+                    level: ERROR
+                threshold: 1
+                period: 60
+        - 
+            type: bunyan-log-scan
+            config: 
+                path: "/var/log/mola-gc-create-links.log"
+                fields: 
+                    level: FATAL
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"mola-gc-create-links" logged an error'
+        description: The "mola-gc-create-links" subsystem has logged an error.
+        severity: minor
+        response: No automated response will be taken.
+        impact: >-
+            Garbage collection may not be running.  Disk space used may
+            accumulate on metadata and storage  nodes until the problem is
+            repaired.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+-
+    event: upset.manta.ops.gc.moray_gc_log_error
+    legacyName: mola-moray-gc-logscan-error,mola-moray-gc-logscan-fatal
+    scope: 
+        service: ops
+    checks: 
+        - 
+            type: bunyan-log-scan
+            config: 
+                path: "/var/log/mola-moray-gc.log"
+                fields: 
+                    level: ERROR
+                threshold: 1
+                period: 60
+        - 
+            type: bunyan-log-scan
+            config: 
+                path: "/var/log/mola-moray-gc.log"
+                fields: 
+                    level: FATAL
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"mola-moray-gc" logged an error'
+        description: The "mola-moray-gc" subsystem has logged an error.
+        severity: minor
+        response: No automated response will be taken.
+        impact: >-
+            Garbage collection may not be running.  Disk space used may
+            accumulate on metadata nodes until the problem is repaired.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+-
+    event: upset.manta.ops.audit.log_error
+    legacyName: mola-audit-logscan-error
+    scope: 
+        service: ops
+    checks: 
+        - 
+            type: bunyan-log-scan
+            config: 
+                path: "/var/log/mola-audit.log"
+                fields: 
+                    level: ERROR
+                threshold: 1
+                period: 60
+        - 
+            type: bunyan-log-scan
+            config: 
+                path: "/var/log/mola-audit.log"
+                fields: 
+                    level: FATAL
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"mola-audit" logged an error'
+        description: The audit subsystem has logged an error.
+        severity: minor
+        response: No automated response will be taken.
+        impact: >-
+            The regularly scheduled audit job may not have been able to run.  If
+            the job did run and complete successfully, then it may have
+            identified objects with missing copies, which would indicate a data
+            integrity issue.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+-
+    event: upset.manta.ops.gc.objects_lingering
+    legacyName: mola-create-link-files-piling-up
+    scope: 
+        service: ops
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "export HOME=/root && . /root/.bashrc && test $(mfind /poseidon/stor/manta_gc/all/do | wc -l) -lt 150"
+                interval: 300
+                period: 1800
+                threshold: 6
+                timeout: 20
+    ka: 
+        title: Garbage collection instructions piling up
+        description: Garbage collection instruction objects are piling up
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            Garbage collection may not be running.  Disk space used may
+            accumulate on metadata and storage nodes until the problem is
+            repaired.
+        action: >-
+            Determine the scope of the problem and resolve the underlying issue.
+
+-
+    event: upset.manta.ops.gc.moray_objects_lingering
+    legacyName: mola-moray-files-piling-up
+    scope: 
+        service: ops
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "export HOME=/root && . /root/.bashrc && for s in $(mls /poseidon/stor/manta_gc/moray); do echo \"$s \"; test $(mls /poseidon/stor/manta_gc/moray/$s | wc -l) -lt 150; if [[ $? -eq 0 ]]; then echo \"success\"; else echo \"fail\"; fi; done"
+                interval: 300
+                period: 1800
+                threshold: 6
+                timeout: 20
+                stdoutMatch: 
+                    pattern: fail
+                    type: substring
+    ka: 
+        title: Garbage collection Moray instructions piling up
+        description: Garbage collection Moray instruction objects are piling up
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            Garbage collection may not be running.  Disk space used may
+            accumulate on metadata nodes until the problem is repaired.
+        action: >-
+            Determine the scope of the problem and resolve the underlying issue.
+
+-
+    event: upset.manta.ops.gc.mako_objects_lingering
+    legacyName: mola-mako-files-piling-up
+    scope: 
+        service: ops
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "export HOME=/root && . /root/.bashrc && for s in $(mls /poseidon/stor/manta_gc/mako); do echo \"$s \"; test $(mls /poseidon/stor/manta_gc/mako/$s | wc -l) -lt 150; if [[ $? -eq 0 ]]; then echo \"success\"; else echo \"fail\"; fi; done"
+                interval: 300
+                period: 1800
+                threshold: 6
+                timeout: 20
+                stdoutMatch: 
+                    pattern: fail
+                    type: substring
+    ka: 
+        title: Garbage collection Mako instructions piling up
+        description: Garbage collection Mako instruction objects are piling up
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            Garbage collection may not be running.  Disk space used may
+            accumulate on storage nodes until the problem is repaired.
+        action: >-
+            Determine the scope of the problem and resolve the underlying issue.
+
+-
+    event: upset.manta.ops.gc.job_lingering
+    legacyName: mola-job-running-too-long
+    scope: 
+        service: ops
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "export HOME=/root && . /root/.bashrc && test $(expr $(date +%s) - $(mjob get $(mjob list -n manta_gc -s running | head -1 | tr -d '/') | json timeCreated | xargs -i date --utc --date \"{}\" +%s)) -lt 10800; if [[ $? -eq 1 ]]; then echo \"fail\"; else echo \"success\"; fi"
+                interval: 300
+                threshold: 1
+                timeout: 20
+                stdoutMatch: 
+                    pattern: fail
+                    type: substring
+    ka: 
+        title: Garbage collection job running too long
+        description: >-
+            The regularly-scheduled garbage collection job has been running for
+            longer than expected.
+        severity: major
+        response: No automated response will be taken.
+        impact: 
+            Garbage collection may not be completing.  Disk space used may
+            accumulate on storage nodes until the problem is repaired.
+        action: >-
+            Determine the scope of the problem and resolve the underlying issue.
+
+-
+    event: upset.manta.ops.dumps_missing
+    legacyName: manatee-backups-failed
+    scope: 
+        service: ops
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "export HOME=/root && . /root/.bashrc && export DATE=$(date +'%Y/%m/%d/00' --date='7 hours ago'); echo $DATE; for s in $(mls /poseidon/stor/manatee_backups | tr -d '/'); do /opt/local/bin/echo -n \"$s \"; until [[ \"$MLS\" != '' ]]; do export MLS=$(mls /poseidon/stor/manatee_backups/$s/$DATE 2>&1); done; echo \"$MLS\" | grep '\\(manta_delete_log-\\)\\|\\(marlin_tasks_v2-\\)' >/dev/null; if [[ $? == 0 ]]; then echo 'pass'; else echo 'fail'; fi; export MLS=''; done"
+                interval: 300
+                threshold: 1
+                period: 3600
+                timeout: 60
+                stdoutMatch: 
+                    pattern: fail
+                    type: substring
+    ka: 
+        title: Unpacked metadata dumps are missing
+        description: Regularly-scheduled metadata dumps have not been unpacked
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            The regularly scheduled metadata backups may not have been unpacked.
+            As a result, regularly scheduled garbage collection, audit, and
+            metering jobs may not be running or their results may be incomplete.
+            Disk usage may accumulate on metadata and storage nodes until the
+            problem is resolved.
+        action: >-
+            The "manta-hk" tool can be used in the "ops" zone to determine
+            whether the regularly-scheduled dumps have been uploaded and
+            unpacked.
+
+-
+    event: upset.manta.ops.metering.summary_missing
+    legacyName: mackerel-summary-missing
+    scope: 
+        service: ops
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "export HOME=/root && . /root/.bashrc && export DATE=$(date +'%Y/%m/%d' --date='30 hours ago'); echo $DATE; mls /poseidon/stor/usage/summary/$DATE 2>&1 | grep .json; if [[ $? == 0 ]]; then echo 'pass'; else echo 'fail'; fi;"
+                interval: 300
+                threshold: 1
+                period: 3600
+                timeout: 60
+                stdoutMatch: 
+                    pattern: fail
+                    type: substring
+    ka: 
+        title: Recent metering summary report missing
+        description: The most recent metering summary report could not be found.
+        severity: minor
+        response: No automated response will be taken.
+        impact: >-
+            The metering subsystem may not be functioning.  Per-user access logs
+            and summary reports may be missing or incomplete.  In some
+            deployments, this can affect customer billing.
+        action:
+            Determine the scope of the problem and resolve the underlying issue.
+
+-
+    event: upset.manta.ops.metering.storage_missing
+    legacyName: mackerel-storage-missing
+    scope: 
+        service: ops
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "export HOME=/root && . /root/.bashrc && export DATE=$(date +'%Y/%m/%d/00' --date='12 hours ago'); echo $DATE; mls /poseidon/stor/usage/storage/$DATE 2>&1 | grep h00.json; if [[ $? == 0 ]]; then echo 'pass'; else echo 'fail'; fi;"
+                interval: 300
+                threshold: 1
+                period: 3600
+                timeout: 60
+                stdoutMatch: 
+                    pattern: fail
+                    type: substring
+    ka: 
+        title: Recent metering storage report missing
+        description: The most recent metering storage report could not be found.
+        severity: minor
+        response: No automated response will be taken.
+        impact: >-
+            The metering subsystem may not be functioning.  In some deployments,
+            this can affect customer billing.
+        action:
+            Determine the scope of the problem and resolve the underlying issue.
+
+-
+    event: upset.manta.ops.metering.request_missing
+    legacyName: mackerel-request-missing
+    scope: 
+        service: ops
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "export HOME=/root && . /root/.bashrc && export DATE=$(date +'%Y/%m/%d/%H' --date='2 hours ago'); echo $DATE; mls /poseidon/stor/usage/request/$DATE 2>&1 | grep .json; if [[ $? == 0 ]]; then echo 'pass'; else echo 'fail'; fi;"
+                interval: 300
+                threshold: 1
+                period: 3600
+                timeout: 60
+                stdoutMatch: 
+                    pattern: fail
+                    type: substring
+    ka: 
+        title: Recent metering request report missing
+        description: The most recent metering request report could not be found.
+        severity: minor
+        response: No automated response will be taken.
+        impact: >-
+            The metering subsystem may not be functioning.  Per-user access logs
+            and summary reports may be missing or incomplete.  In some
+            deployments, this can affect customer billing.
+        action:
+            Determine the scope of the problem and resolve the underlying issue.
+
+-
+    event: upset.manta.ops.metering.compute_missing
+    legacyName: mackerel-compute-missing
+    scope: 
+        service: ops
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "export HOME=/root && . /root/.bashrc && export DATE=$(date +'%Y/%m/%d/%H' --date='2 hours ago'); echo $DATE; mls /poseidon/stor/usage/compute/$DATE 2>&1 | grep .json; if [[ $? == 0 ]]; then echo 'pass'; else echo 'fail'; fi;"
+                interval: 300
+                threshold: 1
+                period: 3600
+                timeout: 60
+                stdoutMatch: 
+                    pattern: fail
+                    type: substring
+    ka: 
+        title: Recent metering compute report missing
+        description: The most recent metering compute report could not be found.
+        severity: minor
+        response: No automated response will be taken.
+        impact: >-
+            The metering subsystem may not be functioning.  Per-user access logs
+            and summary reports may be missing or incomplete.  In some
+            deployments, this can affect customer billing.
+        action:
+            Determine the scope of the problem and resolve the underlying issue.
+
+-
+    event: upset.manta.ops.jobpuller.falling_behind
+    legacyName: wrasse-behind
+    scope: 
+        service: ops
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "/opt/smartdc/mola/amon/checks/check-wrasse-behind"
+                timeout: 180
+                interval: 3600
+                threshold: 1
+    ka: 
+        title: Job archiver is falling behind
+        description: There are too many jobs that have not been archived.
+        severity: minor
+        response: No automated response will be taken.
+        impact: >-
+            The jobs database may be getting large, full of job data that is no
+            longer needed.  Over time, this can significantly decrease
+            performance of compute jobs.
+        action: >-
+            Check the logs for the job archiver to help debug the underlying
+            issue and then repair it.  See also known issue "MANTA-2277".
diff --git a/alarm_metadata/probe_templates/postgres.yaml b/alarm_metadata/probe_templates/postgres.yaml
new file mode 100644
index 0000000..f0a96e2
--- /dev/null
+++ b/alarm_metadata/probe_templates/postgres.yaml
@@ -0,0 +1,228 @@
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2017, Joyent, Inc.
+#
+
+#
+# amon probes for the "postgres" service
+#
+# For background information, see lib/alarms/index.js.  The format of this file
+# is described in lib/alarms/metadata.js.
+#
+
+-
+    event: upset.manta.postgres.db_filesystem_almost_full
+    legacyName: database dataset space running low
+    scope: 
+        service: postgres
+    checks: 
+        - 
+            type: disk-usage
+            config: 
+                path: "/manatee/pg"
+                threshold: "20%"
+                interval: 3600
+    ka: 
+        title: Database filesystem almost full
+        description: A database filesystem is running low on free space
+        severity: critical
+        response: No automated response will be taken.
+        impact: >-
+            There is no immediate impact, but if the filesystem fills up,
+            service may become severely degraded.  End user requests may
+            experience high error rates or increased latency.
+        action: >-
+            Identify the cause of excessive disk usage and resolve the
+            underlying issue.
+
+-
+    event: upset.manta.postgres.sitter.log_error
+    legacyName: manatee-logscan
+    scope: 
+        service: postgres
+    checks: 
+        - 
+            type: bunyan-log-scan
+            config: 
+                smfServiceName: manatee-sitter
+                fields: 
+                    level: FATAL
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"manatee-sitter" logged an error'
+        description: The "manatee-sitter" service has logged an error.
+        severity: major
+        response: No automated response will be taken.
+        impact: Unknown.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+-
+    event: upset.manta.postgres.snapshotter.log_error
+    legacyName: manatee-snapshotter-logscan
+    scope: 
+        service: postgres
+    checks: 
+        - 
+            type: bunyan-log-scan
+            config: 
+                smfServiceName: manatee-snapshotter
+                fields: 
+                    level: FATAL
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"manatee-snapshotter" logged an error'
+        description: The "manatee-snapshotter" service has logged an error.
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            There is no immediate impact, but if snapshots are not being created
+            regularly, then it may impossible to rebuild a Manatee peer and
+            restore fault tolerance after a takeover event.  If snapshots are
+            not being destroyed, disk space may become exhausted, resulting in
+            significant service degradation.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+-
+    event: upset.manta.postgres.backupserver.log_error
+    legacyName: manatee-backupserver-logscan
+    scope: 
+        service: postgres
+    checks: 
+        - 
+            type: bunyan-log-scan
+            config: 
+                smfServiceName: manatee-backupserver
+                fields: 
+                    level: FATAL
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"manatee-backupserver" logged an error'
+        description: The "manatee-backupserver" service has logged an error.
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            There is no immediate impact, but this service is required for
+            Manatee rebuilds, which are necessary to restore fault tolerance
+            after a takeover event.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+-
+    event: upset.manta.postgres.pg_dump.log_error
+    legacyName: "pg_dump-logscan"
+    scope: 
+        service: postgres
+    checks: 
+        - 
+            type: log-scan
+            config: 
+                path: "/var/log/manatee/pg_dump.log"
+                match: 
+                    pattern: fatal
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"pg_dump" logged an error'
+        description: The "pg_dump" service has logged an error.
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            The daily backup for this metadata shard may be missing.  As a
+            result, the garbage collection, auditing, and metering pipelines may
+            not have completed.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+-
+    #
+    # For the shard-wide probes (like the following one that faults when the
+    # shard is unhealthy), it would be nice if this were one probe group per
+    # shard, or at least one per zone, but that's not yet supported.
+    #
+    event: upset.manta.postgres.shard_unhealthy
+    legacyName: manatee-stat
+    scope: 
+        service: postgres
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "/opt/smartdc/manatee/bin/manatee-stat-alarm.sh"
+                interval: 60
+                threshold: 5
+                period: 360
+                timeout: 60
+                stdoutMatch: 
+                    pattern: fail
+                    type: substring
+    ka: 
+        title: Metadata shard is unhealthy
+        description: A metadata shard is unhealthy
+        severity: critical
+        response: No automated response will be taken.
+        impact: >-
+            If the shard is completely offline, then end-user requests or job
+            tasks operating on objects whose metadata is on the affected shard
+            will fail.  If the shard is read-only, then new writes to
+            directories on this shard will fail, but read operations and job
+            tasks using objects on this shard will be unaffected.  If the shard
+            is online for reads and writes, then only its fault tolerance is
+            currently affected.
+        action: >-
+            Determine the scope of the problem and resolve the underying issue.
+            See the "manatee-adm" command inside the PostgreSQL zones for
+            details.
+
+-
+    #
+    # Like the above template, it would be great if this were per-shard.
+    #
+    event: upset.manta.postgres.shard_transition
+    legacyName: manatee-state-transition
+    scope: 
+        service: postgres
+    checks: 
+        - 
+            type: log-scan
+            config: 
+                path: "/var/svc/log/manta-application-manatee-sitter:default.log"
+                match: 
+                    pattern: finished transition
+                threshold: 1
+                period: 60
+    ka: 
+        title: Metadata shard has experienced a takeover event
+        description: >-
+            A metadata shard has gone through a takeover transition.  This may
+            be because the current synchronous peer took over for a primary that
+            it believed had failed, or the current primary peer selected a new
+            synchronous peer to replace one that had failed.  See "manatee-adm
+            history" on this shard for details.
+        severity: minor
+        response: No automated response will be taken.
+        impact: >-
+            During the transition, some end user requests may have failed.  Some
+            job tasks may have experienced higher dispatch latency or (in rarer
+            cases) failures.  The transition has completed, and there should be
+            no ongoing impact.
+
+            If the primary peer has been deposed, then fault tolerance may be
+            compromised until that peer has been rebuilt.
+        action: >-
+            There is likely no immediate action required, but it is recommended
+            to verify the health of this shard using "manatee-adm".  You may
+            need to rebuild a deposed peer.
diff --git a/alarm_metadata/probe_templates/storage.yaml b/alarm_metadata/probe_templates/storage.yaml
new file mode 100644
index 0000000..df237da
--- /dev/null
+++ b/alarm_metadata/probe_templates/storage.yaml
@@ -0,0 +1,137 @@
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2017, Joyent, Inc.
+#
+
+#
+# amon probes for the "storage" service
+#
+# For background information, see lib/alarms/index.js.  The format of this file
+# is described in lib/alarms/metadata.js.
+#
+
+-
+    event: upset.manta.storage.mako_gc.log_error
+    legacyName: mako-gc-logscan
+    scope: 
+        service: storage
+    checks: 
+        - 
+            type: log-scan
+            config: 
+                path: "/var/log/mako_gc.log"
+                match: 
+                    pattern: fatal error
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"mako-gc" logged an error'
+        description: The "mako-gc" subsystem has logged an error.
+        severity: minor
+        response: No automated response will be taken.
+        impact: >-
+            Garbage collection may not be running.  Disk space used may
+            accumulate on metadata nodes until the problem is repaired.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+-
+    event: upset.manta.storage.minnow.logscan
+    legacyName: minnow-logscan
+    scope: 
+        service: storage
+    checks: 
+        - 
+            type: bunyan-log-scan
+            config: 
+                smfServiceName: minnow
+                fields: 
+                    level: ERROR
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"minnow" logged an error'
+        description: The "minnow" service has logged an error.
+        severity: major
+        response: >-
+            No automated response will be taken.
+        impact: >-
+            One or more storage nodes may not be reporting its health.  Affected
+            nodes will not be used for new objects.
+        action:
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+-
+    event: upset.manta.storage.minnow.heartbeat_stale
+    legacyName: minnow heartbeat too old
+    scope: 
+        service: storage
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "/bin/bash -c 'if [[ -f /opt/smartdc/minnow/bin/check-minnow ]]; then /opt/smartdc/minnow/bin/check-minnow; exit $?; fi; let delta=$(date +%s)-$(PATH=/opt/smartdc/minnow/build/node/bin:/opt/smartdc/minnow/node_modules/.bin:$PATH findobjects -h $(cat /opt/smartdc/minnow/etc/config.json | json moray.host) manta_storage hostname=$(hostname)* | json -e _mtime=_mtime/1000 -e _mtime=~~_mtime _mtime) ; test $delta -lt 900'"
+                threshold: 3
+                period: 300
+                timeout: 30
+    ka: 
+        title: Storage zone heartbeat is too old
+        description: The minnow record for a storage zone is out of date
+        severity: minor
+        response: >-
+            Manta will not use the affected storage nodes for new writes.
+        impact: >-
+            Depending on the number of affected storage nodes, the system may
+            have reduced ability to survive additional failures.  If enough
+            storage nodes are affected, the system may not be able to continue
+            accepting writes.
+        action:
+            Resolve the underlying issue.
+
+-
+    event: upset.manta.storage.mako_ping
+    legacyName: shrimp-nginx-ping
+    #
+    # The point of this probe is to identify when storage nodes go down.  For
+    # that to be useful, we need to run the check from somewhere else.  We
+    # select the "nameservice" zone, on the grounds that there should be at
+    # least one per datacenter, and not a whole lot more than that.  It would be
+    # better if Amon supported opening alarms when expected checks did not
+    # complete successfully.
+    #
+    scope: 
+        service: storage
+        checkFrom: nameservice
+    checks: 
+        - 
+            type: cmd
+            config: 
+                autoEnv:
+                    - MANTA_STORAGE_ID
+                cmd: "curl -m 5 -sf http://$MANTA_STORAGE_ID/50x.html -o /dev/null"
+                interval: 60
+                threshold: 3
+                period: 300
+    ka: 
+        title: Storage node HTTP ping failed
+        description: A storage node failed to respond to an HTTP ping request.
+        severity: major
+        response: >-
+            If the storage node is down, then Manta will direct writes to other
+            storage nodes.  Reads for objects with a copy on the affected
+            storage node will be directed to other storage nodes that also have
+            a copy of the object, when possible.
+        impact: >-
+            If the storage node is down, then reads and compute jobs will fail
+            when they operate on objects having only one copy that happens to be
+            stored on the affected node.
+        action: >-
+            Resolve the underlying issue.
+
diff --git a/alarm_metadata/probe_templates/storage_gzs.yaml b/alarm_metadata/probe_templates/storage_gzs.yaml
new file mode 100644
index 0000000..d75b616
--- /dev/null
+++ b/alarm_metadata/probe_templates/storage_gzs.yaml
@@ -0,0 +1,131 @@
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2017, Joyent, Inc.
+#
+
+#
+# amon probes for global zones of storage nodes
+#
+# For background information, see lib/alarms/index.js.  The format of this file
+# is described in lib/alarms/metadata.js.
+#
+
+-
+    event: upset.manta.marlin_agent.log_error
+    legacyName: marlin-agent-logscan-error, marlin-agent-logscan-fatal, marlin-agent-logscan-core
+    scope: 
+        service: storage
+        global: true
+    checks: 
+        - 
+            type: bunyan-log-scan
+            config: 
+                smfServiceName: marlin-agent
+                fields: 
+                    level: error
+                threshold: 1
+                period: 60
+        - 
+            type: bunyan-log-scan
+            config: 
+                smfServiceName: marlin-agent
+                fields: 
+                    level: fatal
+                threshold: 1
+                period: 60
+        - 
+            type: log-scan
+            config: 
+                smfServiceName: marlin-agent
+                match: 
+                    pattern: Stopping because process dumped core.
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"marlin-agent" logged an error'
+        description: The "marlin-agent" service has logged an error.
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            If the problem was transient, there may be no impact.  Otherwise,
+            some jobs may have experienced retries, errors, or additional
+            latency.  It is possible that some jobs are stuck.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
+
+-
+    event: upset.manta.global_zone.logs_lingering
+    legacyName: logs not uploaded
+    scope: 
+        service: storage
+        global: true
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "test ! $(find /var/log/manta/upload -type f | wc -l) -gt 0"
+                interval: 300
+                threshold: 5
+                period: 1800
+                timeout: 30
+    ka: 
+        title: Global zone log files not uploaded
+        description: Some global zone log files have not been uploaded
+        severity: minor
+        response: >-
+            The system automatically retries hourly to upload any internal log
+            files that have not yet been uploaded.
+        impact: >-
+            There is no impact to end-user service.  However, failure to upload
+            files is often indicative of problems affecting end user requests.
+                       
+            If the affected logs are used for metering, then metering reports
+            and access logs for end users may be unavailable or incomplete until
+            the affected logs are uploaded and the relevant metering jobs re-run
+            by an operator.
+        action: >-
+            Identify the reason for the failure and resolve the underlying
+            issue.  If logs used for metering were affected, you may need to
+            re-run the relevant metering jobs once all logs are available.
+            
+            In most components, the log "/var/log/mbackup.log" has a record of
+            recent upload attempts and results.  Another common cause of log
+            upload failure is when a component or service was offline during the
+            scheduled log upload time.
+
+-
+    event: upset.manta.global_zone.smf_maintenance
+    legacyName: "svcs: SMF maintenance"
+    scope: 
+        service: storage
+        global: true
+    checks: 
+        - 
+            type: cmd
+            config: 
+                cmd: "/usr/bin/svcs -x"
+                stdoutMatch: 
+                    pattern: maintenance
+                    matchWord: true
+                threshold: 1
+                period: 60
+                timeout: 30
+    ka: 
+        title: Global zone SMF services in maintenance
+        description: One or more global zone SMF services are in maintenance
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            The impact depends on which services are in maintenance.  In some
+            cases, overall request handling capacity may be reduced.  If enough
+            instances are in maintenance, end users could experience errors.
+        action: >-
+            In the affected global zones, use "svcs -xv" to identify the
+            services in maintenance and see basic instructions for tracking down
+            the problem.
diff --git a/alarm_metadata/probe_templates/webapi.yaml b/alarm_metadata/probe_templates/webapi.yaml
new file mode 100644
index 0000000..fd82940
--- /dev/null
+++ b/alarm_metadata/probe_templates/webapi.yaml
@@ -0,0 +1,42 @@
+#
+# This Source Code Form is subject to the terms of the Mozilla Public
+# License, v. 2.0. If a copy of the MPL was not distributed with this
+# file, You can obtain one at http://mozilla.org/MPL/2.0/.
+#
+
+#
+# Copyright (c) 2017, Joyent, Inc.
+#
+
+#
+# amon probes for the "webapi" service
+#
+# For background information, see lib/alarms/index.js.  The format of this file
+# is described in lib/alarms/metadata.js.
+#
+
+-
+    event: upset.manta.webapi.log_error
+    legacyName: muskie-logscan
+    scope: 
+        service: webapi
+    checks: 
+        - 
+            type: bunyan-log-scan
+            config: 
+                path: "/var/log/muskie.log"
+                fields: 
+                    level: ERROR
+                threshold: 1
+                period: 60
+    ka: 
+        title: '"muskie" logged an error'
+        description: The "muskie" service has logged an error.
+        severity: major
+        response: No automated response will be taken.
+        impact: >-
+            If the problem was transient, there may be no impact.  Otherwise,
+            some end user requests may be experiencing an elevated error rate.
+        action: >-
+            Determine the scope of the problem based on the log message and
+            resolve the underlying issue.
diff --git a/cmd/manta-adm.js b/cmd/manta-adm.js
index b4bc330..96f0d2f 100755
--- a/cmd/manta-adm.js
+++ b/cmd/manta-adm.js
@@ -7,12 +7,14 @@
  */
 
 /*
- * Copyright (c) 2015, Joyent, Inc.
+ * Copyright (c) 2017, Joyent, Inc.
  */
 
 /*
  * manta-adm.js: manage manta deployments.  Provides subcommands:
  *
+ *     alarm		view and configure information about alarms
+ *
  *     cn		show information about CNs
  *
  *     show		show information about all deployed services
@@ -39,6 +41,7 @@ var cmdln = require('cmdln');
 var cmdutil = require('cmdutil');
 var jsprim = require('jsprim');
 var path = require('path');
+var restifyClients = require('restify-clients');
 var util = require('util');
 var vasync = require('vasync');
 var VError = require('verror').VError;
@@ -48,6 +51,59 @@ var madm = require('../lib/adm');
 
 var maArg0 = path.basename(process.argv[1]);
 
+var maDefaultAlarmConcurrency = 10;
+
+/*
+ * These node-cmdln options are used by multiple subcommands.  They're defined
+ * in one place to ensure consistency in names, aliases, and help message.
+ */
+var maCommonOptions = {
+    'columns': {
+	'names': [ 'columns', 'o' ],
+	'type': 'arrayOfString',
+	'help': 'Select columns for output (see below)'
+    },
+    'concurrency': {
+	'names': [ 'concurrency' ],
+	'type': 'positiveInteger',
+	'help': 'Number of concurrent requests to make',
+	'default': maDefaultAlarmConcurrency
+    },
+    'configFile': {
+	'names': [ 'config-file' ],
+	'type': 'string',
+	'help': 'Path to configuration',
+	'default': common.CONFIG_FILE_DEFAULT
+    },
+    'confirm': {
+	'names': [ 'confirm', 'y' ],
+	'type': 'bool',
+	'help': 'Bypass all confirmations (be careful!)'
+    },
+    'dryrun': {
+	'names': [ 'dryrun', 'n' ],
+	'type': 'bool',
+	'help': 'Print what would be done without actually doing it.'
+    },
+    'logFile': {
+	'names': [ 'log_file', 'log-file', 'l' ],
+	'type': 'string',
+	'help': 'Dump logs to this file (or "stdout")',
+	'default': '/var/log/manta-adm.log'
+    },
+    'omitHeader': {
+	'names': [ 'omit-header', 'H'],
+	'type': 'bool',
+	'help': 'Omit the header row for columnar output'
+    },
+    'unconfigure': {
+	'names': [ 'unconfigure' ],
+	'type': 'bool',
+	'help': 'Remove all probes and probe groups instead of updating them',
+	'default': false
+    }
+};
+
 /*
  * node-cmdln interface for the manta-adm tool.
  */
@@ -78,7 +134,7 @@ MantaAdm.prototype.initAdm = function (opts, callback)
 		console.error('logs at ' + opts.log_file);
 	} else {
 		logstreams = [ {
-		    'level': 'fatal',
+		    'level': process.env['LOG_LEVEL'] || 'fatal',
 		    'stream': process.stderr
 		} ];
 	}
@@ -86,7 +142,7 @@ MantaAdm.prototype.initAdm = function (opts, callback)
 	this.madm_log = new bunyan({
 	    'name': maArg0,
 	    'streams': logstreams,
-	    'serializers': bunyan.stdSerializers
+	    'serializers': restifyClients.bunyan.serializers
 	});
 
 	this.madm_adm = new madm.MantaAdm(this.madm_log);
@@ -102,6 +158,8 @@ MantaAdm.prototype.finiAdm = function ()
 	this.madm_adm.close();
 };
 
+MantaAdm.prototype.do_alarm = MantaAdmAlarm;
+
 MantaAdm.prototype.do_cn = function (subcmd, opts, args, callback)
 {
 	var self = this;
@@ -167,27 +225,21 @@ MantaAdm.prototype.do_cn.help =
     '{{options}}\n' +
     'Available columns for -o:\n    ' + madm.cnColumnNames().join(', ');
 
-MantaAdm.prototype.do_cn.options = [ {
-    'names': [ 'omit-header', 'H'],
-    'type': 'bool',
-    'help': 'Omit the header row for columnar output'
-}, {
-    'names': [ 'log_file', 'l' ],
-    'type': 'string',
-    'help': 'Dump logs to this file (or "stdout")'
-}, {
-    'names': [ 'oneachnode', 'n' ],
-    'type': 'bool',
-    'help': 'Emit output suitable for "sdc-oneachnode -n"'
-}, {
-    'names': [ 'columns', 'o' ],
-    'type': 'arrayOfString',
-    'help': 'Select columns for output (see below)'
-}, {
-    'names': [ 'storage-only', 's' ],
-    'type': 'bool',
-    'help': 'Show only nodes used as storage nodes.'
-}];
+MantaAdm.prototype.do_cn.options = [
+    maCommonOptions.omitHeader,
+    maCommonOptions.logFile,
+    {
+	'names': [ 'oneachnode', 'n' ],
+	'type': 'bool',
+	'help': 'Emit output suitable for "sdc-oneachnode -n"'
+    },
+    maCommonOptions.columns,
+    {
+	'names': [ 'storage-only', 's' ],
+	'type': 'bool',
+	'help': 'Show only nodes used as storage nodes.'
+    }
+];
 
 MantaAdm.prototype.do_genconfig = function (subcmd, opts, args, callback)
 {
@@ -254,8 +306,7 @@ MantaAdm.prototype.do_genconfig = function (subcmd, opts, args, callback)
 };
 
 MantaAdm.prototype.do_genconfig.help =
-    'Generate a configuration for COAL or lab deployment or for \n' +
-    'a larger deployment.\n' +
+    'Generate a configuration for COAL, lab, or larger deployment.\n' +
     '\n' +
     'Usage:\n' +
     '\n' +
@@ -364,23 +415,16 @@ MantaAdm.prototype.do_show.options = [ {
     'names': [ 'bycn', 'c' ],
     'type': 'bool',
     'help': 'Show results by compute node, rather than by service.'
-}, {
-    'names': [ 'omit-header', 'H'],
-    'type': 'bool',
-    'help': 'Omit the header row for columnar output'
-}, {
+},
+    maCommonOptions.omitHeader,
+{
     'names': [ 'json', 'j' ],
     'type': 'bool',
     'help': 'Show results in JSON form suitable for importing with "update".'
-}, {
-    'names': [ 'log_file', 'l' ],
-    'type': 'string',
-    'help': 'dump logs to this file (or "stdout")'
-}, {
-    'names': [ 'columns', 'o' ],
-    'type': 'arrayOfString',
-    'help': 'Select columns for output (see below)'
-}, {
+},
+    maCommonOptions.logFile,
+    maCommonOptions.columns,
+ {
     'names': [ 'summary', 's' ],
     'type': 'bool',
     'help': 'Show summary of deployed zones rather than each zone separately.'
@@ -481,20 +525,12 @@ MantaAdm.prototype.do_update = function (subcmd, opts, args, callback)
 MantaAdm.prototype.do_update.help =
     'Update deployment to match a JSON configuration.\n\n{{options}}';
 
-MantaAdm.prototype.do_update.options = [ {
-    'names': [ 'log_file', 'l' ],
-    'type': 'string',
-    'help': 'dump logs to this file (or "stdout")',
-    'default': '/var/log/manta-adm.log'
-}, {
-    'names': [ 'dryrun', 'n' ],
-    'type': 'bool',
-    'help': 'Print what would be done without actually doing it.'
-}, {
-    'names': [ 'confirm', 'y' ],
-    'type': 'bool',
-    'help': 'Bypass all confirmations (be careful!)'
-}, {
+MantaAdm.prototype.do_update.options = [
+    maCommonOptions.logFile,
+    maCommonOptions.dryrun,
+    maCommonOptions.confirm,
+    maCommonOptions.configFile,
+{
     'names': [ 'no-reprovision' ],
     'type': 'bool',
     'help': 'When upgrading a zone, always provision and deprovision ' +
@@ -579,20 +615,11 @@ MantaAdmZk.prototype.do_list.help =
  * as general debug logs.  But the "zk list" subcommand is read-only and only
  * applicable to this user, so we use a path in /var/tmp for the log.
  */
-MantaAdmZk.prototype.do_list.options = [ {
-    'names': [ 'omit-header', 'H'],
-    'type': 'bool',
-    'help': 'Omit the header row for columnar output'
-}, {
-    'names': [ 'log_file', 'l' ],
-    'type': 'string',
-    'help': 'dump logs to this file (or "stdout")',
-    'default': '/var/tmp/manta-adm.log'
-}, {
-    'names': [ 'columns', 'o' ],
-    'type': 'arrayOfString',
-    'help': 'Select columns for output (see below)'
-} ];
+MantaAdmZk.prototype.do_list.options = [
+    maCommonOptions.omitHeader,
+    maCommonOptions.logFile,
+    maCommonOptions.columns
+];
 
 MantaAdmZk.prototype.do_fixup = function (subcmd, opts, args, callback)
 {
@@ -714,19 +741,807 @@ MantaAdmZk.prototype.do_fixup.help = [
     '{{options}}'
 ].join('\n');
 
-MantaAdmZk.prototype.do_fixup.options = [ {
-    'names': [ 'confirm', 'y' ],
-    'type': 'bool',
-    'help': 'Bypass all confirmations (be careful!)'
-}, {
-    'names': [ 'dryrun', 'n' ],
-    'type': 'bool',
-    'help': 'Print what would be done without actually doing it.'
-}, {
-    'names': [ 'log_file', 'l' ],
-    'type': 'string',
-    'help': 'Dump logs to this file (or "stdout")'
-} ];
+MantaAdmZk.prototype.do_fixup.options = [
+    maCommonOptions.confirm,
+    maCommonOptions.dryrun,
+    maCommonOptions.logFile
+];
+
+function MantaAdmAlarm(parent)
+{
+	this.maa_parent = parent;
+	cmdln.Cmdln.call(this, {
+	    'name': 'alarm',
+	    'desc': 'View and configure information about alarms.'
+	});
+}
+
+util.inherits(MantaAdmAlarm, cmdln.Cmdln);
+
+MantaAdmAlarm.prototype.initAdmAndFetchAlarms = function (args, callback)
+{
+	var self = this;
+	var clioptions, skipWarnings, initArgs, funcs;
+
+	assertplus.object(args, 'args');
+	assertplus.object(args.sources, 'args.sources');
+	assertplus.object(args.clioptions, 'clioptions');
+	assertplus.optionalBool(args.skipWarnings, 'args.skipWarnings');
+	assertplus.optionalBool(args.skipFetch, 'args.skipFetch');
+
+	skipWarnings = args.skipWarnings;
+	clioptions = args.clioptions;
+	initArgs = {
+	    'configFile': clioptions.config_file,
+	    'concurrency': clioptions.concurrency || maDefaultAlarmConcurrency,
+	    'sources': args.sources
+	};
+
+	funcs = [];
+	funcs.push(function initAdm(_, stepcb) {
+		self.maa_parent.initAdm(clioptions, stepcb);
+	});
+
+	if (!args.skipFetch) {
+		funcs.push(function fetch(_, stepcb) {
+			self.maa_parent.madm_adm.fetchDeployed(stepcb);
+		});
+	}
+
+	funcs.push(function fetchAmon(_, stepcb) {
+		self.maa_parent.madm_adm.alarmsInit(initArgs, stepcb);
+	});
+
+	vasync.pipeline({
+	    'funcs': funcs
+	}, function (err) {
+		var errors;
+
+		if (err) {
+			fatal(err.message);
+		}
+
+		if (!skipWarnings) {
+			errors = self.maa_parent.madm_adm.alarmWarnings();
+			errors.forEach(function (e) {
+				cmdutil.warn(e);
+			});
+		}
+
+		callback();
+	});
+};
+
+MantaAdmAlarm.prototype.do_close = function (subcmd, opts, args, callback)
+{
+	var parent;
+
+	if (args.length < 1) {
+		callback(new Error('expected ALARMID'));
+		return;
+	}
+
+	parent = this.maa_parent;
+	this.initAdmAndFetchAlarms({
+	    'clioptions': opts,
+	    'sources': {}
+	}, function () {
+		var adm = parent.madm_adm;
+		adm.alarmsClose({
+		    'alarmIds': args,
+		    'concurrency': opts.concurrency
+		}, function (err) {
+			if (err) {
+				common.errorForEach(err, function (e) {
+					console.error('error: %s', e.message);
+				});
+
+				process.exit(1);
+			}
+
+			parent.finiAdm();
+			callback();
+		});
+	});
+};
+
+MantaAdmAlarm.prototype.do_close.help = [
+    'Close open alarms',
+    '',
+    'Usage:',
+    '',
+    '    manta-adm alarm close ALARMID...',
+    '',
+    '{{options}}'
+].join('\n');
+
+MantaAdmAlarm.prototype.do_close.options = [
+    maCommonOptions.concurrency,
+    maCommonOptions.configFile
+];
+
+MantaAdmAlarm.prototype.do_config = MantaAdmAlarmConfig;
+
+MantaAdmAlarm.prototype.do_details = function (subcmd, opts, args, callback)
+{
+	this.doAlarmPrintSubcommand(opts, 1, args, callback);
+};
+
+MantaAdmAlarm.prototype.do_details.help = [
+    'Print details about an alarm',
+    '',
+    'Usage:',
+    '',
+    '    manta-adm alarm details ALARMID...',
+    '',
+    '{{options}}'
+].join('\n');
+
+MantaAdmAlarm.prototype.do_details.options = [
+    maCommonOptions.configFile
+];
+
+MantaAdmAlarm.prototype.do_faults = function (subcmd, opts, args, callback)
+{
+	this.doAlarmPrintSubcommand(opts, undefined, args, callback);
+};
+
+MantaAdmAlarm.prototype.do_faults.help = [
+    'Print information about all of an alarm\'s faults',
+    '',
+    'Usage:',
+    '',
+    '    manta-adm alarm faults ALARMID...',
+    '',
+    '{{options}}'
+].join('\n');
+
+MantaAdmAlarm.prototype.do_faults.options = [
+    maCommonOptions.configFile
+];
+
+MantaAdmAlarm.prototype.doAlarmPrintSubcommand = function
+    doAlarmPrintSubcommand(opts, nmaxfaults, args, callback)
+{
+	var self = this;
+	var sources = {};
+
+	if (args.length < 1) {
+		callback(new Error('expected ALARMID'));
+		return;
+	}
+
+	sources = {
+	    'configBasic': true,
+	    'alarms': {
+		'alarmIds': args
+	    }
+	};
+
+	this.initAdmAndFetchAlarms({
+	    'clioptions': opts,
+	    'sources': sources,
+	    'skipWarnings': true
+	}, function () {
+		var nerrors = 0;
+		args.forEach(function (id) {
+			var error;
+
+			error = self.maa_parent.madm_adm.alarmPrint({
+			    'id': id,
+			    'stream': process.stdout,
+			    'nmaxfaults': nmaxfaults
+			});
+
+			if (error instanceof Error) {
+				cmdutil.warn(error);
+			}
+
+			console.log('');
+		});
+
+		if (nerrors > 0) {
+			process.exit(1);
+		}
+
+		self.maa_parent.finiAdm();
+	});
+};
+
+MantaAdmAlarm.prototype.do_list = function (subcmd, opts, args, callback)
+{
+	var self = this;
+	var options = {};
+	var sources = {};
+
+	if (args.length > 0) {
+		callback(new Error('unexpected arguments'));
+		return;
+	}
+
+	switch (opts.state) {
+	case 'all':
+	case 'closed':
+	case 'open':
+	case 'recent':
+		break;
+
+	default:
+		callback(new VError('unsupported state: %s', opts.state));
+		return;
+	}
+
+	options = listPrepareArgs(opts, madm.alarmColumnNames());
+	if (options instanceof Error) {
+		callback(options);
+		return;
+	}
+
+	sources = {
+	    'configBasic': true,
+	    'alarms': {
+		'state': opts.state
+	    }
+	};
+
+	options.stream = process.stdout;
+	this.initAdmAndFetchAlarms({
+	    'clioptions': opts,
+	    'sources': sources
+	}, function () {
+		self.maa_parent.madm_adm.alarmsList(options);
+		self.maa_parent.finiAdm();
+		callback();
+	});
+};
+
+MantaAdmAlarm.prototype.do_list.help = [
+    'List open alarms',
+    '',
+    'Usage:',
+    '',
+    '    manta-adm alarm list OPTIONS',
+    '',
+    '{{options}}',
+    '',
+    'Available columns for -o:\n    ' + madm.alarmColumnNames().join(', ')
+].join('\n');
+
+MantaAdmAlarm.prototype.do_list.options = [
+    maCommonOptions.configFile,
+    maCommonOptions.omitHeader,
+    maCommonOptions.columns,
+    {
+	'names': [ 'state' ],
+	'type': 'string',
+	'help': 'List only alarms in specified state',
+	'default': 'open'
+    }
+];
+
+MantaAdmAlarm.prototype.do_metadata = MantaAdmAlarmMetadata;
+
+MantaAdmAlarm.prototype.do_notify = function (subcmd, opts, args, callback)
+{
+	var parent;
+	var allowedArg0 = {
+	    'enabled': true,
+	    'enable': true,
+	    'on': true,
+	    'true': true,
+	    'yes': true,
+
+	    'disabled': false,
+	    'disable': false,
+	    'off': false,
+	    'false': false,
+	    'no': false
+	};
+
+	if (args.length < 2) {
+		callback(new Error('expected arguments'));
+		return;
+	}
+
+	if (!allowedArg0.hasOwnProperty(args[0])) {
+		callback(new Error('expected "on" or "off"'));
+		return;
+	}
+
+	parent = this.maa_parent;
+	this.initAdmAndFetchAlarms({
+	    'clioptions': opts,
+	    'sources': {}
+	}, function () {
+		var adm = parent.madm_adm;
+		adm.alarmsUpdateNotification({
+		    'alarmIds': args.slice(1),
+		    'concurrency': opts.concurrency,
+		    'suppressed': !allowedArg0[args[0]]
+		}, function (err) {
+			if (err) {
+				common.errorForEach(err, function (e) {
+					console.error('error: %s', e.message);
+				});
+
+				process.exit(1);
+			}
+
+			parent.finiAdm();
+			callback();
+		});
+	});
+};
+
+MantaAdmAlarm.prototype.do_notify.help = [
+    'Enable or disable alarm notifications',
+    '',
+    'Usage:',
+    '',
+    '    manta-adm alarm notify on|off ALARMID...',
+    '',
+    '{{options}}'
+].join('\n');
+
+MantaAdmAlarm.prototype.do_notify.options = [
+    maCommonOptions.concurrency,
+    maCommonOptions.configFile
+];
+
+MantaAdmAlarm.prototype.do_show = function (subcmd, opts, args, callback)
+{
+	var parent, sources;
+
+	if (args.length > 0) {
+		callback(new Error('unexpected arguments'));
+		return;
+	}
+
+	parent = this.maa_parent;
+	sources = {
+	    'configBasic': true,
+	    'alarms': {
+		'state': 'open'
+	    }
+	};
+
+	this.initAdmAndFetchAlarms({
+	    'clioptions': opts,
+	    'sources': sources
+	}, function () {
+		var showArgs = { 'stream': process.stdout };
+		parent.madm_adm.alarmsShow(showArgs);
+		parent.finiAdm();
+		callback();
+	});
+};
+
+MantaAdmAlarm.prototype.do_show.help = [
+    'Summarize open alarms',
+    '',
+    'Usage:',
+    '',
+    '    manta-adm alarm show',
+    '',
+    '{{options}}'
+].join('\n');
+
+MantaAdmAlarm.prototype.do_show.options = [ maCommonOptions.configFile ];
+
+
+function MantaAdmAlarmConfig(parent)
+{
+	this.maac_parent = parent;
+	this.maac_root = parent.maa_parent;
+
+	cmdln.Cmdln.call(this, {
+	    'name': 'config',
+	    'desc': 'Manage probe and probe group configuration'
+	});
+}
+
+util.inherits(MantaAdmAlarmConfig, cmdln.Cmdln);
+
+MantaAdmAlarmConfig.prototype.do_probegroup = MantaAdmAlarmProbeGroup;
+
+MantaAdmAlarmConfig.prototype.do_show = function (subcmd, opts, args, callback)
+{
+	var root, parent, adm, sources;
+
+	if (args.length > 0) {
+		callback(new Error('unexpected arguments'));
+		return;
+	}
+
+	root = this.maac_root;
+	parent = this.maac_parent;
+	sources = {
+	    'configFull': true
+	};
+
+	parent.initAdmAndFetchAlarms({
+	    'clioptions': opts,
+	    'sources': sources
+	}, function () {
+		adm = root.madm_adm;
+		adm.alarmConfigShow({
+		    'stream': process.stdout
+		});
+
+		root.finiAdm();
+		callback();
+	});
+
+};
+
+MantaAdmAlarmConfig.prototype.do_show.help = [
+    'Summarize configured probes and probe groups',
+    '',
+    'Usage:',
+    '',
+    '    manta-adm alarm config show',
+    '',
+    '{{options}}'
+].join('\n');
+
+MantaAdmAlarmConfig.prototype.do_show.options = [
+    maCommonOptions.concurrency,
+    maCommonOptions.configFile
+];
+
+MantaAdmAlarmConfig.prototype.do_update =
+    function (subcmd, opts, args, callback)
+{
+	if (args.length > 0) {
+		callback(new Error('unexpected arguments'));
+		return;
+	}
+
+	this.amonUpdateSubcommand(opts, opts.dryrun, callback);
+};
+
+MantaAdmAlarmConfig.prototype.do_update.help = [
+    'Update and probes and probe groups that are out of date.',
+    '',
+    'Usage:',
+    '',
+    '    manta-adm alarm config update OPTIONS',
+    '    manta-adm alarm config update OPTIONS --unconfigure',
+    '',
+    '{{options}}'
+].join('\n');
+
+MantaAdmAlarmConfig.prototype.do_update.options = [
+    maCommonOptions.confirm,
+    maCommonOptions.concurrency,
+    maCommonOptions.configFile,
+    maCommonOptions.dryrun,
+    maCommonOptions.unconfigure
+];
+
+MantaAdmAlarmConfig.prototype.do_verify =
+    function (subcmd, opts, args, callback)
+{
+	if (args.length > 0) {
+		callback(new Error('unexpected arguments'));
+		return;
+	}
+
+	this.amonUpdateSubcommand(opts, true, callback);
+};
+
+MantaAdmAlarmConfig.prototype.do_verify.help = [
+    'Check that configured probes and probe groups are up to date.',
+    '',
+    'Usage:',
+    '',
+    '    manta-adm alarm config verify OPTIONS',
+    '',
+    '{{options}}'
+].join('\n');
+
+MantaAdmAlarmConfig.prototype.do_verify.options = [
+    maCommonOptions.concurrency,
+    maCommonOptions.configFile,
+    maCommonOptions.unconfigure
+];
+
+MantaAdmAlarmConfig.prototype.amonUpdateSubcommand =
+    function (clioptions, dryrun, callback) {
+	var self = this;
+	var root, parent, sources, adm, plan;
+
+	assertplus.object(clioptions, 'clioptions');
+	assertplus.number(clioptions.concurrency, 'clioptions.concurrency');
+	assertplus.bool(clioptions.unconfigure, 'clioptions.unconfigure');
+
+	root = self.maac_root;
+	parent = self.maac_parent;
+	sources = {
+	    'configFull': true
+	};
+	vasync.pipeline({
+	    'arg': null,
+	    'funcs': [
+		function init(_, stepcb) {
+			parent.initAdmAndFetchAlarms({
+			    'clioptions': clioptions,
+			    'sources': sources
+			}, stepcb);
+		},
+		function generateAmonPlan(_, stepcb) {
+			var options;
+
+			adm = root.madm_adm;
+			options = {
+				'unconfigure': clioptions.unconfigure
+			};
+			plan = adm.amonUpdatePlanCreate(options);
+			if (plan instanceof Error) {
+				stepcb(plan);
+				return;
+			}
+
+			adm.amonUpdatePlanDump({
+			    'plan': plan,
+			    'stream': process.stderr,
+			    'verbose': false
+			});
+
+			if (!plan.needsChanges()) {
+				console.log('no changes to make');
+				stepcb();
+				return;
+			}
+
+			if (dryrun) {
+				console.log('To apply these changes, ' +
+				    'use the "update" subcommand without ' +
+				    'the -n/--dry-run option.');
+				stepcb();
+				return;
+			}
+
+			if (clioptions.confirm) {
+				stepcb();
+				return;
+			}
+
+			common.confirm(
+			    'Are you sure you want to proceed? (y/N): ',
+			    function (proceed) {
+				if (!proceed) {
+					stepcb(new Error('aborted by user'));
+				} else {
+					stepcb();
+				}
+			    });
+		},
+		function execAmonPlan(_, stepcb) {
+			if (dryrun || !plan.needsChanges()) {
+				stepcb();
+				return;
+			}
+
+			adm.amonUpdatePlanApply({
+			    'concurrency': clioptions.concurrency,
+			    'plan': plan,
+			    'stream': process.stderr
+			}, stepcb);
+		}
+	    ]
+	}, function (err) {
+		root.finiAdm();
+		callback(err);
+	});
+};
+
+function MantaAdmAlarmMetadata(parent)
+{
+	this.maam_parent = parent;
+	this.maam_root = parent.maa_parent;
+
+	cmdln.Cmdln.call(this, {
+	    'name': 'metadata',
+	    'desc': 'View local metadata about alarm config'
+	});
+}
+
+util.inherits(MantaAdmAlarmMetadata, cmdln.Cmdln);
+
+MantaAdmAlarmMetadata.prototype.do_events =
+    function cmdEvents(subcmd, opts, args, callback)
+{
+	var self = this;
+
+	if (args.length > 0) {
+		callback(new Error('unexpected arguments'));
+		return;
+	}
+
+	this.maam_parent.initAdmAndFetchAlarms({
+	    'clioptions': opts,
+	    'sources': {},
+	    'skipFetch': true
+	}, function () {
+		var events = self.maam_root.madm_adm.alarmEventNames();
+		events.forEach(function (eventName) {
+			console.log(eventName);
+		});
+		self.maam_root.finiAdm();
+		callback();
+	});
+};
+
+MantaAdmAlarmMetadata.prototype.do_events.help = [
+    'List known event names',
+    '',
+    'Usage:',
+    '',
+    '    manta-adm alarm events'
+].join('\n');
+
+MantaAdmAlarmMetadata.prototype.do_events.options = [
+    maCommonOptions.configFile
+];
+
+MantaAdmAlarmMetadata.prototype.do_ka = function (subcmd, opts, args, callback)
+{
+	var self = this;
+
+	this.maam_parent.initAdmAndFetchAlarms({
+	    'clioptions': opts,
+	    'sources': {},
+	    'skipFetch': true
+	}, function () {
+		var events, nerrors;
+		var root = self.maam_root;
+
+		if (args.length === 0) {
+			events = root.madm_adm.alarmEventNames();
+		} else {
+			events = args;
+		}
+
+		nerrors = 0;
+		events.forEach(function (eventName) {
+			var error;
+			error = root.madm_adm.alarmKaPrint({
+			    'eventName': eventName,
+			    'stream': process.stdout
+			});
+
+			if (error instanceof Error) {
+				cmdutil.warn(error);
+			}
+
+			console.log('');
+		});
+
+		if (nerrors > 0) {
+			process.exit(1);
+		}
+		root.finiAdm();
+		callback();
+	});
+};
+
+MantaAdmAlarmMetadata.prototype.do_ka.help = [
+    'Print information about an event',
+    '',
+    'Usage:',
+    '',
+    '    manta-adm alarm ka EVENT_NAME'
+].join('\n');
+
+MantaAdmAlarmMetadata.prototype.do_ka.options = [ maCommonOptions.configFile ];
+
+
+function MantaAdmAlarmProbeGroup(parent)
+{
+	this.maap_parent = parent;
+	this.maap_root = parent.maac_root;
+
+	cmdln.Cmdln.call(this, {
+	    'name': 'probegroup',
+	    'desc': 'View and configure information about amon probe groups.'
+	});
+}
+
+util.inherits(MantaAdmAlarmProbeGroup, cmdln.Cmdln);
+
+MantaAdmAlarmProbeGroup.prototype.do_list = function (subcmd,
+    opts, args, callback)
+{
+	var self = this;
+	var options = {};
+	var sources;
+
+	if (args.length > 0) {
+		callback(new Error('unexpected arguments'));
+		return;
+	}
+
+	options = listPrepareArgs(opts, madm.probeGroupColumnNames());
+	if (options instanceof Error) {
+		callback(options);
+		return;
+	}
+
+	/*
+	 * We fetch the list of open alarms in order to count the alarms for
+	 * each probe group.
+	 */
+	sources = {
+	    'configFull': true,
+	    'alarms': {
+		'state': 'open'
+	    }
+	};
+
+	options.stream = process.stdout;
+	this.maap_parent.maac_parent.initAdmAndFetchAlarms({
+	    'clioptions': opts,
+	    'sources': sources
+	}, function () {
+		self.maap_root.madm_adm.alarmsProbeGroupsList(options);
+		self.maap_root.finiAdm();
+		callback();
+	});
+};
+
+MantaAdmAlarmProbeGroup.prototype.do_list.help = [
+    'List open alarms',
+    '',
+    'Usage:',
+    '',
+    '    manta-adm alarm config probegroup list OPTIONS',
+    '',
+    '{{options}}',
+    '',
+    'Available columns for -o:\n',
+    '    ' + madm.probeGroupColumnNames().join(', ')
+].join('\n');
+
+MantaAdmAlarmProbeGroup.prototype.do_list.options = [
+    maCommonOptions.omitHeader,
+    maCommonOptions.columns,
+    maCommonOptions.configFile
+];
+
+
+/*
+ * Named arguments:
+ *
+ *     opts		options provided by cmdln
+ *
+ *     allowed		allowed column names
+ *
+ * Returns either an Error describing invalid command-line arguments or an
+ * object with "columns" and "omitHeader" set according to the options.
+ */
+function listPrepareArgs(opts, allowed)
+{
+	var options, selected;
+
+	options = {};
+	if (opts.columns) {
+		selected = checkColumns(allowed, opts.columns);
+		if (selected instanceof Error) {
+			return (selected);
+		}
+
+		options.columns = selected;
+	}
+
+	if (opts.omit_header) {
+		options.omitHeader = true;
+	} else {
+		options.omitHeader = false;
+	}
+
+	return (options);
+}
 
 function checkColumns(allowed, columns)
 {
diff --git a/docs/man/man1/manta-adm.md b/docs/man/man1/manta-adm.md
index 6c50d43..9c870f2 100644
--- a/docs/man/man1/manta-adm.md
+++ b/docs/man/man1/manta-adm.md
@@ -1,4 +1,4 @@
-# MANTA-ADM 1 "2016" Manta "Manta Operator Commands"
+# MANTA-ADM 1 "2017" Manta "Manta Operator Commands"
 
 ## NAME
 
@@ -6,6 +6,8 @@ manta-adm - administer a Manta deployment
 
 ## SYNOPSIS
 
+`manta-adm alarm SUBCOMMAND... [OPTIONS...]`
+
 `manta-adm cn [-l LOG_FILE] [-H] [-o FIELD...] [-n] [-s] CN_FILTER`
 
 `manta-adm genconfig "lab" | "coal"`
@@ -30,6 +32,9 @@ deployment.  This command only operates on zones within the same datacenter.
 The command may need to be repeated in other datacenters in order to execute it
 across an entire Manta deployment.
 
+`manta-adm alarm`
+  List and configure amon-based alarms for Manta.
+
 `manta-adm cn`
   Show information about Manta servers in this DC.
 
@@ -125,6 +130,14 @@ Many commands also accept:
   Emit verbose log to LOGFILE.  The special string "stdout" causes output to be
   emitted to the program's stdout.
 
+Commands that make changes support:
+
+`-n, --dryrun`
+  Print what changes would be made without actually making them.
+
+`-y, --confirm`
+  Bypass all confirmation prompts.
+
 **Important note for programmatic users:** Except as noted below, the output
 format for this command is subject to change at any time. The only subcommands
 whose output is considered committed are:
@@ -133,13 +146,179 @@ whose output is considered committed are:
 * `manta-adm show`, only when used with either the "-o" or "-j" option
 * `manta-adm zk list`, only when used with the "-o" option
 
-The output for any other commands may change at any time. Documented
+The output for any other commands may change at any time.  The `manta-adm alarm`
+subcommand is still considered an experimental interface.  All other documented
 subcommands, options, and arguments are committed, and you can use the exit
-status of the program to determine success of failure.
+status of the program to determine success or failure.
 
 
 ## SUBCOMMANDS
 
+### "alarm" subcommand
+
+`manta-adm alarm close ALARM_ID...`
+
+`manta-adm alarm config probegroup list [-H] [-o FIELD...]`
+
+`manta-adm alarm config show`
+
+`manta-adm alarm config update [-n] [-y] [--unconfigure]`
+
+`manta-adm alarm config verify [--unconfigure]`
+
+`manta-adm alarm details ALARM_ID...`
+
+`manta-adm alarm faults ALARM_ID...`
+
+`manta-adm alarm list [-H] [-o FIELD...] [--state=STATE]`
+
+`manta-adm alarm metadata events`
+
+`manta-adm alarm metadata ka [EVENT_NAME...]`
+
+`manta-adm alarm notify on|off ALARM_ID...`
+
+`manta-adm alarm show`
+
+The `manta-adm alarm` subcommand provides several tools that allow operators to:
+
+* view and configure amon probes and probe groups (`config` subcommand)
+* view open alarms (`show`, `list`, `details`, and `faults` subcommands)
+* configure notifications for open alarms (`notify` subcommand)
+* view local metadata about alarms and probes (`metadata` subcommand)
+
+The primary commands for working with alarms are:
+
+* `manta-adm alarm config update`: typically used during initial deployment and
+  after other deployment operations to ensure that the right set of probes and
+  probe groups are configured for the deployed components
+* `manta-adm alarm show`: summarize open alarms
+* `manta-adm alarm details ALARM_ID...`: report detailed information (including
+  suggested actions) for the specified alarms
+* `manta-adm alarm close ALARM_ID...`: close open alarms, indicating that they
+  no longer represent issues
+
+For background about Amon itself, probes, probegroups, and alarms, see the
+Triton Amon reference documentation.
+
+As with other subcommands, this command only operates on the current Triton
+datacenter.  In multi-datacenter deployments, alarms are managed separately in
+each datacenter.
+
+Some of the following subcommands can operate on many alarms.  These subcommands
+exit failure if they fail for any of the specified alarms, but the operation may
+have completed successfully for other alarms.  For example, closing 3 alarms is
+not atomic.  If the operation fails, then 1, 2, or 3 alarms may still be open.
+
+`manta-adm alarm close ALARM_ID...`
+
+Close the specified alarms.  These alarms will no longer show up in the
+`manta-adm alarm list` or `manta-adm alarm show` output.  Amon purges closed
+alarms completely after some period of time.
+
+If the underlying issue that caused an alarm is not actually resolved, then a
+new alarm may be opened for the same issue.  In some cases, that can happen
+almost immediately.  In other cases, it may take many hours for the problem to
+resurface.  In the case of transient issues, a new alarm may not open again
+until the issue occurs again, which could be days, weeks, or months later.  That
+does not mean the underlying issue was actually resolved.
+
+`manta-adm alarm config probegroup list [-H] [-o FIELD...]`
+
+List configured probe groups in tabular form.  This is primarily useful in
+debugging unexpected behavior from the alarms themselves.  The `manta-adm alarm
+config show` command provides a more useful summary of the probe groups that are
+configured.
+
+`manta-adm alarm config show`
+
+Shows summary information about the probes and probe groups that are configured.
+This is not generally necessary but it can be useful to verify that probes are
+configured as expected.
+
+`manta-adm alarm config update [-n] [-y] [--unconfigure]`
+
+Examines the Manta components that are deployed and the alarm configuration
+(specifically, the probes and probe groups deployed to monitor those components)
+and compares them with the expected configuration.  If these do not match,
+prints out a summary of proposed changes to the configuration and optionally
+applies those changes.
+
+If `--unconfigure` is specified, then the tool removes all probes and probe
+groups.
+
+This is the primary tool for updating the set of deployed probes and probe
+groups.  Operators would typically use this command:
+
+- during initial deployment to deploy probes and probe groups
+- after deploying (or undeploying) any Manta components to deploy (or remove)
+  probes related to the affected components
+- after updating the `manta-adm` tool itself, which bundles the probe
+  definitions, to deploy any new or updated probes
+- at any time to verify that the configuration matches what's expected
+
+This operation is idempotent.
+
+This command supports the `-n/--dryrun` and `-y/--confirm` options described
+above.
+
+`manta-adm alarm config verify [--unconfigure]`
+
+Behaves exactly like `manta-adm alarm config update --dryrun`.
+
+`manta-adm alarm details ALARM_ID...`
+
+Prints detailed information about any number of alarms.  The detailed
+information includes the time the alarm was opened, the last time an event was
+associated with this alarm, the total number of events associated with the
+alarm, the affected components, and information about the severity, automated
+response, and suggested actions for this issue.
+
+`manta-adm alarm faults ALARM_ID...`
+
+Prints detailed information about the faults associated with any number of
+alarms.  Each fault represents a particular probe failure.  The specific
+information provided depends on the alarm.  If the alarm related to a failed
+health check command, then the exit status, terminating signal, stdout, and
+stderr of the command are provided.  If the alarm relates to an error log entry,
+the contents of the log entry are provided.  There can be many faults associated
+with a single alarm.
+
+`manta-adm alarm list [-H] [-o FIELD...] [--state=STATE]`
+
+Lists alarms in tabular form.  `STATE` controls which alarms are listed, which
+may be any of "open", "closed", "all", or "recent".  The default is "open".
+
+See also the `manta-adm alarm show` command.
+
+`manta-adm alarm metadata events`
+
+List the names for all of the events known to this version of `manta-adm`.  Each
+event corresponds to a distinct kind of problem.  For details about each one,
+see `manta-adm alarm metadata ka`.  The list of events comes from metadata
+bundled with the `manta-adm` tool.
+
+`manta-adm alarm metadata ka [EVENT_NAME...]`
+
+Print out knowledge articles about each of the specified events.  This
+information comes from metadata bundled with the `manta-adm` tool.  If no events
+are specified, prints out knowledge articles about all events.
+
+Knowledge articles include information about the severity of the problem, the
+impact, the automated response, and the suggested action.
+
+`manta-adm alarm notify on|off ALARM_ID...`
+
+Enable or disable notifications for the specified alarms.  Notifications are
+generally configured through Amon, which supports both email and XMPP
+notification for new alarms and new events on existing, open alarms.  This
+command controls whether notifications are enabled for the specified alarms.
+
+`manta-adm alarm show`
+
+Summarize open alarms.  For each alarm, use the `manta-adm alarm details`
+subcommand to view more information about it.
+
 
 ### "cn" subcommand
 
@@ -479,15 +658,8 @@ See above for information about the `-l`, `-H`, and `-o` options for
 ordinal number of each server), "datacenter", "zoneabbr", "zonename", "ip", and
 "port".
 
-The `manta-adm zk fixup` command supports options:
-
-`-n, --dryrun`
-  Print what changes would be made without actually making them.
-
-`-y, --confirm`
-  Bypass all confirmation prompts.
-
-It also supports the `-l/--log_file` option described above.
+The `manta-adm zk fixup` command supports the `-l/--log_file`, `-n/--dryrun`,
+and `-y/--confirm` options described above.
 
 
 ## EXIT STATUS
@@ -504,7 +676,7 @@ It also supports the `-l/--log_file` option described above.
 
 ## COPYRIGHT
 
-Copyright (c) 2016 Joyent Inc.
+Copyright (c) 2017 Joyent Inc.
 
 ## SEE ALSO
 
diff --git a/lib/adm.js b/lib/adm.js
index 984a7e0..2c58f60 100644
--- a/lib/adm.js
+++ b/lib/adm.js
@@ -14,6 +14,7 @@
 
 var assert = require('assert');
 var assertplus = require('assert-plus');
+var extsprintf = require('extsprintf');
 var fs = require('fs');
 var jsprim = require('jsprim');
 var net = require('net');
@@ -21,14 +22,26 @@ var path = require('path');
 var sprintf = require('sprintf-js').sprintf;
 var tab = require('tab');
 var vasync = require('vasync');
+var wordwrap = require('wordwrap');
+
 var VError = require('verror').VError;
 var MultiError = require('verror').MultiError;
-var fprintf = require('extsprintf').fprintf;
+var fprintf = extsprintf.fprintf;
+
+var alarms = require('./alarms');
 var common = require('../lib/common');
 var deploy = require('../lib/deploy');
 var layout = require('./layout');
 var svcs = require('./services');
 
+/* Public interface (used only inside this module) */
+exports.columnNames = columnNames;
+exports.alarmColumnNames = alarmColumnNames;
+exports.cnColumnNames = cnColumnNames;
+exports.probeGroupColumnNames = probeGroupColumnNames;
+exports.zkColumnNames = zkColumnNames;
+exports.MantaAdm = maAdm;
+
 var maMaxConcurrency = 50; /* concurrent requests to SDC services */
 
 /*
@@ -36,11 +49,11 @@ var maMaxConcurrency = 50; /* concurrent requests to SDC services */
  */
 var maZkConfigProp = process.env['ZK_SERVERS_PROPNAME'] || 'ZK_SERVERS';
 
-/* Public interface (used only inside this module) */
-exports.columnNames = columnNames;
-exports.cnColumnNames = cnColumnNames;
-exports.zkColumnNames = zkColumnNames;
-exports.MantaAdm = maAdm;
+/*
+ * Path to default alarm metadata.
+ */
+var maAlarmMetadataDirectory =
+    path.join(__dirname, '..', 'alarm_metadata/probe_templates');
 
 /*
  * Available output columns for the list of zones.
@@ -63,7 +76,7 @@ var maColumns = {
 	'width': 16
     },
     'shard': {
-    	'label': 'SH',
+	'label': 'SH',
 	'width': 2,
 	'align': 'right'
     },
@@ -76,7 +89,7 @@ var maColumns = {
 	'width': 36
     },
     'zoneabbr': {
-    	'label': 'ZONEABBR',
+	'label': 'ZONEABBR',
 	'width': 8
     },
 
@@ -95,7 +108,7 @@ var maColumns = {
 	'align': 'right'
     },
     'indent': {
-    	'label': '',
+	'label': '',
 	'width': 4
     },
     'version': {
@@ -111,6 +124,95 @@ function columnNames()
 	}));
 }
 
+var maAlarmColumns = {
+    'alarm': {
+	'label': 'ALARM',
+	'width': 6
+    },
+    'dateopened': {
+	'label': 'DATE_OPENED',
+	'width': 10
+    },
+    'timeopened': {
+	'label': 'TIME_OPENED',
+	'width': 24
+    },
+    'dateclosed': {
+	'label': 'DATE_CLOSED',
+	'width': 10
+    },
+    'timeclosed': {
+	'label': 'TIME_CLOSED',
+	'width': 24
+    },
+    'datelast': {
+	'label': 'DATE_LAST',
+	'width': 10
+    },
+    'timelast': {
+	'label': 'TIME_LAST',
+	'width': 24
+    },
+    'nevents': {
+	'label': 'NEVENTS',
+	'width': 5,
+	'align': 'right'
+    },
+    'nflts': {
+	'label': 'NFLTS',
+	'width': 5,
+	'align': 'right'
+    },
+    'notify': {
+	'label': 'NFY',
+	'width': 3
+    },
+    'summary': {
+	'label': 'SUMMARY',
+	'width': 30
+    }
+};
+
+function alarmColumnNames()
+{
+	return (Object.keys(maAlarmColumns));
+}
+
+var maProbeGroupColumns = {
+    'uuid': {
+	'label': 'UUID',
+	'width': 36
+    },
+    'name': {
+	'label': 'NAME',
+	'width': 20
+    },
+    'contacts': {
+	'label': 'CONTACTS',
+	'width': 15
+    },
+    'enabled': {
+	'label': 'ENAB',
+	'width': 4,
+	'align': 'right'
+    },
+    'nalarms': {
+	'label': 'NALARMS',
+	'width': 7,
+	'align': 'right'
+    },
+    'nprobes': {
+	'label': 'NPROBES',
+	'width': 7,
+	'align': 'right'
+    }
+};
+
+function probeGroupColumnNames()
+{
+	return (Object.keys(maProbeGroupColumns));
+}
+
 var maCnColumns = {
     'server_uuid': {
 	'label': 'SERVER UUID',
@@ -163,7 +265,7 @@ var maZkColumns = {
 	'width': 10
     },
     'zoneabbr': {
-    	'label': 'ZONEABBR',
+	'label': 'ZONEABBR',
 	'width': 8
     },
     'zonename': {
@@ -208,7 +310,7 @@ function zkColumnNames()
  * it doesn't matter what specific servers they're on.  This is mainly used in
  * development and testing.
  *
- * There are four supported use cases:
+ * There are several supported use cases:
  *
  *     o "genconfig" operation: call loadSdcConfig, then fetchDeployed, then
  *       one of dumpConfigCoal, dumpConfigLab, or genconfigFromFile.
@@ -224,6 +326,10 @@ function zkColumnNames()
  *       fetchDeployed, then some combination of dumpZkServers and
  *       fixupZkServers.
  *
+ *     o alarm configuration: call loadSdcConfig, then likely fetchDeployed,
+ *       then alarmsInit with an appropriate set of sources, then any of the
+ *       alarm-related functions.
+ *
  * Any other sequence of operations (skipping any of these, or duplicating those
  * the ones that can't explicitly be called more than once) is invalid.
  */
@@ -267,6 +373,17 @@ function maAdm(log)
 	 */
 	this.ma_instances_flattened = null;
 
+	/*
+	 * General-purpose information about each instance.  See InstanceInfo.
+	 */
+	this.ma_instance_info = null;
+
+	/*
+	 * Mapping of SAPI service names to the list of local (same-datacenter)
+	 * instances for this service.
+	 */
+	this.ma_instances_local_bysvcname = null;
+
 	/*
 	 * CNAPI server objects, indexed by server_uuid.
 	 */
@@ -277,11 +394,49 @@ function maAdm(log)
 	 */
 	this.ma_vms = null;
 
+	/*
+	 * Translation table from VM or CN uuid to the name of the service to
+	 * which this instance belongs.  CNs, this is just "global zone".
+	 * This only contains translations for the current datacenter.
+	 */
+	this.ma_instance_svcname = {};
+
 	/*
 	 * IMGAPI image objects, indexed by image uuid.
 	 */
 	this.ma_images = null;
 
+	/*
+	 * List of alarm sources that we initially collected data from.
+	 */
+	this.ma_alarm_sources = null;
+
+	/*
+	 * Amon alarm set
+	 */
+	this.ma_alarms = null;
+
+	/*
+	 * Deployed Amon configuration
+	 */
+	this.ma_amon_deployed = null;
+
+	/*
+	 * Warning-level issues encountered while loading alarms information.
+	 */
+	this.ma_alarm_warnings = [];
+
+	/*
+	 * Alarm metadata
+	 */
+	this.ma_alarm_metadata = null;
+
+	/*
+	 * Mapping of alarm levels to the Amon contacts to use for probe groups
+	 * at that level.
+	 */
+	this.ma_alarm_levels = {};
+
 	/*
 	 * Information about global zones, indexed by server_uuid.  This is
 	 * where we keep useful properties derived non-trivially from the CNAPI
@@ -456,9 +611,9 @@ maAdm.prototype.fetchDeployed = function (callback)
 		},
 
 		/*
-		 * XXX want a way to fetch all application instances from VMAPI.
-		 * We currently use owner_uuid as a proxy for that, but that's
-		 * not necessarily correct.
+		 * TODO want a way to fetch all application instances from
+		 * VMAPI.  We currently use owner_uuid as a proxy for that, but
+		 * that's not necessarily correct.
 		 */
 		function fetchVmInfo(_, stepcb) {
 			var params = {
@@ -611,6 +766,916 @@ maAdm.prototype.loadFakeDeployed = function (config)
 	this.loadInstances();
 };
 
+var schemaAlarmConfigLevel = {
+    'type': 'array',
+    'required': true,
+    'items': {
+	'type': 'string'
+    }
+};
+
+var schemaAlarmConfig = {
+    'type': 'object',
+    'properties': {
+	'levels': {
+	    'type': 'object',
+	    'required': true,
+	    'additionalProperties': false,
+	    'properties': {
+		'alert': schemaAlarmConfigLevel,
+		'info': schemaAlarmConfigLevel
+	    }
+	}
+    }
+};
+
+/*
+ * General-purpose function for loading alarm-related data.  There are several
+ * different sources, some of which are expensive to gather, and callers must
+ * specify the data sources they want to load from.  Currently, this function
+ * should only be called once in the lifetime of this object.
+ *
+ * Named arguments:
+ *
+ *    concurrency               maximum number of concurrent requests to make
+ *    (number)
+ *
+ *    configFile		sdc-manta configuration file, which is used for
+ *    				the set of contacts used for alarms
+ *
+ *    sources                   describes which sources to load data from
+ *    (object)
+ *
+ *        configBasic           load probe group information, necessary for
+ *        (boolean)             summarizing basic configuration
+ *
+ *        configFull            load probe information, necessary for actually
+ *        (boolean)             verifying or updating configuration.  This
+ *                              implicitly pulls in "configBasic" as well.
+ *
+ *        alarms                load alarm information.  Exactly one of "state"
+ *        (object)              or "alarmIds" must be specified.
+ *
+ *            state             fetch all alarms in state "state"
+ *            (string)
+ *
+ *            alarmIds          fetch the specified alarm ids
+ *            (array of string)
+ *
+ * In all cases, even if "sources" is empty, local metadata related to alarms is
+ * loaded.
+ *
+ * Callers should invoke alarmWarnings() after calling this to see if there were
+ * any non-fatal issues associated with loading alarms.  Operators should
+ * generally be notified about these issues (as warning-level messages).
+ */
+maAdm.prototype.alarmsInit = function (args, callback)
+{
+	var self = this;
+	var account, configfile, concurrency, funcs, components;
+	var alarmstate = null, alarmIds = null;
+
+	assertplus.object(args, 'args');
+	assertplus.object(args, 'args.sources');
+	assertplus.number(args.concurrency, 'args.concurrency');
+	assertplus.string(args.configFile, 'args.configFile');
+	assertplus.func(callback, 'callback');
+	assertplus.strictEqual(this.ma_alarms, null);
+	assertplus.strictEqual(this.ma_amon_deployed, null);
+	assertplus.strictEqual(this.ma_alarm_metadata, null);
+
+	assertplus.optionalBool(args.sources.configBasic);
+	assertplus.optionalBool(args.sources.configFull);
+	assertplus.optionalObject(args.sources.alarms);
+	if (args.sources.alarms) {
+		assertplus.optionalString(args.sources.alarms.state);
+		if (typeof (args.sources.alarms.state) == 'string') {
+			if (args.sources.alarms.state != 'open' &&
+			    args.sources.alarms.state != 'all' &&
+			    args.sources.alarms.state != 'recent' &&
+			    args.sources.alarms.state != 'closed') {
+				throw (new VError('unsupported alarm state: ' +
+				    '"%s"', args.sources.alarms.state));
+			}
+
+			assertplus.ok(
+			    !args.sources.alarms.hasOwnProperty('alarmIds'),
+			    'cannot specify "sources.alarms.state" and ' +
+			    '"sources.alarms.alarmIds"');
+			alarmstate = args.sources.alarms.state;
+		} else {
+			assertplus.arrayOfString(args.sources.alarms.alarmIds,
+			    'must specify "sources.alarms.state" or ' +
+			    '"sources.alarms.alarmIds');
+			alarmIds = args.sources.alarms.alarmIds;
+		}
+	}
+
+	this.ma_alarm_sources = jsprim.deepCopy(args.sources);
+	configfile = args.configFile;
+	concurrency = args.concurrency;
+	funcs = [];
+
+	/*
+	 * We always want to load the configuration file.
+	 */
+	funcs.push(function loadConfigFile(_, stepcb) {
+		fs.readFile(configfile, function onFileRead(err, contents) {
+			var conf;
+
+			if (err) {
+				err = new VError(err, 'read "%s"', configfile);
+				stepcb(err);
+				return;
+			}
+
+			try {
+				conf = JSON.parse(contents.toString('utf8'));
+			} catch (ex) {
+				err = new VError(ex, 'parse "%s"', configfile);
+				stepcb(err);
+				return;
+			}
+
+			err = jsprim.validateJsonObject(
+			    schemaAlarmConfig, conf);
+			if (err !== null) {
+				stepcb(new VError(
+				    err, 'config "%s"', configfile));
+				return;
+			}
+
+			self.ma_alarm_levels['minor'] = conf.levels.info;
+			self.ma_alarm_levels['major'] = conf.levels.alert;
+			self.ma_alarm_levels['critical'] = conf.levels.alert;
+			stepcb();
+		});
+	});
+
+	/*
+	 * We always want to load metadata.
+	 */
+	funcs.push(function loadMetadata(_, stepcb) {
+		alarms.loadMetadata({
+		    'directory': maAlarmMetadataDirectory
+		}, function onAlarmMetadataLoaded(err, metadata) {
+			if (!err) {
+				self.ma_alarm_metadata = metadata;
+			}
+
+			stepcb(err);
+		});
+	});
+
+	/*
+	 * If the user asked for "configBasic" or "configFull", then we need the
+	 * list of probe groups.
+	 */
+	if (args.sources.configBasic || args.sources.configFull) {
+		assert.ok(this.ma_instances !== null,
+		    'must load deployed first');
+		account = this.ma_app.owner_uuid;
+		funcs.push(function fetchProbeGroups(_, stepcb) {
+			alarms.amonLoadProbeGroups({
+			    'amon': self.ma_sdc.AMON,
+			    'account': account
+			}, function (err, amonconfig) {
+				/*
+				 * This function can emit both an error and a
+				 * result.  In that case, the error represents
+				 * non-fatal (warning-level) issues associated
+				 * with the operation.
+				 */
+				if (amonconfig) {
+					self.ma_amon_deployed = amonconfig;
+				}
+
+				if (err) {
+					common.errorForEach(err,
+					    function (e) {
+						self.ma_alarm_warnings.push(e);
+					    });
+				}
+
+				stepcb();
+			});
+		});
+	}
+
+	/*
+	 * If the user asked for "configFull", then we additionally need
+	 * the list of probes.
+	 */
+	if (args.sources.configFull) {
+		components = Object.keys(this.ma_vms).map(function (vmuuid) {
+			return ({ 'type': 'vm', 'uuid': vmuuid });
+		}).concat(Object.keys(this.ma_cns).map(function (cnuuid) {
+			return ({ 'type': 'cn', 'uuid': cnuuid });
+		}));
+
+		funcs.push(function fetchProbes(_, stepcb) {
+			assertplus.notStrictEqual(self.ma_amon_deployed, null);
+
+			/*
+			 * This function inserts the probe information into
+			 * self.ma_amon_deployed, so we don't need to do
+			 * anything when it completes.
+			 */
+			alarms.amonLoadComponentProbes({
+			    'amonRaw': self.ma_sdc.AMON_RAW,
+			    'amoncfg': self.ma_amon_deployed,
+			    'concurrency': concurrency,
+			    'components': components
+			}, function (err, warnings) {
+				if (warnings) {
+					common.errorForEach(warnings,
+					    function (e) {
+						self.ma_alarm_warnings.push(e);
+					    });
+				}
+
+				stepcb(err);
+			});
+		});
+	}
+
+	/*
+	 * Finally, if the user asked for alarms, then fetch them as requested.
+	 */
+	if (alarmstate !== null) {
+		assert.ok(this.ma_instances !== null,
+		    'must load deployed first');
+		account = this.ma_app.owner_uuid;
+		funcs.push(function fetchAlarms(_, stepcb) {
+			alarms.amonLoadAlarmsForState({
+			    'amon': self.ma_sdc.AMON,
+			    'account': account,
+			    'state': alarmstate
+			}, function (err, alarmset) {
+				if (!alarmset) {
+					stepcb(err);
+					return;
+				}
+
+				self.ma_alarms = alarmset;
+				if (err) {
+					common.errorForEach(err, function (e) {
+						self.ma_alarm_warnings.push(e);
+					});
+				}
+				stepcb();
+			});
+		});
+	} else if (alarmIds !== null) {
+		assert.ok(this.ma_instances !== null,
+		    'must load deployed first');
+		account = this.ma_app.owner_uuid;
+		funcs.push(function fetchAlarmIds(_, stepcb) {
+			alarms.amonLoadAlarmsForIds({
+			    'amon': self.ma_sdc.AMON,
+			    'account': account,
+			    'alarmIds': alarmIds,
+			    'concurrency': concurrency
+			}, function (err, alarmset) {
+				/*
+				 * This function can return warnings (in "err")
+				 * as well as a list of alarms.
+				 */
+				assertplus.strictEqual(self.ma_alarms, null);
+				self.ma_alarms = alarmset;
+				if (err) {
+					common.errorForEach(err, function (e) {
+						self.ma_alarm_warnings.push(e);
+					});
+				}
+				stepcb();
+			});
+		});
+	}
+
+	vasync.pipeline({
+	    'funcs': funcs
+	}, function (err) {
+		callback(err);
+	});
+};
+
+maAdm.prototype.alarmWarnings = function ()
+{
+	return (this.ma_alarm_warnings);
+};
+
+/*
+ * Show information about fetched alarms.
+ */
+maAdm.prototype.alarmsShow = function (args)
+{
+	var self = this;
+	var out;
+
+	assertplus.object(args, 'args');
+	assertplus.object(args.stream, 'args.stream');
+	assertplus.notStrictEqual(this.ma_amon_deployed, null,
+	    'must call alarmsInit() with "configBasic" source first');
+	assertplus.notStrictEqual(this.ma_alarms, null,
+	    'must call alarmsInit() with "alarms" source first');
+
+	out = args.stream;
+	this.ma_alarms.eachAlarm(function (id, alarm) {
+		var details;
+
+		details = self.alarmDetails(alarm);
+		fprintf(out, 'ALARM %-6d  %-16s  %6d event%s (last %s)\n', id,
+		    details.ka !== null ? details.ka.ka_severity.toUpperCase() :
+		    'unknown severity', alarm.a_nevents,
+		    alarm.a_nevents == 1 ? ' ' : 's',
+		    alarm.a_time_last.toISOString());
+		if (alarm.a_suppressed) {
+			fprintf(out, '(note: notifications disabled)\n');
+		}
+
+		fprintf(out, '%s\n\n', details.summary);
+	});
+};
+
+/*
+ * List open alarms.
+ */
+maAdm.prototype.alarmsList = function (args)
+{
+	var self = this;
+	var rows;
+	var nnoprobegroup = 0;
+	var nbadprobegroup = 0;
+
+	assertplus.object(args, 'args');
+	assertplus.object(args.stream, 'args.stream');
+	assertplus.optionalArrayOfString(args.columns, 'args.columns');
+	assertplus.bool(args.omitHeader, 'args.omitHeader');
+
+	assertplus.notStrictEqual(this.ma_amon_deployed, null,
+	    'must call alarmsInit() with "configBasic" source first');
+	assertplus.notStrictEqual(this.ma_alarms, null,
+	    'must call alarmsInit() with "alarms" source first');
+	rows = [];
+	this.ma_alarms.eachAlarm(function (id, alarm) {
+		var details;
+
+		details = self.alarmDetails(alarm);
+		if (details.nogroup) {
+			nnoprobegroup++;
+		}
+		if (details.badgroup) {
+			nbadprobegroup++;
+		}
+
+		rows.push({
+		    'ALARM': id,
+		    'DATE_OPENED': fmtDateOnly(alarm.a_time_opened),
+		    'TIME_OPENED': fmtListDateTime(alarm.a_time_opened),
+		    'DATE_CLOSED': fmtDateOnly(alarm.a_time_closed),
+		    'TIME_CLOSED': fmtListDateTime(alarm.a_time_closed),
+		    'DATE_LAST': fmtDateOnly(alarm.a_time_last),
+		    'TIME_LAST': fmtListDateTime(alarm.a_time_last),
+		    'NFLTS': alarm.a_faults.length,
+		    'NEVENTS': alarm.a_nevents,
+		    'NFY': alarm.a_suppressed ? 'no' : 'yes',
+		    'SUMMARY': details.summary
+		});
+	});
+
+	this.doList({
+	    'stream': args.stream,
+	    'columnsSelected': args.columns,
+	    'columnsDefault': [ 'alarm', 'dateLast', 'nflts', 'summary' ],
+	    'columnMetadata': maAlarmColumns,
+	    'rows': rows,
+	    'omitHeader': args.omitHeader
+	});
+
+	if (nnoprobegroup) {
+		console.error('note: %d alarm%s %s not associated with ' +
+		    'probe groups', nnoprobegroup,
+		    nnoprobegroup == 1 ? '' : 's',
+		    nnoprobegroup == 1 ? 'was' : 'were');
+	}
+
+	if (nbadprobegroup) {
+		console.error('note: %d alarm%s %s associated with ' +
+		    'non-existent probe groups', nbadprobegroup,
+		    nbadprobegroup == 1 ? '' : 's',
+		    nbadprobegroup == 1 ? 'was' : 'were');
+	}
+};
+
+maAdm.prototype.alarmsProbeGroupsList = function (args)
+{
+	var self = this;
+	var nalarmsByGroup, rows;
+
+	assertplus.object(args, 'args');
+	assertplus.object(args.stream, 'args.stream');
+	assertplus.optionalArrayOfString(args.columns, 'args.columns');
+	assertplus.bool(args.omitHeader, 'args.omitHeader');
+	assertplus.notStrictEqual(this.ma_amon_deployed, null,
+	    'must call alarmsInit() with "configBasic" source first');
+	assertplus.notStrictEqual(this.ma_alarms, null,
+	    'must call alarmsInit() with "alarms" source first');
+
+	/*
+	 * First, count the alarms for each probe group.
+	 */
+	nalarmsByGroup = {};
+	this.ma_alarms.eachAlarm(function (id, aa) {
+		if (aa.a_groupid === null) {
+			return;
+		}
+
+		assertplus.string(aa.a_groupid);
+		if (!nalarmsByGroup.hasOwnProperty(aa.a_groupid)) {
+			nalarmsByGroup[aa.a_groupid] = 0;
+		}
+
+		nalarmsByGroup[aa.a_groupid]++;
+	});
+
+	/*
+	 * Construct an output row for each probe group.
+	 */
+	rows = [];
+	this.ma_amon_deployed.eachProbeGroup(function (pg) {
+		var row, nprobes, nalarms;
+
+		nprobes = 0;
+		self.ma_amon_deployed.eachProbeGroupProbe(pg.pg_name,
+		    function () { nprobes++; });
+
+		nalarms = nalarmsByGroup.hasOwnProperty(pg.pg_uuid) ?
+		    nalarmsByGroup[pg.pg_uuid] : 0;
+
+		row = {
+		    'NAME': pg.pg_name,
+		    'UUID': pg.pg_uuid,
+		    'CONTACTS': pg.pg_contacts.join(','),
+		    'NPROBES': nprobes,
+		    'NALARMS': nalarms,
+		    'ENAB': pg.pg_enabled ? 'yes' : 'no'
+		};
+
+		rows.push(row);
+	});
+
+	this.doList({
+	    'stream': args.stream,
+	    'columnsSelected': args.columns,
+	    'columnsDefault': [ 'uuid', 'name' ],
+	    'columnMetadata': maProbeGroupColumns,
+	    'rows': rows,
+	    'omitHeader': args.omitHeader
+	});
+};
+
+maAdm.prototype.alarmsClose = function (args, callback)
+{
+	assertplus.object(args, 'args');
+	assertplus.arrayOfString(args.alarmIds, 'args.alarmIds');
+	assertplus.number(args.concurrency, 'args.concurrency');
+
+	alarms.amonCloseAlarms({
+	    'amon': this.ma_sdc.AMON,
+	    'account': this.ma_app.owner_uuid,
+	    'alarmIds': args.alarmIds,
+	    'concurrency': args.concurrency
+	}, callback);
+};
+
+maAdm.prototype.alarmsUpdateNotification = function (args, callback)
+{
+	assertplus.object(args, 'args');
+	assertplus.arrayOfString(args.alarmIds, 'args.alarmIds');
+	assertplus.number(args.concurrency, 'args.concurrency');
+	assertplus.bool(args.suppressed, 'args.suppressed');
+
+	alarms.amonUpdateAlarmsNotification({
+	    'amonRaw': this.ma_sdc.AMON_RAW,
+	    'account': this.ma_app.owner_uuid,
+	    'alarmIds': args.alarmIds,
+	    'concurrency': args.concurrency,
+	    'suppressed': args.suppressed
+	}, callback);
+};
+
+maAdm.prototype.alarmDetails = function (alarm)
+{
+	var rv, pgid, pgname, eventName;
+	var deployed, metadata;
+
+	assertplus.notStrictEqual(this.ma_amon_deployed, null,
+	    'must call alarmsInit() with "configBasic" source first');
+	deployed = this.ma_amon_deployed;
+	metadata = this.ma_alarm_metadata;
+	pgid = alarm.a_groupid;
+	rv = {
+	    'badgroup': false,
+	    'nogroup': false,
+	    'summary': null,
+	    'ka': null
+	};
+
+	if (pgid !== null) {
+		pgname = deployed.probeGroupNameForUuid(pgid);
+		if (pgname !== null) {
+			eventName = metadata.probeGroupEventName(
+			    pgname);
+			if (rv.eventName !== null) {
+				rv.ka = metadata.eventKa(eventName);
+				if (rv.ka !== null) {
+					rv.summary = rv.ka.ka_title;
+				} else {
+					rv.summary = eventName;
+				}
+			} else {
+				rv.summary = pgname;
+			}
+		} else {
+			rv.badgroup = true;
+		}
+	} else {
+		rv.nogroup = true;
+	}
+
+	if (rv.summary === null) {
+		/*
+		 * Open alarms should have at least one fault, and closed alarms
+		 * should have none.  This is verified on the way in from Amon.
+		 */
+		if (alarm.a_faults.length > 0) {
+			rv.summary = alarm.a_faults[0].aflt_summary;
+		} else {
+			assertplus.ok(alarm.a_closed);
+			rv.summary = '(closed alarm has no faults)';
+		}
+	}
+
+	return (rv);
+};
+
+maAdm.prototype.alarmPrint = function alarmPrint(args)
+{
+	var alarm, out, i, nmax;
+	var details, fault, fltdetail, svcnames, formatted;
+	var self = this;
+	var now = Date.now();
+
+	assertplus.object(args, 'args');
+	assertplus.string(args.id, 'args.id');
+	assertplus.object(args.stream, 'args.stream');
+	assertplus.optionalNumber(args.nmaxfaults, 'args.nmaxfaults');
+	assertplus.notStrictEqual(this.ma_amon_deployed, null,
+	    'must call alarmsInit() with "configBasic" source first');
+
+	out = args.stream;
+	alarm = this.ma_alarms.alarmForId(args.id);
+	if (alarm === null) {
+		return (new VError('no such alarm: "%s"', args.id));
+	}
+
+	details = this.alarmDetails(alarm);
+	if (details.ka !== null) {
+		this.doKaPrint({
+		    'header': 'ALARM ' + args.id,
+		    'stream': out,
+		    'ka': details.ka
+		});
+		fprintf(out, '\n');
+	} else {
+		fprintf(out, 'ALARM %s\n', alarm.a_id);
+	}
+
+	fprintf(out, 'summary:         %s\n', details.summary);
+	fprintf(out, 'state:           %s\n', alarm.a_closed ?
+	    'closed': 'open');
+	fprintf(out, 'opened:          %s (%s ago)\n',
+	    alarm.a_time_opened.toISOString(),
+	    fmtDuration(now - alarm.a_time_opened.getTime()));
+	fprintf(out, 'last event:      %s (%s ago)\n',
+	    alarm.a_time_last.toISOString(),
+	    fmtDuration(now - alarm.a_time_last.getTime()));
+	if (alarm.a_time_closed === null) {
+		fprintf(out, 'closed:          never\n');
+	} else {
+		fprintf(out, 'closed:          %s (%s ago)\n',
+		    alarm.a_time_closed.toISOString(),
+		    fmtDuration(now - alarm.a_time_closed.getTime()));
+	}
+	fprintf(out, 'notifications:   %s\n', alarm.a_suppressed ?
+	    'disabled' : 'enabled');
+	fprintf(out, 'total faults:    %s\n', alarm.a_faults.length);
+	fprintf(out, 'total events:    %s\n', alarm.a_nevents);
+
+	svcnames = {};
+	alarm.a_faults.forEach(function (f) {
+		svcnames[
+		    self.ma_instance_svcname.hasOwnProperty(f.aflt_agent) ?
+		    self.ma_instance_svcname[f.aflt_agent] :
+		    'unknown'] = true;
+	});
+
+	if (alarm.a_faults.length === 0) {
+		svcnames['none (alarm is closed)'] = true;
+	}
+
+	fprintf(out, 'affects zones:   %s\n',
+	    Object.keys(svcnames).join(', '));
+
+	if (alarm.a_faults.length === 0) {
+		return;
+	}
+
+	if (typeof (args.nmaxfaults) == 'number') {
+		nmax = Math.min(alarm.a_faults.length, args.nmaxfaults);
+	} else {
+		nmax = alarm.a_faults.length;
+	}
+
+	for (i = 0; i < nmax; i++) {
+		fprintf(out, '\n    FAULT %d of %d FOR ALARM %d\n',
+		    i + 1, alarm.a_faults.length, alarm.a_id);
+		fault = alarm.a_faults[i];
+		fprintf(out, '    reason:          %s\n', fault.aflt_summary);
+		fprintf(out, '    time:            %s (%s ago)\n',
+		    fault.aflt_time.toISOString(),
+		    fmtDuration(now - fault.aflt_time.getTime()));
+		fprintf(out, '    machine:         %s\n', fault.aflt_machine);
+		fprintf(out, '    agent:           %s\n', fault.aflt_agent);
+		fprintf(out, '    agent alias:     %s\n',
+		    fault.aflt_agent_alias);
+
+		/*
+		 * It's not expected that we would ever see a fault with "clear"
+		 * set to true.  If Amon received that, it would have closed the
+		 * fault and removed it from the list of faults.
+		 */
+		if (fault.aflt_clear) {
+			fprintf(out, '    warn: fault event has "clear" set\n');
+		}
+
+		/*
+		 * Manta uses four different types of probes:
+		 *
+		 *   - "bunyan-log-scan" (scans a file for messages at a
+		 *     specified level)
+		 *   - "cmd" (runs a command periodically)
+		 *   - "disk-usage (checks disk space used)
+		 *   - "log-scan" (scans a file for a pattern)
+		 *
+		 * We can encounter faults for any of these types of probe, and
+		 * we want to print each one differently.  We could reliably
+		 * tell which type we're looking at by looking up the probe, but
+		 * it's expensive to fetch information about all probes, so we
+		 * don't typically do it just to show alarm details.  So
+		 * instead, we use heuristics about the information provided in
+		 * the fault to guess what kind we're looking at.
+		 */
+		fltdetail = fault.aflt_data.details;
+		if (!fltdetail) {
+			continue;
+		}
+
+		if (typeof (fltdetail.cmd) == 'string' &&
+		    fltdetail.hasOwnProperty('stdout') &&
+		    fltdetail.hasOwnProperty('stderr')) {
+			/*
+			 * This is a "cmd" probe.
+			 */
+			fprintf(out, '    cmd exit status: %s\n',
+			    (fltdetail.exitStatus ||
+			    fltdetail.exitStatus === 0) ?
+			    fltdetail.exitStatus : 'none');
+			fprintf(out, '    cmd signal:      %s\n',
+			    fltdetail.signal ? fltdetail.signal : 'none');
+			fprintf(out, '    probe cmd:       %s\n',
+			    JSON.stringify(fltdetail.cmd));
+
+			fprintf(out, '%s', formatCmdOutput(
+			    '    ', 'stdout', fltdetail.stdout));
+			fprintf(out, '%s', formatCmdOutput(
+			    '    ', 'stderr', fltdetail.stderr));
+			continue;
+		}
+
+		if (Array.isArray(fltdetail.matches) &&
+		    fltdetail.matches.length > 0) {
+			if (typeof (fltdetail.matches[0].context) == 'string') {
+				/*
+				 * This is a "log-scan" probe, and the "context"
+				 * field indicates the line that matched.
+				 */
+				fprintf(out, '    first matching message:\n');
+				fprintf(out, '    ------------\n');
+				fprintf(out, '    %s\n', JSON.stringify(
+				    fltdetail.matches[0].context));
+				fprintf(out, '    ------------\n');
+				continue;
+			}
+
+			if (typeof (fltdetail.matches[0].match) == 'object') {
+				/*
+				 * This is a "bunyan-log-scan" probe, and the
+				 * "match" field indicates the entire bunyan
+				 * record that matched.  Note that "log-scan"
+				 * probes also have a "match" field, and it
+				 * means something else, so it's important that
+				 * we checked for this after checking for
+				 * "context" above.
+				 */
+				formatted = JSON.stringify(
+				    fltdetail.matches[0].match, null, '    ');
+				formatted = prependLines(formatted, '    ');
+
+				fprintf(out, '    first matching message:\n');
+				fprintf(out, '    ------------\n');
+				fprintf(out, '%s\n', formatted);
+				fprintf(out, '    ------------\n');
+				continue;
+			}
+		}
+
+		/*
+		 * The only other type of probe currently in use is the
+		 * disk-usage probe, and it has no additional information than
+		 * what's in the message that we already printed out.
+		 */
+	}
+};
+
+maAdm.prototype.alarmEventNames = function alarmEventNames()
+{
+	assertplus.notStrictEqual(this.ma_alarm_metadata, null,
+	    'must call alarmsInit() first');
+	var rv = [];
+	this.ma_alarm_metadata.eachEvent(function (eventName) {
+		rv.push(eventName);
+	});
+	return (rv);
+};
+
+maAdm.prototype.alarmKaPrint = function alarmKaPrint(args)
+{
+	var ka, eventName;
+
+	assertplus.notStrictEqual(this.ma_alarm_metadata, null,
+	    'must call alarmsInit() first');
+
+	assertplus.object(args, 'args');
+	assertplus.object(args.stream, 'args.stream');
+	assertplus.string(args.eventName, 'args.eventName');
+
+	eventName = args.eventName;
+	ka = this.ma_alarm_metadata.eventKa(eventName);
+	if (ka === null) {
+		return (new VError('no such event: "%s"', eventName));
+	}
+
+	this.doKaPrint({
+	    'stream': args.stream,
+	    'ka': ka
+	});
+};
+
+maAdm.prototype.doKaPrint = function doKaPrint(args)
+{
+	var out, ka, header;
+	var wrapper = wordwrap(4, 80);
+
+	assertplus.object(args, 'args');
+	assertplus.object(args.ka, 'args.ka');
+	assertplus.object(args.stream, 'args.stream');
+	assertplus.optionalString(args.header, 'args.header');
+
+	ka = args.ka;
+	out = args.stream;
+	header = args.header ? args.header : 'TITLE';
+	fprintf(out, '%s: %s\n', header, ka.ka_title);
+	fprintf(out, 'SEVERITY: %s\n', ka.ka_severity);
+	fprintf(out, 'DESC:\n%s\n', wrapper(ka.ka_description));
+	fprintf(out, 'IMPACT:\n%s\n', wrapper(ka.ka_impact));
+	fprintf(out, 'AUTOMATED RESPONSE:\n%s\n', wrapper(ka.ka_response));
+	fprintf(out, 'SUGGESTED ACTION:\n%s\n', wrapper(ka.ka_action));
+};
+
+/*
+ * General-purpose function for listing rows.
+ */
+maAdm.prototype.doList = function doList(args)
+{
+	var colnames, columns, taboptions, tabstream;
+
+	assertplus.object(args, 'args');
+	assertplus.object(args.stream, 'args.stream');
+	assertplus.optionalArrayOfString(
+	    args.columnsSelected, 'args.columnsSelected');
+	assertplus.arrayOfString(args.columnsDefault, 'args.columnsDefault');
+	assertplus.object(args.columnMetadata, 'args.columnMetadata');
+	assertplus.arrayOfObject(args.rows, 'args.rows');
+	assertplus.optionalBool(args.omitHeader, 'args.omitHeader');
+
+	colnames = args.columnsSelected || args.columnsDefault;
+	columns = colnames.map(function (colname) {
+		colname = colname.toLowerCase();
+		assertplus.ok(args.columnMetadata.hasOwnProperty(colname),
+		    'no property "' + colname + '"');
+		return (args.columnMetadata[colname]);
+	});
+	taboptions = {
+	    'stream': args.stream,
+	    'omitHeader': args.omitHeader,
+	    'columns': columns
+	};
+	tabstream = new tab.TableOutputStream(taboptions);
+	args.rows.forEach(function (row) {
+		tabstream.writeRow(row);
+	});
+};
+
+/*
+ * Print a summary about the currently configured probe groups and probes.
+ */
+maAdm.prototype.alarmConfigShow = function (args)
+{
+	assertplus.object(args, 'args');
+	assertplus.object(args.stream, 'args.stream');
+	assertplus.notStrictEqual(this.ma_amon_deployed, null,
+	    'must call alarmsInit() with "configBasic" source first');
+	alarms.amonConfigSummarize({
+	    'config': this.ma_amon_deployed,
+	    'metadata': this.ma_alarm_metadata,
+	    'instanceSvcname': this.ma_instance_svcname,
+	    'stream': args.stream
+	});
+};
+
+/*
+ * Create a plan for updating the Amon configuration.
+ */
+maAdm.prototype.amonUpdatePlanCreate = function amonUpdatePlanCreate(options)
+{
+	assertplus.notStrictEqual(this.ma_amon_deployed, null,
+	    'must call alarmsInit() with "configFull" source first');
+	assertplus.object(options, 'args');
+	assertplus.bool(options.unconfigure, 'options.unconfigure');
+
+	return (alarms.amonUpdatePlanCreate({
+	    'account': this.ma_app.owner_uuid,
+	    'contactsBySeverity': this.ma_alarm_levels,
+	    'metadata': this.ma_alarm_metadata,
+	    'instances': this.ma_instance_info,
+	    'instancesBySvc': this.ma_instances_local_bysvcname,
+	    'deployed': this.ma_amon_deployed,
+	    'unconfigure': options.unconfigure
+	}));
+};
+
+/*
+ * Summarize a plan for updating the Amon configuration.
+ */
+maAdm.prototype.amonUpdatePlanDump = function amonUpdatePlanDump(args)
+{
+	assertplus.object(args, 'args');
+	assertplus.object(args.plan, 'args.plan');
+	assertplus.object(args.stream, 'args.stream');
+	assertplus.bool(args.verbose, 'args.verbose');
+
+	alarms.amonUpdatePlanSummarize({
+	    'stream': args.stream,
+	    'plan': args.plan,
+	    'instances': this.ma_instance_info,
+	    'cns': this.ma_cns,
+	    'metadata': this.ma_alarm_metadata,
+	    'verbose': args.verbose
+	});
+};
+
+/*
+ * Execute a plan for updating the Amon configuration.
+ */
+maAdm.prototype.amonUpdatePlanApply =
+    function amonUpdatePlanApply(args, callback)
+{
+	assertplus.object(args, 'args');
+	assertplus.object(args.plan, 'args.plan');
+	assertplus.object(args.stream, 'args.stream');
+	assertplus.number(args.concurrency, 'args.concurrency');
+
+	alarms.amonUpdatePlanApply({
+	    'account': this.ma_app.owner_uuid,
+	    'amon': this.ma_sdc.AMON,
+	    'concurrency': args.concurrency,
+	    'stream': args.stream,
+	    'plan': args.plan
+	}, callback);
+};
+
 /*
  * The dumpConfig.* functions dump sample configurations based on common
  * deployments in development and test.
@@ -1254,7 +2319,7 @@ maAdm.prototype.dumpCns = function (sout, conf)
 		    'ADMIN IP': gz['admin_ip'],
 		    'COMPUTE ID': gz['compute_id'] || '-',
 		    'STORAGE IDS': kind == 'storage' ?
-		        gz['storage_ids'].sort().join(',') : '-',
+			gz['storage_ids'].sort().join(',') : '-',
 		    'KIND': kind
 		});
 	});
@@ -1301,7 +2366,7 @@ maAdm.prototype.dumpZkServers = function (sout, conf)
 		    'ZONENAME': instance ? instance.uuid : '-',
 		    'ZONEABBR': instance ? instance.uuid.substr(0, 8) : '-',
 		    'DATACENTER': instance ?
-		        instance.metadata['DATACENTER'] : '-'
+			instance.metadata['DATACENTER'] : '-'
 		});
 	});
 
@@ -1361,6 +2426,8 @@ maAdm.prototype.loadInstances = function ()
 	rv = [];
 	this.ma_config_bycn = {};
 	this.ma_config_bycfg = {};
+	this.ma_instance_info = {};
+	this.ma_instances_local_bysvcname = {};
 
 	for (svcid in this.ma_instances) {
 		svcname = services[svcid]['name'];
@@ -1369,6 +2436,7 @@ maAdm.prototype.loadInstances = function ()
 		this.ma_config_bycfg[svcid] =
 		    new svcs.ServiceConfiguration(svckey);
 		this.ma_config_bycn[svcid] = {};
+		this.ma_instances_local_bysvcname[svcname] = [];
 
 		for (i = 0; i < this.ma_instances[svcid].length; i++) {
 			instance = this.ma_instances[svcid][i];
@@ -1377,6 +2445,10 @@ maAdm.prototype.loadInstances = function ()
 				server = this.ma_vms[instance['uuid']]
 				    ['server_uuid'];
 				gz = this.ma_gzinfo[server];
+				assertplus.ok(!this.ma_instance_svcname.
+				    hasOwnProperty(instance['uuid']));
+				this.ma_instance_svcname[
+				    instance['uuid']] = svcname;
 			} else {
 				server = '-';
 				gz = null;
@@ -1402,7 +2474,7 @@ maAdm.prototype.loadInstances = function ()
 			row = {
 			    'SERVICE': svcname,
 			    'SH': instance['metadata']['SHARD'] ?
-			        instance['metadata']['SHARD'].toString() : '-',
+				instance['metadata']['SHARD'].toString() : '-',
 			    'DATACENTER': metadata['DATACENTER'] || '-',
 			    'ZONENAME': instance['uuid'],
 			    'GZ HOST': gz ? gz['hostname'] : '-',
@@ -1416,6 +2488,21 @@ maAdm.prototype.loadInstances = function ()
 			};
 			rv.push(row);
 
+			this.ma_instance_info[instance['uuid']] =
+			    new InstanceInfo({
+				'uuid': instance['uuid'],
+				'svcname': svcname,
+				'metadata': instance['metadata'],
+				'local': gz !== null,
+				'server_uuid': gz !== null ?
+				    server : null
+			    });
+
+			if (gz !== null) {
+				this.ma_instances_local_bysvcname[
+				    svcname].push(instance['uuid']);
+			}
+
 			if (image === '-')
 				continue;
 
@@ -1467,6 +2554,9 @@ maAdm.prototype.loadCns = function ()
 
 			gzinfo[cnid]['admin_ip'] = iface['ip4addr'];
 		}
+
+		assertplus.ok(!this.ma_instance_svcname.hasOwnProperty(cnid));
+		this.ma_instance_svcname[cnid] = 'global zone';
 	}
 
 	if (this.ma_app['metadata']) {
@@ -1940,7 +3030,7 @@ maAdm.prototype.dumpPlan = function ()
 				    'zonename': p['zonename'],
 				    'image': p['IMAGE'] || p['new_image'],
 				    'shard': p['SH'] || p['shard']
-			        });
+				});
 			    }));
 	});
 
@@ -2185,15 +3275,15 @@ maAdm.prototype.close = function ()
  * Each element in the ZK_SERVERS array has these properties:
  *
  *     "num"	Ordinal number of this ZooKeeper instance.  These are
- *      	allocated starting from 1 when an instance is deployed.
- *      	This value is used by _all_ ZooKeeper instances when
- *      	writing out their ZK configuration files.  It must be
- *      	unique within the cluster.  It's unclear from the ZooKeeper
- *      	documentation if it must also be consecutive and start at 1.
+ *		allocated starting from 1 when an instance is deployed.
+ *		This value is used by _all_ ZooKeeper instances when
+ *		writing out their ZK configuration files.  It must be
+ *		unique within the cluster.  It's unclear from the ZooKeeper
+ *		documentation if it must also be consecutive and start at 1.
  *
- *      	This value MUST match the ZK_ID metadata property on the
- *      	corresponding SAPI instance, which is also used in ZK
- *      	configuration.
+ *		This value MUST match the ZK_ID metadata property on the
+ *		corresponding SAPI instance, which is also used in ZK
+ *		configuration.
  *
  *     "host"	IP address of this ZooKeeper instance.
  *
@@ -2203,8 +3293,8 @@ maAdm.prototype.close = function ()
  * other elements must not have this property):
  *
  *     "last"	Indicates that this is the last entry in the list.  This is used
- *     		in templates for JSON configuration files to avoid including a
- *     		trailing comma.
+ *		in templates for JSON configuration files to avoid including a
+ *		trailing comma.
  *
  * Because ZK_SERVERS is used so directly to write out ZooKeeper configuration
  * files, this property value inherits constraints associated with ZooKeeper
@@ -2300,14 +3390,14 @@ maAdm.prototype.fixupZkServers = function (callback)
  * The return value is an object with:
  *
  *     validationErrors		List of Error objects describing serious
- *     				validation errors like those described above.
- *     				There is no support for fixing these
- *     				automatically.
+ *				validation errors like those described above.
+ *				There is no support for fixing these
+ *				automatically.
  *
  *     configuredInstances	List of objects describing the entries in
- *     				ZK_SERVERS, each having:
+ *				ZK_SERVERS, each having:
  *
- *     		instance		SAPI instance metadata
+ *		instance		SAPI instance metadata
  *
  *		zkid			ZK_ID metadata
  *
@@ -2316,12 +3406,12 @@ maAdm.prototype.fixupZkServers = function (callback)
  *		port			PORT (from ZK_SERVERS)
  *
  *     missingInstances		List of indexes into ZK_SERVERS identifying
- *     				elements with no matching SAPI instance.
+ *				elements with no matching SAPI instance.
  *
  *     nforeign			Number of ZK_SERVERS instances for which we have
- *     				no metadata about the corresponding compute
- *     				node.  This usually means that the instance is
- *     				deployed inside another datacenter.
+ *				no metadata about the corresponding compute
+ *				node.  This usually means that the instance is
+ *				deployed inside another datacenter.
  */
 maAdm.prototype.auditZkServers = function ()
 {
@@ -2334,7 +3424,7 @@ maAdm.prototype.auditZkServers = function ()
 	if (!this.ma_app.metadata.hasOwnProperty(maZkConfigProp)) {
 		return ({
 		    'validationErrors': [
-		        new VError('%s not found in metadata', maZkConfigProp)
+			new VError('%s not found in metadata', maZkConfigProp)
 		    ],
 		    'configuredInstances': [],
 		    'missingInstances': [],
@@ -2487,3 +3577,146 @@ maAdm.prototype.auditZkServers = function ()
 
 	return (rv);
 };
+
+function fmtListDateTime(ts)
+{
+	if (ts === null) {
+		return ('-');
+	}
+
+	return (new Date(ts).toISOString());
+}
+
+function fmtDateOnly(ts)
+{
+	if (ts === null) {
+		return ('-');
+	}
+
+	return (new Date(ts).toISOString().substr(0, '2017-02-06'.length));
+}
+
+/*
+ * TODO This implementation is copied from manta-marlin.  It should be moved to
+ * node-jsprim.
+ */
+function fmtDuration(ms)
+{
+	var hour, min, sec, rv;
+
+	/* compute totals in each unit */
+	assertplus.number(ms, 'ms');
+	sec = Math.floor(ms / 1000);
+	min = Math.floor(sec / 60);
+	hour = Math.floor(min / 60);
+
+	/* compute offsets for each unit */
+	ms %= 1000;
+	sec %= 60;
+	min %= 60;
+
+	rv = '';
+	if (hour > 0)
+		rv += hour + 'h';
+
+	if (hour > 0 || min > 0) {
+		if (hour > 0 && min < 10)
+			rv += '0' + min + 'm';
+		else
+			rv += min + 'm';
+	}
+
+	if ((hour > 0 || min > 0) && sec < 10)
+		rv += '0' + sec;
+	else
+		rv += sec;
+
+	rv += 's';
+	return (rv);
+}
+
+function formatCmdOutput(prefix, streamname, str)
+{
+	var trimmed;
+
+	/*
+	 * Don't bother printing empty outputs.
+	 */
+	trimmed = str.trim();
+	if (trimmed.length === 0) {
+		return ('');
+	}
+
+	if (trimmed.indexOf('\n') == -1) {
+		return (extsprintf.sprintf('%s%6s:           %s\n',
+		    prefix, streamname, JSON.stringify(str)));
+	}
+
+	return (extsprintf.sprintf('%s%6s:\n%s\n',
+	    prefix, streamname, prependLines(str, '        | ')));
+}
+
+function prependLines(str, prefix)
+{
+	var lines, i;
+
+	/*
+	 * A simple "map" would be concise and elegant, but wouldn't handle the
+	 * trailing newline case very well.
+	 */
+	lines = str.split('\n');
+	for (i = 0; i < lines.length - 1; i++) {
+		lines[i] = prefix + lines[i];
+	}
+
+	if (lines.length > 0 && lines[lines.length - 1].length > 0) {
+		lines[lines.length - 1] = prefix + lines[lines.length - 1];
+	}
+
+	return (lines.join('\n'));
+}
+
+/*
+ * InstanceInfo represents information about a specific instance.  This should
+ * eventually include SAPI and VMAPI information.  This is the abstract object
+ * we should pass around between different subsystems, rather than raw VMAPI and
+ * SAPI objects.  For now, this is only used for the alarms subsystem.
+ */
+function InstanceInfo(args)
+{
+	assertplus.object(args, 'args');
+	assertplus.string(args.uuid, 'args.uuid');
+	assertplus.string(args.svcname, 'args.svcname');
+	assertplus.object(args.metadata, 'args.metadata');
+	assertplus.bool(args.local, 'args.local');
+	assertplus.optionalString(args.server_uuid, 'args.server_uuid');
+
+	if (args.local) {
+		assertplus.string(args.server_uuid, 'args.server_uuid');
+	} else {
+		/*
+		 * There's no way for us to authoritatively know the server_uuid
+		 * for remote instances.  There's a server uuid in the SAPI
+		 * information, but it may be out of date if the VM has been
+		 * migrated or the server's chassis has been swapped or the
+		 * like.  The only authoritative place for this information is
+		 * in the VMAPI data, which we don't have for remote instances.
+		 */
+		assertplus.strictEqual(args.server_uuid, null);
+	}
+
+	/* VM uuid */
+	this.inst_uuid = args.uuid;
+
+	/* SAPI service name to which this instance belongs */
+	this.inst_svcname = args.svcname;
+
+	/* SAPI metadata for this instance */
+	this.inst_metadata = args.metadata;
+
+	/* Boolean indicating whether this instance is in this datacenter. */
+	this.inst_local = args.local;
+
+	/* Server uuid where this VM lives, or "null" for non-local instances */
+	this.inst_server_uuid = args.server_uuid;
+}
diff --git a/lib/alarms/alarms.js b/lib/alarms/alarms.js
new file mode 100644
index 0000000..06b82af
--- /dev/null
+++ b/lib/alarms/alarms.js
@@ -0,0 +1,341 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright 2017, Joyent, Inc.
+ */
+
+/*
+ * lib/alarms/alarms.js: facilities for working with amon alarms.
+ */
+
+var assertplus = require('assert-plus');
+var jsprim = require('jsprim');
+var vasync = require('vasync');
+
+var sprintf = require('extsprintf').sprintf;
+var VError = require('verror');
+
+var amon_objects = require('./amon_objects');
+var common = require('../common');
+
+/* Exported interface */
+exports.amonLoadAlarmsForState = amonLoadAlarmsForState;
+exports.amonLoadAlarmsForIds = amonLoadAlarmsForIds;
+exports.amonCloseAlarms = amonCloseAlarms;
+exports.amonUpdateAlarmsNotification = amonUpdateAlarmsNotification;
+
+
+/*
+ * Load information about Amon alarms in the specified state.
+ *
+ * Named arguments:
+ *
+ *     account         Triton account uuid whose open alarms to load
+ *
+ *     amon            Amon client (from sdc-clients)
+ *
+ *     state           one of "open", "closed", "all", or "recent"
+ */
+function amonLoadAlarmsForState(args, callback)
+{
+	var amon, account, rv, options;
+
+	assertplus.object(args, 'args');
+	assertplus.string(args.account, 'args.account');
+	assertplus.object(args.amon, 'args.amon');
+	assertplus.string(args.state, 'args.state');
+
+	amon = args.amon;
+	account = args.account;
+	rv = new MantaAlarmSet();
+	options = { 'state': args.state };
+
+	amon.listAlarms(account, options, function (err, rawalarms) {
+		var errors;
+
+		if (err) {
+			err = new VError(err, 'listing open alarms');
+			callback(err);
+			return;
+		}
+
+		errors = [];
+		rawalarms.forEach(function (rawalarm) {
+			var alarm;
+
+			alarm = amon_objects.loadAlarmObject(rawalarm);
+			if (alarm instanceof Error) {
+				errors.push(new VError(alarm,
+				    'bad alarm from server'));
+				return;
+			}
+
+			if (rv.mas_alarms_byid.hasOwnProperty(alarm.a_id)) {
+				errors.push(new VError('server reported ' +
+				    'more than one alarm with id %d',
+				    alarm.a_id));
+				return;
+			}
+
+			rv.mas_alarms.push(alarm);
+			rv.mas_alarms_byid[alarm.a_id] = alarm;
+		});
+
+		err = common.errorForList(errors);
+		callback(err, rv);
+	});
+}
+
+/*
+ * Iterate the specified "alarmIds" and invoke "func" for each one.
+ *
+ * External to this file, we avoid assuming that alarm ids are positive
+ * integers.  That's an amon-ism.  But the Amon client library does assume that,
+ * so here's where we have to validate it.  This function also manages a queue
+ * of the requested concurrency.
+ */
+function amonAlarmForEach(args, callback)
+{
+	var errors, queue, func;
+
+	assertplus.object(args, 'args');
+	assertplus.arrayOfString(args.alarmIds, 'args.alarmIds');
+	assertplus.number(args.concurrency, 'args.concurrency');
+	assertplus.func(args.func, 'args.func');
+
+	errors = [];
+	func = args.func;
+	queue = vasync.queuev({
+	    'concurrency': args.concurrency,
+	    'worker': function iterAlarm(alarmid, qcallback) {
+		var num;
+
+		num = jsprim.parseInteger(alarmid);
+		if (typeof (num) == 'number' && num < 1) {
+			num = VError('not a positive integer');
+		}
+
+		if (num instanceof Error) {
+			errors.push(new VError(num, 'alarm "%s"', alarmid));
+			qcallback();
+			return;
+		}
+
+		func(num, function onFuncDone(err) {
+			if (err) {
+				errors.push(err);
+			}
+
+			qcallback();
+		});
+	    }
+	});
+
+	args.alarmIds.forEach(function (a) { queue.push(a); });
+	queue.on('end', function () {
+		callback(common.errorForList(errors));
+	});
+
+	queue.close();
+}
+
+/*
+ * Closes the specified open alarms.
+ *
+ * Named arguments:
+ *
+ *     account         Triton account uuid whose open alarms to load
+ *
+ *     amon            Amon client (from sdc-clients)
+ *
+ *     alarmIds	       array of alarm ids to close
+ *
+ *     concurrency     maximum request concurrency
+ *
+ * This is an array-based interface in order to better support parallelizing
+ * operations.  This could also expose an object-mode stream interface.
+ */
+function amonCloseAlarms(args, callback)
+{
+	var account, amon;
+
+	assertplus.object(args, 'args');
+	assertplus.string(args.account, 'args.account');
+	assertplus.object(args.amon, 'args.amon');
+	assertplus.arrayOfString(args.alarmIds, 'args.alarmIds');
+	assertplus.number(args.concurrency, 'args.concurrency');
+
+	account = args.account;
+	amon = args.amon;
+	amonAlarmForEach({
+	    'alarmIds': args.alarmIds,
+	    'concurrency': args.concurrency,
+	    'func': function amonCloseOne(alarmid, subcallback) {
+		assertplus.number(alarmid);
+		amon.closeAlarm(account, alarmid, function onAmonClose(err) {
+			if (err) {
+				err = new VError(err,
+				    'close alarm "%d"', alarmid);
+			}
+
+			subcallback(err);
+		});
+	    }
+	}, callback);
+}
+
+
+/*
+ * Updates the "suppressed" property on the specified alarms.
+ *
+ * Named arguments:
+ *
+ *     account         Triton account uuid whose open alarms to load
+ *
+ *     amonRaw         a restify JSON client for the AMON master API.
+ *                     This is different from most other consumers, which use an
+ *                     actual Amon client.
+ *
+ *     alarmIds        array of alarm ids to close
+ *
+ *     concurrency     maximum request concurrency
+ *
+ *     suppressed      new value for the "suppressed" property
+ *
+ * This is an array-based interface in order to better support parallelizing
+ * operations.  This could also expose an object-mode stream interface.
+ */
+function amonUpdateAlarmsNotification(args, callback)
+{
+	var account, amon, suppressed;
+
+	assertplus.object(args, 'args');
+	assertplus.string(args.account, 'args.account');
+	assertplus.object(args.amonRaw, 'args.amonRaw');
+	assertplus.arrayOfString(args.alarmIds, 'args.alarmIds');
+	assertplus.number(args.concurrency, 'args.concurrency');
+	assertplus.bool(args.suppressed, 'args.suppressed');
+
+	account = args.account;
+	amon = args.amonRaw;
+	suppressed = args.suppressed;
+
+	amonAlarmForEach({
+	    'alarmIds': args.alarmIds,
+	    'concurrency': args.concurrency,
+	    'func': function amonUpdateOne(alarmid, subcallback) {
+		/*
+		 * Unfortunately, sdc-client's Amon client does not support this
+		 * operation, so we need to hit the API directly.
+		 *
+		 * The server also doesn't recognize POST parameters specified
+		 * in the body, so we have to put them into the query string.
+		 */
+		var action, resource;
+		action = suppressed ? 'suppress' : 'unsuppress';
+		resource = sprintf('/pub/%s/alarms/%d?action=%s',
+		    encodeURIComponent(account), alarmid, action);
+		amon.post(resource, function (err) {
+			subcallback(err);
+		});
+	    }
+	}, callback);
+}
+
+/*
+ * Fetches details about the specified alarms.  Named arguments:
+ *
+ *     account         Triton account uuid whose open alarms to load
+ *
+ *     amon            Amon client (from sdc-clients)
+ *
+ *     alarmIds	       array of alarm ids to close
+ *
+ *     concurrency     maximum request concurrency
+ */
+function amonLoadAlarmsForIds(args, callback)
+{
+	var account, amon, fetching, rv;
+
+	assertplus.object(args, 'args');
+	assertplus.string(args.account, 'args.account');
+	assertplus.object(args.amon, 'args.amon');
+	assertplus.arrayOfString(args.alarmIds, 'args.alarmIds');
+	assertplus.number(args.concurrency, 'args.concurrency');
+
+	account = args.account;
+	amon = args.amon;
+	fetching = {};
+	rv = new MantaAlarmSet();
+	amonAlarmForEach({
+	    'alarmIds': args.alarmIds,
+	    'concurrency': args.concurrency,
+	    'func': function amonLoadOne(alarmid, subcallback) {
+		/*
+		 * Ignore duplicates.
+		 */
+		if (fetching[alarmid]) {
+			setImmediate(subcallback);
+			return;
+		}
+
+		fetching[alarmid] = true;
+		amon.getAlarm(account, alarmid, function (err, rawalarm) {
+			var alarm;
+
+			if (err) {
+				subcallback(
+				    new VError('fetch alarm "%d"', alarmid));
+				return;
+			}
+
+			alarm = amon_objects.loadAlarmObject(rawalarm);
+			if (alarm instanceof Error) {
+				subcallback(new VError(
+				    'bad alarm from server'));
+				return;
+			}
+
+			/*
+			 * We checked for duplicates before we made the request.
+			 */
+			assertplus.ok(!rv.mas_alarms_byid.hasOwnProperty(
+			    alarm.a_id));
+			rv.mas_alarms.push(alarm);
+			rv.mas_alarms_byid[alarm.a_id] = alarm;
+			subcallback();
+		});
+	    }
+	}, function (err) {
+		callback(err, rv);
+	});
+}
+
+/*
+ * Represents a set of open amon alarms.
+ */
+function MantaAlarmSet()
+{
+	/* list of open alarms */
+	this.mas_alarms = [];
+
+	/* alarms indexed by id */
+	this.mas_alarms_byid = {};
+}
+
+MantaAlarmSet.prototype.eachAlarm = function (func)
+{
+	this.mas_alarms.forEach(function (aa) {
+		func(aa.a_id, aa);
+	});
+};
+
+MantaAlarmSet.prototype.alarmForId = function (id)
+{
+	return (this.mas_alarms_byid.hasOwnProperty(id) ?
+	    this.mas_alarms_byid[id] : null);
+};
diff --git a/lib/alarms/amon_objects.js b/lib/alarms/amon_objects.js
new file mode 100644
index 0000000..dfa088e
--- /dev/null
+++ b/lib/alarms/amon_objects.js
@@ -0,0 +1,401 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright 2017, Joyent, Inc.
+ */
+
+/*
+ * lib/alarms/amon_objects.js: low-level Amon objects and their schemas
+ * The classes in this file are used as simple structs, mostly with details
+ * private to this subsystem.  Each class's fields mirror those in the Amon API.
+ */
+
+var assertplus = require('assert-plus');
+var jsprim = require('jsprim');
+
+var sprintf = require('extsprintf').sprintf;
+var VError = require('verror');
+
+var common = require('../common');
+
+exports.loadAlarmObject = loadAlarmObject;
+exports.loadProbeObject = loadProbeObject;
+exports.loadProbeGroupObject = loadProbeGroupObject;
+
+/*
+ * This class is used as a struct, with details exposed to the next-level
+ * subsystem (lib/adm.js).  The fields here mirror those in the Amon API for
+ * Alarms.
+ */
+function AmonAlarm(alarmdef)
+{
+	var self = this;
+
+	assertplus.object(alarmdef, 'alarmdef');
+	assertplus.number(alarmdef.id, 'alarmdef.id');
+	assertplus.string(alarmdef.user, 'alarmdef.user');
+	assertplus.optionalString(alarmdef.probeGroup, 'alarmdef.probeGroup');
+	assertplus.bool(alarmdef.closed, 'alarmdef.closed');
+	assertplus.bool(alarmdef.suppressed, 'alarmdef.suppressed');
+	assertplus.number(alarmdef.timeOpened, 'alarmdef.timeOpened');
+	assertplus.optionalNumber(alarmdef.timeClosed, 'alarmdef.timeClosed');
+	assertplus.number(alarmdef.timeLastEvent, 'alarmdef.timeLastEvent');
+	assertplus.number(alarmdef.numEvents, 'alarmdef.numEvents');
+	assertplus.arrayOfObject(alarmdef.faults, 'alarmdef.faults');
+
+	this.a_id = alarmdef.id;
+	this.a_user = alarmdef.user;
+	this.a_groupid = alarmdef.probeGroup || null;
+	this.a_closed = alarmdef.closed;
+	this.a_suppressed = alarmdef.suppressed;
+	this.a_time_opened = new Date(alarmdef.timeOpened);
+	this.a_time_closed = alarmdef.timeClosed ?
+	    new Date(alarmdef.timeClosed) : null;
+	this.a_time_last = new Date(alarmdef.timeLastEvent);
+	this.a_nevents = alarmdef.numEvents;
+	this.a_faults = alarmdef.faults.map(function (f) {
+		return (new AmonFault(self, f));
+	});
+}
+
+/*
+ * This class is used as a struct, with details exposed to the next-level
+ * subsystem (lib/adm.js).  The fields here mirror those in the Amon API for
+ * Alarms.
+ */
+function AmonFault(alarm, faultdef)
+{
+	assertplus.object(alarm, 'alarm');
+	assertplus.ok(alarm instanceof AmonAlarm);
+	assertplus.object(faultdef, 'faultdef');
+	assertplus.string(faultdef.type, 'faultdef.type');
+	assertplus.equal(faultdef.type, 'probe');
+	assertplus.string(faultdef.probe, 'faultdef.probe');
+	assertplus.object(faultdef.event, 'faultdef.event');
+	assertplus.equal(faultdef.event.v, '1');
+	assertplus.string(faultdef.event.type, 'faultdef.event.type');
+	assertplus.equal(faultdef.event.type, 'probe');
+	assertplus.bool(faultdef.event.clear, 'faultdef.event.clear');
+	assertplus.string(faultdef.event.machine, 'faultdef.event.machine');
+	assertplus.string(faultdef.event.uuid, 'faultdef.event.uuid');
+	assertplus.string(faultdef.event.agent, 'faultdef.event.agent');
+	assertplus.string(faultdef.event.agentAlias,
+	    'faultdef.event.agentAlias');
+	assertplus.number(faultdef.event.time, 'faultdef.event.time');
+	assertplus.object(faultdef.event.data, 'faultdef.event.data');
+	assertplus.string(faultdef.event.data.message,
+	    'faultdef.event.data.message');
+
+	this.aflt_alarm = alarm;
+	this.aflt_probeid = faultdef.probe;
+	this.aflt_clear = faultdef.event.clear;
+	this.aflt_uuid = faultdef.event.uuid;
+	this.aflt_machine = faultdef.event.machine;
+	this.aflt_agent = faultdef.event.agent;
+	this.aflt_agent_alias = faultdef.event.agentAlias;
+	this.aflt_time = new Date(faultdef.event.time);
+	this.aflt_summary = faultdef.event.data.message;
+	this.aflt_data = faultdef.event.data;
+}
+
+/*
+ * This class is used as a struct, with details private to this subsystem.
+ * The fields here mirror those in the Amon API for Probes.  Because this can
+ * represent probes that have not yet been created, "uuid" is not required.
+ * Most optional fields are "null" when not present, though "contacts" may
+ * actually be not present.
+ */
+function AmonProbe(probedef)
+{
+	assertplus.object(probedef, 'probedef');
+	assertplus.optionalString(probedef.uuid, 'probedef.uuid');
+	assertplus.optionalString(probedef.name, 'probedef.name');
+	assertplus.string(probedef.type, 'probedef.type');
+	assertplus.object(probedef.config, 'probedef.config');
+	assertplus.string(probedef.agent, 'probedef.agent');
+	assertplus.optionalString(probedef.machine, 'probedef.machine');
+	assertplus.optionalString(probedef.group, 'probedef.group');
+	assertplus.optionalArrayOfString(probedef.contacts,
+	    'probedef.contacts');
+	assertplus.optionalBool(probedef.groupEvents, 'probedef.groupEvents');
+
+	this.p_uuid = probedef.hasOwnProperty('uuid') ? probedef.uuid : null;
+	this.p_name = probedef.hasOwnProperty('name') ? probedef.name : null;
+	this.p_type = probedef.type;
+	this.p_config = jsprim.deepCopy(probedef.config);
+	this.p_agent = probedef.agent;
+	this.p_machine = probedef.machine || null;
+	this.p_groupid = probedef.group || null;
+	this.p_contacts = probedef.contacts || null;
+	this.p_group_events = probedef.groupEvents || false;
+}
+
+/*
+ * This class is used as a struct, with details private to this subsystem.
+ * The fields here mirror those in the Amon API for Probe Groups.  Because this
+ * can represent probes that have not yet been created, the uuid is not
+ * required.  Optional fields are "null" when not present.
+ */
+function AmonProbeGroup(groupdef)
+{
+	assertplus.object(groupdef, 'groupdef');
+	assertplus.string(groupdef.user, 'groupdef.user');
+	assertplus.string(groupdef.uuid, 'groupdef.uuid');
+	assertplus.string(groupdef.name, 'groupdef.name');
+	assertplus.bool(groupdef.disabled, 'groupdef.disabled');
+	assertplus.arrayOfString(groupdef.contacts, 'groupdef.contacts');
+
+	this.pg_name = groupdef.name;
+	this.pg_user = groupdef.user;
+	this.pg_uuid = groupdef.uuid;
+	this.pg_contacts = groupdef.contacts.slice(0);
+	this.pg_enabled = groupdef.disabled ? false : true;
+}
+
+
+/*
+ * Schema helper functions
+ *
+ * A note on optional properties: in some cases, when a value is unspecified
+ * (e.g., a probe with no probe group), the property is just missing.  The
+ * schema handles this by making the corresponding property not required.  In
+ * other cases (e.g., the timeClosed for an alarm that has not yet been closed),
+ * Amon includes the property with value "null".  The schema handles this by
+ * explicitly allowing "null" as one of the allowed types.
+ *
+ * By convention, schema types that we define in order to be able to reuse them
+ * are always named with a suffix of either "Required" (if the schema represents
+ * a required property) or "Optional" (if not).  We use the Optional version of
+ * a type when Amon might leave the property out entirely.  On the other hand,
+ * we use the schemaAllowNull() function to take any Required schema type and
+ * return one that can be null.
+ */
+
+/*
+ * Given a JSON schema, return a schema that is exactly equivalent, but also
+ * allows "null" values.
+ */
+function schemaAllowNull(schemaType) {
+	var rv;
+
+	assertplus.object(schemaType, 'schemaType');
+	assertplus.string(schemaType.type, 'schemaType.type');
+	assertplus.strictEqual(schemaType.required, true,
+	    'can only allow "required" properties to be null');
+	rv = jsprim.deepCopy(schemaType);
+	rv.type = [ 'null', rv.type ];
+	return (rv);
+}
+
+/*
+ * Schemas
+ */
+
+var schemaTypeNonNegativeIntegerRequired = {
+    'type': 'integer',
+    'required': true,
+    'minimum': 0
+};
+
+var schemaTypeUuidOptional = {
+    'type': 'string',
+    'minLength': 36,
+    'maxLength': 36
+};
+
+var schemaTypeUuidRequired = {
+    'type': 'string',
+    'required': true,
+    'minLength': 36,
+    'maxLength': 36
+};
+
+var schemaTypeTimestampAsNumberRequired = {
+    'type': 'number',
+    'required': true,
+    'minimum': 0
+};
+
+/*
+ * It's important that these Amon schemas be strict enough that we don't pass
+ * through objects that will cause us to crash when we go to dereference fields
+ * that are missing or have the wrong type.  They can only be stricter than Amon
+ * itself allows as long as consumers can handle ignoring objects that don't
+ * conform to our stricter schema.  This happens when we encounter probe groups
+ * with no name, for example.
+ *
+ * It's important that we not reject valid objects from Amon in a way that would
+ * cause entire operations to fail.  For example, we currently bail out any
+ * "verify"/"update" operation if we encounter any invalid probes.  We don't
+ * want that to happen in a real deployment, since it renders the tool useless.
+ * But we also want to err on the side of caution and not plow ahead with probes
+ * we don't know what to do with.
+ *
+ * The point of all this is that we need to be very careful about situations
+ * where this schema is stricter than Amon's.  The known cases are documented
+ * below.
+ */
+
+var schemaTypeAmonFault = {
+    'type': 'object',
+    'properties': {
+	'type': { 'type': 'string', 'required': true },
+	'probe': schemaTypeUuidRequired,
+	'event': {
+	    'type': 'object',
+	    'required': true,
+	    'properties': {
+		'clear': { 'type': 'boolean', 'required': true },
+		'machine': schemaTypeUuidRequired,
+		'uuid': schemaTypeUuidRequired,
+		'agent': schemaTypeUuidRequired,
+		'agentAlias': { 'type': 'string', 'required': true },
+		'time': schemaTypeTimestampAsNumberRequired,
+		'data': {
+		    'type': 'object',
+		    'required': true,
+		    'properties': {
+			'message': {
+			    'type': 'string',
+			    'required': true
+			}
+		    }
+		}
+	    }
+	}
+    }
+};
+
+var schemaTypeAmonAlarm = {
+    'type': 'object',
+    'properties': {
+	'id': schemaTypeNonNegativeIntegerRequired,
+	'user': schemaTypeUuidRequired,
+	'probeGroup': schemaTypeUuidOptional,
+	'closed': { 'type': 'boolean', 'required': true },
+	'suppressed': { 'type': 'boolean', 'required': true },
+	'timeOpened': schemaTypeTimestampAsNumberRequired,
+	'timeClosed': schemaAllowNull(schemaTypeTimestampAsNumberRequired),
+	'timeLastEvent': schemaTypeTimestampAsNumberRequired,
+	'numEvents': schemaTypeNonNegativeIntegerRequired,
+	'faults': {
+	    'type': 'array',
+	    'items': schemaTypeAmonFault
+	}
+    }
+};
+
+var schemaTypeAmonContacts = {
+    'type': 'array',
+    'items': {
+	'type': 'string'
+    }
+};
+
+var schemaTypeAmonProbe = {
+    'type': 'object',
+    'properties': {
+	/*
+	 * See the comment on the AmonProbe class definition.  We allow "uuid"
+	 * to be omitted for probes that have not yet been created.
+	 */
+	'uuid': { 'type': 'string' },
+	'name': { 'type': 'string' },
+	'type': { 'type': 'string', 'required': true },
+	'config': { 'type': 'object', 'required': true },
+	'agent': schemaTypeUuidRequired,
+	'groupEvents': schemaAllowNull({ 'type': 'boolean', 'required': true }),
+	'machine': schemaAllowNull(schemaTypeUuidRequired),
+
+	/*
+	 * "group" is not explicitly a uuid in the case of uncreated probes.
+	 */
+	'group': schemaAllowNull({ 'type': 'string', 'required': true }),
+	'contacts': schemaTypeAmonContacts
+    }
+};
+
+var schemaTypeAmonProbeGroup = {
+    'type': 'object',
+    'properties': {
+	/*
+	 * As with probes, the structure is a little looser to accommodate
+	 * uncreated probe groups.  We allow strings instead of requiring them
+	 * to be uuids.  However, we do require both of these.  It's technically
+	 * allowed for users to create probe groups with no names.  These will
+	 * fail validation, and we will ignore them.  (That's generally fine,
+	 * because we also handle alarms for probe groups that we don't know
+	 * about.)
+	 */
+	'uuid': { 'type': 'string', 'required': true },
+	'name': { 'type': 'string', 'required': true },
+	'user': schemaTypeUuidRequired,
+	'contacts': schemaTypeAmonContacts,
+	'disabled': { 'type': 'boolean', 'required': true }
+    }
+};
+
+function loadAlarmObject(alarmdef)
+{
+	var error;
+
+	error = jsprim.validateJsonObject(schemaTypeAmonAlarm, alarmdef);
+	if (error === null &&
+	    !alarmdef.closed && alarmdef.faults.length === 0) {
+		error = new VError('alarm open with no faults');
+	}
+
+	if (error === null &&
+	    (alarmdef.closed && alarmdef.timeClosed === null) ||
+	    (!alarmdef.closed && alarmdef.timeClosed !== null)) {
+		error = new VError('alarm\'s "closed" is not consistent ' +
+		    'with "timeClosed"');
+	}
+
+	if (error !== null) {
+		if (typeof (alarmdef.id) == 'number') {
+			error = new VError(error, 'alarm %d', alarmdef.id);
+		}
+
+		return (error);
+	}
+
+	return (new AmonAlarm(alarmdef));
+}
+
+function loadProbeObject(probedef)
+{
+	var error;
+
+	error = jsprim.validateJsonObject(schemaTypeAmonProbe, probedef);
+	if (error !== null) {
+		if (typeof (probedef.uuid) == 'string') {
+			error = new VError(error, 'probe "%s"', probedef.uuid);
+		}
+
+		return (error);
+	}
+
+	return (new AmonProbe(probedef));
+}
+
+function loadProbeGroupObject(groupdef)
+{
+	var error;
+
+	error = jsprim.validateJsonObject(schemaTypeAmonProbeGroup, groupdef);
+	if (error !== null) {
+		if (typeof (groupdef.uuid) == 'string') {
+			error = new VError(error, 'probe group "%s"',
+			    groupdef.uuid);
+		}
+
+		return (error);
+	}
+
+	return (new AmonProbeGroup(groupdef));
+}
diff --git a/lib/alarms/config.js b/lib/alarms/config.js
new file mode 100644
index 0000000..807b7bb
--- /dev/null
+++ b/lib/alarms/config.js
@@ -0,0 +1,545 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+/*
+ * lib/alarms/config.js: facilities for representing a set of amon
+ * configuration, which essentially means a set of probes and probe groups.
+ */
+
+var assertplus = require('assert-plus');
+var jsprim = require('jsprim');
+var progbar = require('progbar');
+var vasync = require('vasync');
+var VError = require('verror');
+var fprintf = require('extsprintf').fprintf;
+
+var amon_objects = require('./amon_objects');
+var common = require('../common');
+var services = require('../services');
+
+/* Exported interface */
+exports.amonLoadProbeGroups = amonLoadProbeGroups;
+exports.amonLoadComponentProbes = amonLoadComponentProbes;
+exports.amonConfigSummarize = amonConfigSummarize;
+exports.MantaAmonConfig = MantaAmonConfig;
+
+/*
+ * Fetches Amon probe groups.
+ *
+ *     amon             an Amon client
+ *
+ *     account		Triton account uuid whose probes to fetch
+ *
+ * callback is invoked as "callback(err, amonconfig)", where on success
+ * "amonconfig" is an instance of MantaAmonConfig.  Note that "err" and
+ * "amonconfig" can both be null, in which case "err" represents non-fatal
+ * (warning-level) issues encountered.
+ */
+function amonLoadProbeGroups(args, callback)
+{
+	var account, amon;
+
+	assertplus.object(args, 'args');
+	assertplus.object(args.amon, 'args.amon');
+	assertplus.func(callback, 'callback');
+
+	account = args.account;
+	amon = args.amon;
+	amon.listProbeGroups(account, function (err, rawgroups) {
+		var amoncfg, errors;
+
+		if (err) {
+			err = new VError(err, 'listing probegroups');
+			callback(err);
+			return;
+		}
+
+		amoncfg = new MantaAmonConfig();
+		errors = [];
+		rawgroups.forEach(function (rawgroup) {
+			var error = amoncfg.addProbeGroup(rawgroup);
+			if (error instanceof Error) {
+				errors.push(
+				    new VError(error, 'ignoring group'));
+			}
+		});
+
+		err = common.errorForList(errors);
+		callback(err, amoncfg);
+	});
+}
+
+/*
+ * Fetches Amon probe objects for all probes for the specified components.
+ * Named arguments:
+ *
+ *     amonRaw          a restify JSON client for the AMON master API.
+ *     			This is different from most other consumers, which use
+ *     			an actual Amon client.
+ *
+ *     amoncfg          an instance of MantaAmonConfig with probe groups
+ *                      configured already.  This configuration will be updated
+ *                      with probe details.
+ *
+ *     components	an array of objects describing the components.  Each
+ *     			component should have properties:
+ *
+ *     		"type"	either "cn" (for compute nodes) or "vm" (for containers)
+ *
+ *     		"uuid"  the server_uuid (for type "cn") or VM uuid (for
+ *     			containers)
+ *
+ *     concurrency	an integer number for the maximum concurrent requests
+ *
+ * "callback" is invoked as "callback(err)".
+ *
+ * Amon has an API for listing probes, but it's limited to 1000 probes, which is
+ * too small for large Manta deployments.  Additionally, that API has no support
+ * for pagination.  Instead, we use the private Amon agent API to fetch the list
+ * of probes for each agent.  That number is generally much smaller.  This
+ * results in a lot more requests, but we don't have a better option.
+ */
+function amonLoadComponentProbes(args, callback)
+{
+	var amoncfg, client, queue, errors, warnings, progress, ndone;
+
+	assertplus.object(args, 'args');
+	assertplus.object(args.amonRaw, 'args.amonRaw');
+	assertplus.object(args.amoncfg, 'args.amoncfg');
+	assertplus.ok(args.amoncfg instanceof MantaAmonConfig);
+	assertplus.number(args.concurrency, 'args.concurrency');
+	assertplus.arrayOfObject(args.components, 'args.components');
+	assertplus.func(callback, 'callback');
+
+	amoncfg = args.amoncfg;
+	client = args.amonRaw;
+	errors = [];
+	warnings = [];
+	ndone = 0;
+	if (process.stderr.isTTY) {
+		progress = new progbar.ProgressBar({
+		    'filename': 'fetching probes for each agent',
+		    'bytes': false,
+		    'size': args.components.length
+		});
+	}
+
+	queue = vasync.queuev({
+	    'concurrency': args.concurrency,
+	    'worker': function fetchProbeQueueWorker(component, qcallback) {
+		assertplus.object(component, 'component');
+		assertplus.string(component.type, 'component.type');
+		assertplus.string(component.uuid, 'component.uuid');
+
+		amonFetchAgentProbes({
+		    'amon': client,
+		    'agentUuid': component.uuid
+		}, function (err, probes) {
+			if (err) {
+				err = new VError(err, 'fetching probes for ' +
+				    'agent on %s "%s"', component.type,
+				    component.uuid);
+				errors.push(err);
+				qcallback();
+				return;
+			}
+
+			probes.forEach(function (p) {
+				var error = amoncfg.addProbe(p);
+				if (error !== null) {
+					warnings.push(new VError(error,
+					    'ignoring probe'));
+				}
+			});
+
+			ndone++;
+			if (progress !== undefined) {
+				progress.advance(ndone);
+			}
+
+			qcallback();
+		});
+	    }
+	});
+
+	args.components.forEach(function (c, i) {
+		var label = 'args.components[' + i + ']';
+		assertplus.string(c.type, label + '.type');
+		assertplus.string(c.uuid, label + '.uuid');
+		queue.push({ 'type': c.type, 'uuid': c.uuid });
+	});
+
+	queue.on('end', function () {
+		if (progress !== undefined) {
+			progress.end();
+		}
+
+		callback(common.errorForList(errors),
+		    common.errorForList(warnings));
+	});
+
+	queue.close();
+}
+
+/*
+ * Uses the amon (private) relay API to list the probes associated with the
+ * given agent.
+ *
+ * Named arguments:
+ *
+ *     amon             a restify JSON client for the AMON master API
+ *
+ *     agentUuid        uuid of the agent whose probes should be fetched
+ */
+function amonFetchAgentProbes(args, callback)
+{
+	var client, uripath;
+
+	assertplus.object(args, 'args');
+	assertplus.object(args.amon, 'args.amon');
+	assertplus.string(args.agentUuid, 'args.agentUuid');
+	assertplus.func(callback, 'callback');
+
+	client = args.amon;
+	uripath = '/agentprobes?agent=' + encodeURIComponent(args.agentUuid);
+	client.get(uripath, function (err, req, res, result) {
+		/*
+		 * This API has the same problem as most Amon API "list"
+		 * operations, which is that they implicitly have a limit on the
+		 * number of results, there's no way to override that, and
+		 * there's no way to paginate the list.  As a result, we can
+		 * never see more than that many results.  Today, this number is
+		 * 1000.  We attempt to at least detect that this might have
+		 * happened.
+		 */
+		var limit = 1000;
+		if (!err && result.length == limit) {
+			err = new VError('got %d results, ' +
+			    'assuming truncation', limit);
+		}
+
+		if (err) {
+			err = new VError(err, 'amon: get "%s"', uripath);
+			callback(err);
+			return;
+		}
+
+		callback(null, result);
+	});
+}
+
+
+/*
+ * Print a human-readable summary of configured probes and probe groups.
+ */
+function amonConfigSummarize(args, callback)
+{
+	var out, config, metadata, instanceSvcname;
+	var ngroups, nagents, nprobes, norphans;
+	var svcs, rows;
+
+	assertplus.object(args, 'args');
+	assertplus.object(args.config, 'args.config');
+	assertplus.ok(args.config instanceof MantaAmonConfig);
+	assertplus.object(args.stream, 'args.stream');
+	assertplus.object(args.metadata, 'args.metadata');
+	assertplus.object(args.instanceSvcname, 'args.instanceSvcname');
+
+	out = args.stream;
+	config = args.config;
+	metadata = args.metadata;
+	instanceSvcname = args.instanceSvcname;
+
+	ngroups = 0;
+	nprobes = 0;
+	nagents = 0;
+	norphans = 0;
+	svcs = {};
+	svcs['unknown'] = {
+	    'svc_groups': {},
+	    'svc_nprobes': 0,
+	    'svc_ninstances': 0,
+	    'svc_norphans': 0,
+	    'svc_agents': {}
+	};
+	svcs['global zone'] = jsprim.deepCopy(svcs['unknown']);
+	services.mSvcNamesProbes.forEach(function (svcname) {
+		svcs[svcname] = jsprim.deepCopy(svcs['unknown']);
+	});
+
+	/*
+	 * Print a count of probes and agents affected for each probe group.
+	 */
+	rows = [];
+	config.eachProbeGroup(function (pg) {
+		var eventName, ka, name;
+		var ngroupprobes, agents, ngroupagents;
+
+		agents = {};
+		ngroupprobes = 0;
+		config.eachProbeGroupProbe(pg.pg_name, function iterProbe(p) {
+			var svcname, svc;
+
+			agents[p.p_agent] = true;
+			ngroupprobes++;
+			svcname = instanceSvcname.hasOwnProperty(p.p_agent) &&
+			    svcs.hasOwnProperty(instanceSvcname[p.p_agent]) ?
+			    instanceSvcname[p.p_agent] : 'unknown';
+			svc = svcs[svcname];
+			svc.svc_nprobes++;
+			svc.svc_groups[pg.pg_uuid] = true;
+			svc.svc_agents[p.p_agent] = true;
+		});
+
+		ngroupagents = Object.keys(agents).length;
+		nprobes += ngroupprobes;
+		ngroups++;
+
+		eventName = metadata.probeGroupEventName(pg.pg_name);
+		if (eventName !== null) {
+			ka = metadata.eventKa(eventName);
+			if (ka !== null) {
+				name = ka.ka_title;
+			} else {
+				name = eventName;
+			}
+		} else {
+			name = pg.pg_name;
+		}
+
+		rows.push({
+		    'nprobes': ngroupprobes,
+		    'nagents': ngroupagents,
+		    'name': name
+		});
+	});
+
+	config.eachOrphanProbe(function (p) {
+		var svcname, svc;
+
+		nprobes++;
+		norphans++;
+		svcname = instanceSvcname.hasOwnProperty(p.p_agent) &&
+		    svcs.hasOwnProperty(instanceSvcname[p.p_agent]) ?
+		    instanceSvcname[p.p_agent] : 'unknown';
+		svc = svcs[svcname];
+		svc.svc_nprobes++;
+		svc.svc_norphans++;
+	});
+
+	fprintf(out, 'Configuration by probe group:\n\n');
+	fprintf(out, '    %7s  %7s  %s\n', 'NPROBES', 'NAGENTS', 'PROBE GROUP');
+	rows.sort(function (r1, r2) {
+		return (r1.name.localeCompare(r2.name));
+	}).forEach(function (row) {
+		fprintf(out, '    %7d  %7d  %s\n',
+		    row.nprobes, row.nagents, row.name);
+	});
+
+	/*
+	 * Now print a summary of probes by service name.
+	 */
+	fprintf(out, '\nConfiguration by service:\n\n');
+	fprintf(out, '    %-16s  %7s  %7s  %7s  %8s\n',
+	    'SERVICE', 'NGROUPS', 'NAGENTS', 'NPROBES', 'NORPHANS');
+	Object.keys(svcs).sort(function (a1, a2) {
+		if (a1 == 'unknown') {
+			return (1);
+		} else if (a2 == 'unknown') {
+			return (-1);
+		}
+		return (a1.localeCompare(a2));
+	}).forEach(function (svcname) {
+		var svc, nsvcagents;
+
+		svc = svcs[svcname];
+		nsvcagents = Object.keys(svc.svc_agents).length;
+		fprintf(out, '    %-16s  %7d  %7d  %7d  %8d\n', svcname,
+		    Object.keys(svc.svc_groups).length, nsvcagents,
+		    svc.svc_nprobes, svc.svc_norphans);
+		nagents += nsvcagents;
+	});
+
+	fprintf(out, '    %-16s  %7d  %7d  %7d  %8d\n\n', 'TOTAL',
+	    ngroups, nagents, nprobes, norphans);
+}
+
+
+/*
+ * Amon configuration
+ *
+ * The MantaAmonConfig class represents a set of probes and probe groups.  See
+ * the block comment in lib/alarms/index.js for more information.
+ *
+ * This implementation requires that probe group names be unique, and that probe
+ * groups be added before probes.  The name uniqueness constraint is important
+ * because the only way to compare what we expect to be deployed against what's
+ * really deployed is based on the probe group names.  If we have more than one
+ * probe group with the same name, then it would be much harder to tell whether
+ * the right probes were deployed.
+ */
+function MantaAmonConfig()
+{
+	/*
+	 * mapping of probe group name -> probe group object
+	 * This is the canonical set of probe groups represented by this object.
+	 */
+	this.mac_probegroups_by_name = {};
+
+	/*
+	 * mapping of probe group uuid -> probe group name
+	 * This set is updated as callers add probe groups, but it's primarily
+	 * used as callers subsequently add probes in order to map those probes
+	 * to corresponding probe groups.  It's also used for deployed probe
+	 * groups for consumers to map group uuids to group names.
+	 */
+	this.mac_probegroups_by_uuid = {};
+
+	/*
+	 * mapping of probe group name -> list of probes
+	 * Along with mac_probes_orphan below, this is the canonical set of
+	 * probes represented by this object.
+	 */
+	this.mac_probes_by_probegroup = {};
+
+	/* List of probes having no group */
+	this.mac_probes_orphan = [];
+}
+
+/*
+ * Adds a probe.  The "probedef" object must match the Amon schema for a probe.
+ */
+MantaAmonConfig.prototype.addProbe = function (probedef)
+{
+	var probe, pgname;
+
+	probe = new amon_objects.loadProbeObject(probedef);
+	if (probe instanceof Error) {
+		return (probe);
+	}
+
+	if (probe.p_groupid === null) {
+		this.mac_probes_orphan.push(probe);
+		return (null);
+	}
+
+	if (!this.mac_probegroups_by_uuid.hasOwnProperty(probe.p_groupid)) {
+		return (new VError('probe "%s": unknown probe group "%s"',
+		    probe.p_uuid, probe.p_groupid));
+	}
+
+	pgname = this.mac_probegroups_by_uuid[probe.p_groupid];
+	assertplus.ok(this.mac_probes_by_probegroup.hasOwnProperty(pgname));
+	this.mac_probes_by_probegroup[pgname].push(probe);
+	return (null);
+};
+
+/*
+ * Adds a probe group.  The "groupdef" object must match the Amon schema for a
+ * probe group.
+ */
+MantaAmonConfig.prototype.addProbeGroup = function (groupdef)
+{
+	var probegroup;
+
+	probegroup = amon_objects.loadProbeGroupObject(groupdef);
+	if (probegroup instanceof Error) {
+		return (probegroup);
+	}
+
+	if (this.mac_probegroups_by_name.hasOwnProperty(probegroup.pg_name)) {
+		return (new VError('duplicate probe group name: "%s"',
+		    probegroup.pg_name));
+	}
+
+	if (this.mac_probegroups_by_uuid.hasOwnProperty(probegroup.pg_uuid)) {
+		return (new VError('duplicate probe group uuid: "%s"',
+		    probegroup.pg_uuid));
+	}
+
+	assertplus.ok(!this.mac_probes_by_probegroup.hasOwnProperty(
+	    probegroup.pg_name));
+	this.mac_probegroups_by_name[probegroup.pg_name] = probegroup;
+	this.mac_probegroups_by_uuid[probegroup.pg_uuid] = probegroup.pg_name;
+	this.mac_probes_by_probegroup[probegroup.pg_name] = [];
+	return (null);
+};
+
+/*
+ * Returns the specified probe group, if it exists.  Otherwise, returns null.
+ */
+MantaAmonConfig.prototype.probeGroupForName = function (pgname)
+{
+	assertplus.string(pgname, 'pgname');
+	if (!this.mac_probegroups_by_name.hasOwnProperty(pgname)) {
+		return (null);
+	}
+
+	return (this.mac_probegroups_by_name[pgname]);
+};
+
+/*
+ * Returns the probe group name for the given probe group id.
+ */
+MantaAmonConfig.prototype.probeGroupNameForUuid = function (pgid)
+{
+	if (!this.mac_probegroups_by_uuid.hasOwnProperty(pgid)) {
+		return (null);
+	}
+
+	return (this.mac_probegroups_by_uuid[pgid]);
+};
+
+MantaAmonConfig.prototype.hasProbeGroup = function (pgname)
+{
+	assertplus.string(pgname);
+	return (this.mac_probes_by_probegroup.hasOwnProperty(pgname));
+};
+
+/*
+ * Iterates all of the probe groups in this configuration and invokes
+ * "func(probegroup)".
+ */
+MantaAmonConfig.prototype.eachProbeGroup = function (func)
+{
+	var probesbypg;
+
+	assertplus.func(func, 'func');
+	probesbypg = this.mac_probes_by_probegroup;
+	jsprim.forEachKey(this.mac_probegroups_by_name, function (name, pg) {
+		assertplus.ok(probesbypg.hasOwnProperty(name));
+		func(pg);
+	});
+};
+
+/*
+ * Iterates all probes in this configuration that are associated with probe
+ * group "pgname" and invokes "func(probe)" for each one.
+ */
+MantaAmonConfig.prototype.eachProbeGroupProbe = function (pgname, func)
+{
+	var probes;
+	assertplus.string(pgname, 'pgname');
+	assertplus.func(func, 'func');
+	assertplus.ok(this.mac_probes_by_probegroup.hasOwnProperty(pgname),
+	    'unknown probe group name: "' + pgname + '"');
+	probes = this.mac_probes_by_probegroup[pgname];
+	probes.forEach(function (p) { func(p); });
+};
+
+/*
+ * Iterates all probes in this configuration that have no associated probe
+ * group and invokes "func(probe)" for each one.
+ */
+MantaAmonConfig.prototype.eachOrphanProbe = function (func)
+{
+	assertplus.func(func, 'func');
+	this.mac_probes_orphan.forEach(function (p) { func(p); });
+};
diff --git a/lib/alarms/index.js b/lib/alarms/index.js
new file mode 100644
index 0000000..78207bc
--- /dev/null
+++ b/lib/alarms/index.js
@@ -0,0 +1,240 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+/*
+ * Manta Alarm Management
+ *
+ *
+ * INTRODUCTION
+ *
+ * Manta uses Triton's Amon facilities to define checks and other conditions
+ * that will raise alarms and notify operators when problems arise.  Operators
+ * are expected to run "manta-adm" during deployment to configure Amon with all
+ * of the expected checks.  (Other than running the command itself, this process
+ * is fully automatic.)  Later, when alarms are opened in response to failures,
+ * operators also use "manta-adm" to list alarms, fetch details about them,
+ * suspend notifications in cases of known issues, and ultimately close alarms
+ * for which the underlying issue is believed to be resolved.
+ *
+ * Amon is configured in terms of probes.  Most probes are either commands that
+ * run periodically or log file watchers that continuously monitor the contents
+ * of log files.  Each probe is attached to a specific instance, which is either
+ * a Triton VM or a Triton CN.  Of course, the set of VMs and CNs used for Manta
+ * is not static, and certainly not known at build time, so probes must be
+ * dynamically generated based on metadata (which is stored in this repository)
+ * and the set of components actually deployed at any given time.  This process
+ * is described in more detail below.
+ *
+ * Within Amon's configuration, probes are gathered into probe groups, which are
+ * mainly useful because they define how the corresponding alarms get organized.
+ * When multiple probes in the same group fail, those failures are generally
+ * collected into a single alarm.  That's useful to group multiple instances of
+ * the same problem (e.g., multiple "webapi" components reporting the same
+ * error).  Unlike previous versions of this software, distinct failure modes
+ * generally result in different alarms.  This makes it possible to silence
+ * individual alarms corresponding to known issues without squelching
+ * notifications about new issues as well.
+ *
+ *
+ * LOCAL METADATA AND KNOWN FAILURE MODES
+ *
+ * The metadata contained in this repository enumerates all of the known Manta
+ * failure modes, defines checks for identifying them, and provides useful
+ * information for an operator about each one.  Inspired by the illumos Fault
+ * Management Architecture ("FMA"), the metadata describes one or more _event
+ * classes_, each having a unique name in a hierarchical, dot-delimited
+ * namespace.  Each of these corresponds to a distinct failure mode.  For
+ * example, the event:
+ *
+ *     upset.manta.loadbalancer.no_backends
+ *
+ * indicates that a loadbalancer has no available backends.  ("upset" is an
+ * existing top-level FMA event class that most closely matches the kinds of
+ * problems we're representing here.)
+ *
+ * In our model, these event classes correspond one-to-one with Amon probe
+ * groups.
+ *
+ * For operator-visible events like these, FMA supports the idea of _knowledge
+ * articles_, which provide content suitable for an operator that describes a
+ * problem's severity, impact, any automated response (if any), and suggested
+ * action.
+ *
+ * Putting all this together: each piece of metadata defined in this repository
+ * is called a _probe template_.  Each template describes a known Manta failure
+ * mode, a list of checks for identifying it, and knowledge article content.
+ * Specifically, each template has:
+ *
+ *     - an FMA event class name unique to this failure mode.  This is used as a
+ *       primary key to refer to this particular failure mode.
+ *
+ *     - a scope, which describes what kinds of components this template applies
+ *       to (e.g., "loadbalancer" zones)
+ *
+ *     - a list of checks for identifying this failure mode.  These are used to
+ *       create Amon probes to detect this failure.
+ *
+ *     - knowledge article content
+ *
+ * The specific format is described in lib/alarms/metadata.js.
+ *
+ * As an example, the aforementioned event has FMA event class
+ * "upset.manta.loadbalancer.no_backends".  Its scope would be "loadbalancer",
+ * and it would define a check script to run in each loadbalancer zone to count
+ * the backends and fail if the count is zero.  To implement this, the
+ * deployment tooling creates one probe group for the failure mode itself and
+ * a probe to run the check _for each_ loadbalancer instance.
+ *
+ * More sophisticated configurations are also possible.  See the comments in
+ * config.js for details.
+ *
+ * FMA supports a sophisticated system of telemetry, diagnosis, reporting, and
+ * retirement of faulty components.  We only use the concepts of events, the
+ * event hierarchy, and knowledge articles, and this implementation shares no
+ * code with FMA itself.
+ *
+ *
+ * IMPLEMENTATION OVERVIEW
+ *
+ * Putting this together, there are basically three sources of information
+ * related to probes and alarms:
+ *
+ *    (1) The list of instances of each component that are currently deployed.
+ *        This includes the lists of SAPI instances, VMs, and CNs, and the
+ *        information comes from SAPI, VMAPI, and CNAPI.
+ *
+ *    (2) Local metadata that describes the probe groups and probes that should
+ *        exist in a Manta deployment.  This metadata also includes knowledge
+ *        articles that provide additional information for the operator for each
+ *        failure mode (like instructions about how to respond to various types
+ *        of alarms).
+ *
+ *    (3) The list of probes and probe groups that are actually deployed, and
+ *        the list of open alarms and the events associated with each alarm.
+ *        This comes from Amon, but Amon only knows about its own agents, which
+ *        have uuids corresponding to VM and CN uuids.  To make sense of this
+ *        information, it has to be at least joined with the list of components
+ *        deployed, but likely also the local metadata associated with probe
+ *        groups.
+ *
+ *        This source can be split further into the list of alarms and probe
+ *        groups and (separately) the list of probes.  The list of probes is
+ *        much more expensive to gather, and is only necessary when
+ *        checking or updating the Amon configuration.
+ *
+ * Using this information, we want to support a few different stories:
+ *
+ *    (1) List open alarms or detailed information about specific alarms.
+ *        ("manta-adm alarm show" and related commands)
+ *
+ *        We want to present the list of known, active problems.  This is the
+ *        list of open alarms, which we can fetch from Amon.  We want to
+ *        associate each problem with the affected components using their
+ *        service names.  That requires joining the "machine" that's provided
+ *        in each fault with the information we fetched separately about
+ *        deployed VMs and CNs.  We also want to provide knowledge article
+ *        content about each alarm by joining with the local configuration,
+ *        based on the alarm's probe group name.
+ *
+ *    (2) List configured probes and probe groups.
+ *        ("manta-adm alarm config show" and related commands)
+ *
+ *        It's useful for operators to see what probes have been configured.
+ *        This involves fetching probes and probe groups from Amon and combining
+ *        that information with the local knowledge articles for each one and
+ *        possibly the list of VMs and CNs deployed.
+ *
+ *    (3) Update the probe and probe group configuration.
+ *        ("manta-adm alarm config verify", "manta-adm alarm config update")
+ *
+ *        For both initial deployment and subsequent updates, it's important to
+ *        have an idempotent operation that compares what probes and probe
+ *        groups are supposed to be configured with what's actually deployed and
+ *        then updates the deployment to match what's expected.  This also
+ *        involves joining all three sources of information.
+ *
+ * Adding to the complexity, there are several other types of probes or probe
+ * groups that we may encounter:
+ *
+ *     - Probes and probe groups added by operators for their own custom
+ *       monitoring.  This is fully supported, though it cannot be configured
+ *       using the Manta tools.  We present these as best we can -- using
+ *       whatever metadata is in the probe groups rather than knowledge article
+ *       information.
+ *
+ *     - Probes and probe groups added by previous versions of this software
+ *       before any of the local metadata was provided.  These groups are
+ *       explicitly deprecated: we want operators to move away from them because
+ *       they're very hard to use.  For display, we treat these like probes and
+ *       probe groups that operators added, where we have no local knowledge
+ *       article information about them.  For update, we'll remove these
+ *       altogether, since they're replaced by other probes and groups that we
+ *       deploy.
+ *
+ *     - Other probes and probe groups added by other versions of this software
+ *       (either older or newer) that had local metadata at the time.  We can
+ *       distinguish these because of the way probe groups are named.  We treat
+ *       these similar to probes and probe groups that were added before this
+ *       metadata was available: we'll consider them removable during "manta-adm
+ *       alarm config verify/update".
+ *
+ * The implementation of these facilities is divided into:
+ *
+ *     - lib/alarms/index.js (this file): general documentation and symbols
+ *       exported from this subsystem.
+ *
+ *     - lib/alarms/alarms.js: defines data structures and functions for
+ *       managing alarms themselves.
+ *
+ *     - lib/alarms/amon_objects.js: defines classes and loaders for low-level
+ *       Amon objects, including input validation.
+ *
+ *     - lib/alarms/config.js: defines data structures and functions for
+ *       managing Amon configuration (namely, a set of probes and probe groups).
+ *       This includes functions that fetch probes and probe groups from Amon,
+ *       and classes for walking these structures for the purpose of verifying
+ *       or updating the configuration.
+ *
+ *     - lib/alarms/metadata.js: defines data structures and functions for
+ *       working with the locally provided metadata for known failure modes.
+ *
+ *     - lib/alarms/update.js: defines data structures and functions for
+ *       updating the Amon configuration.  This includes functions for comparing
+ *       two sets of configuration (usually a "deployed" configuration and a
+ *       "desired" configuration), generating a plan to move from one to
+ *       another, and applying that plan.
+ */
+
+var alarm_metadata = require('./metadata');
+var alarm_alarms = require('./alarms');
+var alarm_config = require('./config');
+var alarm_update = require('./update');
+
+/* Exported interfaces */
+
+/* Alarms */
+exports.amonLoadAlarmsForState = alarm_alarms.amonLoadAlarmsForState;
+exports.amonLoadAlarmsForIds = alarm_alarms.amonLoadAlarmsForIds;
+exports.amonCloseAlarms = alarm_alarms.amonCloseAlarms;
+exports.amonUpdateAlarmsNotification =
+    alarm_alarms.amonUpdateAlarmsNotification;
+
+/* Configuration */
+exports.amonLoadProbeGroups = alarm_config.amonLoadProbeGroups;
+exports.amonLoadComponentProbes = alarm_config.amonLoadComponentProbes;
+exports.amonConfigSummarize = alarm_config.amonConfigSummarize;
+
+/* Configuration updates */
+exports.amonUpdatePlanCreate = alarm_update.amonUpdatePlanCreate;
+exports.amonUpdatePlanSummarize = alarm_update.amonUpdatePlanSummarize;
+exports.amonUpdatePlanApply = alarm_update.amonUpdatePlanApply;
+
+/* Local metadata */
+exports.loadMetadata = alarm_metadata.loadMetadata;
diff --git a/lib/alarms/metadata.js b/lib/alarms/metadata.js
new file mode 100644
index 0000000..c412464
--- /dev/null
+++ b/lib/alarms/metadata.js
@@ -0,0 +1,930 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright 2017, Joyent, Inc.
+ */
+
+/*
+ * lib/alarms/metadata.js: facilities for working with locally-provided metadata
+ * about probes and probe groups.  See the block comment in lib/alarms/index.js
+ * for details.
+ *
+ *
+ * INTERFACES
+ *
+ * This module exposes this function publicly:
+ *
+ *     loadMetadata: loads locally-provided metadata from files into a
+ *     MantaAmonMetadata object
+ *
+ * that implicitly exposes this class:
+ *
+ *     MantaAmonMetadata: a class that provides basic methods for iterating the
+ *     locally-provided metadata.  Instances of this class are immutable once
+ *     constructed.
+ *
+ * This module exposes this function semi-privately (to other modules in this
+ * directory):
+ *
+ *     probeGroupNameForTemplate: constructs a probe group name based on a probe
+ *     template
+ *
+ * as well as the "MetadataLoader" for tools.
+ *
+ *
+ * PROBE TEMPLATE FILES
+ *
+ * In this repo, the directory "alarm_metadata/probe_templates" contains a
+ * number of probe template files written in YAML.  Each file describes an array
+ * of probe templates.  The probe templates from all of these files are combined
+ * into a single configuration; the organization into separate files is purely
+ * for readers.  Each probe template describes a distinct failure mode for a
+ * Manta deployment and implicitly specifies a group of probes to be created at
+ * deployment-time.  These concepts and the broad design are described in much
+ * more detail in lib/alarms/index.js.
+ *
+ * We use YAML because of its reasonable support of strings with embedded
+ * newlines.  These probe template files contain both ordinary configuration
+ * (that could be specified in JSON as well as anything else) and a bunch of
+ * human-readable strings (each containing potentially a few paragraphs) that
+ * are closely associated with that configuration.  These files are not used by
+ * tools outside this repository, so the format can be changed in the future.
+ *
+ * Each probe template MUST contain these top-level properties:
+ *
+ *    "event"                   a unique, FMA-style event class name.  This
+ *    (required string)         ultimately defines a probe group.  All probes
+ *                              created from this template will be part of the
+ *                              same probe group (and there will be no other
+ *                              probes in this probe group).
+ *
+ *    "scope"                   describes the set of components that this
+ *    (required object)         probe template is intended to monitor, as well
+ *                              as how to distribute probes for those components
+ *
+ *        "scope.service"       identifies the SAPI service being monitored.
+ *        (required string)     In most cases, this template will generate an
+ *                              Amon probe for each instance of the specified
+ *                              service.
+ *
+ *        "scope.global"        if true, then instead of creating a probe for
+ *        (optional boolean)    each instance of the SAPI service indicated by
+ *                              "scope.service" (which would all be non-global
+ *                              zones), the system creates probes for every
+ *                              global zone that hosts those instance.
+ *
+ *        "scope.checkFrom"     if specified, this field identifies the SAPI
+ *        (optional string)     service for which probes will be created.  That
+ *                              would normally be the same as "scope.service",
+ *                              but in some cases, we monitor one service using
+ *                              probes associated with another.  For example, we
+ *                              monitor each storage zone with probes associated
+ *                              with nameservice zones.
+ *
+ *                              Specifying this causes a probe to be generated
+ *                              for each instance of "scope.service" for each
+ *                              instance of "scope.checkFrom".  That is, it's
+ *                              O(m * n) probes, where "m" and "n" are the
+ *                              numbers of instances of the two services.
+ *                              Generally, one or both of these services should
+ *                              have a fixed, small number of instances (like
+ *                              "nameservice" or "ops"), rather than both being
+ *                              services that are intended to scale to large
+ *                              numbers (like "webapi").
+ *
+ *    "checks"                  array of objects describing Amon probes to
+ *    (required array)          create.  There will be one Amon probe generated
+ *                              for each "check".  There must be at least one
+ *                              check.
+ *
+ *        "checks[i].type"      See Amon probe's "type" field.
+ *        (required string)
+ *
+ *        "checks[i].config"    See Amon probe's "config" field.
+ *        (required object)
+ *
+ *                              The "type" and "config" properties of each check
+ *                              are the same as for the corresponding properties
+ *                              of Amon probes, except that we only support a
+ *                              limited subset of types, and we support an
+ *                              additional "config" property: probes of type
+ *                              "cmd" may specify the special property:
+ *
+ *        "checks[i].config.autoEnv"    If specified, then this should
+ *        (optional array of strings)   be an array of SAPI metadata variables.
+ *                                      These metadata variables will be made
+ *                                      available in the process environment of
+ *                                      the command that runs as part of the
+ *                                      probe.  That means you can use them in
+ *                                      the probe's script.
+ *
+ *                                      When this property is specified on
+ *                                      supported probes, the "env" property of
+ *                                      the probe is filled in with the current
+ *                                      values of the specified SAPI metadata,
+ *                                      and the "autoEnv" property itself is not
+ *                                      passed through to Amon.
+ *
+ *    "ka"                      Specified knowledge article content.  This is
+ *    (required object)         prose text intended for operators trying to
+ *                              understand an open alarm.
+ *
+ *        "ka.title"            A very short summary of the problem.  This
+ *        (required string)     should fit comfortably in about 30 columns of
+ *                              text.
+ *
+ *        "ka.severity"         The severity of the problem, which must be one
+ *        (required string)     of the following:
+ *
+ *                                  "critical"  the data path or jobs path
+ *                                              may be significantly affected or
+ *                                              may be imminently so.
+ *                                              "Affected" here means increased
+ *                                              error rate or latency for end
+ *                                              user operations.
+ *
+ *                                  "major"     the data path or jobs path may
+ *                                              be affected, but the impact is
+ *                                              likely minor or limited to a
+ *                                              small number of requests.  Even
+ *                                              if not currently affected, these
+ *                                              paths are at risk for a major
+ *                                              disruption.
+ *
+ *                                  "minor"     the data path and jobs path are
+ *                                              likely not currently affected
+ *                                              (not more than a bounded, fixed
+ *                                              number of requests).
+ *
+ *                                  The severity is also used to determine the
+ *                                  Amon contacts applied to the probe group.
+ *                                  The specific contacts for each severity
+ *                                  level are contained in the top-level
+ *                                  configuration file.
+ *
+ *        "ka.description"      A summary of the problem in more detail than
+ *        (required string)     "ka.title" if desired.
+ *
+ *        "ka.response"         A description of any automated response taken by
+ *        (required string)     the system in response to this event.
+ *
+ *        "ka.impact"           A description of the impact to the system of
+ *        (required string)     this event.  This is a good place to describe
+ *                              how this event affects the error rate, latency,
+ *                              or anything else that's affected by this event.
+ *
+ *        "ka.action"           A description of actions recommended for the
+ *        (required string)     operator.
+ *
+ * To summarize, an Amon probe is created for each element of "checks" for each
+ * instance of SAPI service "scope.service".   If "scope.checkFrom" is
+ * specified, then all of _those_ probes are created for each instance of
+ * "checkFrom".
+ *
+ * Probe templates MAY also contain these top-level properties
+ *
+ *    "legacyName"              a string describing the legacy mantamon
+ *    (string)                  probe names that correspond to the probes
+ *                              for this template.  This is currently not used
+ *                              by anything, but is potentially useful for
+ *                              readers to understand the history of particular
+ *                              templates.
+ */
+
+var assertplus = require('assert-plus');
+var fs = require('fs');
+var jsprim = require('jsprim');
+var jsyaml = require('js-yaml');
+var path = require('path');
+var vasync = require('vasync');
+var VError = require('verror');
+
+var common = require('../common');
+var services = require('../services');
+
+/* Exported interface */
+exports.loadMetadata = loadMetadata;
+exports.probeGroupNameForTemplate = probeGroupNameForTemplate;
+exports.MetadataLoader = MetadataLoader;
+
+/* Exposed for testing only */
+exports.testingParseProbeGroupName = parseProbeGroupName;
+
+/*
+ * Concurrency with which we load probe template files.
+ */
+var PTS_CONCURRENCY_FILES = 10;
+
+/*
+ * Load all of the probe template metadata from the specified directory.
+ *
+ * Named arguments include:
+ *
+ *     directory	path to directory containing all probe template files
+ *     (string)
+ *
+ * "callback" is invoked upon completion as callback(err, metadata).
+ */
+function loadMetadata(args, callback)
+{
+	var mdl;
+
+	assertplus.object(args, 'args');
+	assertplus.string(args.directory, 'args.directory');
+
+	mdl = new MetadataLoader();
+	mdl.loadFromDirectory(args.directory, function onLoadDone() {
+		var error = common.errorForList(mdl.errors());
+		callback(error, error === null ? mdl.mdl_amoncfg : null);
+	});
+}
+
+/*
+ * An instance of MantaAmonMetadata represents the local metadata associated
+ * with probes and probe groups.  This is the primary exposed interface from
+ * this module, though objects are only exposed through the loading interfaces.
+ * (Outside consumers cannot create instances of this class directly.)
+ */
+function MantaAmonMetadata()
+{
+	/* Probe group information keyed by the configured event name. */
+	this.mam_templates_byevent = {};
+
+	/*
+	 * A single template can be used to define multiple probe groups with
+	 * the service name filled into the event name, which makes it different
+	 * for each service.  For example, the "SMF maintenance" template
+	 * has a scope of "each", which causes us to create one probe group per
+	 * distinct service.  The event name in the template is:
+	 *
+	 *     upset.manta.$service.smf_maintenance
+	 *
+	 * This creates one event per distinct service, which look like:
+	 *
+	 *     upset.manta.postgres.smf_maintenance
+	 *     upset.manta.moray.smf_maintenance
+	 *     ...
+	 *
+	 * To be able to recognize these expanded names, we expand them as we
+	 * process each template and store aliases here.
+	 */
+	this.mam_event_aliases = {};
+}
+
+
+/*
+ * Public interfaces: for all callers
+ */
+
+/*
+ * Public interface to return the knowledge article for an event called
+ * "eventName".  Returns null if there is no knowledge article registered for
+ * this event.
+ *
+ * See above for allowed callers.
+ */
+MantaAmonMetadata.prototype.eventKa = function eventKa(eventName)
+{
+	var resolved = this.resolveEventName(eventName);
+	if (resolved === null) {
+		return (null);
+	}
+
+	return (this.mam_templates_byevent[resolved].pt_ka);
+};
+
+/*
+ * Iterate the probe group events represented in this metadta.
+ *
+ * See above for allowed callers.
+ */
+MantaAmonMetadata.prototype.eachEvent = function (func)
+{
+	jsprim.forEachKey(this.mam_templates_byevent, function (evt, pt) {
+		if (pt.pt_aliases.length === 0) {
+			func(evt);
+		}
+	});
+
+	jsprim.forEachKey(this.mam_event_aliases, function (alias) {
+		func(alias);
+	});
+};
+
+
+/*
+ * Semi-private interfaces: for other files in this directory.
+ */
+
+/*
+ * Iterate all registered probe templates.
+ *
+ * See above for allowed callers.
+ */
+MantaAmonMetadata.prototype.eachTemplate = function (func)
+{
+	jsprim.forEachKey(this.mam_templates_byevent, function (_, pt) {
+		func(pt);
+	});
+};
+
+/*
+ * Given a probe group with name "probeGroupName", return the string name of the
+ * event that is emitted when an alarm for this group fires.  This is primarily
+ * useful for passing to the eventKa() function to get the knowledge article
+ * associated with this probe group.  This function returns null if the event
+ * name is unknown or not applicable (because it's an operator-created probe
+ * group or the like).
+ *
+ * See above for allowed callers.
+ */
+MantaAmonMetadata.prototype.probeGroupEventName =
+    function probeGroupEventName(probeGroupName)
+{
+	var result;
+
+	result = parseProbeGroupName(probeGroupName);
+	if (result.error !== null || result.isLegacy || result.isOther) {
+		return (null);
+	}
+
+	assertplus.string(result.eventName);
+	return (result.eventName);
+};
+
+/*
+ * Given a probe group with name "probeGroupName", determine whether it should
+ * be removed as part of a configuration update operation.  See the block
+ * comment at the top of this file for an explanation of why we mark different
+ * types of groups for removal.
+ *
+ * See above for allowed callers.
+ */
+MantaAmonMetadata.prototype.probeGroupIsRemovable =
+    function probeGroupIsRemovable(probeGroupName)
+{
+	var result, eventName;
+
+	result = parseProbeGroupName(probeGroupName);
+	if (result.error !== null || result.isOther) {
+		return (false);
+	}
+
+	if (result.isLegacy) {
+		return (true);
+	}
+
+	assertplus.string(result.eventName);
+	eventName = this.resolveEventName(result.eventName);
+	return (eventName === null);
+};
+
+
+/*
+ * Private interfaces: for this file only
+ */
+
+/*
+ * Private interface to load a probe template into this data structure.  This
+ * normally comes from the probe template files checked into this repository,
+ * though the test suite can use this to load specific templates.
+ *
+ * See above for allowed callers.
+ */
+MantaAmonMetadata.prototype.addTemplate = function addTemplate(args)
+{
+	var inp, eventName, pt, error, nsubs;
+	var self = this;
+
+	assertplus.object(args, 'args');
+	assertplus.object(args.input, 'args.input');
+	assertplus.string(args.originLabel, 'args.originLabel');
+
+	inp = args.input;
+	eventName = inp.event;
+
+	if (this.mam_templates_byevent.hasOwnProperty(eventName)) {
+		return (new VError('%s: re-uses event name "%s" previously ' +
+		    'used in template "%s"', args.originLabel, eventName,
+		    this.mam_templates_byevent[eventName].pt_origin_label));
+	}
+
+	pt = new ProbeTemplate({
+	    'input': inp,
+	    'originLabel': args.originLabel
+	});
+
+	if (pt.pt_scope.ptsc_service != 'each') {
+		if (/[^a-zA-Z0-9_.]/.test(eventName)) {
+			return (new VError('%s: event name contains ' +
+			    'unsupported characters', args.originLabel));
+		}
+
+		this.mam_templates_byevent[eventName] = pt;
+		return (null);
+	}
+
+	this.mam_templates_byevent[eventName] = pt;
+
+	/*
+	 * Generate per-service aliases for probe groups that generate more than
+	 * one event name.
+	 */
+	nsubs = 0;
+	error = null;
+	services.mSvcNamesProbes.forEach(function (svcname) {
+		var fmasvcname;
+
+		/*
+		 * For FMA event names, we adopt the convention of using
+		 * underscores instead of dashes.  We need to translate service
+		 * names accordingly.
+		 */
+		assertplus.ok(svcname !== 'marlin');
+		fmasvcname = svcname.replace(/-/g, '_');
+
+		/*
+		 * We support certain limited expansions within the FMA event
+		 * name.  Each expansion is an ASCII string beginning with '$',
+		 * followed by an alphabetic character or "_", followed by any
+		 * number of alphanumeric characters or "_".  This is perhaps
+		 * simplistic, but because these fields are otherwise plaintext,
+		 * there's nothing else to confuse the interpretation here
+		 * (e.g., quoted strings or escape characters).  Note that the
+		 * use of '$' or '$3' or the like will work fine, though a
+		 * literal string like $ABC that is not intended to be expanded
+		 * will not work.  This would be a little strange for an FMA
+		 * event name.
+		 */
+		var aliasname = pt.pt_event.replace(
+		    /\$([a-zA-Z_][a-zA-Z0-9_]*)/g,
+		    function onMatch(substr, varname) {
+			assertplus.equal('$' + varname, substr);
+			if (varname == 'service') {
+				nsubs++;
+				return (fmasvcname);
+			}
+
+			if (error === null) {
+				error = new VError('template "%s": unknown ' +
+				    'variable "%s" in event name',
+				    pt.pt_origin_label, substr);
+			}
+
+			return ('INVALID');
+		    });
+
+
+		if (/[^a-zA-Z0-9_.]/.test(aliasname)) {
+			if (error === null) {
+				error = new VError('%s: expanded event name ' +
+				    'contains unsupported characters: "%s"',
+				    args.originLabel, aliasname);
+			}
+		} else {
+			pt.pt_aliases.push({
+			    'pta_event': aliasname,
+			    'pta_service': svcname
+			});
+		}
+	});
+
+	if (error === null && nsubs === 0) {
+		return (new VError('template "%s": templates with scope ' +
+		    '"each" must use "$service" in event name to ensure ' +
+		    'uniqueness', pt.pt_origin_label));
+	}
+
+	if (error !== null) {
+		return (error);
+	}
+
+	pt.pt_aliases.forEach(function (alias) {
+		assertplus.ok(!self.mam_event_aliases.hasOwnProperty(
+		    alias.pta_event), 'duplicate alias: ' + alias.pta_event);
+		self.mam_event_aliases[alias.pta_event] = pt.pt_event;
+	});
+
+	return (null);
+};
+
+/*
+ * Resolve an event name that may be an alias to the underlying event name.
+ * Returns null if this event is not known in this metadata.
+ *
+ * See above for allowed callers.
+ */
+MantaAmonMetadata.prototype.resolveEventName = function (eventName)
+{
+	if (this.mam_event_aliases.hasOwnProperty(eventName)) {
+		assertplus.ok(this.mam_templates_byevent.hasOwnProperty(
+		    this.mam_event_aliases[eventName]));
+		return (this.mam_event_aliases[eventName]);
+	}
+
+	if (this.mam_templates_byevent.hasOwnProperty(eventName)) {
+		return (eventName);
+	}
+
+	return (null);
+};
+
+
+var schemaProbeTemplateFile = {
+    'type': 'array',
+    'required': true,
+    'items': {
+	'type': 'object',
+	'additionalProperties': false,
+	'properties': {
+	    'event': {
+	        'type': 'string',
+		'required': true,
+		'minLength': 'upset.'.length
+	    },
+	    'legacyName': { 'type': 'string' },
+	    'scope': {
+		'type': 'object',
+		'additionalProperties': false,
+		'properties': {
+		    'service': {
+		        'type': 'string',
+			'required': true,
+			'enum': [ 'each', 'all' ].concat(
+			    services.mSvcNamesProbes)
+		    },
+		    'global': {
+			'type': 'boolean'
+		    },
+		    'checkFrom': {
+		        'type': 'string',
+			'enum': services.mSvcNamesProbes
+		    }
+		}
+	    },
+	    'checks': {
+		'type': 'array',
+		'minItems': 1,
+		'items': {
+		    'type': 'object',
+		    'additionalProperties': false,
+		    'properties': {
+			'type': {
+			    'type': 'string',
+			    'enum': [
+			        'bunyan-log-scan',
+				'cmd',
+				'disk-usage',
+				'log-scan'
+			    ]
+			},
+			'config': {
+			    'type': 'object'
+			}
+		    }
+		}
+	    },
+	    'ka': {
+		'type': 'object',
+		'additionalProperties': false,
+		'properties': {
+		    'title': { 'type': 'string', 'required': true },
+		    'description': { 'type': 'string', 'required': true },
+		    'severity': { 'type': 'string', 'required': true },
+		    'response': { 'type': 'string', 'required': true },
+		    'impact': { 'type': 'string', 'required': true },
+		    'action': { 'type': 'string', 'required': true }
+		}
+	    }
+	}
+    }
+};
+
+
+/*
+ * This class is used as a struct, with details private to this subsystem.
+ * The fields here closely mirror those in the probe template schema.  For
+ * details, see the documentation for that.
+ *
+ * The constructor takes arguments in the form as it comes out of the the
+ * YAML-parsed files.  These structures should have already been validated.
+ */
+function ProbeTemplate(args)
+{
+	var self = this;
+	var inp;
+
+	assertplus.object(args, 'args');
+	assertplus.object(args.input, 'args.input');
+	assertplus.string(args.originLabel, 'args.originLabel');
+
+	inp = args.input;
+
+	/*
+	 * The origin label is a string describing the source of this template.
+	 * It's generally a filename and potentially an index into the templates
+	 * listed in the file.  This is used in error messages that result from
+	 * building a configuration based on this template.
+	 */
+	this.pt_origin_label = args.originLabel;
+
+	/* FMA-style event class for this probe template. */
+	this.pt_event = inp.event;
+
+	/*
+	 * The scope object describes which components this probe monitors (and
+	 * potentially from which other components, if those are different).
+	 */
+	this.pt_scope = {};
+	this.pt_scope.ptsc_service = inp.scope.service;
+	this.pt_scope.ptsc_global = (inp.scope.global === true);
+	this.pt_scope.ptsc_check_from = inp.scope.checkFrom || null;
+
+	this.pt_checks = [];
+	inp.checks.forEach(function (c) {
+		var cc;
+
+		cc = {};
+		cc.ptc_type = c.type;
+		cc.ptc_config = jsprim.deepCopy(c.config);
+		self.pt_checks.push(cc);
+	});
+
+	this.pt_ka = {};
+	this.pt_ka.ka_title = inp.ka.title;
+	this.pt_ka.ka_description = inp.ka.description;
+	this.pt_ka.ka_severity = inp.ka.severity;
+	this.pt_ka.ka_response = inp.ka.response;
+	this.pt_ka.ka_impact = inp.ka.impact;
+	this.pt_ka.ka_action = inp.ka.action;
+	this.pt_aliases = [];
+}
+
+
+/*
+ * Represents the operation of loading a bunch of probe templates from
+ * configuration files.
+ */
+function MetadataLoader()
+{
+	/* problems encountered during load */
+	this.mdl_load_errors = [];
+
+	/* probe templates found */
+	this.mdl_amoncfg = new MantaAmonMetadata();
+
+	/* for debugging only */
+	this.mdl_load_pipeline = null;
+}
+
+/*
+ * Read YAML files in "directory" and load them.  Invokes "callback" upon
+ * completion.  Errors and warnings are not passed to the callback.  See the
+ * separate public methods for accessing those.
+ */
+MetadataLoader.prototype.loadFromDirectory =
+    function loadFromDirectory(directory, callback)
+{
+	var files;
+	var queue;
+
+	assertplus.string(directory, 'directory');
+	assertplus.func(callback, 'callback');
+
+	this.mdl_load_pipeline = vasync.pipeline({
+	    'arg': this,
+	    'funcs': [
+		function listDirectory(self, subcallback) {
+			fs.readdir(directory,
+			    function onReaddirDone(err, entries) {
+				if (err) {
+					err = new VError(err, 'readdir "%s"',
+					    directory);
+					self.mdl_load_errors.push(err);
+					subcallback();
+					return;
+				}
+
+				files = entries.filter(function (e) {
+					return (jsprim.endsWith(e, '.yaml'));
+				}).map(function (e) {
+					return (path.join(directory, e));
+				});
+
+				subcallback();
+			    });
+		},
+
+		function readFiles(self, subcallback) {
+			if (self.mdl_load_errors.length > 0) {
+				setImmediate(subcallback);
+				return;
+			}
+
+			queue = vasync.queuev({
+			    'concurrency': PTS_CONCURRENCY_FILES,
+			    'worker': function loadQueueCallback(f, qcallback) {
+				self.loadFromFile(f, qcallback);
+			    }
+			});
+
+			files.forEach(function (f) { queue.push(f); });
+			queue.on('end', function () { subcallback(); });
+			queue.close();
+		}
+	    ]
+	}, function (err) {
+		/*
+		 * Errors should be pushed onto mdl_load_errors, not emitted
+		 * here.
+		 */
+		assertplus.ok(!err);
+		callback();
+	});
+};
+
+/*
+ * Read a single YAML file and load it.  Invokes "callback" upon completion.
+ * Like loadFromDirectory(), errors and warnings are not passed to the callback,
+ * but recorded for later.
+ */
+MetadataLoader.prototype.loadFromFile =
+    function loadFromFile(filename, callback)
+{
+	var self = this;
+	var readoptions;
+
+	assertplus.string(filename, 'filename');
+	assertplus.func(callback, 'callback');
+
+	readoptions = { 'encoding': 'utf8' };
+	fs.readFile(filename, readoptions, function (err, contents) {
+		var parsed;
+
+		if (err) {
+			err = new VError(err, 'read "%s"', filename);
+			self.mdl_load_errors.push(err);
+			callback();
+			return;
+		}
+
+		try {
+			parsed = jsyaml.safeLoad(contents, {
+			    'filename': filename
+			});
+		} catch (ex) {
+			err = new VError(ex, 'parse "%s"', filename);
+			self.mdl_load_errors.push(err);
+			callback();
+			return;
+		}
+
+		err = jsprim.validateJsonObject(
+		    schemaProbeTemplateFile, parsed);
+		if (err instanceof Error) {
+			err = new VError(err, 'parse "%s"', filename);
+			self.mdl_load_errors.push(err);
+			callback();
+			return;
+		}
+
+		parsed.forEach(function (p, i) {
+			var label, error, k;
+
+			/*
+			 * In order to format the various messages, it's
+			 * important that they be consistent with respect to
+			 * trailing newlines.  It's easy to get this wrong in
+			 * YAML, so we check for it here.
+			 */
+			label = filename + ': probe ' + (i + 1);
+			for (k in p.ka) {
+				if (jsprim.endsWith(p.ka[k], '\n')) {
+					error = new VError('%s: field ka.%s: ' +
+					    'ends with trailing newline',
+					    label, k);
+					break;
+				}
+			}
+
+			if (!error) {
+				error = self.mdl_amoncfg.addTemplate({
+				    'input': p,
+				    'originLabel': label
+				});
+			}
+
+			if (error) {
+				self.mdl_load_errors.push(error);
+			}
+		});
+
+		callback();
+	});
+};
+
+MetadataLoader.prototype.errors = function ()
+{
+	return (this.mdl_load_errors.slice());
+};
+
+/*
+ * List of unversioned probe group names used by previous versions of this
+ * software.
+ */
+var MAM_LEGACY_PROBEGROUP_NAMES = [
+    'authcache-alert',
+    'compute-alert',
+    'electric-moray-alert',
+    'jobsupervisor-alert',
+    'loadbalancer-alert',
+    'moray-alert',
+    'nameservice-alert',
+    'ops-alert',
+    'ops-info',
+    'postgres-alert',
+    'storage-alert',
+    'webapi-alert'
+];
+
+/*
+ * Probe group names
+ *
+ * Probe templates are defined in the source code configuration.  Each template
+ * is expected to correspond to a distinct failure mode.  There may be more than
+ * one probe group for each template, depending on the scope.  These probe
+ * groups need to have names, and those names link them to the metadata we have
+ * (i.e., the knowledge articles).  To do this, we use FMA-style event names
+ * (e.g., upset.manta.$service.$problem).  Since this information will be
+ * programmatically parsed, we want to include a version number.  Together, we
+ * construct the probe group name for a given template by taking the FMA-style
+ * event name, substituting the service name if requested, and appending a
+ * version suffix.
+ *
+ * Given an arbitrary probe group name, we can classify it into one of a few
+ * buckets:
+ *
+ *    - If it matches one of the well-known probe groups used by previous
+ *      versions of this software, we call that "legacy".  We don't have
+ *      metadata about these groups, and they should be removed if we're making
+ *      updates to the probe configuration.
+ *
+ *    - Otherwise, if we cannot find the ";v=" suffix, then we assume this not a
+ *      probe created by this software.  This is likely something operators
+ *      created.  We'll generally leave these alone.
+ *
+ *    - Otherwise, if we find the suffix, but the version is newer than one we
+ *      recognize, then we'll not touch this probe group.  In the future, if we
+ *      decide to change the encoding (e.g., to include additional information
+ *      in the probe group name), then we can do so as long as we preserve a
+ *      ";v=" suffix with a new version number.
+ *
+ *    - Finally, if we find an acceptable version suffix, then this is a probe
+ *      group that we know how to manage.
+ *
+ * See the block comment at the top of this file for details on these different
+ * kinds of probe groups.
+ */
+
+function probeGroupNameForTemplate(pt, eventname)
+{
+	assertplus.object(pt, 'pt');
+	assertplus.string(eventname, 'eventname');
+	return (eventname + ';v=1');
+}
+
+function parseProbeGroupName(probeGroupName)
+{
+	var result, i, verpart;
+
+	result = {};
+	result.error = null;		/* failure to parse (bad version) */
+	result.isLegacy = null;		/* from "mantamon" era */
+	result.isOther = null;		/* operator-created */
+	result.eventName = null;	/* software-created, versioned era */
+
+	if (MAM_LEGACY_PROBEGROUP_NAMES.indexOf(probeGroupName) != -1) {
+		result.isLegacy = true;
+		return (result);
+	}
+
+	i = probeGroupName.indexOf(';');
+	if (i == -1 || probeGroupName.substr(i + 1, 2) != 'v=') {
+		result.isOther = true;
+		return (result);
+	}
+
+	verpart = probeGroupName.substr(i + 3);
+	if (verpart != '1') {
+		result.error = new VError('unrecognized version "%s" in ' +
+		    'probe group with name "%s"', verpart, probeGroupName);
+		return (result);
+	}
+
+	result.eventName = probeGroupName.slice(0, i);
+	return (result);
+}
diff --git a/lib/alarms/update.js b/lib/alarms/update.js
new file mode 100644
index 0000000..932feef
--- /dev/null
+++ b/lib/alarms/update.js
@@ -0,0 +1,1306 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+/*
+ * lib/alarms/update.js: facilities for updating a deployed set of Amon probes
+ * and probe groups.  This module builds on the facilities provided by
+ * amon_config.js.
+ */
+
+var assertplus = require('assert-plus');
+var jsprim = require('jsprim');
+var vasync = require('vasync');
+var VError = require('verror');
+var extsprintf = require('extsprintf');
+
+var fprintf = extsprintf.fprintf;
+var sprintf = extsprintf.sprintf;
+
+var common = require('../common');
+var services = require('../services');
+
+var alarm_metadata = require('./metadata');
+var alarm_config = require('./config');
+
+/* Exported interface */
+exports.amonUpdatePlanCreate = amonUpdatePlanCreate;
+exports.amonUpdatePlanSummarize = amonUpdatePlanSummarize;
+exports.amonUpdatePlanApply = amonUpdatePlanApply;
+
+/*
+ * Amon update plan
+ *
+ * The MantaAmonUpdatePlan class represents a set of probes and probe groups to
+ * be removed and a set of probes and probe groups to be added in order to
+ * update the Amon configuration for the Manta service.
+ */
+function MantaAmonUpdatePlan()
+{
+	/*
+	 * The actual plan is represented by the lists of probes and groups to
+	 * be added and removed.
+	 */
+
+	this.mup_probes_remove = []; 	/* probes to remove */
+	this.mup_groups_remove = []; 	/* probe groups to remove */
+	this.mup_groups_add = []; 	/* groups to add */
+	this.mup_probes_add = []; 	/* probes to add */
+
+	/*
+	 * Statistics kept about the update
+	 */
+
+	/* count of probe groups that were deployed and wanted */
+	this.mup_ngroupsmatch = 0;
+	/* count of probes that were deployed and wanted */
+	this.mup_nprobesmatch = 0;
+	/* count of probes ignored because they were orphans */
+	this.mup_nprobesorphan = 0;
+	/* count of probe groups that were deployed, unwanted, but kept */
+	this.mup_ngroupsignore = 0;
+
+	/*
+	 * Counts of probes added and removed and agents affected, by group id.
+	 */
+
+	this.mup_nadd_bygroup = {};
+	this.mup_nremove_bygroup = {};
+	this.mup_agents_bygroup = {};
+
+	/* warning messages to display to the operator */
+	this.mup_warnings = [];
+
+	/*
+	 * MantaAmonConfig objects used to generate this plan.
+	 */
+	this.mup_deployed = null;	/* found configuration */
+	this.mup_wanted = null;		/* normal wanted configuration */
+	this.mup_unconfigure = false;	/* unconfigure operation */
+}
+
+/*
+ * This is one of only two methods that may be called from outside of this file.
+ * Returns true if the update plan indicates that any changes need to be made.
+ */
+MantaAmonUpdatePlan.prototype.needsChanges = function ()
+{
+	return (this.mup_groups_remove.length > 0 ||
+	    this.mup_probes_remove.length > 0 ||
+	    this.mup_probes_add.length > 0 ||
+	    this.mup_groups_add.length > 0);
+};
+
+/*
+ * This is one of only two methods that may be called from outside of this file.
+ * Returns a list of Error objects describing problems found constructing the
+ * update plan.  These are generally non-fatal, but should be presented to an
+ * operator.
+ */
+MantaAmonUpdatePlan.prototype.warnings = function ()
+{
+	return (this.mup_warnings.slice(0));
+};
+
+MantaAmonUpdatePlan.prototype.probeUpdate = function (probe, counters, list)
+{
+	var groupid, agent;
+
+	assertplus.string(probe.p_groupid,
+	    'probe has no group id (adding and removing probes ' +
+	    'without groups is not supported');
+	groupid = probe.p_groupid;
+	assertplus.string(probe.p_agent);
+	agent = probe.p_agent;
+
+	if (!counters.hasOwnProperty(groupid)) {
+		counters[groupid] = 0;
+	}
+	counters[groupid]++;
+
+	if (!this.mup_agents_bygroup.hasOwnProperty(groupid)) {
+		this.mup_agents_bygroup[groupid] = {};
+	}
+	this.mup_agents_bygroup[groupid][agent] = true;
+
+	list.push(probe);
+};
+
+MantaAmonUpdatePlan.prototype.groupAdd = function groupAdd(group)
+{
+	this.mup_groups_add.push(group);
+};
+
+MantaAmonUpdatePlan.prototype.groupRemove = function groupRemove(group)
+{
+	this.mup_groups_remove.push(group);
+};
+
+MantaAmonUpdatePlan.prototype.probeAdd = function probeAdd(probe)
+{
+	this.probeUpdate(probe, this.mup_nadd_bygroup, this.mup_probes_add);
+};
+
+MantaAmonUpdatePlan.prototype.probeRemove = function probeRemove(probe)
+{
+	this.probeUpdate(probe, this.mup_nremove_bygroup,
+	    this.mup_probes_remove);
+};
+
+/*
+ * Given information about a current deployment, determine the set of updates to
+ * Amon necessary to update the configuration to what it should be.  See the
+ * block comment at the top of this file for a discussion of the goals and
+ * constraints of this operation.
+ *
+ * Named arguments:
+ *
+ *     account          Triton account uuid to use for wanted Amon probes
+ *
+ *     contactsBySeverity object mapping event severity levels to the associated
+ *     			set of amon contacts
+ *
+ *     instances        object mapping instance uuids to InstanceInfo objects
+ *
+ *     instancesBySvc   object mapping SAPI service names to array of instance
+ *                      uuids for instances in this datacenter
+ *
+ *     deployed         MantaAmonConfig object describing the set of probes and
+ *                      probe groups curently deployed
+ *
+ *     metadata         MantaAmonMetadata object describing the set of probes
+ *                      and probe groups that should be deployed
+ *
+ *     unconfigure      if specified, then all probes and probe groups should
+ *                      be removed, rather than updated to what would normally
+ *                      be configured
+ *
+ * This function returns either an Error (on failure) or a MantaAmonUpdatePlan.
+ */
+function amonUpdatePlanCreate(args)
+{
+	var deployed, metadata, wanted, rv;
+
+	assertplus.object(args, 'args');
+	assertplus.string(args.account, 'args.account');
+	assertplus.object(args.contactsBySeverity, 'args.contactsBySeverity');
+	assertplus.object(args.instances, 'args.instances');
+	assertplus.object(args.instancesBySvc, 'args.instancesBySvc');
+	assertplus.object(args.deployed, 'args.deployed');
+	assertplus.ok(args.deployed instanceof alarm_config.MantaAmonConfig);
+	assertplus.object(args.metadata, 'args.metadata');
+	assertplus.bool(args.unconfigure, 'args.unconfigure');
+
+	deployed = args.deployed;
+	metadata = args.metadata;
+	wanted = amonGenerateWanted({
+	    'account': args.account,
+	    'contactsBySeverity': args.contactsBySeverity,
+	    'metadata': metadata,
+	    'instances': args.instances,
+	    'instancesBySvc': args.instancesBySvc
+	});
+
+	if (wanted instanceof Error) {
+		return (new VError(wanted,
+		    'generating wanted amon configuration'));
+	}
+
+	rv = new MantaAmonUpdatePlan();
+	rv.mup_deployed = deployed;
+	rv.mup_wanted = wanted;
+
+	/*
+	 * We don't expect to deploy any probes that don't have probe groups
+	 * associated with them.
+	 */
+	wanted.eachOrphanProbe(function (p) {
+		throw (new VError(
+		    'unexpected orphan probe in "wanted" set'));
+	});
+
+	if (args.unconfigure) {
+		amonUpdatePlanCreateUnconfigure({
+		    'metadata': metadata,
+		    'plan': rv
+		});
+
+		return (rv);
+	}
+
+	/*
+	 * Iterate the "wanted" set and create any probe groups and probes that
+	 * are missing from the "deployed" set.
+	 */
+	wanted.eachProbeGroup(function iterWProbeGroup(wpg) {
+		var pgname, dpg, probesByAgent;
+
+		pgname = wpg.pg_name;
+		dpg = deployed.probeGroupForName(pgname);
+
+		if (dpg !== null) {
+			rv.mup_ngroupsmatch++;
+
+			if (!jsprim.deepEqual(wpg.pg_contacts.slice(0).sort(),
+			    dpg.pg_contacts.slice(0).sort())) {
+				/*
+				 * If the contacts on the probe group differ,
+				 * then notify the user.  We don't have a way to
+				 * update it (see MON-355).  We could remove
+				 * everything, but that would make open alarms
+				 * harder to grok, so we ask the operator to do
+				 * that.
+				 */
+				rv.mup_warnings.push(new VError('probe group ' +
+				    'with name "%s" (deployed with uuid %s): ' +
+				    'contacts do not match expected.',
+				    pgname, dpg.pg_user));
+			}
+
+			if (wpg.pg_user != dpg.pg_user) {
+				/*
+				 * This is theoretically similar to the
+				 * "contacts" case above, but there's no way the
+				 * user account should ever change, even with a
+				 * reconfiguration.  We'll just ignore that
+				 * these are different (but let the operator
+				 * know).
+				 */
+				rv.mup_warnings.push(new VError('probe group ' +
+				    'with name "%s" (deployed with uuid %s): ' +
+				    'user does not match expected',
+				    pgname, dpg.pg_user));
+			}
+		} else {
+			rv.groupAdd(wpg);
+		}
+
+		/*
+		 * In order to tell which probes need to be added and removed,
+		 * we need to be able to match up probes that are deployed with
+		 * probes that are wanted.  For our purposes, we will consider
+		 * a deployed probe and a wanted probe equivalent if they have
+		 * the same value for all of the immutable, configurable fields
+		 * that we expect not to change: the probe group name, "type",
+		 * "config", "agent", and "machine".  We'll warn if "contacts"
+		 * or "groupEvents" don't match what we expect.  If a new
+		 * version of the software changes the configuration (e.g., by
+		 * changing the bash script executed or the frequency of
+		 * execution), the deployed and wanted probes won't match, and
+		 * we'll end up removing the deployed one and adding the wanted
+		 * one.
+		 *
+		 * In order to keep this search relatively efficient, we first
+		 * build a list of probes for each agent for this probe group.
+		 * This should generally correspond to the list of checks
+		 * configured in the local metadata.  That's usually just one
+		 * probe, but might be a handful.
+		 */
+		probesByAgent = {};
+		if (deployed.hasProbeGroup(pgname)) {
+			deployed.eachProbeGroupProbe(pgname,
+			    function iterDProbe(p) {
+				if (!probesByAgent.hasOwnProperty(p.p_agent)) {
+					probesByAgent[p.p_agent] = [];
+				}
+
+				probesByAgent[p.p_agent].push(p);
+			    });
+		}
+
+		wanted.eachProbeGroupProbe(pgname, function iterWProbe(wp) {
+			var agent, dprobes, i, dp;
+
+			/*
+			 * Try to find a match for this wanted probe in the list
+			 * of deployed probes for the same agent.
+			 */
+			agent = wp.p_agent;
+			if (!probesByAgent.hasOwnProperty(agent)) {
+				rv.probeAdd(wp);
+				return;
+			}
+
+			dprobes = probesByAgent[agent];
+			for (i = 0; i < dprobes.length; i++) {
+				dp = dprobes[i];
+				if (dp.p_type == wp.p_type &&
+				    jsprim.deepEqual(dp.p_config,
+				    wp.p_config) &&
+				    dp.p_machine == wp.p_machine) {
+					break;
+				}
+			}
+
+			if (i == dprobes.length) {
+				rv.probeAdd(wp);
+				return;
+			}
+
+			/*
+			 * We've found a match, but if it differs in fields we
+			 * would never expect to change, warn the administrator.
+			 */
+			rv.mup_nprobesmatch++;
+			if (wp.p_group_events != dp.p_group_events ||
+			    (dp.p_contacts === null &&
+			    wp.p_contacts !== null) ||
+			    (dp.p_contacts !== null &&
+			    wp.p_contacts === null) ||
+			    (dp.p_contacts !== null &&
+			    !jsprim.deepEqual(dp.p_contacts.slice(0).sort(),
+			    wp.p_contacts.slice(0).sort()))) {
+				rv.mup_warnings.push(new VError('probe group ' +
+				    '"%s" (deployed with uuid "%s"): probe ' +
+				    'for agent "%s": found match that ' +
+				    'differs in "groupEvents" or "contacts"',
+				    pgname, dpg.pg_uuid, agent));
+			}
+
+
+			/*
+			 * Since we've found a match, there's no action to take
+			 * for this probe.  Remove the entry for the deployed
+			 * probe so that we can identify all of the deployed
+			 * probes that weren't wanted by just iterating what's
+			 * left.  This also prevents us from re-using the same
+			 * deployed probe to match multiple wanted probes, but
+			 * that shouldn't be possible anyway.
+			 */
+			if (dprobes.length == 1) {
+				assertplus.equal(i, 0);
+				delete (probesByAgent[agent]);
+			} else {
+				dprobes.splice(i, 1);
+			}
+		});
+
+		/*
+		 * Remove whatever deployed probes did not match any of the
+		 * wanted probes.  We only create each agent's array when we're
+		 * going to add to it, and we delete the array entirely when we
+		 * would remove its last element, so each array we find here
+		 * should be non-empty.
+		 */
+		jsprim.forEachKey(probesByAgent, function (agent, dprobes) {
+			assertplus.ok(dprobes.length > 0);
+			dprobes.forEach(function (p) {
+				rv.probeRemove(p);
+			});
+		});
+	});
+
+	/*
+	 * Now iterate the "deployed" set and remove probes and probe groups
+	 * that are both unwanted and eligible for removal.
+	 */
+	deployed.eachProbeGroup(function iterDProbeGroup(dpg) {
+		var pgname;
+
+		pgname = dpg.pg_name;
+		if (wanted.probeGroupForName(pgname) !== null) {
+			/*
+			 * This group was handled when we iterated the wanted
+			 * probe groups.
+			 */
+			return;
+		}
+
+		if (!metadata.probeGroupIsRemovable(pgname)) {
+			rv.mup_ngroupsignore++;
+			return;
+		}
+
+		rv.groupRemove(dpg);
+		deployed.eachProbeGroupProbe(pgname, function iterDProbe(p) {
+			rv.probeRemove(p);
+		});
+	});
+
+	deployed.eachOrphanProbe(function (p) {
+		rv.mup_nprobesorphan++;
+	});
+
+	return (rv);
+}
+
+/*
+ * Given information about deployed VMs and CNs and the local metadata about
+ * which probes are to be deployed to which types of components, construct a
+ * MantaAmonConfig that represents the desired set of Amon configuration.
+ */
+function amonGenerateWanted(args)
+{
+	var contactsBySeverity, wanted, errors, error;
+
+	assertplus.object(args, 'args');
+	assertplus.string(args.account, 'args.account');
+	assertplus.object(args.contactsBySeverity, 'args.contactsBySeverity');
+	assertplus.object(args.instances, 'args.instances');
+	assertplus.object(args.instancesBySvc, 'args.instancesBySvc');
+	assertplus.object(args.metadata, 'args.metadata');
+
+	contactsBySeverity = args.contactsBySeverity;
+	wanted = new alarm_config.MantaAmonConfig();
+	errors = [];
+
+	args.metadata.eachTemplate(function iterMetadataEvent(pt) {
+		var sev;
+
+		sev = pt.pt_ka.ka_severity;
+		if (!contactsBySeverity.hasOwnProperty(sev)) {
+			/*
+			 * Since we construct contactsBySeverity in lib/adm.js,
+			 * it's a bug either there or in metadata validation if
+			 * we encounter a probe template with an unknown
+			 * severity level.
+			 */
+			throw (new VError(
+			    'no contacts defined by caller for alarms with ' +
+			    'severity level "%s" (used in %s)', sev,
+			    pt.pt_origin_label));
+		}
+
+		amonGenerateWantedTemplate({
+		    'account': args.account,
+		    'contacts': contactsBySeverity[sev].slice(0),
+		    'instances': args.instances,
+		    'instancesBySvc': args.instancesBySvc,
+		    'wanted': wanted,
+		    'probeTemplate': pt,
+		    'errors': errors
+		});
+	});
+
+	error = common.errorForList(errors);
+	return (error !== null ? error : wanted);
+}
+
+function amonGenerateWantedTemplate(args)
+{
+	var events, eventForSvc;
+	var instances, instancesBySvc, pt, wanted, errors;
+
+	assertplus.object(args, 'args');
+	assertplus.string(args.account, 'args.account');
+	assertplus.arrayOfString(args.contacts, 'args.contacts');
+	assertplus.object(args.instances, 'args.instances');
+	assertplus.object(args.instancesBySvc, 'args.instancesBySvc');
+	assertplus.object(args.wanted, 'args.wanted');
+	assertplus.ok(args.wanted instanceof alarm_config.MantaAmonConfig);
+	assertplus.object(args.probeTemplate, 'args.probeTemplate');
+	assertplus.arrayOfObject(args.errors, 'args.errors');
+
+	instances = args.instances;
+	instancesBySvc = args.instancesBySvc;
+	pt = args.probeTemplate;
+	wanted = args.wanted;
+	errors = args.errors;
+
+	eventForSvc = {};
+	if (pt.pt_scope.ptsc_service == 'each') {
+		assertplus.ok(pt.pt_aliases.length > 0);
+		events = [];
+		pt.pt_aliases.forEach(function (alias) {
+			events.push(alias.pta_event);
+			eventForSvc[alias.pta_service] =
+			    alias.pta_event;
+		});
+	} else if (pt.pt_scope.ptsc_service == 'all') {
+		assertplus.ok(pt.pt_aliases.length === 0);
+		events = [ pt.pt_event ];
+		jsprim.forEachKey(instancesBySvc, function (svcname) {
+			if (services.serviceSupportsProbes(svcname)) {
+				eventForSvc[svcname] = pt.pt_event;
+			}
+		});
+	} else if (pt.pt_scope.ptsc_check_from !== null) {
+		assertplus.ok(pt.pt_aliases.length === 0);
+		events = [ pt.pt_event ];
+		eventForSvc[pt.pt_scope.ptsc_check_from] = pt.pt_event;
+	} else {
+		assertplus.ok(pt.pt_aliases.length === 0);
+		events = [ pt.pt_event ];
+		eventForSvc[pt.pt_scope.ptsc_service] = pt.pt_event;
+	}
+
+	events.forEach(function (eventName) {
+		var pgname, error;
+		pgname = alarm_metadata.probeGroupNameForTemplate(
+		    pt, eventName);
+
+		/*
+		 * Undeployed probe groups have no uuid yet.  However, it's
+		 * useful for other code to be able to organize data structures
+		 * by probe group id.  (They could use probe group names, but
+		 * it's not required that probe groups have names, even though
+		 * we do require that for our own groups.  Additionally,
+		 * sometimes it's useful to organize by probe group uuid even
+		 * before a name is known, as when enumerating a bunch of
+		 * probes, some of which may even belong to probe groups that no
+		 * longer exist, and all we have about them is the uuid.)
+		 *
+		 * So it's perhaps dicey, but we just fake up a uuid that
+		 * matches the name.  We guarantee elsewhere that probe group
+		 * names are unique among our own probe groups.
+		 */
+		error = wanted.addProbeGroup({
+		    'uuid': pgname,
+		    'name': pgname,
+		    'user': args.account,
+		    'contacts': args.contacts,
+		    'disabled': false
+		});
+
+		/*
+		 * The only reasons that addProbeGroup can fail are because we
+		 * constructed an invalid probe group or one with a duplicate
+		 * uuid or name.  That would indicate a bug in this code.
+		 */
+		if (error !== null) {
+			assertplus.ok(error instanceof Error);
+			throw (error);
+		}
+	});
+
+	jsprim.forEachKey(eventForSvc, function (svcname, eventName) {
+		var targets, checkers, probeargs, gzs;
+
+		if (!instancesBySvc.hasOwnProperty(svcname)) {
+			/*
+			 * We have no locally deployed zones for whatever
+			 * service we would deploy these probes.  This is likely
+			 * to happen if someone is deploying probes in a
+			 * partially-deployed Manta, or if this is a
+			 * multi-datacenter deployment where some services are
+			 * only in a subset of datacenters.  There's nothing
+			 * wrong with this; we just have no probes to deploy
+			 * here.
+			 */
+			return;
+		}
+
+		checkers = instancesBySvc[svcname];
+
+		if (pt.pt_scope.ptsc_global) {
+			/*
+			 * If "global" was specified on the scope, then this
+			 * probe targets not the zones for the specified
+			 * service, but all global zones where this service
+			 * runs.  There may be more than one instance on each
+			 * CN, so we need to deduplicate this list.
+			 */
+			gzs = {};
+			checkers.forEach(function (c) {
+				assertplus.ok(instances.hasOwnProperty(c));
+				assertplus.ok(instances[c].inst_local);
+				assertplus.string(
+				    instances[c].inst_server_uuid);
+				gzs[instances[c].inst_server_uuid] = true;
+			});
+			checkers = Object.keys(gzs);
+		}
+
+		if (pt.pt_scope.ptsc_check_from !== null) {
+			if (!instancesBySvc.hasOwnProperty(
+			    pt.pt_scope.ptsc_check_from)) {
+				return;
+			}
+
+			targets = instancesBySvc[pt.pt_scope.ptsc_service];
+			probeargs = [];
+			checkers.forEach(function (c) {
+				targets.forEach(function (t) {
+					/*
+					 * We might expect the machine to be the
+					 * "target" here, but amon does not
+					 * allow that for probes of type "cmd",
+					 * and it's not all that meaningful here
+					 * anyway.
+					 */
+					probeargs.push({
+					    'agent': c,
+					    'machine': c,
+					    'target': t
+					});
+				});
+			});
+		} else {
+			probeargs = checkers.map(function (c) {
+				return ({
+				    'agent': c,
+				    'machine': c,
+				    'target': c
+				});
+			});
+		}
+
+		probeargs.forEach(function (p) {
+			pt.pt_checks.forEach(function (check, i) {
+				var conf, probe, label, md, error;
+
+				conf = jsprim.deepCopy(check.ptc_config);
+				probe = {
+				    'name': eventName + i,
+				    'type': check.ptc_type,
+				    'config': conf,
+				    'agent': p.agent,
+				    'machine': p.machine,
+				    'group': alarm_metadata.
+				        probeGroupNameForTemplate(
+				        pt, eventName),
+				    'groupEvents': true
+				};
+
+				/*
+				 * Augment probe configurations with information
+				 * from SAPI metadata.
+				 */
+				label = sprintf('probe for group "%s", ' +
+				    'check %d, machine "%s"', probe.group,
+				    i + 1, p.machine);
+				md = instances.hasOwnProperty(p.target) ?
+				    instances[p.target].inst_metadata : null;
+				amonProbePopulateAutoEnv(label, probe, md,
+				    errors);
+
+				/*
+				 * As with the call to addProbeGroup() above,
+				 * the only reasons this can fail would be
+				 * because of bugs in this code.
+				 */
+				error = wanted.addProbe(probe);
+				if (error !== null) {
+					assertplus.ok(error instanceof Error);
+					throw (error);
+				}
+			});
+		});
+	});
+}
+
+/*
+ * For probes of type "cmd", we support a special configuration property called
+ * "autoEnv".  The value of this property is a list of variable names.  We
+ * populate the probe's shell environment with corresponding values from the
+ * corresponding instance's SAPI metadata.
+ */
+function amonProbePopulateAutoEnv(label, probe, metadata, errors)
+{
+	var vars;
+
+	if (probe.type != 'cmd' ||
+	    !probe.config.hasOwnProperty('autoEnv')) {
+		return;
+	}
+
+	/*
+	 * Remove the autoEnv property itself since Amon doesn't know anything
+	 * about that.
+	 */
+	vars = probe.config.autoEnv;
+	delete (probe.config.autoEnv);
+	if (vars.length === 0) {
+		return;
+	}
+	if (!probe.config.env) {
+		probe.config.env = {};
+	}
+
+	if (metadata === null) {
+		errors.push(new VError('%s: "autoEnv" specified but no ' +
+		    'metadata found for instance', label));
+		return;
+	}
+
+	vars.forEach(function (v) {
+		if (!metadata.hasOwnProperty(v)) {
+			errors.push(new VError('%s: "autoEnv" variable "%s": ' +
+			    'metadata value not found', label, v));
+			return;
+		}
+
+		if (typeof (metadata[v]) != 'string') {
+			errors.push(new VError('%s: "autoEnv" variable "%s": ' +
+			    'metadata value is not a string', label, v));
+			return;
+		}
+
+		probe.config.env[v] = metadata[v];
+	});
+}
+
+/*
+ * Flesh out an update plan that should unconfigure all of the probes and probe
+ * groups that we would normally create.
+ *
+ * Named arguments:
+ *
+ *    plan	the update plan to flesh out
+ *
+ *    metadata	an instanceof MantaAmonMetadata
+ */
+function amonUpdatePlanCreateUnconfigure(args)
+{
+	var metadata, plan, wanted, deployed;
+
+	assertplus.object(args, 'args');
+	assertplus.object(args.metadata, 'args.metadata');
+	assertplus.object(args.plan, 'args.plan');
+	assertplus.ok(args.plan instanceof MantaAmonUpdatePlan);
+
+	/*
+	 * Unconfiguring isn't quite as simple as it seems.  We want to remove
+	 * probe groups that we would normally have configured, as well as probe
+	 * groups that we would normally remove (because they were created by
+	 * older versions of the software).  But we want to leave in place any
+	 * probes and probe groups created by an operator.
+	 *
+	 * It would be tempting to just create an empty "wanted" configuration
+	 * and then run through the usual update plan generation process, but
+	 * that process relies on knowing which probe groups are considered
+	 * removable (see probeGroupIsRemovable()), and the definition of that
+	 * differs for this case because our normal probe groups are removable
+	 * when unconfiguring, but not otherwise.
+	 */
+	metadata = args.metadata;
+	plan = args.plan;
+	plan.mup_unconfigure = true;
+	wanted = plan.mup_wanted;
+	deployed = plan.mup_deployed;
+	deployed.eachProbeGroup(function iterDProbeGroup(dpg) {
+		var pgname, wpg;
+
+		pgname = dpg.pg_name;
+		wpg = wanted.probeGroupForName(pgname);
+		if (wpg === null &&
+		    !metadata.probeGroupIsRemovable(pgname)) {
+			plan.mup_ngroupsignore++;
+			return (null);
+		}
+
+		plan.groupRemove(dpg);
+		deployed.eachProbeGroupProbe(pgname, function iterDProbe(p) {
+			plan.probeRemove(p);
+		});
+	});
+}
+
+/*
+ * Print a human-readable summary of an update plan.  Named arguments:
+ *
+ *    plan             The update plan to print
+ *
+ *    stream           Node stream to which to write the summary
+ *
+ *    instances        object mapping instance uuids to InstanceInfo objects
+ *
+ *    cns              set of valid CN uuids in this datacenter
+ *
+ *    metadata         An instance of MantaAmonMetadata, used to translate
+ *                     internal names to more useful titles.
+ *
+ *    verbose          If true, print detailed information about probes changed
+ */
+function amonUpdatePlanSummarize(args)
+{
+	var metadata, out, plan, verbose, instances;
+	var nagents, nprobes, ntotagents, ntotprobes, probes;
+	var ntotbefore, ntotafter, delta;
+	var countsByService = {};
+
+	assertplus.object(args, 'args');
+	assertplus.object(args.stream, 'args.stream');
+	assertplus.object(args.metadata, 'args.metadata');
+	assertplus.object(args.plan, 'args.plan');
+	assertplus.ok(args.plan instanceof MantaAmonUpdatePlan);
+	assertplus.object(args.instances, 'args.instances');
+	assertplus.object(args.cns, 'args.cns');
+	assertplus.bool(args.verbose, 'args.verbose');
+
+	metadata = args.metadata;
+	out = args.stream;
+	plan = args.plan;
+	verbose = args.verbose;
+	instances = args.instances;
+
+	fprintf(out, 'Probe groups to REMOVE: ');
+	if (plan.mup_groups_remove.length === 0) {
+		fprintf(out, 'none\n\n');
+	} else {
+		fprintf(out, '\n\n');
+		fprintf(out, '%7s %7s %s\n', 'NPROBES', 'NAGENTS', 'GROUP');
+		ntotagents = 0;
+		ntotprobes = 0;
+		plan.mup_groups_remove.forEach(function (pg) {
+			assertplus.ok(plan.mup_nremove_bygroup.hasOwnProperty(
+			    pg.pg_uuid));
+			assertplus.ok(!plan.mup_nadd_bygroup.hasOwnProperty(
+			    pg.pg_uuid));
+			nprobes = plan.mup_nremove_bygroup[pg.pg_uuid];
+
+			assertplus.ok(plan.mup_agents_bygroup.hasOwnProperty(
+			    pg.pg_uuid));
+			nagents = Object.keys(plan.mup_agents_bygroup[
+			    pg.pg_uuid]).length;
+
+			fprintf(out, '%7d %7d %s\n',
+			    nprobes, nagents, pg.pg_name);
+
+			ntotprobes += nprobes;
+			ntotagents += nagents;
+		});
+		fprintf(out, '%7d %7d TOTAL\n\n', ntotprobes, ntotagents);
+	}
+
+	fprintf(out, 'Probe groups to ADD: ');
+	if (plan.mup_groups_add.length === 0) {
+		fprintf(out, 'none\n\n');
+	} else {
+		fprintf(out, '\n\n');
+		fprintf(out, '%7s %7s %s\n', 'NPROBES', 'NAGENTS', 'GROUP');
+		ntotagents = 0;
+		ntotprobes = 0;
+		plan.mup_groups_add.forEach(function (pg) {
+			var evt, ka, name;
+
+			assertplus.ok(!plan.mup_nremove_bygroup.hasOwnProperty(
+			    pg.pg_uuid));
+
+			/*
+			 * It's possible that we would create a probe group that
+			 * has no probes.  This likely means there are no
+			 * instances of the zone that this group is associated
+			 * with.  This would usually happen in a multi-DC
+			 * deployment where there happen to be no instances in
+			 * this datacenter.
+			 */
+			if (plan.mup_nadd_bygroup.hasOwnProperty(pg.pg_uuid)) {
+				nprobes = plan.mup_nadd_bygroup[pg.pg_uuid];
+				assertplus.ok(plan.mup_agents_bygroup.
+				    hasOwnProperty(pg.pg_uuid));
+				nagents = Object.keys(plan.mup_agents_bygroup[
+				    pg.pg_uuid]).length;
+			} else {
+				assertplus.ok(!plan.mup_agents_bygroup.
+				    hasOwnProperty(pg.pg_uuid));
+				nprobes = 0;
+				nagents = 0;
+			}
+
+			name = pg.pg_name;
+			evt = metadata.probeGroupEventName(pg.pg_name);
+			if (evt !== null) {
+				ka = metadata.eventKa(evt);
+				if (ka !== null) {
+					name = ka.ka_title;
+				}
+			}
+
+			fprintf(out, '%7d %7d %s\n', nprobes, nagents, name);
+
+			ntotprobes += nprobes;
+			ntotagents += nagents;
+		});
+		fprintf(out, '%7d %7d TOTAL\n\n', ntotprobes, ntotagents);
+	}
+
+	fprintf(out, 'Count of probes by service:\n\n');
+	fprintf(out, '    %-16s  %6s  %6s  %6s\n', 'SERVICE', 'BEFORE', 'AFTER',
+	    'DELTA');
+	services.mSvcNamesProbes.forEach(function (svcname) {
+		countsByService[svcname] = { 'sc_before': 0, 'sc_after': 0 };
+	});
+	countsByService['global zones'] = { 'sc_before': 0, 'sc_after': 0 };
+
+	ntotbefore = amonUpdatePlanSummarizeConfig({
+	    'config': plan.mup_deployed,
+	    'cns': args.cns,
+	    'countsByService': countsByService,
+	    'instances': instances,
+	    'out': out,
+	    'propname': 'sc_before'
+	});
+
+	ntotafter = 0;
+	if (!plan.mup_unconfigure) {
+		ntotafter = amonUpdatePlanSummarizeConfig({
+		    'config': plan.mup_wanted,
+		    'cns': args.cns,
+		    'countsByService': countsByService,
+		    'instances': instances,
+		    'out': out,
+		    'propname': 'sc_after'
+		});
+	}
+
+	jsprim.forEachKey(countsByService, function (svcname, counts) {
+		delta = counts.sc_after - counts.sc_before;
+		fprintf(out, '    %-16s  %6d  %6d  %6s\n', svcname,
+		    counts.sc_before, counts.sc_after,
+		    delta > 0 ? '+' + delta : delta);
+	});
+	delta = ntotafter - ntotbefore;
+	fprintf(out, '    %-16s  %6d  %6d  %6s\n', 'TOTAL',
+	    ntotbefore, ntotafter,
+	    delta > 0 ? '+' + delta : delta);
+	fprintf(out, '\n');
+
+	if (verbose) {
+		fprintf(out, 'Probes to ADD:\n');
+		probes = plan.mup_probes_add.slice(0).sort(function (p1, p2) {
+			var s1, s2, rv;
+
+			s1 = instances[p1.p_agent].inst_svcname;
+			s2 = instances[p2.p_agent].inst_svcname;
+			rv = s1.localeCompare(s2);
+			if (rv !== 0) {
+				return (rv);
+			}
+
+			rv = p1.p_agent.localeCompare(p2.p_agent);
+			if (rv !== 0) {
+				return (rv);
+			}
+
+			/*
+			 * We do not allow our own probes to be nameless.
+			 */
+			assertplus.string(p1.p_name);
+			assertplus.string(p2.p_name);
+			return (p1.p_name.localeCompare(p2.p_name));
+		});
+
+		probes.forEach(function (p) {
+			fprintf(out, '    %s %-16s %s\n', p.p_agent,
+			    instances[p.p_agent].inst_svcname, p.p_name);
+		});
+		fprintf(out, '\n');
+	}
+
+	fprintf(out, 'Summary:\n\n');
+	fprintf(out, '%6d wanted probe groups matched existing groups\n',
+	    plan.mup_ngroupsmatch);
+	fprintf(out, '%6d wanted probes matched existing probes\n',
+	    plan.mup_nprobesmatch);
+	fprintf(out, '%6d probes ignored because they were in no probe group\n',
+	    plan.mup_nprobesorphan);
+	fprintf(out, '%6d probe groups ignored (operator-added)\n',
+	    plan.mup_ngroupsignore);
+	fprintf(out, '%6d total probe groups to remove\n',
+	    plan.mup_groups_remove.length);
+	fprintf(out, '%6d total probes to remove\n',
+	    plan.mup_probes_remove.length);
+	fprintf(out, '%6d total probe groups to add\n',
+	    plan.mup_groups_add.length);
+	fprintf(out, '%6d total probes to add\n', plan.mup_probes_add.length);
+	fprintf(out, '%6d warnings\n\n', plan.mup_warnings.length);
+
+	plan.mup_warnings.forEach(function (w) {
+		fprintf(out, 'warn: %s\n', w.message);
+	});
+}
+
+function amonUpdatePlanSummarizeConfig(args)
+{
+	var config, countsByService, cns, instances, out, propname;
+	var total = 0;
+
+	assertplus.object(args.config, 'args.config');
+	assertplus.object(args.cns, 'args.cns');
+	assertplus.object(args.instances, 'args.instances');
+	assertplus.object(args.countsByService, 'args.countsByService');
+	assertplus.object(args.out, 'args.out');
+	assertplus.string(args.propname, 'args.propname');
+
+	config = args.config;
+	countsByService = args.countsByService;
+	cns = args.cns;
+	instances = args.instances;
+	out = args.out;
+	propname = args.propname;
+
+	config.eachProbeGroup(function (pg) {
+		config.eachProbeGroupProbe(pg.pg_name, function (p) {
+			var agent, svcname;
+
+			assertplus.string(p.p_agent);
+			agent = p.p_agent;
+			if (instances.hasOwnProperty(agent)) {
+				svcname = instances[agent].inst_svcname;
+			} else if (cns.hasOwnProperty(agent)) {
+				svcname = 'global zones';
+			} else {
+				fprintf(out, 'warning: probe "%s": agent ' +
+				    '"%s" is not a known VM or CN',
+				    p.p_uuid, p.p_agent);
+				return;
+			}
+
+			assertplus.ok(countsByService.hasOwnProperty(svcname));
+			countsByService[svcname][propname]++;
+			total++;
+		});
+	});
+
+	return (total);
+}
+
+/*
+ * Apply the changes described by a MantaUpdatePlan.  This removes old probes
+ * and probe groups and creates new ones to replace them.  This operation is not
+ * atomic, and can wind up in basically any intermediate state.  However, the
+ * broader operation (where we construct the update plan and then apply it) is
+ * idempotent.  In the face of only transient errors, this process can be
+ * re-applied to converge to the desired state.
+ */
+function amonUpdatePlanApply(args, callback)
+{
+	var plan, out, au;
+
+	assertplus.object(args, 'args');
+	assertplus.object(args.plan, 'args.plan');
+	assertplus.object(args.amon, 'args.amon');
+	assertplus.number(args.concurrency, 'args.concurrency');
+	assertplus.string(args.account, 'args.account');
+	assertplus.object(args.stream, 'args.stream');
+
+	plan = args.plan;
+	out = args.stream;
+	au = new AmonUpdate(args);
+
+	/*
+	 * We always create probes inside probe groups.  In order to represent
+	 * probes before we've created those probe groups, the "p_groupid"
+	 * property for new probes identifies the name (not uuid) of the group
+	 * they will be in.  (We assume that group names are unique, and this is
+	 * validated elsewhere.)  When we create these probes shortly, we'll
+	 * need to look up the real uuid of the group.  There are two cases:
+	 * either the probe group already exists, in which case we have its uuid
+	 * right now, or the probe group will be created by this process, in
+	 * which case we'll need to record that and use it later.
+	 *
+	 * Here, we collect the names and uuids of probe groups that already
+	 * exist and add them to mau_groups_byname.  As we create new probe
+	 * groups, we'll add their names and uuids to the same structure.  We'll
+	 * consult this when we go create new probes.
+	 */
+	jsprim.forEachKey(au.mau_plan.mup_deployed.mac_probegroups_by_name,
+	    function forEachDeployedProbeGroup(name, group) {
+		au.mau_groups_byname[name] = group.pg_uuid;
+	    });
+
+	/*
+	 * Although Amon may tolerate probes whose groups are missing, we avoid
+	 * creating such a state by processing each of these phases separately.
+	 * Strictly speaking, we only need three phases to do this: remove old
+	 * probes, remove and create probe groups, and create new probes.  It's
+	 * simpler (and not much slower) to split this middle phase.
+	 */
+	fprintf(out, 'Applying changes ... ');
+	vasync.pipeline({
+	    'input': null,
+	    'funcs': [
+		function amonUpdateRemoveProbes(_, subcallback) {
+			amonUpdateQueue(au, plan.mup_probes_remove,
+			    amonUpdateProbeRemove, subcallback);
+		},
+		function amonUpdateRemoveProbeGroups(_, subcallback) {
+			amonUpdateQueue(au, plan.mup_groups_remove,
+			    amonUpdateGroupRemove, subcallback);
+		},
+		function amonUpdateAddProbeGroups(_, subcallback) {
+			amonUpdateQueue(au, plan.mup_groups_add,
+			    amonUpdateGroupAdd, subcallback);
+		},
+		function amonUpdateAddProbes(_, subcallback) {
+			amonUpdateQueue(au, plan.mup_probes_add,
+			    amonUpdateProbeAdd, subcallback);
+		}
+	    ]
+	}, function (err) {
+		fprintf(out, 'done.\n');
+		fprintf(out, 'probes removed: %5d\n', au.mau_nprobes_removed);
+		fprintf(out, 'groups removed: %5d\n', au.mau_ngroups_removed);
+		fprintf(out, 'groups added:   %5d\n', au.mau_ngroups_added);
+		fprintf(out, 'probes added:   %5d\n', au.mau_nprobes_added);
+		callback(err);
+	});
+}
+
+/*
+ * Represents the state associated with a single amon update operation.
+ * This class is used as a struct, with details private to this subsystem.
+ */
+function AmonUpdate(args)
+{
+	assertplus.object(args, 'args');
+	assertplus.object(args.amon, 'args.amon');
+	assertplus.object(args.plan, 'args.plan');
+	assertplus.number(args.concurrency, 'args.concurrency');
+	assertplus.string(args.account, 'args.account');
+
+	this.mau_amon = args.amon;
+	this.mau_concurrency = args.concurrency;
+	this.mau_plan = args.plan;
+	this.mau_account = args.account;
+	this.mau_queues = [];
+	this.mau_errors = [];
+	this.mau_groups_byname = {};
+
+	/* for debugging */
+	this.mau_nprobes_removed = 0;
+	this.mau_ngroups_removed = 0;
+	this.mau_ngroups_added = 0;
+	this.mau_nprobes_added = 0;
+}
+
+/*
+ * Given a worker function, pushes all of the specified inputs through a queue.
+ */
+function amonUpdateQueue(au, tasks, worker, callback)
+{
+	var queue;
+
+	queue = vasync.queuev({
+	    'concurrency': au.mau_concurrency,
+	    'worker': function queueWorker(task, qcallback) {
+		worker(au, task, function onWorkDone(err) {
+			if (err) {
+				au.mau_errors.push(err);
+			}
+
+			qcallback();
+		});
+	    }
+	});
+
+	au.mau_queues.push(queue);
+
+	tasks.forEach(function (t) {
+		queue.push(t);
+	});
+
+	queue.on('end', function () {
+		callback(common.errorForList(au.mau_errors));
+	});
+	queue.close();
+}
+
+function amonUpdateProbeAdd(au, probe, callback)
+{
+	var newprobe;
+
+	/*
+	 * We must not have assigned a uuid by this point, but we only create
+	 * probes that have names.
+	 */
+	assertplus.strictEqual(probe.p_uuid, null);
+	assertplus.notStrictEqual(probe.p_name, null);
+	newprobe = {
+	    'name': probe.p_name,
+	    'type': probe.p_type,
+	    'config': probe.p_config,
+	    'agent': probe.p_agent,
+	    'machine': probe.p_machine || undefined,
+	    'contacts': probe.p_contacts,
+	    'groupEvents': probe.p_group_events
+	};
+
+	/*
+	 * By this point in the process, we must have a name -> uuid mapping for
+	 * the group associated with this probe.
+	 */
+	assertplus.ok(au.mau_groups_byname.hasOwnProperty(probe.p_groupid));
+	newprobe.group = au.mau_groups_byname[probe.p_groupid];
+
+	au.mau_amon.createProbe(au.mau_account, newprobe,
+	    function onAmonProbeAdd(err) {
+		if (err) {
+			err = new VError(err, 'add probe "%s"', probe.p_name);
+		} else {
+			au.mau_nprobes_added++;
+		}
+
+		callback(err);
+	    });
+}
+
+function amonUpdateProbeRemove(au, probe, callback)
+{
+	assertplus.string(probe.p_uuid);
+	au.mau_amon.deleteProbe(au.mau_account, probe.p_uuid,
+	    function onAmonProbeRemove(err) {
+		if (err) {
+			err = new VError(err, 'remove probe "%s"',
+			    probe.p_uuid);
+		} else {
+			au.mau_nprobes_removed++;
+		}
+
+		callback(err);
+	    });
+}
+
+function amonUpdateGroupAdd(au, group, callback)
+{
+	var newgroup;
+
+	/*
+	 * Prior to this point, the uuid matches the name.
+	 */
+	assertplus.strictEqual(group.pg_uuid, group.pg_name);
+	newgroup = {
+	    'name': group.pg_name,
+	    'user': group.pg_user,
+	    'contacts': group.pg_contacts
+	};
+
+	assertplus.ok(!au.mau_groups_byname.hasOwnProperty(group.pg_name));
+	au.mau_amon.createProbeGroup(au.mau_account, newgroup,
+	    function onAmonGroupAdd(err, amongroup) {
+		if (!err && typeof (amongroup.uuid) != 'string') {
+			err = new VError('amon returned group with bad or ' +
+			    'missing uuid');
+		}
+
+		if (!err && amongroup.name != group.pg_name) {
+			err = new VError('amon returned group with a ' +
+			    'different name (uuid "%s")', amongroup.uuid);
+		}
+
+		if (err) {
+			err = new VError(err, 'add group "%s"', group.pg_uuid);
+			callback(err);
+			return;
+		}
+
+		assertplus.ok(!au.mau_groups_byname.hasOwnProperty(
+		    group.pg_name));
+		au.mau_groups_byname[group.pg_name] = amongroup.uuid;
+		au.mau_ngroups_added++;
+		callback(err);
+	    });
+}
+
+function amonUpdateGroupRemove(au, group, callback)
+{
+	assertplus.string(group.pg_uuid);
+	au.mau_amon.deleteProbeGroup(au.mau_account, group.pg_uuid,
+	    function onAmonGroupRemove(err) {
+		if (err) {
+			err = new VError(err, 'remove group "%s"',
+			    group.pg_uuid);
+		} else {
+			au.mau_ngroups_removed++;
+		}
+
+		callback(err);
+	    });
+}
diff --git a/lib/common.js b/lib/common.js
index 2d81d1b..5e37a5b 100644
--- a/lib/common.js
+++ b/lib/common.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright 2017 Joyent, Inc.
+ * Copyright (c) 2017, Joyent, Inc.
  */
 
 /*
@@ -15,9 +15,12 @@
 var assert = require('assert-plus');
 var fs = require('fs');
 var path = require('path');
+var restifyClients = require('restify-clients');
 var sdc = require('sdc-clients');
 var vasync = require('vasync');
-var VError = require('verror').VError;
+var verror = require('verror');
+var VError = verror.VError;
+var MultiError = verror.MultiError;
 
 var exec = require('child_process').exec;
 var sprintf = require('util').format;
@@ -45,6 +48,8 @@ exports.INDEX_SHARDS = 'INDEX_MORAY_SHARDS';
 exports.HASH_RING_IMAGE = 'HASH_RING_IMAGE';
 exports.HASH_RING_IMGAPI_SERVICE = 'HASH_RING_IMGAPI_SERVICE';
 
+exports.CONFIG_FILE_DEFAULT = path.join(__dirname, '..', 'etc', 'config.json');
+
 
 // -- Helper functions
 
@@ -84,22 +89,12 @@ function domainToPath(domain) {
 	return ('/' + domain.split('.').reverse().join('/'));
 }
 
-function initLogger(filename) {
-	var __file = path.basename(filename, '.js');
-	var log = new Logger({
-		name: __file,
-		streams: [
-			{
-				level: 'debug',
-				path: path.join(LOG_DIR, __file + '.log')
-			}
-		],
-		serializers: Logger.stdSerializers
-	});
-
-	return (log);
-}
-
+/*
+ * Note that this function instantiates clients configured according to a config
+ * file relative to the root of this repository.  Some commands accept the
+ * configuration file as a command-line option, but those are not respected
+ * here.  This function should be generalized.
+ */
 function initSdcClients(cb) {
 	var self = this;
 
@@ -153,6 +148,12 @@ function initSdcClients(cb) {
 			agent: false
 		});
 
+		self.AMON_RAW = restifyClients.createJsonClient({
+			log: self.log,
+			url: config.amon.url,
+			agent: false
+		});
+
 		self.UFDS = new sdc.UFDS({
 			log: self.log,
 			url: config.ufds.url,
@@ -196,6 +197,7 @@ function finiSdcClients(cb) {
 	this.NAPI.close();
 	this.SAPI.close();
 	this.AMON.close();
+	this.AMON_RAW.close();
 	this.UFDS.close(cb);
 }
 
@@ -660,9 +662,45 @@ function sortObjectsByProps(rows, comparators)
 	}));
 }
 
+/*
+ * Given an array of errors (usually from several operations run in parallel),
+ * returns a single error describing them.  There are three cases:
+ *
+ *    - if the array is empty, returns null (no error)
+ *    - if the array has one element, returns that element (one error)
+ *    - otherwise, returns a MultiError constructed from the whole array
+ */
+function errorForList(errors)
+{
+	assert.arrayOfObject(errors, 'errors');
+
+	if (errors.length === 0) {
+		return (null);
+	}
+
+	if (errors.length == 1) {
+		return (errors[0]);
+	}
+
+	return (new MultiError(errors));
+}
+
+/*
+ * Iterate an Error or a MultiError.  Given an Error, invokes "func" on the
+ * error.  Given a MultiError, invokes "func" on each of the underlying errors.
+ */
+function errorForEach(err, func)
+{
+	assert.ok(err instanceof Error, 'err must be an Error');
+	if (err instanceof MultiError) {
+		err.errors().forEach(function iterError(e) { func(e); });
+	} else {
+		func(err);
+	}
+}
+
 exports.shuffle = shuffle;
 exports.domainToPath = domainToPath;
-exports.initLogger = initLogger;
 exports.initSdcClients = initSdcClients;
 exports.finiSdcClients = finiSdcClients;
 exports.getMantaApplication = getMantaApplication;
@@ -674,3 +712,5 @@ exports.commandExecute = commandExecute;
 exports.insert = insert;
 exports.stripe = stripe;
 exports.sortObjectsByProps = sortObjectsByProps;
+exports.errorForList = errorForList;
+exports.errorForEach = errorForEach;
diff --git a/lib/services.js b/lib/services.js
index 06cd613..64bf761 100644
--- a/lib/services.js
+++ b/lib/services.js
@@ -27,12 +27,13 @@ exports.ServiceConfiguration = ServiceConfiguration;
 exports.serviceNameIsValid = serviceNameIsValid;
 exports.serviceIsSharded = serviceIsSharded;
 exports.serviceSupportsOneach = serviceSupportsOneach;
+exports.serviceSupportsProbes = serviceSupportsProbes;
 exports.serviceConfigProperties = serviceConfigProperties;
 
 /*
  * Service names deployed by default, in the order they get deployed.  This must
  * be kept in sync with mSvcConfigs below and the various parameters in
- * lib/layout.js
+ * lib/layout.js.
  */
 var mSvcNames = [
     'nameservice',
@@ -57,25 +58,26 @@ var mSvcNames = [
  * manta-oneach to be used with "marlin" zones.
  */
 var mSvcConfigs = {
-    'nameservice':	{ 'oneach': true,  'sharded': false },
-    'postgres':		{ 'oneach': true,  'sharded': true  },
-    'moray':		{ 'oneach': true,  'sharded': true  },
-    'electric-moray':	{ 'oneach': true,  'sharded': false },
-    'storage':		{ 'oneach': true,  'sharded': false },
-    'authcache':	{ 'oneach': true,  'sharded': false },
-    'webapi':		{ 'oneach': true,  'sharded': false },
-    'loadbalancer':	{ 'oneach': true,  'sharded': false },
-    'jobsupervisor':	{ 'oneach': true,  'sharded': false },
-    'jobpuller':	{ 'oneach': true,  'sharded': false },
-    'medusa':		{ 'oneach': true,  'sharded': false },
-    'ops':		{ 'oneach': true,  'sharded': false },
-    'madtom':		{ 'oneach': true,  'sharded': false },
-    'marlin-dashboard':	{ 'oneach': true,  'sharded': false },
-    'marlin':		{ 'oneach': false, 'sharded': false },
-    'propeller':	{ 'oneach': true,  'sharded': false }
+    'nameservice':	{ 'oneach': true,  'probes': true,  'sharded': false },
+    'postgres':		{ 'oneach': true,  'probes': true,  'sharded': true  },
+    'moray':		{ 'oneach': true,  'probes': true,  'sharded': true  },
+    'electric-moray':	{ 'oneach': true,  'probes': true,  'sharded': false },
+    'storage':		{ 'oneach': true,  'probes': true,  'sharded': false },
+    'authcache':	{ 'oneach': true,  'probes': true,  'sharded': false },
+    'webapi':		{ 'oneach': true,  'probes': true,  'sharded': false },
+    'loadbalancer':	{ 'oneach': true,  'probes': true,  'sharded': false },
+    'jobsupervisor':	{ 'oneach': true,  'probes': true,  'sharded': false },
+    'jobpuller':	{ 'oneach': true,  'probes': true,  'sharded': false },
+    'medusa':		{ 'oneach': true,  'probes': true,  'sharded': false },
+    'ops':		{ 'oneach': true,  'probes': true,  'sharded': false },
+    'madtom':		{ 'oneach': true,  'probes': true,  'sharded': false },
+    'marlin-dashboard':	{ 'oneach': true,  'probes': true,  'sharded': false },
+    'marlin':		{ 'oneach': false, 'probes': false, 'sharded': false },
+    'propeller':	{ 'oneach': true,  'probes': false, 'sharded': false }
 };
 
 exports.mSvcNames = mSvcNames;
+exports.mSvcNamesProbes = mSvcNames.filter(serviceSupportsProbes);
 
 /*
  * This is exposed for testing only!  There are functional interfaces for
@@ -281,3 +283,12 @@ function serviceSupportsOneach(svcname)
 	assertplus.ok(serviceNameIsValid(svcname));
 	return (mSvcConfigs[svcname].oneach);
 }
+
+/*
+ * Returns true if the given service can support Amon probes.
+ */
+function serviceSupportsProbes(svcname)
+{
+	assertplus.ok(serviceNameIsValid(svcname));
+	return (mSvcConfigs[svcname].probes);
+}
diff --git a/man/man1/manta-adm.1 b/man/man1/manta-adm.1
index 41a1375..01e392c 100644
--- a/man/man1/manta-adm.1
+++ b/man/man1/manta-adm.1
@@ -1,9 +1,11 @@
-.TH MANTA\-ADM 1 "2016" Manta "Manta Operator Commands"
+.TH MANTA\-ADM 1 "2017" Manta "Manta Operator Commands"
 .SH NAME
 .PP
 manta\-adm \- administer a Manta deployment
 .SH SYNOPSIS
 .PP
+\fB\fCmanta\-adm alarm SUBCOMMAND... [OPTIONS...]\fR
+.PP
 \fB\fCmanta\-adm cn [\-l LOG_FILE] [\-H] [\-o FIELD...] [\-n] [\-s] CN_FILTER\fR
 .PP
 \fB\fCmanta\-adm genconfig "lab" | "coal"\fR
@@ -26,6 +28,9 @@ deployment.  This command only operates on zones within the same datacenter.
 The command may need to be repeated in other datacenters in order to execute it
 across an entire Manta deployment.
 .TP
+\fB\fCmanta\-adm alarm\fR
+List and configure amon\-based alarms for Manta.
+.TP
 \fB\fCmanta\-adm cn\fR
 Show information about Manta servers in this DC.
 .TP
@@ -121,6 +126,14 @@ Many commands also accept:
 Emit verbose log to LOGFILE.  The special string "stdout" causes output to be
 emitted to the program's stdout.
 .PP
+Commands that make changes support:
+.TP
+\fB\fC\-n, \-\-dryrun\fR
+Print what changes would be made without actually making them.
+.TP
+\fB\fC\-y, \-\-confirm\fR
+Bypass all confirmation prompts.
+.PP
 \fBImportant note for programmatic users:\fP Except as noted below, the output
 format for this command is subject to change at any time. The only subcommands
 whose output is considered committed are:
@@ -133,10 +146,187 @@ whose output is considered committed are:
 \fB\fCmanta\-adm zk list\fR, only when used with the "\-o" option
 .RE
 .PP
-The output for any other commands may change at any time. Documented
+The output for any other commands may change at any time.  The \fB\fCmanta\-adm alarm\fR
+subcommand is still considered an experimental interface.  All other documented
 subcommands, options, and arguments are committed, and you can use the exit
-status of the program to determine success of failure.
+status of the program to determine success or failure.
 .SH SUBCOMMANDS
+.SS "alarm" subcommand
+.PP
+\fB\fCmanta\-adm alarm close ALARM_ID...\fR
+.PP
+\fB\fCmanta\-adm alarm config probegroups list [\-H] [\-o FIELD...]\fR
+.PP
+\fB\fCmanta\-adm alarm config show\fR
+.PP
+\fB\fCmanta\-adm alarm config update [\-n] [\-y] [\-\-unconfigure]\fR
+.PP
+\fB\fCmanta\-adm alarm config verify [\-\-unconfigure]\fR
+.PP
+\fB\fCmanta\-adm alarm details ALARM_ID...\fR
+.PP
+\fB\fCmanta\-adm alarm faults ALARM_ID...\fR
+.PP
+\fB\fCmanta\-adm alarm list [\-H] [\-o FIELD...]\fR
+.PP
+\fB\fCmanta\-adm alarm metadata events\fR
+.PP
+\fB\fCmanta\-adm alarm metadata ka [EVENT_NAME...]\fR
+.PP
+\fB\fCmanta\-adm alarm notify on|off ALARM_ID...\fR
+.PP
+\fB\fCmanta\-adm alarm show\fR
+.PP
+The \fB\fCmanta\-adm alarm\fR subcommand provides several tools that allow operators to:
+.RS
+.IP \(bu 2
+view and configure amon probes and probe groups (\fB\fCconfig\fR subcommand)
+.IP \(bu 2
+view open alarms (\fB\fCshow\fR, \fB\fClist\fR, \fB\fCdetails\fR, and \fB\fCfaults\fR subcommands)
+.IP \(bu 2
+configure notifications for open alarms (\fB\fCnotify\fR subcommand)
+.IP \(bu 2
+view local metadata about alarms and probes (\fB\fCmetadata\fR subcommand)
+.RE
+.PP
+The primary commands for working with alarms are:
+.RS
+.IP \(bu 2
+\fB\fCmanta\-adm alarm config update\fR: typically used during initial deployment and
+after other deployment operations to ensure that the right set of probes and
+probe groups are configured for the deployed components
+.IP \(bu 2
+\fB\fCmanta\-adm alarm show\fR: summarize open alarms
+.IP \(bu 2
+\fB\fCmanta\-adm alarm details ALARM_ID...\fR: report detailed information (including
+suggested actions) for the specified alarms
+.IP \(bu 2
+\fB\fCmanta\-adm alarm close ALARM_ID...\fR: close open alarms, indicating that they
+no longer represent issues
+.RE
+.PP
+For background about Amon itself, probes, probegroups, and alarms, see the
+Triton Amon reference documentation.
+.PP
+As with other subcommands, this command only operates on the current Triton
+datacenter.  In multi\-datacenter deployments, alarms are managed separately in
+each datacenter.
+.PP
+Some of the following subcommands can operate on many alarms.  These subcommands
+exit failure if they fail for any of the specified alarms, but the operation may
+have completed successfully for other alarms.  For example, closing 3 alarms is
+not atomic.  If the operation fails, then 1, 2, or 3 alarms may still be open.
+.PP
+\fB\fCmanta\-adm alarm close ALARM_ID...\fR
+.PP
+Close the specified alarms.  These alarms will no longer show up in the
+\fB\fCmanta\-adm alarm list\fR or \fB\fCmanta\-adm alarm show\fR output.  Amon purges closed
+alarms completely after some period of time.
+.PP
+If the underlying issue that caused an alarm is not actually resolved, then a
+new alarm may be opened for the same issue.  In some cases, that can happen
+almost immediately.  In other cases, it may take many hours for the problem to
+resurface.  In the case of transient issues, a new alarm may not open again
+until the issue occurs again, which could be days, weeks, or months later.  That
+does not mean the underlying issue was actually resolved.
+.PP
+\fB\fCmanta\-adm alarm config probegroups list [\-H] [\-o FIELD...]\fR
+.PP
+List configured probe groups in tabular form.  This is primarily useful in
+debugging unexpected behavior from the alarms themselves.  The \fB\fCmanta\-adm alarm
+config show\fR command provides a more useful summary of the probe groups that are
+configured.
+.PP
+\fB\fCmanta\-adm alarm config show\fR
+.PP
+Shows summary information about the probes and probe groups that are configured.
+This is not generally necessary but it can be useful to verify that probes are
+configured as expected.
+.PP
+\fB\fCmanta\-adm alarm config update [\-n] [\-y] [\-\-unconfigure]\fR
+.PP
+Examines the Manta components that are deployed and the alarm configuration
+(specifically, the probes and probe groups deployed to monitor those components)
+and compares them with the expected configuration.  If these do not match,
+prints out a summary of proposed changes to the configuration and optionally
+applies those changes.
+.PP
+If \fB\fC\-\-unconfigure\fR is specified, then the tool removes all probes and probe
+groups.
+.PP
+This is the primary tool for updating the set of deployed probes and probe
+groups.  Operators would typically use this command:
+.RS
+.IP \(bu 2
+during initial deployment to deploy probes and probe groups
+.IP \(bu 2
+after deploying (or undeploying) any Manta components to deploy (or remove)
+probes related to the affected components
+.IP \(bu 2
+after updating the \fB\fCmanta\-adm\fR tool itself, which bundles the probe
+definitions, to deploy any new or updated probes
+.IP \(bu 2
+at any time to verify that the configuration matches what's expected
+.RE
+.PP
+This operation is idempotent.
+.PP
+This command supports the \fB\fC\-n/\-\-dryrun\fR and \fB\fC\-y/\-\-confirm\fR options described
+above.
+.PP
+\fB\fCmanta\-adm alarm config verify [\-\-unconfigure]\fR
+.PP
+Behaves exactly like \fB\fCmanta\-adm alarm config update \-\-dryrun\fR\&.
+.PP
+\fB\fCmanta\-adm alarm details ALARM_ID...\fR
+.PP
+Prints detailed information about any number of alarms.  The detailed
+information includes the time the alarm was opened, the last time an event was
+associated with this alarm, the total number of events associated with the
+alarm, the affected components, and information about the severity, automated
+response, and suggested actions for this issue.
+.PP
+\fB\fCmanta\-adm alarm faults ALARM_ID...\fR
+.PP
+Prints detailed information about the events (faults) associated with any number
+of alarms.  The specific information provided depends on the alarm.  If the
+alarm related to a failed health check command, then the exit status,
+terminating signal, stdout, and stderr of the command are provided.  If the
+alarm relates to an error log entry, the contents of the log entry are provided.
+There can be many faults associated with a single alarm, though not all of them
+are reported by this command.
+.PP
+\fB\fCmanta\-adm alarm list [\-H] [\-o FIELD...]\fR
+.PP
+Lists open alarms in tabular form.  See also the \fB\fCmanta\-adm alarm show\fR command.
+.PP
+\fB\fCmanta\-adm alarm metadata events\fR
+.PP
+List the names for all of the events known to this version of \fB\fCmanta\-adm\fR\&.  Each
+event corresponds to a distinct kind of problem.  For details about each one,
+see \fB\fCmanta\-adm alarm metadata ka\fR\&.  The list of events comes from metadata
+bundled with the \fB\fCmanta\-adm\fR tool.
+.PP
+\fB\fCmanta\-adm alarm metadata ka [EVENT_NAME...]\fR
+.PP
+Print out knowledge articles about each of the specified events.  This
+information comes from metadata bundled with the \fB\fCmanta\-adm\fR tool.  If no events
+are specified, prints out knowledge articles about all events.
+.PP
+Knowledge articles include information about the severity of the problem, the
+impact, the automated response, and the suggested action.
+.PP
+\fB\fCmanta\-adm alarm notify on|off ALARM_ID...\fR
+.PP
+Enable or disable notifications for the specified alarms.  Notifications are
+generally configured through Amon, which supports both email and XMPP
+notification for new alarms and new events on existing, open alarms.  This
+command controls whether notifications are enabled for the specified alarms.
+.PP
+\fB\fCmanta\-adm alarm show\fR
+.PP
+Summarize open alarms.  For each alarm, use the \fB\fCmanta\-adm alarm details\fR
+subcommand to view more information about it.
 .SS "cn" subcommand
 .PP
 \fB\fCmanta\-adm cn [\-l LOG_FILE] [\-H] [\-o FIELD...] [\-n] [\-s] [CN_FILTER]\fR
@@ -550,15 +740,8 @@ See above for information about the \fB\fC\-l\fR, \fB\fC\-H\fR, and \fB\fC\-o\fR
 ordinal number of each server), "datacenter", "zoneabbr", "zonename", "ip", and
 "port".
 .PP
-The \fB\fCmanta\-adm zk fixup\fR command supports options:
-.TP
-\fB\fC\-n, \-\-dryrun\fR
-Print what changes would be made without actually making them.
-.TP
-\fB\fC\-y, \-\-confirm\fR
-Bypass all confirmation prompts.
-.PP
-It also supports the \fB\fC\-l/\-\-log_file\fR option described above.
+The \fB\fCmanta\-adm zk fixup\fR command supports the \fB\fC\-l/\-\-log_file\fR, \fB\fC\-n/\-\-dryrun\fR,
+and \fB\fC\-y/\-\-confirm\fR options described above.
 .SH EXIT STATUS
 .TP
 \fB\fC0\fR
@@ -571,7 +754,7 @@ Generic failure.
 The command\-line options were not valid.
 .SH COPYRIGHT
 .PP
-Copyright (c) 2016 Joyent Inc.
+Copyright (c) 2017 Joyent Inc.
 .SH SEE ALSO
 .PP
 .BR json (1), 
diff --git a/package.json b/package.json
index 9a7e894..0f559b9 100644
--- a/package.json
+++ b/package.json
@@ -16,19 +16,23 @@
         "forkexec": "0.1.0",
         "hogan.js": "2.0.0",
         "imgapi-cli": "git+https://github.com/joyent/sdc-imgapi-cli.git#65bba66818",
-        "jsprim": "0.5.1",
+        "jsprim": "^1.4.0",
+        "js-yaml": "3.8.2",
         "mantamon": "git+https://github.com/joyent/mantamon.git#master",
         "node-uuid": "1.4.0",
         "optimist": "0.3.5",
         "once": "1.3.0",
         "posix-getopt": "1.2.0",
+        "progbar": "1.1.1",
         "readable-stream": "1.0.26",
+	"restify-clients": "1.5.0",
         "sdc-clients": "git+https://github.com/joyent/node-sdc-clients.git#42ea07d3ca",
         "sprintf-js": "0.0.7",
         "tab": "0.1.0",
         "urclient": "1.0.0",
         "vasync": "1.6.3",
-        "verror": "1.3.7",
+        "verror": "1.9.0",
+        "wordwrap": "1.0.0",
         "zonename": "1.1.0"
     }
 }
diff --git a/tools/probecfgchk.js b/tools/probecfgchk.js
new file mode 100755
index 0000000..067fee9
--- /dev/null
+++ b/tools/probecfgchk.js
@@ -0,0 +1,64 @@
+#!/usr/bin/env node
+
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+/*
+ * probecfgchk: validates a given probe configuration file
+ */
+
+var cmdutil = require('cmdutil');
+var vasync = require('vasync');
+var VError = require('verror');
+
+var alarm_metadata = require('../lib/alarms/metadata');
+var nerrors = 0;
+
+function main()
+{
+	cmdutil.configure({
+	    'synopses': [ 'FILENAME...' ],
+	    'usageMessage': 'validates one or more probe template files'
+	});
+
+	if (process.argv.length < 3) {
+		cmdutil.usage();
+	}
+
+	vasync.forEachPipeline({
+	    'func': validateOneFile,
+	    'inputs': process.argv.slice(2)
+	}, function () {
+		process.exit(nerrors === 0 ? 0 : 1);
+	});
+}
+
+function validateOneFile(filename, callback)
+{
+	var pts;
+
+	pts = new alarm_metadata.MetadataLoader();
+	pts.loadFromFile(filename, function onLoaded() {
+		var errors;
+
+		errors = pts.errors();
+		nerrors += errors.length;
+
+		if (errors.length === 0) {
+			console.error('%s okay', filename);
+		} else {
+			errors.forEach(function (e) { cmdutil.warn(e); });
+		}
+
+		callback();
+	});
+}
+
+main();
