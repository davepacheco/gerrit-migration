commit 2e6bbff7a52fb0a4ec3f58ff77bc8c1dcd6d00c7 (refs/changes/36/3936/5)
Author: Kody A Kantor <kody@kkantor.com>
Date:   2018-05-29T21:33:58+00:00 (1 year, 4 months ago)
    
    joyent/pgstatsmon#10 connection count query isn't always accurate
    joyent/pgstatsmon#11 create scripts to get basic stats from pgstatsmon
    joyent/pgstatsmon#12 postgres peers could report their own WAL positions

diff --git a/CHANGES.md b/CHANGES.md
index a6f9936..7881ffb 100644
--- a/CHANGES.md
+++ b/CHANGES.md
@@ -1,6 +1,12 @@
 # pgstatsmon Changelog
 
 ## Not yet released.
+None
+
+## 1.0.0
+* #12 postgres peers could report their own WAL positions
+* #11 create scripts to get basic stats from pgstatsmon
+* #10 connection count query isn't always accurate
 * #8 create a Postgres user for pgstatsmon
 * #7 add 'release' and 'publish' targets
 * #5 add and improve queries
diff --git a/Makefile b/Makefile
index f04b799..a9f3a92 100644
--- a/Makefile
+++ b/Makefile
@@ -84,8 +84,12 @@ SAPI_MANIFESTS_DIRS	= $(SAPI_MANIFESTS:%=$(PREFIX)/sapi_manifests/%)
 SMF_MANIFEST		= pgstatsmon
 SMF_MANIFEST_DIR	= $(PREFIX)/smf/manifests
 
+DTRACE_SCRIPTS		= backendstat.d querystat.d walstat.d
+DTRACE_SCRIPTS_DIR	= $(PREFIX)/bin/dtrace
+
 INSTALL_FILES	= $(addprefix $(PROTO), \
 		  $(PREFIX)/bin/pgstatsmon.js \
+		  $(DTRACE_SCRIPTS:%=$(DTRACE_SCRIPTS_DIR)/%) \
 		  $(BOOT_SCRIPTS:%=$(BOOT_SCRIPTS_DIR)/%) \
 		  $(SCRIPTS:%=$(SCRIPTS_DIR)/%) \
 		  $(LIB_FILES:%=$(PREFIX)/lib/%) \
@@ -100,6 +104,7 @@ INSTALL_FILES	= $(addprefix $(PROTO), \
 
 INSTALL_DIRS	= $(addprefix $(PROTO), \
 		  $(PREFIX)/bin \
+		  $(PREFIX)/bin/dtrace \
 		  $(PREFIX)/lib \
 		  $(PREFIX)/etc \
 		  $(SCRIPTS_DIR) \
@@ -165,9 +170,12 @@ $(GUARD):
 $(INSTALL_DIRS):
 	mkdir -p $@
 
-$(PROTO)$(PREFIX)/bin/%: bin/% | $(INSTALL_DIRS)
+$(PROTO)$(PREFIX)/bin/%.js: bin/%.js | $(INSTALL_DIRS)
 	$(INSTALL_FILE)
 
+$(PROTO)$(DTRACE_SCRIPTS_DIR)/%.d: bin/dtrace/%.d | $(INSTALL_DIRS)
+	$(INSTALL_EXEC)
+
 $(PROTO)$(PREFIX)/lib/%: lib/% | $(INSTALL_DIRS)
 	$(INSTALL_FILE)
 
diff --git a/README.md b/README.md
index 7d324fa..aacc88d 100644
--- a/README.md
+++ b/README.md
@@ -195,6 +195,14 @@ the [lib/dtrace.js](./lib/dtrace.js) file.
 [node-artedi](https://github.com/joyent/node-artedi), which pgstatsmon uses to
 perform aggregation and serialize metrics, also exposes DTrace probes.
 
+There are dtrace scripts bundled with this repository in the `bin/dtrace`
+directory:
+
+* backendstat.d reports latency and errors for all of pgstatsmon's backends
+* querystat.d reports latency and errors for a given backend's queries
+* walstat.d reports WAL throughput stats for all of pgstatsmon's backends
+
+
 ## License
 MPL-v2. See the LICENSE file.
 
diff --git a/bin/dtrace/backendstat.d b/bin/dtrace/backendstat.d
new file mode 100755
index 0000000..cf1eac9
--- /dev/null
+++ b/bin/dtrace/backendstat.d
@@ -0,0 +1,125 @@
+#!/usr/sbin/dtrace -Cs
+
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ *
+ * Copyright (c) 2018, Joyent, Inc.
+ */
+
+/*
+ * backendstat.d - print stats from pgstatsmon broken down by backend.
+ *
+ * This script prints information about the backends that pgstatsmon is
+ * scraping metrics from. This is probably a good first step to finding what's
+ * causing ticks to slow down. From here you can dive deeper using querystat.d
+ * to determine which queries are slowest and causing errors.
+ *
+ * Arguments: none
+ *
+ * Fields:
+ * BACKEND - name or IP and port combination of Postgres backend
+ * LAT     - scrape duration from start to finish in milliseconds
+ * QTIM    - queries timed out in the last tick
+ * QERR    - query errors in the last tick
+ * CERR    - connection errors since the last tick
+ * NaN     - bogus values received over the wire
+ *
+ * The last line of output should have a backend named 'tick.' This represents
+ * the end-to-end latency from querying _every_ backend. Backends are queried
+ * with some parallelism, so the sum of the LAT column won't equal the LAT
+ * value for 'tick.'
+ *
+ * If any of the values to the right of LAT are non-zero, that indicates there's
+ * a problem somewhere.
+ *
+ * This script prints refreshed stats every tick. Output is sorted by LAT.
+ *
+ */
+
+#pragma D option quiet
+#pragma D option zdefs
+
+/* track end-to-end tick latency */
+pgstatsmon*:::tick-start
+{
+	self->startts["tick"] = timestamp;
+}
+
+pgstatsmon*:::tick-done
+/self->startts["tick"]/
+{
+	@tick_lat["tick"] = sum(
+	    (timestamp - self->startts["tick"]) / (1000000)
+	);
+}
+
+/* track latency of individual backends */
+pgstatsmon*:::backend-start
+{
+	self->bestartts[copyinstr(arg0)] = timestamp;
+}
+
+pgstatsmon*:::backend-done
+/self->bestartts[copyinstr(arg0)]/
+{
+	backend = copyinstr(arg0);
+	@backend_lat[backend] = sum(
+	    (timestamp - self->bestartts[backend]) / (1000000)
+	);
+}
+
+/* count the errors that pgstatsmon has reported */
+artedi*:::counter-add
+/copyinstr(arg0) == "pg_query_error"/
+{
+	backend = json(copyinstr(arg2), "backend");
+	@query_errors[backend] = count();
+}
+
+artedi*:::counter-add
+/copyinstr(arg0) == "pg_connect_error"/
+{
+	backend = json(copyinstr(arg2), "backend");
+	@conn_errors[backend] = count();
+}
+
+artedi*:::counter-add
+/copyinstr(arg0) == "pg_NaN_error"/
+{
+	backend = json(copyinstr(arg2), "backend");
+	@NaN_errors[backend] = count();
+}
+
+artedi*:::counter-add
+/copyinstr(arg0) == "pg_query_timeout"/
+{
+	backend = json(copyinstr(arg2), "backend");
+	@query_timeouts[backend] = count();
+}
+
+/*
+ * print the results every tick.
+ *
+ * This prints one line for every backend, and then one final line displaying
+ * the end-to-end tick latency.
+ */
+pgstatsmon*:::tick-done
+{
+	/* we use %40s because that's how long backend names are in prod */
+	printf("%40s %6s %4s %4s %4s %4s\n", "BACKEND", "LAT", "QTIM", "QERR",
+	    "CERR", "NaN");
+	printa("%40s %@6u %@4u %@4u %@4u %@4u \n", @backend_lat,
+	    @query_timeouts, @query_errors, @conn_errors, @NaN_errors);
+	printa("%40s %@6u\n", @tick_lat);
+	printf("\n");
+
+	/* reset the stats for the new tick */
+	clear(@backend_lat);
+	clear(@query_timeouts);
+	clear(@query_errors);
+	clear(@conn_errors);
+	clear(@NaN_errors);
+	clear(@tick_lat);
+}
\ No newline at end of file
diff --git a/bin/dtrace/querystat.d b/bin/dtrace/querystat.d
new file mode 100755
index 0000000..d4cb620
--- /dev/null
+++ b/bin/dtrace/querystat.d
@@ -0,0 +1,98 @@
+#!/usr/sbin/dtrace -Cs
+
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ *
+ * Copyright (c) 2018, Joyent, Inc.
+ */
+
+/*
+ * querystat.d - print stats about queries for a given backend.
+ *
+ * This script prints information about the queries that are performed on a
+ * given Postgres backend by pgstatsmon. This is a good second step after
+ * looking at the output from backendstat.d.
+ *
+ * Arguments:
+ *  - Name of backend (BACKEND column from backendstat.d)
+ *
+ * Fields:
+ *
+ * QUERY - name of the query being performed
+ * LAT   - duration of query from the standpoint of pgstatsmon
+ * QTIM  - queries timed out
+ * QERR  - queries that resulted in error
+ * NaN   - queries that returned NaN
+ *
+ * The name of the query generally corresponds to the Postgres table being
+ * queried. If pgstatsmon joins tables the name is made up to represent the data
+ * being returned.
+ *
+ * The possible values of QTIM, QERR, and NaN are zero and one.
+ *
+ * The last line of the output for every tick is the name of the backend.
+ *
+ * This script prints refreshed stats every tick. Output is sorted by LAT.
+ *
+ */
+
+#pragma D option quiet
+#pragma D option zdefs
+
+pgstatsmon*:::backend-query-start
+/copyinstr(arg0) == $1/
+{
+	self->startts[copyinstr(arg1)] = timestamp;
+}
+
+pgstatsmon*:::backend-query-done
+/self->startts[copyinstr(arg1)] && copyinstr(arg0) == $1/
+{
+	query = copyinstr(arg1);
+	@lat[query] = sum((timestamp - self->startts[query]) / 1000000);
+}
+
+artedi*:::counter-add
+/
+json(copyinstr(arg2), "backend") == $1 &&
+copyinstr(arg0) == "pg_query_error"
+/
+{
+	@query_errors[json(copyinstr(arg2), "query")] = count();
+}
+
+artedi*:::counter-add
+/
+json(copyinstr(arg2), "backend") == $1 &&
+copyinstr(arg0) == "pg_query_timeout"
+/
+{
+	@query_timeouts[json(copyinstr(arg2), "query")] = count();
+}
+
+artedi*:::counter-add
+/
+json(copyinstr(arg2), "backend") == $1 &&
+copyinstr(arg0) == "pg_NaN_error"
+/
+{
+	@NaN_errors[json(copyinstr(arg2), "query")] = count();
+}
+
+pgstatsmon*:::tick-done
+{
+	printf("%s\n", $1);
+	printf("%20s %6s %4s %4s %4s\n", "QUERY", "LAT", "QTIM", "QERR",
+	    "NaN");
+	printa("%20s %@6u %@4u %@4u %@4u \n", @lat, @query_timeouts,
+	    @query_errors, @NaN_errors);
+	printf("\n");
+
+	/* reset the stats for the new tick */
+	clear(@lat);
+	clear(@query_timeouts);
+	clear(@query_errors);
+	clear(@NaN_errors);
+}
diff --git a/bin/dtrace/walstat.d b/bin/dtrace/walstat.d
new file mode 100755
index 0000000..7a1f157
--- /dev/null
+++ b/bin/dtrace/walstat.d
@@ -0,0 +1,116 @@
+#!/usr/sbin/dtrace -Cs
+
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ *
+ * Copyright (c) 2018, Joyent, Inc.
+ */
+
+/*
+ * walstat.d - print WAL stats from backends monitored by pgstatsmon
+ *
+ * This script makes WAL information available. This is useful if someone is
+ * trying to quickly understand the throughput and lag (both in terms of bytes)
+ * of various Postgres instances.
+ *
+ * Arguments: none
+ *
+ * Fields:
+ * SHARD    - name or number of Postgres shard
+ * ZONE     - first portion of the backend's zonename
+ * REPL     - replication role of WAL replica's server process
+ * SENT     - WAL bytes sent to replica
+ * WRITTEN  - WAL bytes written to disk by replica
+ * FLUSHED  - WAL bytes flushed to disk by replica
+ * REPLAYED - WAL bytes replayed by replica
+ *
+ * SENT, WRITTEN, FLUSHED, and REPLAYED are all the amount written since the
+ * last scrape by pgstatsmon.
+ *
+ * WRITTEN, FLUSHED, and REPLAYED are the values reported by the downstream
+ * replica. This is the very similar to the output format that
+ * `manatee-adm pg-status` uses.
+ *
+ * Note: This script currently is only useful in sharded deployments that have
+ * dot delineated shard names (e.g. 1.moray.mydomain.com).
+ *
+ */
+
+#pragma D option quiet
+#pragma D option zdefs
+#pragma D option aggsortkey
+
+artedi*:::counter-add
+/copyinstr(arg0) == "pg_stat_replication_wal_sent_bytes"/
+{
+	labels = copyinstr(arg2);
+	shard = strtok(json(labels, "backend"), ".");
+	sync_state = json(labels, "sync_state");
+
+	strtok(json(labels, "backend"), "-");
+	zone = strtok(NULL, "-");
+
+	@sent[shard, zone, sync_state] = sum(arg1);
+}
+
+artedi*:::counter-add
+/copyinstr(arg0) == "pg_stat_replication_replica_wal_written_bytes"/
+{
+	labels = copyinstr(arg2);
+	shard = strtok(json(labels, "backend"), ".");
+	sync_state = json(labels, "sync_state");
+
+	strtok(json(labels, "backend"), "-");
+	zone = strtok(NULL, "-");
+
+	@written[shard, zone, sync_state] = sum(arg1);
+}
+
+artedi*:::counter-add
+/copyinstr(arg0) == "pg_stat_replication_replica_wal_flushed_bytes"/
+{
+	labels = copyinstr(arg2);
+	shard = strtok(json(labels, "backend"), ".");
+	sync_state = json(labels, "sync_state");
+
+	strtok(json(labels, "backend"), "-");
+	zone = strtok(NULL, "-");
+
+	@flushed[shard, zone, sync_state] = sum(arg1);
+}
+
+artedi*:::counter-add
+/copyinstr(arg0) == "pg_stat_replication_replica_wal_replayed_bytes"/
+{
+	labels = copyinstr(arg2);
+	shard = strtok(json(labels, "backend"), ".");
+	sync_state = json(labels, "sync_state");
+
+	strtok(json(labels, "backend"), "-");
+	zone = strtok(NULL, "-");
+
+	@replayed[shard, zone, sync_state] = sum(arg1);
+}
+
+pgstatsmon*:::tick-done
+{
+	/* headers */
+	printf("%30s %40s\n", "----- CLUSTER STATE -----",
+	    "----- THROUGHPUT (Bytes/tick) -----");
+
+	printf("%10s %10s %10s %10s %10s %10s %10s \n", "SHARD", "ZONE", "REPL",
+	    "SENT", "WRITTEN", "FLUSHED", "REPLAYED");
+
+	/* data */
+	printa("%10s %10s %10s %@10u %@10u %@10u %@10u \n",
+	    @sent, @written, @flushed, @replayed);
+
+	printf("\n");
+
+	clear(@sent);
+	clear(@written);
+	clear(@flushed);
+	clear(@replayed);
+}
diff --git a/docs/overview.md b/docs/overview.md
index fbedd9c..61b67b2 100644
--- a/docs/overview.md
+++ b/docs/overview.md
@@ -121,10 +121,11 @@ own set of labels that will be added in addition to whatever the user provides
 in the configuration file. Usually this includes at least the name of the
 backend that the stat came from (e.g. 2.moray.us-east.joyent.us-12345).
 
-| Table | Values | Metadata | Notes |
-|-----------------------------------|
+| Data Source | Values | Metadata | Notes |
+|-------------|--------|----------|-------|
 |pg_stat_user_tables | Information about things done on a relation. This includes number of dead tuples, number of vacuum operations, table scan counts, and more | relname| |
-|pg_stat_replication | Absolute WAL positions in bytes | sync_state | Only works on PG 9.4+ |
+|pg_stat_replication | Absolute WAL positions in bytes (mostly downstream peers)| sync_state | Only works on PG 9.4+, recovering peers return little data |
+|WAL admin functions | Absolute WAL positions in bytes (local peer only) | | Data returned varies depending on whether or not the backend is in recovery |
 |pg_stat_activity    | Connection counts | datname, state | |
 |pg_stat_database    | Information about given databases. This includes transaction counts, tuple counts, the time spent reading from and writing to disk, and more | datname | |
 |pg_class            | Size of relations in bytes | relname | |
diff --git a/lib/pgstatsmon.js b/lib/pgstatsmon.js
index 2a4454b..b6b074e 100644
--- a/lib/pgstatsmon.js
+++ b/lib/pgstatsmon.js
@@ -638,6 +638,10 @@ PgMon.prototype.tick = function (callback)
 		var pool = mon.pm_pools[pi].pool;
 		var backend = mon.pm_pgs[pi].name;
 
+		dtrace['backend-start'].fire(function () {
+			return ([backend]);
+		});
+
 		if (mon.pm_pools[pi].needs_setup) {
 			setImmediate(function () {
 				mon.setup_backend(pi);
@@ -680,22 +684,7 @@ PgMon.prototype.tick = function (callback)
 				    backend, pi, qi);
 				barrier.start(query_id);
 
-				dtrace['backend-query-start'].fire(function () {
-					return ([
-					    backend,
-					    query.q_name
-					]);
-				});
-
 				mon.tickPgQuery(pi, qi, function (err2) {
-					dtrace['backend-query-done'].fire(
-					    function () {
-
-						return ([
-						    backend,
-						    query.q_name
-						]);
-					});
 					if (err2) {
 						errors.push(err2);
 					}
@@ -774,9 +763,6 @@ PgMon.prototype.tick = function (callback)
 
 	/* enqueue all of the Postgres instances */
 	mon.pm_pgs.forEach(function (pg, pi) {
-		dtrace['backend-start'].fire(function () {
-			return ([pg.name]);
-		});
 		queue.push(pi);
 	});
 
@@ -844,6 +830,13 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 	    'query': query.q_name
 	}, 'check: start');
 
+	dtrace['backend-query-start'].fire(function () {
+		return ([
+		    backend,
+		    query.q_name
+		]);
+	});
+
 	res = client.query(query.q_sql);
 	res.on('row', function on_query_row(row) {
 		if (aborted) {
@@ -862,6 +855,10 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 	});
 
 	res.on('error', function on_query_error(err) {
+		dtrace['backend-query-done'].fire(function () {
+			return ([backend, query.q_name]);
+		});
+
 		mon.pm_state[pi][qi] = null;
 		aborted = true;
 
@@ -879,6 +876,7 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 				}
 			};
 			mon.emitCounter(errmetric, 1);
+
 			setImmediate(cb, err);
 			return;
 		}
@@ -905,6 +903,9 @@ PgMon.prototype.tickPgQuery = function (pi, qi, cb)
 	});
 
 	res.once('end', function on_query_end() {
+		dtrace['backend-query-done'].fire(function () {
+			return ([backend, query.q_name]);
+		});
 		res.removeAllListeners();
 		if (client.isDestroyed()) {
 			mon.pm_log.info({
@@ -955,6 +956,25 @@ PgMon.prototype.record = function (pi, qi, datum)
 	var new_value, old_value;
 	var metric;
 
+	function record_NaN(met) {
+		mon.pm_log.warn({
+			'metric': met
+		}, 'NaN value observed');
+
+		mon.pm_targets.forEach(function (t) {
+			t.emitCounter({
+				'name': 'pg_NaN_error',
+				'help': 'pgstatsmon read a bad'
+				    + ' value from a SQL query',
+				'metadata': {
+					'name': met.name,
+					'backend': backend,
+					'query': query.q_name
+				}
+			}, 1);
+		});
+	}
+
 	oldresult = mon.pm_data[pi][qi];
 	mon.pm_old_data[pi][qi] = oldresult;
 	mon.pm_data[pi][qi] = {};
@@ -998,6 +1018,7 @@ PgMon.prototype.record = function (pi, qi, datum)
 		}
 
 		query.q_counters.forEach(function emit_counters(c) {
+			metric = mon.qstatname(pi, qi, row, c);
 			new_value = row[c.attr];
 			old_value = oldrow[c.attr];
 
@@ -1011,25 +1032,19 @@ PgMon.prototype.record = function (pi, qi, datum)
 			 * counters or gauges, but will log a warning and
 			 * increment a separate counter to track this behavior.
 			 */
-			if (isNaN(new_value) || new_value === null) {
-				metric = mon.qstatname(pi, qi, row, c);
+			if (isNaN(new_value)) {
+				record_NaN(metric);
+				return;
+			}
 
-				mon.pm_log.warn({
+			/*
+			 * Some queries return null values under normal
+			 * operation (pg_recovery).
+			 */
+			if (new_value === null) {
+				mon.pm_log.debug({
 					'metric': metric
-				}, 'NaN value observed');
-
-				mon.pm_targets.forEach(function (t) {
-					t.emitCounter({
-						'name': 'pg_NaN_error',
-						'help': 'pgstatsmon read a bad'
-						    + ' value from a SQL query',
-						'metadata': {
-							'name': metric.name,
-							'backend': backend,
-							'query': query.q_name
-						}
-					}, 1);
-				});
+				}, 'null value observed');
 				return;
 			}
 
@@ -1043,38 +1058,27 @@ PgMon.prototype.record = function (pi, qi, datum)
 				    + ' skipping');
 				return;
 			}
-			mon.emitCounter(
-			    mon.qstatname(pi, qi, row, c),
-			    new_value - old_value);
+			mon.emitCounter(metric, new_value - old_value);
 		});
 
 		query.q_gauges.forEach(function emit_gauges(g) {
+			metric = mon.qstatname(pi, qi, row, g);
 			new_value = row[g.attr];
+
 			/* see previous comment for explanation */
-			if (isNaN(new_value) || new_value === null) {
-				metric = mon.qstatname(pi, qi, row, g);
+			if (isNaN(new_value)) {
+				record_NaN(metric);
+				return;
+			}
 
-				mon.pm_log.warn({
+			if (new_value === null) {
+				mon.pm_log.debug({
 					'metric': metric
-				}, 'NaN value observed');
-
-				mon.pm_targets.forEach(function (t) {
-					t.emitCounter({
-						'name': 'pg_NaN_error',
-						'help': 'pgstatsmon read a bad'
-						    + ' value from a SQL query',
-						'metadata': {
-							'name': metric.name,
-							'backend': backend,
-							'query': query.q_name
-						}
-					}, 1);
-				});
+				}, 'null value observed');
 				return;
 			}
 
-			mon.emitGauge(
-			    mon.qstatname(pi, qi, row, g), new_value);
+			mon.emitGauge(metric, new_value);
 		});
 	});
 };
diff --git a/lib/queries.js b/lib/queries.js
index 077978e..5341466 100644
--- a/lib/queries.js
+++ b/lib/queries.js
@@ -106,14 +106,61 @@ var queries = [ {
 	    'help': 'wal bytes replayed into database by replica',
 	    'unit': 'bytes' }
     ]
+}, {
+    'name': 'pg_recovery',
+    'statkey': 'recovery',
+    'metadata': [],
+    'sql': [
+	'SELECT \'recovery\' as recovery, ',
+	'pg_last_xlog_replay_location() - CAST (\'0/0\' AS pg_lsn) ',
+	'	AS wal_replayed_bytes, ',
+	'',
+	'CASE pg_is_in_recovery() WHEN \'t\' ',
+	'THEN (SELECT pg_last_xlog_receive_location() - ',
+	'	CAST (\'0/0\' AS pg_lsn))',
+	'ELSE (NULL) END AS wal_received_bytes, ',
+	'',
+	'CASE pg_is_in_recovery() WHEN \'t\' ',
+	'THEN (NULL) ',
+	'ELSE (SELECT pg_current_xlog_flush_location() - ',
+	'	CAST (\'0/0\' AS pg_lsn)) END AS wal_flushed_bytes, ',
+	'',
+	'CASE pg_is_in_recovery() WHEN \'t\' ',
+	'THEN (NULL) ',
+	'ELSE (SELECT pg_current_xlog_insert_location() - ',
+	'	CAST (\'0/0\' AS pg_lsn)) END AS wal_inserted_bytes;'
+    ].join('\n'),
+    'counters': [
+	{ 'attr': 'wal_inserted_bytes', 'help': 'WAL bytes inserted' },
+	{ 'attr': 'wal_replayed_bytes', 'help': 'WAL bytes replayed into DB' },
+	{ 'attr': 'wal_received_bytes',
+	    'help': 'WAL bytes received from upstream server' },
+	{ 'attr': 'wal_flushed_bytes', 'help': 'WAL bytes flushed to disk' }
+    ]
 }, {
     'name': 'pg_stat_activity',
     'statkey': 'datname',
     'metadata': [ 'datname', 'state' ],
     'sql': [
-	'SELECT datname, state, count(*) AS connections ',
-	'FROM get_stat_activity() ',
-	'GROUP BY datname, state;'
+	'SELECT ',
+	'pg_database.datname, states.state, ',
+	'COALESCE(connections, 0) as connections ',
+	'FROM ( ',
+	'	VALUES ',
+	'	(\'active\'), ',
+	'	(\'idle\'), ',
+	'	(\'idle in transaction\'), ',
+	'	(\'idle in transaction (aborted)\'), ',
+	'	(\'fastpath function call\'), ',
+	'	(\'disabled\') ',
+	') AS states(state) CROSS JOIN pg_database ',
+	'LEFT JOIN ( ',
+	'	SELECT ',
+	'	datname, state, count(*) AS connections ',
+	'	FROM get_stat_activity() GROUP BY datname,state) AS active ',
+	'ON states.state = active.state ',
+	'AND pg_database.datname = active.datname ',
+	'WHERE pg_database.datname NOT LIKE \'template%\';'
     ].join('\n'),
     'gauges': [ { 'attr': 'connections', 'help': 'worker process state' } ]
 }, {
