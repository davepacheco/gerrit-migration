From a3d55648ea64daa002fe840fea4c6a2d39ea25bf Mon Sep 17 00:00:00 2001
From: Patrick Mooney <pmooney@pfmooney.com>
Date: Tue, 31 Jul 2018 17:37:11 +0000
Subject: [PATCH] OS-7080 bhyve and KVM should coexist in peace OS-7126 KVM
 queries host MSRs dangerously

---
 kvm.c      | 144 ++++-------------------
 kvm_host.h |   7 +-
 kvm_vmx.c  | 332 ++++++++++++++---------------------------------------
 kvm_x86.c  | 113 +++++++-----------
 kvm_x86.h  |  15 +--
 5 files changed, 153 insertions(+), 458 deletions(-)

diff --git a/kvm.c b/kvm.c
index e5ffc65..2939b6e 100644
--- a/kvm.c
+++ b/kvm.c
@@ -24,7 +24,7 @@
  *   Yaniv Kamay  <yaniv@qumranet.com>
  *
  * Ported to illumos by Joyent
- * Copyright 2017 Joyent, Inc.
+ * Copyright 2018 Joyent, Inc.
  *
  * Authors:
  *   Max Bruning	<max@joyent.com>
@@ -313,7 +313,7 @@
 #include <sys/xc_levels.h>
 #include <asm/cpu.h>
 #include <sys/id_space.h>
-#include <sys/pc_hvm.h>
+#include <sys/hma.h>
 
 #include "kvm_bitops.h"
 #include "kvm_vmx.h"
@@ -365,12 +365,9 @@ static int kvm_hiwat = 0x1000000;
 static void *kvm_state;		/* DDI state */
 static id_space_t *kvm_minors;	/* minor number arena */
 static dev_info_t *kvm_dip;	/* global devinfo hanlde */
-static boolean_t kvm_init_failed; /* track vm hardware init failure */
+static hma_reg_t *kvm_hma_reg;
 static int kvmid;		/* monotonically increasing, unique per vm */
 static int largepages_enabled = 1;
-static cpuset_t cpus_hardware_enabled;
-static kmutex_t cpus_hardware_enabled_mp;
-static volatile uint32_t hardware_enable_failed;
 static uint_t kvm_usage_count;
 static list_t vm_list;
 static kmutex_t kvm_lock;
@@ -454,10 +451,11 @@ kvm_ringbuf_record(kvm_ringbuf_t *ringbuf, uint32_t tag, uint64_t payload)
  * Called when we've been asked to save our context. i.e. we're being swapped
  * out.
  */
-void
+static void
 kvm_ctx_save(void *arg)
 {
 	struct kvm_vcpu *vcpu = arg;
+
 	kvm_ringbuf_record(&vcpu->kvcpu_ringbuf,
 	    KVM_RINGBUF_TAG_CTXSAVE, vcpu->cpu);
 	kvm_arch_vcpu_put(vcpu);
@@ -468,13 +466,12 @@ kvm_ctx_save(void *arg)
  * Called when we're being asked to restore our context. i.e. we're returning
  * from being swapped out.
  */
-void
+static void
 kvm_ctx_restore(void *arg)
 {
-	int cpu;
-
-	cpu = CPU->cpu_seqid;
 	struct kvm_vcpu *vcpu = arg;
+	const int cpu = CPU->cpu_seqid;
+
 	kvm_ringbuf_record(&vcpu->kvcpu_ringbuf,
 	    KVM_RINGBUF_TAG_CTXRESTORE, vcpu->cpu);
 	kvm_arch_vcpu_load(vcpu, cpu);
@@ -492,15 +489,13 @@ kvm_is_mmio_pfn(pfn_t pfn)
 void
 vcpu_load(struct kvm_vcpu *vcpu)
 {
-	int cpu;
-
 	mutex_enter(&vcpu->mutex);
+
+	kpreempt_disable();
 	installctx(curthread, vcpu, kvm_ctx_save, kvm_ctx_restore, NULL,
 	    NULL, NULL, NULL);
 
-	kpreempt_disable();
-	cpu = CPU->cpu_seqid;
-	kvm_arch_vcpu_load(vcpu, cpu);
+	kvm_arch_vcpu_load(vcpu, CPU->cpu_seqid);
 	kvm_ringbuf_record(&vcpu->kvcpu_ringbuf,
 	    KVM_RINGBUF_TAG_VCPULOAD, vcpu->cpu);
 	kpreempt_enable();
@@ -516,13 +511,15 @@ kvm_get_vcpu(struct kvm *kvm, int i)
 void
 vcpu_put(struct kvm_vcpu *vcpu)
 {
+	int cpu;
+
 	kpreempt_disable();
+	cpu = vcpu->cpu;
 	kvm_arch_vcpu_put(vcpu);
 	kvm_fire_urn(vcpu);
 	removectx(curthread, vcpu, kvm_ctx_save, kvm_ctx_restore, NULL,
 	    NULL, NULL, NULL);
-	kvm_ringbuf_record(&vcpu->kvcpu_ringbuf,
-	    KVM_RINGBUF_TAG_VCPUPUT, vcpu->cpu);
+	kvm_ringbuf_record(&vcpu->kvcpu_ringbuf, KVM_RINGBUF_TAG_VCPUPUT, cpu);
 	kpreempt_enable();
 	mutex_exit(&vcpu->mutex);
 }
@@ -1519,74 +1516,10 @@ kvm_dev_ioctl_check_extension_generic(long arg, int *rv)
 	return (kvm_dev_ioctl_check_extension(arg, rv));
 }
 
-static void
-hardware_enable(void *junk)
-{
-	int cpu;
-	int r;
-
-	cpu = curthread->t_cpu->cpu_id;
-
-	mutex_enter(&cpus_hardware_enabled_mp);
-	if (CPU_IN_SET(cpus_hardware_enabled, cpu)) {
-		mutex_exit(&cpus_hardware_enabled_mp);
-		return;
-	}
-
-	CPUSET_ADD(cpus_hardware_enabled, cpu);
-	mutex_exit(&cpus_hardware_enabled_mp);
-
-	r = kvm_arch_hardware_enable(NULL);
-
-	if (r) {
-		mutex_enter(&cpus_hardware_enabled_mp);
-		CPUSET_DEL(cpus_hardware_enabled, cpu);
-		mutex_exit(&cpus_hardware_enabled_mp);
-		atomic_inc_32(&hardware_enable_failed);
-		cmn_err(CE_WARN, "kvm: enabling virtualization CPU%d failed\n",
-			cpu);
-	}
-}
-
-void
-hardware_disable(void *junk)
-{
-	int cpu = curthread->t_cpu->cpu_id;
-
-	mutex_enter(&cpus_hardware_enabled_mp);
-	if (!CPU_IN_SET(cpus_hardware_enabled, cpu)) {
-		mutex_exit(&cpus_hardware_enabled_mp);
-		return;
-	}
-
-	CPUSET_DEL(cpus_hardware_enabled, cpu);
-	mutex_exit(&cpus_hardware_enabled_mp);
-	kvm_arch_hardware_disable(NULL);
-}
-
-static void
-hardware_disable_all(void)
-{
-	ASSERT(MUTEX_HELD(&kvm_lock));
-
-	on_each_cpu(hardware_disable, NULL, 1);
-}
 
-static int
-hardware_enable_all(void)
-{
-	ASSERT(MUTEX_HELD(&kvm_lock));
 
-	hardware_enable_failed = 0;
-	on_each_cpu(hardware_enable, NULL, 1);
 
-	if (hardware_enable_failed) {
-		hardware_disable_all();
-		return (EBUSY);
-	}
 
-	return (0);
-}
 
 /* kvm_io_bus_write - called under kvm->slots_lock */
 int
@@ -1785,45 +1718,25 @@ zero_constructor(void *buf, void *arg, int tags)
 	return (0);
 }
 
-static const char *kvm_excl_ident = "SmartOS KVM";
+static const char *kvm_hma_ident = "SmartOS KVM";
 
 static boolean_t
 kvm_hvm_init(void)
 {
+	hma_reg_t *reg;
+
 	ASSERT(MUTEX_HELD(&kvm_lock));
 
-	/*
-	 * If initialization failed on a previous open attempt, do not repeatedly
-	 * try again (which could incur additional cmn_err noise).  Detaching the
-	 * driver will lead this state to be cleared, allowing for subsequent
-	 * attempts, if desired.
-	 */
-	if (kvm_init_failed) {
+	if ((reg = hma_vmx_register(kvm_hma_ident)) == NULL) {
 		return (B_FALSE);
 	}
-
-	/*
-	 * Demand exclusivity over the HVM resources of this machine.  A
-	 * failure to acquire this advisory lock does preclude a potential
-	 * success in the future. (So kvm_init_failed is not asserted.)
-	 */
-	if (!hvm_excl_hold(kvm_excl_ident)) {
+	if (vmx_init() != DDI_SUCCESS) {
+		hma_vmx_unregister(reg);
 		return (B_FALSE);
 	}
 
-	if (vmx_init() != DDI_SUCCESS) {
-		goto fail;
-	}
-	if (hardware_enable_all() != 0) {
-		vmx_fini();
-		goto fail;
-	}
+	kvm_hma_reg = reg;
 	return (B_TRUE);
-
-fail:
-	kvm_init_failed = B_TRUE;
-	hvm_excl_rele(kvm_excl_ident);
-	return (B_FALSE);
 }
 
 static void
@@ -1831,8 +1744,8 @@ kvm_hvm_fini(void)
 {
 	ASSERT(MUTEX_HELD(&kvm_lock));
 	ASSERT(kvm_usage_count == 0);
+	ASSERT3P(kvm_hma_reg, !=, NULL);
 
-	hardware_disable_all();
 	kvm_arch_hardware_unsetup();
 	vmx_fini();
 
@@ -1847,11 +1760,8 @@ kvm_hvm_fini(void)
 
 	kvm_arch_exit();
 
-	/*
-	 * Only once all resources directly related to HVM are released can the
-	 * advisory lock be dropped.
-	 */
-	hvm_excl_rele(kvm_excl_ident);
+	hma_vmx_unregister(kvm_hma_reg);
+	kvm_hma_reg = NULL;
 }
 
 static boolean_t
@@ -1909,8 +1819,6 @@ kvm_attach(dev_info_t *dip, ddi_attach_cmd_t cmd)
 	}
 
 	mutex_init(&kvm_lock, NULL, MUTEX_DRIVER, 0);
-	mutex_init(&cpus_hardware_enabled_mp, NULL, MUTEX_DRIVER,
-	    (void *)XC_HI_PIL);
 
 	list_create(&vm_list, sizeof (struct kvm),
 	    offsetof(struct kvm, vm_list));
@@ -1918,7 +1826,6 @@ kvm_attach(dev_info_t *dip, ddi_attach_cmd_t cmd)
 
 	kvm_dip = dip;
 	ddi_report_dev(dip);
-	kvm_init_failed = B_FALSE;
 
 	return (DDI_SUCCESS);
 }
@@ -1938,7 +1845,6 @@ kvm_detach(dev_info_t *dip, ddi_detach_cmd_t cmd)
 	id_space_destroy(kvm_minors);
 	kvm_dip = NULL;
 
-	mutex_destroy(&cpus_hardware_enabled_mp);
 	mutex_destroy(&kvm_lock);
 	ddi_soft_state_fini(&kvm_state);
 
diff --git a/kvm_host.h b/kvm_host.h
index 20bd089..58715cc 100644
--- a/kvm_host.h
+++ b/kvm_host.h
@@ -3,7 +3,7 @@
  * COPYING file in the top-level directory.
  *
  * Copyright 2011 various Linux Kernel contributors.
- * Copyright 2011 Joyent, Inc. All Rights Reserved.
+ * Copyright 2018 Joyent, Inc.
  */
 
 #ifndef __KVM_HOST_H
@@ -63,14 +63,13 @@ typedef struct kvm_shared_msrs_global {
 typedef struct kvm_shared_msrs {
 	struct kvm_user_return_notifier urn;
 	int registered;
+	uint_t host_saved;
 	struct kvm_shared_msr_values {
 		uint64_t host;
 		uint64_t curr;
 	} values[KVM_NR_SHARED_MSRS];
 } kvm_shared_msrs_t;
 
-extern struct kvm_shared_msrs **shared_msrs;
-
 /*
  * It would be nice to use something smarter than a linear search, TBD...
  * Thankfully we dont expect many devices to register (famous last words :),
@@ -403,8 +402,6 @@ extern int kvm_arch_vcpu_setup(struct kvm_vcpu *);
 extern void kvm_arch_vcpu_destroy(struct kvm_vcpu *);
 
 extern int kvm_arch_vcpu_reset(struct kvm_vcpu *);
-extern int kvm_arch_hardware_enable(void *);
-extern void kvm_arch_hardware_disable(void *);
 extern int kvm_arch_hardware_setup(void);
 extern void kvm_arch_hardware_unsetup(void);
 extern void kvm_arch_check_processor_compat(void *);
diff --git a/kvm_vmx.c b/kvm_vmx.c
index ffac0c6..6bc651b 100644
--- a/kvm_vmx.c
+++ b/kvm_vmx.c
@@ -13,7 +13,7 @@
  * This work is licensed under the terms of the GNU GPL, version 2.  See
  * the COPYING file in the top-level directory.
  *
- * Copyright (c) 2015 Joyent, Inc. All rights reserved.
+ * Copyright 2018 Joyent, Inc.
  */
 
 #include <sys/sysmacros.h>
@@ -23,6 +23,7 @@
 #include <sys/x86_archext.h>
 #include <sys/xc_levels.h>
 #include <sys/machsystm.h>
+#include <sys/hma.h>
 
 #include "kvm_bitops.h"
 #include "kvm_msr.h"
@@ -40,12 +41,8 @@
 /*
  * Globals
  */
-struct kvm_shared_msrs **shared_msrs;
 
 #define	VMX_NR_VPIDS				(1 << 16)
-static kmutex_t vmx_vpid_lock;
-static ulong_t *vmx_vpid_bitmap;
-static size_t vpid_bitmap_words;
 static int bypass_guest_pf = 1;
 static int enable_vpid = 1;
 static int flexpriority_enabled = 1;
@@ -73,9 +70,11 @@ __attribute__((__aligned__(PAGESIZE)))static unsigned long
     vmx_msr_bitmap_longmode[PAGESIZE / sizeof (unsigned long)];
 #endif
 
-static struct vmcs **vmxarea;  /* 1 per cpu */
-static struct vmcs **current_vmcs;
-static uint64_t *vmxarea_pa;   /* physical address of each vmxarea */
+static uintptr_t vmx_io_bitmap_a_pa;
+static uintptr_t vmx_io_bitmap_b_pa;
+static uintptr_t vmx_msr_bitmap_legacy_pa;
+static uintptr_t vmx_msr_bitmap_longmode_pa;
+
 static int vmx_has_kvm_support_override = 0;
 
 #define	KVM_GUEST_CR0_MASK_UNRESTRICTED_GUEST				\
@@ -164,6 +163,7 @@ typedef struct vcpu_vmx {
 		} irq;
 	} rmode;
 	int vpid;
+	int cpu_lastrun;
 	char emulation_required;
 
 	/* Support for vnmi-less CPUs */
@@ -474,50 +474,37 @@ find_msr_entry(struct vcpu_vmx *vmx, uint32_t msr)
 	return (NULL);
 }
 
+
 static void
-vmcs_clear(uint64_t vmcs_pa)
+vmcs_load(uint64_t vmcs_pa)
 {
-	unsigned char error;
+	uint8_t error;
 
-	KVM_TRACE1(vmx__vmclear, uint64_t, vmcs_pa);
+	KVM_TRACE1(vmx__vmptrld, uint64_t, vmcs_pa);
 
 	/*CSTYLED*/
-	__asm__ volatile (__ex(ASM_VMX_VMCLEAR_RAX) "\n\tsetna %0\n"
+	__asm__ volatile (ASM_VMX_VMPTRLD_RAX "; setna %0"
 	    : "=g"(error) : "a"(&vmcs_pa), "m"(vmcs_pa)
-	    : "cc", "memory");
+	    : "cc");
 
 	if (error)
-		cmn_err(CE_PANIC, "kvm: vmclear fail: %lx\n", vmcs_pa);
+		cmn_err(CE_PANIC, "kvm: vmptrld fail: %lx\n", vmcs_pa);
 }
 
 static void
-__vcpu_clear(void *arg)
+vmcs_clear(uint64_t vmcs_pa)
 {
-	struct vcpu_vmx *vmx = arg;
-	int cpu = CPU->cpu_id;
-
-	vmx->vmcs->revision_id = vmcs_config.revision_id;
-
-	kvm_ringbuf_record(&vmx->vcpu.kvcpu_ringbuf,
-	    KVM_RINGBUF_TAG_VCPUCLEAR, vmx->vcpu.cpu);
-
-	if (vmx->vcpu.cpu == cpu)
-		vmcs_clear(vmx->vmcs_pa);
-
-	if (current_vmcs[cpu] == vmx->vmcs)
-		current_vmcs[cpu] = NULL;
+	unsigned char error;
 
-	vmx->vcpu.cpu = -1;
-	vmx->launched = 0;
-}
+	KVM_TRACE1(vmx__vmclear, uint64_t, vmcs_pa);
 
-static void
-vcpu_clear(struct vcpu_vmx *vmx)
-{
-	if (vmx->vcpu.cpu == -1)
-		return;
+	/*CSTYLED*/
+	__asm__ volatile (__ex(ASM_VMX_VMCLEAR_RAX) "\n\tsetna %0\n"
+	    : "=g"(error) : "a"(&vmcs_pa), "m"(vmcs_pa)
+	    : "cc", "memory");
 
-	kvm_xcall(vmx->vcpu.cpu, __vcpu_clear, vmx);
+	if (error)
+		cmn_err(CE_PANIC, "kvm: vmclear fail: %lx\n", vmcs_pa);
 }
 
 static void
@@ -834,38 +821,22 @@ static void
 vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
-	uint64_t phys_addr = vmx->vmcs_pa;
-	uint64_t tsc_this, delta, new_offset;
 
-	if (vcpu->cpu != cpu) {
-		vcpu_clear(vmx);
-		set_bit(KVM_REQ_TLB_FLUSH, &vcpu->requests);
-	}
-
-	if (current_vmcs[cpu] != vmx->vmcs) {
-		uint8_t error;
-
-		kvm_ringbuf_record(&vcpu->kvcpu_ringbuf,
-		    KVM_RINGBUF_TAG_VMPTRLD, (uint64_t)current_vmcs[cpu]);
-
-		current_vmcs[cpu] = vmx->vmcs;
+	vmcs_load(vmx->vmcs_pa);
+	vcpu->cpu = cpu;
 
-		KVM_TRACE1(vmx__vmptrld, uint64_t, phys_addr);
-
-		/*CSTYLED*/
-		__asm__ volatile (ASM_VMX_VMPTRLD_RAX "; setna %0"
-		    : "=g"(error) : "a"(&phys_addr), "m"(phys_addr)
-		    : "cc");
-	}
-
-	if (vcpu->cpu != cpu) {
+	/*
+	 * Load per-CPU context into the VMCS if this vCPU previously ran on a
+	 * different host CPU.
+	 */
+	if (vmx->cpu_lastrun != cpu) {
 		struct descriptor_table dt;
 		unsigned long sysenter_esp;
 
 		kvm_ringbuf_record(&vcpu->kvcpu_ringbuf,
-		    KVM_RINGBUF_TAG_VCPUMIGRATE, vcpu->cpu);
+		    KVM_RINGBUF_TAG_VCPUMIGRATE, cpu);
 
-		vcpu->cpu = cpu;
+		set_bit(KVM_REQ_TLB_FLUSH, &vcpu->requests);
 
 		/*
 		 * We have a per-CPU TSS, GDT, IDT and GSBASE -- so we reset
@@ -891,12 +862,22 @@ vmx_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 		vmcs_write64(TSC_OFFSET, tsc_gethrtime_tick_delta() +
 		    vcpu->arch.tsc_offset);
 	}
+	vmx->cpu_lastrun = cpu;
 }
 
 static void
 vmx_vcpu_put(struct kvm_vcpu *vcpu)
 {
-	__vmx_load_host_state(to_vmx(vcpu));
+	struct vcpu_vmx *vmx = to_vmx(vcpu);
+
+	__vmx_load_host_state(vmx);
+	vmcs_clear(vmx->vmcs_pa);
+
+	/*
+	 * Having VMCLEARed the VMCS, a subsequent VM entry must use VMLAUNCH
+	 * rather than VMRESUME.
+	 */
+	vmx->launched = 0;
 }
 
 static void
@@ -1061,11 +1042,10 @@ move_msr_up(struct vcpu_vmx *vmx, int from, int to)
  * msrs.  Don't touch the 64-bit msrs if the guest is in legacy
  * mode, as fiddling with msrs is very expensive.
  */
-void
+static void
 setup_msrs(struct vcpu_vmx *vmx)
 {
 	int save_nmsrs, index;
-	unsigned long *msr_bitmap;
 
 	vmx_load_host_state(vmx);
 	save_nmsrs = 0;
@@ -1098,12 +1078,14 @@ setup_msrs(struct vcpu_vmx *vmx)
 	vmx->save_nmsrs = save_nmsrs;
 
 	if (cpu_has_vmx_msr_bitmap()) {
+		uintptr_t msr_bitmap;
+
 		if (is_long_mode(&vmx->vcpu))
-			msr_bitmap = vmx_msr_bitmap_longmode;
+			msr_bitmap = vmx_msr_bitmap_longmode_pa;
 		else
-			msr_bitmap = vmx_msr_bitmap_legacy;
+			msr_bitmap = vmx_msr_bitmap_legacy_pa;
 
-		vmcs_write64(MSR_BITMAP, kvm_va2pa((caddr_t)msr_bitmap));
+		vmcs_write64(MSR_BITMAP, msr_bitmap);
 	}
 }
 
@@ -1344,68 +1326,6 @@ vmx_disabled_by_bios(void)
 	/* locked but not enabled */
 }
 
-static int
-vmx_hardware_enable(void *garbage)
-{
-	int cpu = curthread->t_cpu->cpu_seqid;
-	pfn_t pfn;
-	uint64_t old;
-#ifdef XXX
-	uint64_t phys_addr = kvtop(per_cpu(vmxarea, cpu));
-#else
-	uint64_t phys_addr;
-	XXX_KVM_PROBE;
-	phys_addr = vmxarea_pa[cpu];
-
-#endif
-
-	((struct vmcs *)(vmxarea[cpu]))->revision_id = vmcs_config.revision_id;
-
-	if (getcr4() & X86_CR4_VMXE)
-		return (DDI_FAILURE);
-
-	rdmsrl(MSR_IA32_FEATURE_CONTROL, old);
-	if ((old & (FEATURE_CONTROL_LOCKED | FEATURE_CONTROL_VMXON_ENABLED)) !=
-	    (FEATURE_CONTROL_LOCKED | FEATURE_CONTROL_VMXON_ENABLED)) {
-		/* enable and lock */
-		wrmsrl(MSR_IA32_FEATURE_CONTROL, old | FEATURE_CONTROL_LOCKED |
-		    FEATURE_CONTROL_VMXON_ENABLED);
-	}
-
-	KVM_TRACE1(vmx__vmxon, uint64_t, phys_addr);
-
-	setcr4(getcr4() | X86_CR4_VMXE); /* FIXME: not cpu hotplug safe */
-	/* BEGIN CSTYLED */
-	__asm__ volatile (ASM_VMX_VMXON_RAX
-		      : : "a"(&phys_addr), "m"(phys_addr)
-		      : "memory", "cc");
-	/* END CSTYLED */
-
-	ept_sync_global();
-
-	return (0);
-}
-
-/*
- * Just like cpu_vmxoff(), but with the __kvm_handle_fault_on_reboot()
- * tricks.
- */
-static void
-kvm_cpu_vmxoff(void)
-{
-	KVM_TRACE(vmx__vmxoff);
-
-	/* BEGIN CSTYLED */
-	__asm__ volatile (ASM_VMX_VMXOFF : : : "cc");
-	/* END CSTYLED */
-	setcr4(getcr4() & ~X86_CR4_VMXE);
-}
-
-static void vmx_hardware_disable(void *garbage)
-{
-	kvm_cpu_vmxoff();
-}
-
 static int
 adjust_vmx_controls(uint32_t ctl_min, uint32_t ctl_opt,
     uint32_t msr, uint32_t *result)
@@ -1551,60 +1471,6 @@ setup_vmcs_config(struct vmcs_config *vmcs_conf)
 	return (0);
 }
 
-static int
-alloc_kvm_area(void)
-{
-	int i, j;
-	pfn_t pfn;
-
-	/*
-	 * linux seems to do the allocations in a numa-aware
-	 * fashion.  We'll just allocate...
-	 */
-	vmxarea = kmem_alloc(ncpus * sizeof (struct vmcs *), KM_SLEEP);
-	vmxarea_pa = kmem_alloc(ncpus * sizeof (uint64_t *), KM_SLEEP);
-	current_vmcs = kmem_alloc(ncpus * sizeof (struct vmcs *), KM_SLEEP);
-	shared_msrs = kmem_alloc(ncpus * sizeof (struct kvm_shared_msrs *),
-	    KM_SLEEP);
-
-	for (i = 0; i < ncpus; i++) {
-		struct vmcs *vmcs;
-
-		/* XXX the following assumes PAGESIZE allocations */
-		/* are PAGESIZE aligned.  We could enforce this */
-		/* via kmem_cache_create, but I'm lazy */
-		vmcs = kmem_zalloc(PAGESIZE, KM_SLEEP);
-		vmxarea[i] = vmcs;
-		current_vmcs[i] = vmcs;
-		pfn = hat_getpfnum(kas.a_hat, (caddr_t)vmcs);
-		vmxarea_pa[i] = ((uint64_t)pfn << PAGESHIFT) |
-			((uint64_t)vmxarea[i] & PAGEOFFSET);
-		shared_msrs[i] = kmem_zalloc(sizeof (struct kvm_shared_msrs),
-		    KM_SLEEP);
-	}
-
-	return (0);
-}
-
-static void
-free_kvm_area(void)
-{
-	int cpu;
-
-	for (cpu = 0; cpu < ncpus; cpu++) {
-		kmem_free(vmxarea[cpu], PAGESIZE);
-		kmem_free(shared_msrs[cpu], sizeof (struct kvm_shared_msrs));
-	}
-	kmem_free(shared_msrs, ncpus * sizeof (struct kvm_shared_msrs *));
-	shared_msrs = NULL;
-	kmem_free(current_vmcs, ncpus * sizeof (struct vmcs *));
-	current_vmcs = NULL;
-	kmem_free(vmxarea_pa, ncpus * sizeof (uint64_t *));
-	vmxarea_pa = NULL;
-	kmem_free(vmxarea, ncpus * sizeof (struct vmcs *));
-	vmxarea = NULL;
-}
-
 static int
 vmx_hardware_setup(void)
 {
@@ -1642,14 +1508,7 @@ vmx_hardware_setup(void)
 	if (!cpu_has_vmx_ple())
 		ple_gap = 0;
 
-
-	return (alloc_kvm_area());
-}
-
-static void
-vmx_hardware_unsetup(void)
-{
-	free_kvm_area();
+	return (0);
 }
 
 static void
@@ -2542,13 +2401,8 @@ allocate_vpid(struct vcpu_vmx *vmx)
 	vmx->vpid = 0;
 	if (!enable_vpid)
 		return;
-	mutex_enter(&vmx_vpid_lock);
-	vpid = find_first_zero_bit(vmx_vpid_bitmap, VMX_NR_VPIDS);
-	if (vpid < VMX_NR_VPIDS) {
-		vmx->vpid = vpid;
-		__set_bit(vpid, vmx_vpid_bitmap);
-	}
-	mutex_exit(&vmx_vpid_lock);
+
+	vmx->vpid = hma_vmx_vpid_alloc();
 }
 
 static void
@@ -2585,7 +2439,7 @@ vmx_disable_intercept_for_msr(uint32_t msr, int longmode_only)
 /*
  * Sets up the vmcs for emulated real mode.
  */
-static int
+static void
 vmx_vcpu_setup(struct vcpu_vmx *vmx)
 {
 	uint32_t host_sysenter_cs, msr_low, msr_high;
@@ -2598,12 +2452,11 @@ vmx_vcpu_setup(struct vcpu_vmx *vmx)
 	uint32_t exec_control;
 
 	/* I/O */
-	vmcs_write64(IO_BITMAP_A, kvm_va2pa((caddr_t)vmx_io_bitmap_a));
-	vmcs_write64(IO_BITMAP_B, kvm_va2pa((caddr_t)vmx_io_bitmap_b));
+	vmcs_write64(IO_BITMAP_A, vmx_io_bitmap_a_pa);
+	vmcs_write64(IO_BITMAP_B, vmx_io_bitmap_b_pa);
 
 	if (cpu_has_vmx_msr_bitmap()) {
-		vmcs_write64(MSR_BITMAP,
-		    kvm_va2pa((caddr_t)vmx_msr_bitmap_legacy));
+		vmcs_write64(MSR_BITMAP, vmx_msr_bitmap_legacy_pa);
 	}
 
 	vmcs_write64(VMCS_LINK_POINTER, -1ull); /* 22.3.1.5 */
@@ -2754,8 +2607,6 @@ vmx_vcpu_setup(struct vcpu_vmx *vmx)
 		vmcs_write64(TSC_OFFSET, tsc_gethrtime_tick_delta() +
 		    vmx->vcpu.arch.tsc_offset);
 	}
-
-	return (0);
 }
 
 static int
@@ -4341,31 +4192,29 @@ vmx_destroy_vcpu(struct kvm_vcpu *vcpu)
 	vcpu_vmx_t *vmx = to_vmx(vcpu);
 
 	if (vmx->vmcs != NULL) {
-		vcpu_clear(vmx);
 		kmem_free(vmx->vmcs, PAGESIZE);
 		vmx->vmcs = NULL;
 	}
 	if (vmx->guest_msrs != NULL)
 		kmem_free(vmx->guest_msrs, PAGESIZE);
 	kvm_vcpu_uninit(vcpu);
-	mutex_enter(&vmx_vpid_lock);
-	if (vmx->vpid != 0)
-		__clear_bit(vmx->vpid, vmx_vpid_bitmap);
-	mutex_exit(&vmx_vpid_lock);
+	if (vmx->vpid != 0) {
+		hma_vmx_vpid_free(vmx->vpid);
+	}
 	kmem_cache_free(kvm_vcpu_cache, vmx);
 }
 
 struct kvm_vcpu *
 vmx_create_vcpu(struct kvm *kvm, unsigned int id)
 {
-	int err;
 	struct vcpu_vmx *vmx = kmem_cache_alloc(kvm_vcpu_cache, KM_SLEEP);
-	int cpu;
+	int err;
 
 	if (!vmx)
 		return (NULL);
 
 	bzero(vmx, sizeof (struct vcpu_vmx));
+	vmx->cpu_lastrun = -1;
 
 	allocate_vpid(vmx);
 	err = kvm_vcpu_init(&vmx->vcpu, kvm, id);
@@ -4375,31 +4224,27 @@ vmx_create_vcpu(struct kvm *kvm, unsigned int id)
 	}
 
 	vmx->guest_msrs = kmem_zalloc(PAGESIZE, KM_SLEEP);
-
 	vmx->vmcs = kmem_zalloc(PAGESIZE, KM_SLEEP);
 
-	vmx->vmcs_pa = (hat_getpfnum(kas.a_hat, (caddr_t)vmx->vmcs) <<
-	    PAGESHIFT) | ((int64_t)(vmx->vmcs) & 0xfff);
-
-	kpreempt_disable();
-
-	cpu = curthread->t_cpu->cpu_seqid;
-
-	cmn_err(CE_CONT, "!vmcs revision_id = %x\n", vmcs_config.revision_id);
+	vmx->vmcs_pa = kvm_va2pa((caddr_t)vmx->vmcs);
 	vmx->vmcs->revision_id = vmcs_config.revision_id;
+	cmn_err(CE_CONT, "!vmcs revision_id = %x\n", vmcs_config.revision_id);
 
-	vmcs_clear(vmx->vmcs_pa);
-
-	vmx_vcpu_load(&vmx->vcpu, cpu);
-	err = vmx_vcpu_setup(vmx);
+	/*
+	 * Without the protection of save/restore ctxops, kpreempt_disable is
+	 * only effective if none of the code in the critical section
+	 * voluntarily goes off-cpu (such as blocking for a lock).
+	 */
+	kpreempt_disable();
+	vmx_vcpu_load(&vmx->vcpu, CPU->cpu_seqid);
+	vmx_vcpu_setup(vmx);
 	vmx_vcpu_put(&vmx->vcpu);
-
 	kpreempt_enable();
-	if (err)
-		vmx->vmcs = NULL;
-	if (vm_need_virtualize_apic_accesses(kvm))
+
+	if (vm_need_virtualize_apic_accesses(kvm)) {
 		if (alloc_apic_access_page(kvm) != 0)
 			goto free_vmcs;
+	}
 
 	if (enable_ept) {
 		if (!kvm->arch.ept_identity_map_addr)
@@ -4554,15 +4399,10 @@ struct kvm_x86_ops vmx_x86_ops = {
 	.cpu_has_kvm_support = vmx_has_kvm_support,
 	.disabled_by_bios = vmx_disabled_by_bios,
 
-	.hardware_enable = vmx_hardware_enable,
-	.hardware_disable = vmx_hardware_disable,
-
 	.check_processor_compatibility = vmx_check_processor_compat,
 
 	.hardware_setup = vmx_hardware_setup,
 
-	.hardware_unsetup = vmx_hardware_unsetup,
-
 	.cpu_has_accelerated_tpr = report_flexpriority,
 	.vcpu_create = vmx_create_vcpu,
 	.vcpu_free = vmx_destroy_vcpu, /* XXX */
@@ -4638,13 +4478,6 @@ vmx_init(void)
 	for (i = 0; i < NR_VMX_MSR; ++i)
 		kvm_define_shared_msr(i, vmx_msr_index[i]);
 
-	if (enable_vpid) {
-		vpid_bitmap_words = howmany(VMX_NR_VPIDS, 64);
-		vmx_vpid_bitmap = kmem_zalloc(sizeof (ulong_t) *
-		    vpid_bitmap_words, KM_SLEEP);
-		mutex_init(&vmx_vpid_lock, NULL, MUTEX_DRIVER, NULL);
-	}
-
 	/* A kmem cache lets us meet the alignment requirements of fx_save. */
 	kvm_vcpu_cache = kmem_cache_create("kvm_vcpu", sizeof (struct vcpu_vmx),
 	    (size_t)PAGESIZE,
@@ -4668,7 +4501,15 @@ vmx_init(void)
 	memset(vmx_msr_bitmap_legacy, 0xff, PAGESIZE);
 	memset(vmx_msr_bitmap_longmode, 0xff, PAGESIZE);
 
-	set_bit(0, vmx_vpid_bitmap); /* 0 is reserved for host */
+	/*
+	 * Cache PAs of these elements so they need not be looked up when in
+	 * the sensitive context preceeding a VMCS write.
+	 */
+	vmx_io_bitmap_a_pa = kvm_va2pa((caddr_t)vmx_io_bitmap_a);
+	vmx_io_bitmap_b_pa = kvm_va2pa((caddr_t)vmx_io_bitmap_b);
+	vmx_msr_bitmap_legacy_pa = kvm_va2pa((caddr_t)vmx_msr_bitmap_legacy);
+	vmx_msr_bitmap_longmode_pa =
+	    kvm_va2pa((caddr_t)vmx_msr_bitmap_longmode);
 
 	r = kvm_init(&vmx_x86_ops);
 
@@ -4707,10 +4548,5 @@ out:
 void
 vmx_fini(void)
 {
-	if (enable_vpid) {
-		mutex_destroy(&vmx_vpid_lock);
-		kmem_free(vmx_vpid_bitmap, sizeof (ulong_t) *
-		    vpid_bitmap_words);
-	}
 	kmem_cache_destroy(kvm_vcpu_cache);
 }
diff --git a/kvm_x86.c b/kvm_x86.c
index 791d82b..0c56233 100644
--- a/kvm_x86.c
+++ b/kvm_x86.c
@@ -95,6 +95,7 @@ static uint64_t efer_reserved_bits = 0xfffffffffffffafeULL;
 
 static void update_cr8_intercept(struct kvm_vcpu *);
 static struct kvm_shared_msrs_global shared_msrs_global;
+static struct kvm_shared_msrs *shared_msrs;
 
 void
 kvm_sigprocmask(int how, sigset_t *setp, sigset_t *osetp)
@@ -130,28 +131,13 @@ kvm_on_user_return(struct kvm_vcpu *vcpu, struct kvm_user_return_notifier *urn)
 		}
 	}
 	locals->registered = 0;
-	kvm_user_return_notifier_unregister(vcpu, urn);
-}
-
-static void
-shared_msr_update(unsigned slot, uint32_t msr)
-{
-	struct kvm_shared_msrs *smsr;
-	uint64_t value;
-	smsr = shared_msrs[CPU->cpu_id];
-
 	/*
-	 * only read, and nobody should modify it at this time,
-	 * so don't need lock
+	 * As the on-user-return handler indicates that this thread is either
+	 * returning to userspace or going off-cpu, the host MSR values should
+	 * be queried again prior to the next VM entry.
 	 */
-	if (slot >= shared_msrs_global.nr) {
-		cmn_err(CE_WARN, "kvm: invalid MSR slot!");
-		return;
-	}
-
-	rdmsrl_safe(msr, (unsigned long long *)&value);
-	smsr->values[slot].host = value;
-	smsr->values[slot].curr = value;
+	locals->host_saved = 0;
+	kvm_user_return_notifier_unregister(vcpu, urn);
 }
 
 void
@@ -165,26 +151,31 @@ kvm_define_shared_msr(unsigned slot, uint32_t msr)
 	smp_wmb();
 }
 
-static void
-kvm_shared_msr_cpu_online(void)
-{
-	unsigned i;
-
-	for (i = 0; i < shared_msrs_global.nr; i++)
-		shared_msr_update(i, shared_msrs_global.msrs[i]);
-}
-
 void
 kvm_set_shared_msr(struct kvm_vcpu *vcpu, unsigned slot, uint64_t value,
     uint64_t mask)
 {
-	struct kvm_shared_msrs *smsr = shared_msrs[CPU->cpu_id];
+	struct kvm_shared_msrs *smsr = &shared_msrs[CPU->cpu_id];
+	const uint32_t msr = shared_msrs_global.msrs[slot];
+	const uint_t slot_bit = 1 << slot;
+
+	ASSERT(slot < KVM_NR_SHARED_MSRS);
+
+	/* Preserve host MSR values prior to loading the guest data. */
+	if ((smsr->host_saved & slot_bit) == 0) {
+		uint64_t temp;
+
+		rdmsrl_safe(msr, (unsigned long long *)&temp);
+		smsr->values[slot].host = temp;
+		smsr->values[slot].curr = temp;
+		smsr->host_saved |= slot_bit;
+	}
 
 	if (((value ^ smsr->values[slot].curr) & mask) == 0)
 		return;
 
 	smsr->values[slot].curr = value;
-	wrmsrl(shared_msrs_global.msrs[slot], value);
+	wrmsrl(msr, value);
 
 	if (!smsr->registered) {
 		smsr->urn.on_user_return = kvm_on_user_return;
@@ -3374,8 +3365,7 @@ native_set_debugreg(int regno, unsigned long value)
 static int
 vcpu_enter_guest(struct kvm_vcpu *vcpu)
 {
-	int r, loaded;
-
+	int r;
 	int req_int_win = !irqchip_in_kernel(vcpu->kvm) &&
 	    vcpu->run->request_interrupt_window;
 
@@ -3421,13 +3411,19 @@ vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		}
 	}
 
-	kpreempt_disable();
+	/*
+	 * There are some narrow circumstances in which the event injection
+	 * process might sleep on a lock.  Since its logic does not require
+	 * guest-switch preparation or FPU data, complete the injection now
+	 * before entering the kpreempt-disabled critical section.
+	 */
+	inject_pending_event(vcpu);
 
+	kpreempt_disable();
 	kvm_x86_ops->prepare_guest_switch(vcpu);
 	if (vcpu->fpu_active)
 		kvm_load_guest_fpu(vcpu);
 
-	loaded = CPU->cpu_id;
 	clear_bit(KVM_REQ_KICK, &vcpu->requests);
 
 	if (vcpu->requests || issig(JUSTLOOKING)) {
@@ -3437,25 +3433,6 @@ vcpu_enter_guest(struct kvm_vcpu *vcpu)
 		goto out;
 	}
 
-	inject_pending_event(vcpu);
-
-	if (CPU->cpu_id != loaded) {
-		/*
-		 * The kpreempt_disable(), above, disables kernel migration --
-		 * but it doesn't disable migration when we block.  The call
-		 * to inject_pending_event() can, through a circuitous path,
-		 * block, and we may therefore have moved to a different CPU.
-		 * That's actually okay -- we just need to reload our state
-		 * in this case.
-		 */
-		kvm_ringbuf_record(&vcpu->kvcpu_ringbuf,
-		    KVM_RINGBUF_TAG_RELOAD, loaded);
-		kvm_x86_ops->prepare_guest_switch(vcpu);
-
-		if (vcpu->fpu_active)
-			kvm_load_guest_fpu(vcpu);
-	}
-
 	cli();
 
 	if ((r = ht_acquire()) != 1) {
@@ -4577,11 +4554,9 @@ kvm_put_guest_fpu(struct kvm_vcpu *vcpu)
 	if (vcpu->kvm->arch.need_xcr0) {
 		set_xcr(XFEATURE_ENABLED_MASK, vcpu->kvm->arch.host_xcr0);
 	}
-	KVM_TRACE1(fpu, int, 1);
 	hma_fpu_stop_guest(vcpu->arch.guest_fpu);
 	KVM_VCPU_KSTAT_INC(vcpu, kvmvs_fpu_reload);
 	set_bit(KVM_REQ_DEACTIVATE_FPU, &vcpu->requests);
-	KVM_TRACE1(fpu, int, 0);
 }
 
 void
@@ -4708,30 +4683,24 @@ kvm_arch_vcpu_reset(struct kvm_vcpu *vcpu)
 	return (kvm_x86_ops->vcpu_reset(vcpu));
 }
 
-int
-kvm_arch_hardware_enable(void *garbage)
-{
-	kvm_shared_msr_cpu_online();
-
-	return (kvm_x86_ops->hardware_enable(garbage));
-}
-
-void
-kvm_arch_hardware_disable(void *garbage)
-{
-	kvm_x86_ops->hardware_disable(garbage);
-}
-
 int
 kvm_arch_hardware_setup(void)
 {
-	return (kvm_x86_ops->hardware_setup());
+	int res;
+
+	res = kvm_x86_ops->hardware_setup();
+	if (res == 0) {
+		shared_msrs = kmem_zalloc(
+		    ncpus * sizeof (struct kvm_shared_msrs), KM_SLEEP);
+	}
+	return (res);
 }
 
 void
 kvm_arch_hardware_unsetup(void)
 {
-	kvm_x86_ops->hardware_unsetup();
+	kmem_free(shared_msrs, ncpus * sizeof (struct kvm_shared_msrs));
+	shared_msrs = NULL;
 }
 
 void
diff --git a/kvm_x86.h b/kvm_x86.h
index b90f506..668a2ee 100644
--- a/kvm_x86.h
+++ b/kvm_x86.h
@@ -17,7 +17,7 @@
  * GPL HEADER END
  *
  * Copyright 2011 various Linux Kernel contributors.
- * Copyright 2011 Joyent, Inc. All Rights Reserved.
+ * Copyright 2018 Joyent, Inc.
  */
 
 #ifndef __KVM_X86_H
@@ -283,19 +283,6 @@ typedef struct kvm_vcpu_events {
 	uint32_t reserved[10];
 } kvm_vcpu_events_t;
 
-/*
- * The following needs to run on each cpu.  Currently,
- * wait is always 1, so we use the kvm_xcall() routine which
- * calls xc_sync.  Later, if needed, the implementation can be
- * changed to use xc_call or xc_call_nowait.
- */
-#define	on_each_cpu(func, info, wait)	\
-	/*CSTYLED*/			\
-	({				\
-		kvm_xcall(KVM_CPUALL, func, info);	\
-	0;				\
-	})
-
 /*
  * The following should provide an optimization barrier.
  * If the system does reorder loads and stores, this needs to be changed.
-- 
2.21.0

