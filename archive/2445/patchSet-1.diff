commit e79b1271072fd8e1444ae211325f8e6017053821 (refs/changes/45/2445/1)
Author: Isaac Davis <isaac.davis@joyent.com>
Date:   2017-08-23T16:32:52+00:00 (2 years, 2 months ago)
    
    OS-6013 want more efficient id_space_t

diff --git a/usr/src/common/idspace/id_space.c b/usr/src/common/idspace/id_space.c
index 7d28a8f533..fffa4e911b 100644
--- a/usr/src/common/idspace/id_space.c
+++ b/usr/src/common/idspace/id_space.c
@@ -22,46 +22,280 @@
  * Copyright (c) 2000, 2010, Oracle and/or its affiliates. All rights reserved.
  */
 
-#include <sys/types.h>
-#include <sys/id_space.h>
+#include <sys/ddi.h>
 #include <sys/debug.h>
+#include <sys/id_space.h>
+#include <sys/types.h>
 
+#ifdef _KERNEL
+#include <sys/bitmap.h>
+#include <sys/kmem.h>
+#else
+#include <math.h>
+#include <stddef.h>
+#include <stdio.h>
+#include <string.h>
+#include <umem.h>
+#endif
 /*
- * ID Spaces
+ * ID Space Big Theory Statement
  *
- *   The id_space_t provides a simple implementation of a managed range of
- *   integer identifiers using a vmem arena.  An ID space guarantees that the
- *   next identifer returned by an allocation is larger than the previous one,
- *   unless there are no larger slots remaining in the range.  In this case,
- *   the ID space will return the first available slot in the lower part of the
- *   range (viewing the previous identifier as a partitioning element).  If no
- *   slots are available, id_alloc()/id_allocff() will sleep until an
- *   identifier becomes available.  Accordingly, id_space allocations must be
- *   initiated from contexts where sleeping is acceptable.  id_alloc_nosleep()/
- *   id_allocff_nosleep() will return -1 if no slots are available or if the
- *   system is low on memory.  If id_alloc_nosleep() fails, callers should
- *   not try to extend the ID space.  This is to avoid making a possible
- *   low-memory situation worse.
+ * 1. Overview
  *
- *   As an ID space is designed for representing a range of id_t's, there
- *   is a preexisting maximal range: [0, MAXUID].  ID space requests outside
- *   that range will fail on a DEBUG kernel.  The id_allocff*() functions
- *   return the first available id, and should be used when there is benefit
- *   to having a compact allocated range.
+ * The id_space_t provides an implementation of a managed range of
+ * integer identifiers.  An ID space guarantees that the
+ * next identifer returned by an allocation is larger than the previous one,
+ * unless there are no larger slots remaining in the range.  In this case,
+ * the ID space will return the first available slot in the lower part of the
+ * range (viewing the previous identifier as a partitioning element).  If no
+ * slots are available, id_alloc()/id_allocff() will sleep until an
+ * identifier becomes available.  Accordingly, id_space allocations must be
+ * initiated from contexts where sleeping is acceptable.  id_alloc_nosleep()/
+ * id_allocff_nosleep() will return -1 if no slots are available or if the
+ * system is low on memory.  If id_alloc_nosleep() fails, callers should
+ * not try to extend the ID space.  This is to avoid making a possible
+ * low-memory situation worse.
  *
- *   (Presently, the id_space_t abstraction supports only direct allocations; ID
- *   reservation, in which an ID is allocated but placed in a internal
- *   dictionary for later use, should be added when a consuming subsystem
- *   arrives.)
+ * As an ID space is designed for representing a range of id_t's, there
+ * is a preexisting maximal range: [0, MAXUID].  ID space requests outside
+ * that range will fail on a DEBUG kernel.  The id_allocff*() functions
+ * return the first available id, and should be used when there is benefit
+ * to having a compact allocated range.
  *
- *   This code is also shared with userland. In userland, we don't have the same
- *   ability to have sleeping variants, so we effectively turn the normal
- *   versions without _nosleep into _nosleep.
+ * (Presently, the id_space_t abstraction supports only direct allocations; ID
+ * reservation, in which an ID is allocated but placed in a internal
+ * dictionary for later use, should be added when a consuming subsystem
+ * arrives.)
+ *
+ * This code is also shared with userland. In userland, we don't have the same
+ * ability to have sleeping variants, so we effectively turn the normal
+ * versions without _nosleep into _nosleep.
+ *
+ * 2. Implementation
+ *
+ * The only outward-facing struct is the id_space_t - this is returned by
+ * id_space_create and must be passed to the various id space allocation
+ * functions. The struct contains an AVL tree of id_tree_t's - each id_tree_t
+ * represents a continuous range of IDs. A freshly created id_space_t will have
+ * just one id_tree_t in its AVL tree, and an additional id_tree_t will be
+ * added with every call to id_space_extend. The id_tree_t's are ordered in
+ * the AVL tree by their starting IDs, low-to-high. The ranges of ID trees
+ * do not overlap.
+ *
+ * TODO ISAAC: finish Big Theory Statement
  */
 
 #define	ID_TO_ADDR(id) ((void *)(uintptr_t)(id + 1))
 #define	ADDR_TO_ID(addr) ((id_t)((uintptr_t)addr - 1))
 
+#ifndef _KERNEL
+/*
+ * Get index of highest nonzero bit
+ */
+int
+highbit(ulong_t i)
+{
+	register int h = 1;
+
+	if (i == 0)
+		return (0);
+#ifdef _LP64
+	if (i & 0xffffffff00000000ul) {
+		h += 32; i >>= 32;
+	}
+#endif
+	if (i & 0xffff0000) {
+		h += 16; i >>= 16;
+	}
+	if (i & 0xff00) {
+		h += 8; i >>= 8;
+	}
+	if (i & 0xf0) {
+		h += 4; i >>= 4;
+	}
+	if (i & 0xc) {
+		h += 2; i >>= 2;
+	}
+	if (i & 0x2) {
+		h += 1;
+	}
+	return (h);
+}
+#endif
+
+/*
+ * Allocate an id_node_t. Returns NULL if unable to allocate.
+ */
+static id_node_t *
+id_node_alloc(boolean_t sleep)
+{
+#ifdef _KERNEL
+	int flag = sleep ? KM_SLEEP : KM_NOSLEEP;
+	return kmem_alloc(sizeof(id_node_t), flag);
+#else
+	(void) sleep;
+	return umem_alloc(sizeof(id_node_t), UMEM_DEFAULT);
+#endif
+}
+
+/*
+ * Allocate an id_tree_t. Returns NULL if unable to allocate.
+ */
+static id_tree_t *
+id_tree_alloc(boolean_t sleep)
+{
+#ifdef _KERNEL
+	int flag = sleep ? KM_SLEEP : KM_NOSLEEP;
+	return kmem_alloc(sizeof(id_tree_t), flag);
+#else
+	(void) sleep;
+	return umem_alloc(sizeof(id_tree_t), UMEM_DEFAULT);
+#endif
+}
+
+/*
+ * Allocate an id_space_t. Returns NULL if unable to allocate.
+ */
+static id_space_t *
+id_space_alloc(boolean_t sleep)
+{
+#ifdef _KERNEL
+	int flag = sleep ? KM_SLEEP : KM_NOSLEEP;
+	return kmem_alloc(sizeof(id_space_t), flag);
+#else
+	(void) sleep;
+	return umem_alloc(sizeof(id_space_t), UMEM_DEFAULT);
+#endif
+}
+
+/*
+ * Free a previously allocated id_node_t.
+ */
+static void
+id_node_free(id_node_t *inp)
+{
+#ifdef _KERNEL
+	kmem_free(inp, sizeof(id_node_t));
+#else
+	umem_free(inp, sizeof(id_node_t));
+#endif
+}
+
+/*
+ * Free a previously allocated id_tree_t.
+ */
+static void
+id_tree_free(id_tree_t *itp)
+{
+#ifdef _KERNEL
+	kmem_free(itp, sizeof(id_tree_t));
+#else
+	umem_free(itp, sizeof(id_tree_t));
+#endif
+}
+
+/*
+ * Free a previously allocated id_space_t.
+ */
+static void
+id_space_free(id_space_t *isp)
+{
+#ifdef _KERNEL
+	kmem_free(isp, sizeof(id_space_t));
+#else
+	umem_free(isp, sizeof(id_space_t));
+#endif
+}
+
+/*
+ * Allocate and initialize an id_node_t.
+ */
+static id_node_t *
+id_node_create(boolean_t sleep)
+{
+	id_node_t *np = id_node_alloc(sleep);
+	if (np == NULL) {
+		return NULL;
+	}
+	(void) memset(np, 0, sizeof(id_node_t));
+	return np;
+}
+
+/*
+ * Destroy an id_node_t previously allocated with id_node_create. Recursively
+ * destroys child nodes.
+ */
+static void
+id_node_destroy(id_node_t *np)
+{
+	int		i;
+	id_node_t	*childp;
+
+	for (i = 0; i < ID_BRANCH_FACTOR; i++) {
+		if ((childp = np->idn_children[i]) != NULL) {
+			id_node_destroy(childp);
+		}
+	}
+	id_node_free(np);
+}
+
+/*
+ * Allocate and initialize an id_tree_t.
+ */
+static id_tree_t *
+id_tree_create(boolean_t sleep, id_t low, id_t high)
+{
+	id_tree_t *itp = id_tree_alloc(sleep);
+	if (itp == NULL) {
+		return NULL;
+	}
+	if ((itp->idt_root = id_node_create(sleep)) == NULL) {
+		id_tree_free(itp);
+		return NULL;
+	}
+	itp->idt_height	= 0;
+	itp->idt_offset = low;
+	itp->idt_size = 0;
+	/*
+	 * Note that max IS NOT the highest ID contained in the tree - it is the
+	 * maximum number of IDs the tree contains.
+	 */
+	itp->idt_max = high - low;
+	return itp;
+}
+
+/*
+ * Destroy an id_tree_t previously allocated with id_tree_create. Kicks off
+ * destruction of all nodes by destroying root.
+ */
+static void
+id_tree_destroy(id_tree_t *tp)
+{
+	id_node_destroy(tp->idt_root);
+	id_tree_free(tp);
+}
+
+/*
+ * Comparator for use in id_space_t's AVL tree of id_tree_t's. A tree with a
+ * lower offset (e.g. lower starting ID) is "less" than a tree with a higher
+ * offset.
+ */
+static int
+id_tree_compare(const void *first, const void *second)
+{
+	const id_tree_t	*tree1 = first;
+	const id_tree_t	*tree2 = second;
+	uint32_t	result = 0;
+
+	if (tree1->idt_offset < tree2->idt_offset) {
+		result = -1;
+	} else if (tree1->idt_offset > tree2->idt_offset) {
+		result = 1;
+	}
+
+	return result;
+}
+
 /*
  * Create an arena to represent the range [low, high).
  * Caller must be in a context in which VM_SLEEP is legal,
@@ -70,16 +304,40 @@
 id_space_t *
 id_space_create(const char *name, id_t low, id_t high)
 {
-#ifdef _KERNEL
-	int flag = VM_SLEEP;
-#else
-	int flag = VM_NOSLEEP;
-#endif
+	id_space_t	*isp;
+	id_tree_t	*itp;
+
 	ASSERT(low >= 0);
 	ASSERT(low < high);
+	ASSERT(high <= MAXUID + 1);
+
+	if ((isp = id_space_alloc(B_TRUE)) == NULL) {
+		return NULL;
+	}
+	if ((itp = id_tree_create(B_TRUE, low, high)) == NULL) {
+		id_space_free(isp);
+		return NULL;
+	}
+
+	/* Create AVL tree and add id_tree_t */
+	avl_create(&isp->id_trees, id_tree_compare, sizeof(id_tree_t),
+		offsetof(id_tree_t, idt_avl));
+	avl_add(&isp->id_trees, itp);
+
+	(void) snprintf(isp->id_name, ID_NAMELEN, "%s", name);
+	isp->id_high = high;
+	isp->id_low = low;
+	isp->id_next_free = low;
 
-	return (vmem_create(name, ID_TO_ADDR(low), high - low, 1,
-	    NULL, NULL, NULL, 0, flag | VMC_IDENTIFIER));
+	/* Initialize mutexes and condvar as appropriate*/
+#ifdef _KERNEL
+	mutex_init(&isp->id_lock, NULL, MUTEX_DEFAULT, NULL);
+	mutex_init(&isp->id_cond_lock, NULL, MUTEX_DEFAULT, NULL);
+	cv_init(&isp->id_cond, NULL, CV_DEFAULT, NULL);
+#else
+	(void) mutex_init(&isp->id_lock, USYNC_THREAD | LOCK_ERRORCHECK, NULL);
+#endif
+	return isp;
 }
 
 /*
@@ -89,18 +347,522 @@ id_space_create(const char *name, id_t low, id_t high)
 void
 id_space_destroy(id_space_t *isp)
 {
-	vmem_destroy(isp);
+	id_tree_t	*itp;
+	void		*c = NULL;
+
+	/* Destroy all id_tree_t's in AVL tree */
+	while((itp = avl_destroy_nodes(&isp->id_trees, &c)) != NULL) {
+		id_tree_destroy(itp);
+	}
+
+	avl_destroy(&isp->id_trees);
+
+#ifdef _KERNEL
+	mutex_destroy(&isp->id_lock);
+	mutex_destroy(&isp->id_cond_lock);
+	cv_destroy(&isp->id_cond);
+#else
+	(void) mutex_destroy(&isp->id_lock);
+#endif
+	id_space_free(isp);
 }
 
+/*
+ * Add the specified range of IDs to an existing ID space. The new range is
+ * allowed to overlap arbitrarily with existing ranges. This significantly
+ * complicates the logic required of this function.
+ *
+ * As we iterate through the existing ID ranges, the new ID range will fall into
+ * one of four cases, as illustrated below:
+ *
+ * key:
+ *          [             ]   Existing ID range
+ *        *******             Range to be added
+ *
+ * Case 1:  [   ******    ]
+ * The range to be added lies completely inside the existing range. We do
+ * nothing.
+ *
+ * Case 2:  [      *******]**
+ * The new range overlaps the end of the old range. We truncate the beginning of
+ * the new range.
+ *
+ * Case 3: *[*****        ]
+ * The new range overlaps the beginning of the old range. We truncate the
+ * end of the new range.
+ *
+ * Case 4: *[*************]**
+ * The new range completely contains the old range. We split the new range into
+ * two smaller ranges on either side of the old range.
+ *
+ * The structure and documentation of this function are shamelessly lifted from
+ * the virtual memory implementation in Brown University's Weenix.
+ */
 void
 id_space_extend(id_space_t *isp, id_t low, id_t high)
 {
-#ifdef _KERNEL
-	int flag = VM_SLEEP;
-#else
-	int flag = VM_NOSLEEP;
-#endif
-	(void) vmem_add(isp, ID_TO_ADDR(low), high - low, flag);
+	id_t		adjusted_low = low;
+	id_t		adjusted_high = high;
+	id_t		itp_low;
+	id_t		itp_high;
+	id_tree_t	*itp;
+	id_tree_t	*new_tree;
+
+	ASSERT(isp != NULL);
+	ASSERT(low >= 0);
+	ASSERT(high <= MAXUID + 1);
+
+	mutex_enter(&isp->id_lock);
+
+	/* Iterate through all existing id_tree_t's in the AVL tree */
+	for (itp = avl_first(&isp->id_trees); itp != NULL;
+		itp = AVL_NEXT(&isp->id_trees, itp)) {
+
+		/* Get ID range of current tree */
+		itp_low = itp->idt_offset;
+		itp_high = itp->idt_max + itp_low;
+
+		/* We're past the new range */
+                if (adjusted_high <= itp_low) {
+                        break;
+		}
+
+                /* We haven't reached the new range */
+		if (adjusted_low >= itp_high) {
+			continue;
+		}
+
+                /*
+		 * Start of new range is less than start of existing range, but
+		 * end of new range is not less than start of existing range
+                 *   --> new range overlaps in SOME way
+                 */
+                if (adjusted_low < itp_low) {
+                        if (adjusted_high > itp_high) {
+                                /*
+				 * Case 4: old range completely covered - split
+				 * new range, allocate lower part, and keep
+				 * iterating through trees
+				 */
+                                new_tree = id_tree_create(B_TRUE, adjusted_low,
+					itp_low);
+				avl_add(&isp->id_trees, new_tree);
+				if (adjusted_low < isp->id_low) {
+					isp->id_low = adjusted_low;
+				}
+				adjusted_low = itp_high;
+                        } else {
+                                /* Case 3: truncate end of new range */
+				adjusted_high = itp_low;
+                        }
+                } else if (adjusted_high > itp_high) {
+			/* Case 2: truncate beginning of new range */
+			adjusted_low = itp_high;
+		} else {
+			/*
+			 * Case 1: new range completely encompassed by old
+			 * range - do nothing
+			 */
+			mutex_exit(&isp->id_lock);
+			return;
+		}
+	}
+
+	/* Allocate the free range and update space's high and low */
+	new_tree = id_tree_create(B_TRUE, adjusted_low, adjusted_high);
+	avl_add(&isp->id_trees, new_tree);
+	if (adjusted_high > isp->id_high) {
+		isp->id_high = adjusted_high;
+	}
+	if (adjusted_low < isp->id_low) {
+		isp->id_low = adjusted_low;
+	}
+
+	mutex_exit(&isp->id_lock);
+	return;
+}
+
+/*
+ * Get index of first 0 in bitfield between [low, high)
+ */
+static uint32_t
+get_open_slot(ulong_t *bitfield, uint32_t low, uint32_t high)
+{
+	uint32_t	result = 0;
+	uint32_t 	long_bits = sizeof(ulong_t) * 8;
+	/* Number of longs in bitfield*/
+	uint32_t	arr_len = ID_BRANCH_FACTOR / long_bits;
+	uint32_t	i;
+	/* Offsets of low and high relative to a single long */
+	uint32_t	low_offset = low % long_bits;
+	uint32_t	high_offset = high % long_bits;
+	/* Indices of longs for which masks must be applied */
+	uint32_t	low_mask_idx = low / long_bits;
+	uint32_t	high_mask_idx = high / long_bits;
+	/* Masks for range outside of [low, high) */
+	ulong_t		low_mask;
+	ulong_t		high_mask;
+
+	ASSERT(low < high);
+
+	for (i = low/long_bits; i < arr_len; i++) {
+		/* Low mask is irrelevant - we're past it */
+		if (i > low_mask_idx) {
+			low_mask = 0;
+		/* Mask out start of long */
+		} else {
+			/* Avoid shifting by long_bits - this is undefined */
+			low_mask = (low_offset == 0) ? 0
+				: -1L << (long_bits - low_offset);
+		}
+
+		/* High mask is irrelevant - we haven't hit it yet */
+		if (i < high_mask_idx) {
+			high_mask = 0;
+		/* We haven't hit the valid range yet - keep going */
+		} else if (i > high_mask_idx) {
+			continue;
+		/* Mask out end of long */
+		} else {
+			/* Avoid shifting by long_bits - this is undefined */
+			high_mask = (high_offset == 0) ? 0
+				: (1L << (long_bits - (high % long_bits))) - 1;
+		}
+
+		/* Get index of first bit that is a 0, after applying mask */
+		result = long_bits
+			- highbit(~(bitfield[i] | low_mask | high_mask));
+
+		/*
+		 * If the first open index == long_bits, there _is_ no open
+		 * index, so we continue. Otherwise, we return the result,
+		 * adding i * long_bits to account for where we're at in
+		 * the array of longs.
+		 */
+		if (result < long_bits) {
+			ASSERT((i * long_bits) + result >= low);
+			return (i * long_bits) + result;
+		}
+	}
+
+	/* Return -1 if we don't find any open index */
+	return (uint32_t) -1;
+}
+
+/*
+ * Sets bit in bitfield to value passed in - 0 or 1.
+ */
+static void
+set_bit(ulong_t *bitfield, uint32_t index, uint32_t value)
+{
+	uint32_t	long_bits = sizeof(ulong_t) * 8;
+	ulong_t 	mask = 1L << (long_bits - (index % long_bits) - 1);
+
+	ASSERT(index < ID_BRANCH_FACTOR);
+	ASSERT((value == 0) || (value == 1));
+
+	if (value) {
+		bitfield[index / long_bits] |= mask;
+	} else {
+		bitfield[index / long_bits] &= ~mask;
+	}
+	return;
+}
+
+/*
+ * Grows height of tree by one level if tree is currently full but has not
+ * yet reached max number of IDs it is allowed to hold. Returns -1 if the tree
+ * has reached its maximum size or cannot be expanded.
+ */
+static int
+expand_tree(id_tree_t *itp, boolean_t sleep)
+{
+	id_node_t *new_root;
+
+	/* We're not allowed to make the tree bigger */
+	if (itp->idt_size == itp->idt_max) {
+		return -1;
+	}
+
+	/*
+	 * tree is currently full - note that this doesn't cover the case where
+	 * the tree is at its maximum size but the root's bitfield isn't
+	 * totally full - this is handled by the "if" statement above.
+	 */
+	if (get_open_slot(itp->idt_root->idn_bitfield, 0, ID_BRANCH_FACTOR)
+		== -1) {
+
+		/*
+		 * Make a new root and make the current root the first child of
+		 * the new root.
+		 */
+		if ((new_root = id_node_create(sleep)) == NULL) {
+			return -1;
+		}
+		new_root->idn_children[0] = itp->idt_root;
+		set_bit(new_root->idn_bitfield, 0, 1);
+		itp->idt_root = new_root;
+		itp->idt_height++;
+		return 0;
+	}
+
+	/* We didn't do anything, but that's ok */
+	return 0;
+}
+
+/*
+ * This is where most of the magic happens. Walks the given tree looking for an
+ * ID to allocate or free. Note that the wrap-around behavior DOES NOT occur
+ * in this function - this is because this function only involves a single
+ * id_tree_t, and we must wrap around ALL id_tree_t's in the AVL tree.
+ *
+ * Arguments:
+ *	itp:	The tree to walk.
+ *	start:	The inded of the first ID to consider. For id_alloc, this will
+ *		be continuously incremented, and for id_allocff, it will be 0.
+ *	exact:	Whether or not we're looking for the _exact_ id passed in, or
+ *		just an ID larger than the id passed in. Should be B_TRUE for
+ *		id_alloc_specific and id_free and B_FALSE for everything else.
+ *	alloc:	Whether we're allocating or freeing an ID
+ *	sleep:	Specifies whether we sleep or fail if we can't alloc or free an
+ *		ID.
+ */
+static id_t
+id_tree_walk(id_tree_t *itp, id_t start, boolean_t exact, boolean_t alloc,
+	boolean_t sleep)
+{
+	/* Variables for tree traversal */
+	uint32_t	idx = start;
+	uint32_t	scale_power;
+	uint32_t	tree_levels;
+	uint32_t 	i;
+	id_node_t	*curr_node;
+	id_node_t	*new_node;
+	/* Variables to keep track of path we've taken */
+	id_node_t	*path_nodes[ID_MAX_HEIGHT];
+	uint32_t	path_slots[ID_MAX_HEIGHT];
+	boolean_t	backtrack = B_FALSE;
+	id_t		child_state;
+	/* Variables for maintaining next bitfield slot to look for */
+	int32_t		slot;
+	uint32_t	maxslot;
+	id_t		next_leaf_start;
+	id_t		result = 0;
+
+	/* Add a layer to the tree if we need to */
+	if (alloc && expand_tree(itp, sleep) == -1) {
+		return -1;
+	}
+
+	curr_node = itp->idt_root;
+	tree_levels = itp->idt_height + 1;
+	scale_power = pow(ID_BRANCH_FACTOR, tree_levels);
+
+	/* Iterate down tree */
+	for (i = 0; i < tree_levels; i++) {
+		/*
+		 * idx is an index into the combined bitfields of the current
+		 * level of the tree, treated as one big array. For example,
+		 * if the current level of the tree had two nodes, each with a
+		 * bitfield of length 4, then the first index of the first
+		 * node's bitfield would have idx == 0, and the first index of
+		 * the second node's bitfield would have idx == 5.
+		 */
+		idx %= scale_power;
+		scale_power /= ID_BRANCH_FACTOR;
+
+		/*
+		 * maxslot puts an upper limit on any bitfields that should
+		 * never be fully filled because the size of the ID space is
+		 * in-between tree sizes. It will only ever be relevant at the
+		 * right edge of the tree.
+		 */
+		maxslot = min(((itp->idt_max - 1 - result) / scale_power) + 1,
+			ID_BRANCH_FACTOR);
+		/* Try to take exact path down tree, return -1 if we can't */
+		if (exact) {
+			slot = idx / scale_power;
+			if (alloc && get_open_slot(curr_node->idn_bitfield,
+				slot, maxslot) != slot) {
+				return -1;
+			}
+		/* Get next open slot*/
+		} else {
+			ASSERT(alloc); /* Inexact frees are not permitted */
+			slot = get_open_slot(curr_node->idn_bitfield,
+				idx / scale_power, maxslot);
+			if (slot == -1) {
+				/*
+				 * If we get here, it means we've reached a leaf
+				 * that DOES have open slots, but the open slots
+				 * are all smaller than the "start" ID. This
+				 * means we should check the next leaf over in
+				 * the tree. We do this recursively.
+				 *
+				 * This case should be fairly rare.
+				 */
+				next_leaf_start = start + ID_BRANCH_FACTOR -
+					(start % ID_BRANCH_FACTOR);
+				if (next_leaf_start > itp->idt_max) {
+					return -1;
+				}
+				return id_tree_walk(itp, next_leaf_start, exact,
+					alloc, sleep);
+			}
+			/*
+			 * If the next open slot is larger than the intended
+			 * slot (we didn't get the exact ID we wanted), we
+			 * update idx to correspond to the actual slot that we
+			 * got - this becomes relevant when idx is scaled on the
+			 * next iteration of the loop. 
+			 */
+			if (slot > idx/scale_power) {
+				idx = slot * scale_power;
+			}
+		}
+
+		/* Fill in our path bookkeeping in case we need to backtrack */
+		path_nodes[i] = curr_node;
+		path_slots[i] = slot;
+		/* Accumulate our slot in the final result */
+		result += slot * scale_power;
+
+		/*
+		 * If we're not yet at a leaf, we advance curr_node to the
+		 * child corresponding to slot. If the child hasn't been
+		 * allocated yet, we allocate it.
+		 */
+		if (i != tree_levels - 1) {
+			if (curr_node->idn_children[slot] == NULL) {
+				new_node = id_node_create(sleep);
+				if (!sleep && new_node == NULL) {
+					return -1;
+				}
+				curr_node->idn_children[slot] = new_node;
+			}
+			curr_node = curr_node->idn_children[slot];
+		}
+	}
+
+	/* If we're freeing from a fully allocated leaf, we backtrack */
+	if (!alloc
+		&& (get_open_slot(curr_node->idn_bitfield, 0, maxslot) == -1)) {
+		backtrack = B_TRUE;
+	}
+	/* Update the bit in the leaf */
+	set_bit(curr_node->idn_bitfield, slot, alloc);
+	/* If the leaf has become full after we allocated, we backtrack */
+	if (alloc
+		&& (get_open_slot(curr_node->idn_bitfield, 0, maxslot) == -1)) {
+		backtrack = B_TRUE;
+	}
+
+	/* Follow our path back up the tree, updating parents as necessary */
+	if (backtrack) {
+		for (i = tree_levels - 1; i >= 1; i--) {
+			curr_node = path_nodes[i];
+			child_state = get_open_slot(curr_node->idn_bitfield, 0,
+				maxslot);
+			if ((alloc && child_state == -1)
+				|| (!alloc && child_state != -1)) {
+
+				slot = path_slots[i-1];
+				set_bit(path_nodes[i-1]->idn_bitfield, slot,
+					alloc);
+			}
+		}
+	}
+
+	/* Update tree size */
+	if (alloc) {
+		itp->idt_size++;
+	} else {
+		itp->idt_size--;
+	}
+
+	ASSERT(result >= start); /* No wrapping here */
+	return result;
+}
+
+/*
+ * Called by the user-facing ID space functions.
+ * Traverses AVL tree of id_tree_t's to find suitable tree for ID allocation,
+ * then calls id_walk_tree to allocate an ID. This function is where the
+ * wrap-around from high to low happens.
+ *
+ * Arguments:
+ *	isp:	The ID space to allocate from.
+ *	start:	The inded of the first ID to consider. For id_alloc, this will
+ *		be continuously incremented, and for id_allocff, it will be 0.
+ *	exact:	Whether or not we're looking for the _exact_ id passed in, or
+ *		just an ID larger than the id passed in. Should be B_TRUE for
+ *		id_alloc_specific and id_free and B_FALSE for everything else.
+ *		If it's B_FALSE, we'll automatically wrap around to the
+ *		beginning of the AVL tree if there are no IDs larger than start
+ * 		that meet our criteria.
+ *	alloc:	Whether we're allocating or freeing an ID
+ *	sleep:	Specifies whether we sleep or fail if we can't alloc or free an
+ *		ID.
+ */
+static id_t
+id_get_id(id_space_t *isp, id_t start, id_t exact, boolean_t alloc,
+	boolean_t sleep)
+{
+	id_tree_t	*itp;
+	uint32_t	i;
+	uint32_t	start_tree; /* position of first viable tree */
+	id_t		id;
+
+	/* Find first candidate tree */
+	for (itp = avl_first(&isp->id_trees), i = 0; itp != NULL;
+		itp = AVL_NEXT(&isp->id_trees, itp), i++) {
+
+		start_tree = i;
+		/* This tree has a suitable range - try to allocate an ID */
+		if (itp->idt_offset <= start
+			&& (itp->idt_max + itp->idt_offset) > start) {
+
+			id = id_tree_walk(itp, start - itp->idt_offset, exact, alloc, sleep);
+			/* If this tree is full, move to the next one */
+			if (id == -1) {
+				break;
+			}
+			/* We found an id! */
+			return id + itp->idt_offset;
+		}
+		/*
+		 * We've gone too far so we should move to the next loop because
+		 * everything is fair game now
+		 */
+		if (itp->idt_offset > start) {
+			break;
+		}
+	}
+
+	/* Only look at other trees if we're not looking for a specific id */
+	if (!exact) {
+		/* First loop through the trees after the starting tree */
+		for (; itp != NULL; itp = AVL_NEXT(&isp->id_trees, itp)) {
+			id = id_tree_walk(itp, 0, B_FALSE, alloc, sleep);
+			if (id != -1) {
+				ASSERT(id > start);
+				return id + itp->idt_offset;
+			}
+		}
+
+		/* Then loop through the trees BEFORE the starting tree */
+		for (itp = avl_first(&isp->id_trees), i = 0; i <= start_tree;
+			itp = AVL_NEXT(&isp->id_trees, itp), i++) {
+			id = id_tree_walk(itp, 0, B_FALSE, alloc, sleep);
+			if (id != -1) {
+				ASSERT(id < start);
+				return id + itp->idt_offset;
+			}
+		}
+	}
+
+	/* Everything is totally full */
+	return -1;
 }
 
 /*
@@ -110,12 +872,27 @@ id_space_extend(id_space_t *isp, id_t low, id_t high)
 id_t
 id_alloc(id_space_t *isp)
 {
+	id_t result;
+	mutex_enter(&isp->id_lock);
 #ifdef _KERNEL
-	int flag = VM_SLEEP;
+	/* Try to get an ID until we succeed, sleeping in-between attempts */
+	mutex_enter(&isp->id_cond_lock);
+	while ((result = id_get_id(isp, isp->id_next_free, B_FALSE, B_TRUE,
+		B_TRUE)) == -1) {
+		cv_wait(&isp->id_cond, &isp->id_cond_lock);
+	}
+	mutex_exit(&isp->id_cond_lock);
 #else
-	int flag = VM_NOSLEEP;
+	result = id_get_id(isp, isp->id_next_free, B_FALSE, B_TRUE, B_TRUE);
 #endif
-	return (ADDR_TO_ID(vmem_alloc(isp, 1, flag | VM_NEXTFIT)));
+	if (result != -1) {
+		isp->id_next_free = result + 1;
+		if (isp->id_next_free == isp->id_high) {
+			isp->id_next_free = isp->id_low;
+		}
+	}
+	mutex_exit(&isp->id_lock);
+	return result;
 }
 
 /*
@@ -126,7 +903,17 @@ id_alloc(id_space_t *isp)
 id_t
 id_alloc_nosleep(id_space_t *isp)
 {
-	return (ADDR_TO_ID(vmem_alloc(isp, 1, VM_NOSLEEP | VM_NEXTFIT)));
+	id_t result;
+	mutex_enter(&isp->id_lock);
+	result = id_get_id(isp, isp->id_next_free, B_FALSE, B_TRUE, B_FALSE);
+	if (result != -1) {
+		isp->id_next_free = result + 1;
+		if (isp->id_next_free == isp->id_high) {
+			isp->id_next_free = isp->id_low;
+		}
+	}
+	mutex_exit(&isp->id_lock);
+	return result;
 }
 
 /*
@@ -136,12 +923,21 @@ id_alloc_nosleep(id_space_t *isp)
 id_t
 id_allocff(id_space_t *isp)
 {
+	id_t result;
+	mutex_enter(&isp->id_lock);
 #ifdef _KERNEL
-	int flag = VM_SLEEP;
+	/* Try to get an ID until we succeed, sleeping in-between attempts */
+	mutex_enter(&isp->id_cond_lock);
+	while ((result = id_get_id(isp, isp->id_low, B_FALSE, B_TRUE, B_TRUE))
+		== -1) {
+		cv_wait(&isp->id_cond, &isp->id_cond_lock);
+	}
+	mutex_exit(&isp->id_cond_lock);
 #else
-	int flag = VM_NOSLEEP;
+	result = id_get_id(isp, isp->id_low, B_FALSE, B_TRUE, B_TRUE);
 #endif
-	return (ADDR_TO_ID(vmem_alloc(isp, 1, flag | VM_FIRSTFIT)));
+	mutex_exit(&isp->id_lock);
+	return result;
 }
 
 /*
@@ -152,7 +948,11 @@ id_allocff(id_space_t *isp)
 id_t
 id_allocff_nosleep(id_space_t *isp)
 {
-	return (ADDR_TO_ID(vmem_alloc(isp, 1, VM_NOSLEEP | VM_FIRSTFIT)));
+	id_t result;
+	mutex_enter(&isp->id_lock);	
+	result = id_get_id(isp, isp->id_low, B_FALSE, B_TRUE, B_FALSE);
+	mutex_exit(&isp->id_lock);
+	return result;
 }
 
 /*
@@ -162,15 +962,11 @@ id_allocff_nosleep(id_space_t *isp)
 id_t
 id_alloc_specific_nosleep(id_space_t *isp, id_t id)
 {
-	void *minaddr = ID_TO_ADDR(id);
-	void *maxaddr = ID_TO_ADDR(id + 1);
-
-	/*
-	 * Note that even though we're vmem_free()ing this later, it
-	 * should be OK, since there's no quantum cache.
-	 */
-	return (ADDR_TO_ID(vmem_xalloc(isp, 1, 1, 0, 0,
-	    minaddr, maxaddr, VM_NOSLEEP)));
+	id_t	result;
+	mutex_enter(&isp->id_lock);
+	result = id_get_id(isp, id, B_TRUE, B_TRUE, B_TRUE);
+	mutex_exit(&isp->id_lock);
+	return result;
 }
 
 /*
@@ -180,5 +976,13 @@ id_alloc_specific_nosleep(id_space_t *isp, id_t id)
 void
 id_free(id_space_t *isp, id_t id)
 {
-	vmem_free(isp, ID_TO_ADDR(id), 1);
+	mutex_enter(&isp->id_lock);
+#ifdef _KERNEL
+	/* Wake up a single waiting thread */
+	mutex_enter(&isp->id_cond_lock);
+	cv_signal(&isp->id_cond);
+	mutex_exit(&isp->id_cond_lock);
+#endif
+	(void) id_get_id(isp, id, B_TRUE, B_FALSE, B_FALSE);
+	mutex_exit(&isp->id_lock);
 }
diff --git a/usr/src/uts/common/sys/id_space.h b/usr/src/uts/common/sys/id_space.h
index 46d25f207f..7b58190711 100644
--- a/usr/src/uts/common/sys/id_space.h
+++ b/usr/src/uts/common/sys/id_space.h
@@ -20,7 +20,7 @@
  */
 /*
  * Copyright (c) 2000, 2010, Oracle and/or its affiliates. All rights reserved.
- * Copyright (c) 2014, Joyent, Inc.  All Rights reserved.
+ * Copyright (c) 2017, Joyent, Inc.  All rights reserved.
  */
 
 #ifndef	_ID_SPACE_H
@@ -32,10 +32,48 @@ extern "C" {
 
 #include <sys/param.h>
 #include <sys/types.h>
+#include <sys/avl.h>
+
+#ifdef _KERNEL
 #include <sys/mutex.h>
-#include <sys/vmem.h>
+#include <sys/ksynch.h>
+#else
+#include <thread.h>
+#endif
+
+#define ID_BRANCH_FACTOR 128
+#define ID_MAX_HEIGHT 10
+#define ID_NAMELEN 30
+
+typedef struct id_node {
+	ulong_t		idn_bitfield[ID_BRANCH_FACTOR/(8 * sizeof(ulong_t))];
+	struct id_node	*idn_children[ID_BRANCH_FACTOR];
+} id_node_t;
 
-typedef vmem_t id_space_t;
+typedef struct id_tree {
+	avl_node_t	idt_avl;
+	id_node_t	*idt_root;
+	size_t		idt_height;
+	id_t		idt_offset;
+	id_t		idt_max;	/* max size, NOT highest value */
+	size_t		idt_size;
+} id_tree_t;
+
+typedef struct id_space {
+	char 		id_name[ID_NAMELEN];
+	avl_tree_t	id_trees;		/* an AVL tree of id_trees */
+	id_t		id_next_free;
+	id_t		id_high;
+	id_t		id_low;
+
+#ifdef _KERNEL
+	kmutex_t	id_lock;
+	kmutex_t	id_cond_lock;
+	kcondvar_t	id_cond;
+#else
+	mutex_t		id_lock;
+#endif
+} id_space_t;
 
 id_space_t *id_space_create(const char *, id_t, id_t);
 void id_space_destroy(id_space_t *);
