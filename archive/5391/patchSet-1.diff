From beffa6d67cd1f8a68044d871b7163fa7f3848687 Mon Sep 17 00:00:00 2001
From: Patrick Mooney <pmooney@pfmooney.com>
Date: Wed, 12 Dec 2018 22:32:29 +0000
Subject: [PATCH] OS-7447 formalize bhyve resource exclusion

---
 usr/src/uts/i86pc/io/viona/viona.c     | 305 ++++++++++++++-----
 usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c | 387 ++++++++++++++++++-------
 usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c  |  12 +-
 usr/src/uts/i86pc/sys/vmm_drv.h        |  17 +-
 usr/src/uts/i86pc/sys/vmm_impl.h       |  17 +-
 5 files changed, 539 insertions(+), 199 deletions(-)

diff --git a/usr/src/uts/i86pc/io/viona/viona.c b/usr/src/uts/i86pc/io/viona/viona.c
index 0a8fd9f141..8dc1221a55 100644
--- a/usr/src/uts/i86pc/io/viona/viona.c
+++ b/usr/src/uts/i86pc/io/viona/viona.c
@@ -34,7 +34,7 @@
  * http://www.illumos.org/license/CDDL.
  *
  * Copyright 2015 Pluribus Networks Inc.
- * Copyright 2018 Joyent, Inc.
+ * Copyright 2019 Joyent, Inc.
  */
 
 /*
@@ -411,6 +411,7 @@ typedef struct viona_vring {
 	uint_t		vr_xfer_outstanding;
 	kthread_t	*vr_worker_thread;
 	viona_desb_t	*vr_desb;
+	vmm_lease_t	*vr_lease;
 
 	uint_t		vr_intr_enabled;
 	uint64_t	vr_msi_addr;
@@ -419,6 +420,7 @@ typedef struct viona_vring {
 	/* Internal ring-related state */
 	kmutex_t	vr_a_mutex;	/* sync consumers of 'avail' */
 	kmutex_t	vr_u_mutex;	/* sync consumers of 'used' */
+	uint64_t	vr_pa;
 	uint16_t	vr_size;
 	uint16_t	vr_mask;	/* cached from vr_size */
 	uint16_t	vr_cur_aidx;	/* trails behind 'avail_idx' */
@@ -568,12 +570,14 @@ static int viona_chpoll(dev_t dev, short events, int anyyet, short *reventsp,
 static int viona_ioc_create(viona_soft_state_t *, void *, int, cred_t *);
 static int viona_ioc_delete(viona_soft_state_t *, boolean_t);
 
-static void *viona_gpa2kva(viona_link_t *link, uint64_t gpa, size_t len);
+static void *viona_gpa2kva(viona_vring_t *, uint64_t, size_t);
 
 static void viona_ring_alloc(viona_link_t *, viona_vring_t *);
 static void viona_ring_free(viona_vring_t *);
 static int viona_ring_reset(viona_vring_t *, boolean_t);
 static kthread_t *viona_create_worker(viona_vring_t *);
+static boolean_t viona_ring_map(viona_vring_t *);
+static void viona_ring_unmap(viona_vring_t *);
 
 static int viona_ioc_set_notify_ioport(viona_link_t *, uint_t);
 static int viona_ioc_ring_init(viona_link_t *, void *, int);
@@ -586,6 +590,7 @@ static int viona_ioc_intr_poll(viona_link_t *, void *, int, int *);
 static void viona_intr_ring(viona_vring_t *);
 
 static void viona_desb_release(viona_desb_t *);
+static void viona_tx_wait_outstanding(viona_vring_t *);
 static void viona_rx(void *, mac_resource_handle_t, mblk_t *, boolean_t);
 static void viona_tx(viona_link_t *, viona_vring_t *);
 
@@ -882,7 +887,7 @@ viona_ioctl(dev_t dev, int cmd, intptr_t data, int md, cred_t *cr, int *rv)
 
 	mutex_enter(&ss->ss_lock);
 	if ((link = ss->ss_link) == NULL || link->l_destroyed ||
-	    vmm_drv_expired(link->l_vm_hold)) {
+	    vmm_drv_release_reqd(link->l_vm_hold)) {
 		mutex_exit(&ss->ss_lock);
 		return (ENXIO);
 	}
@@ -1182,9 +1187,72 @@ viona_ioc_delete(viona_soft_state_t *ss, boolean_t on_close)
  * Translate a guest physical address into a kernel virtual address.
  */
 static void *
-viona_gpa2kva(viona_link_t *link, uint64_t gpa, size_t len)
+viona_gpa2kva(viona_vring_t *ring, uint64_t gpa, size_t len)
 {
-	return (vmm_drv_gpa2kva(link->l_vm_hold, gpa, len));
+	ASSERT3P(ring->vr_lease, !=, NULL);
+
+	return (vmm_drv_gpa2kva(ring->vr_lease, gpa, len));
+}
+
+static void
+viona_ring_lease_expire_cb(void *arg)
+{
+	viona_vring_t *ring = arg;
+
+	cv_broadcast(&ring->vr_cv);
+}
+
+static void
+viona_ring_lease_drop(viona_vring_t *ring)
+{
+	ASSERT(MUTEX_HELD(&ring->vr_lock));
+
+	if (ring->vr_lease != NULL) {
+		vmm_hold_t *hold = ring->vr_link->l_vm_hold;
+
+		ASSERT(hold != NULL);
+
+		/*
+		 * Without an active lease, the ring mappings cannot be
+		 * considered valid.
+		 */
+		viona_ring_unmap(ring);
+
+		vmm_drv_lease_break(hold, ring->vr_lease);
+		ring->vr_lease = NULL;
+	}
+}
+
+static boolean_t
+viona_ring_lease_renew(viona_vring_t *ring)
+{
+	vmm_hold_t *hold = ring->vr_link->l_vm_hold;
+
+	ASSERT(hold != NULL);
+	ASSERT(MUTEX_HELD(&ring->vr_lock));
+
+	viona_ring_lease_drop(ring);
+
+	/*
+	 * Lease renewal will fail if the VM has requested that all holds be
+	 * cleaned up.
+	 */
+	ring->vr_lease = vmm_drv_lease_sign(hold, viona_ring_lease_expire_cb,
+	    ring);
+	if (ring->vr_lease != NULL) {
+		/* A ring undergoing renewal will need valid guest mappings */
+		if (ring->vr_pa != 0 && ring->vr_size != 0) {
+			/*
+			 * If new mappings cannot be established, consider the
+			 * lease renewal a failure.
+			 */
+			if (!viona_ring_map(ring)) {
+				viona_ring_lease_drop(ring);
+				return (B_FALSE);
+			}
+		}
+	}
+	return (ring->vr_lease != NULL);
 }
 
 static void
@@ -1246,19 +1314,78 @@ viona_ring_reset(viona_vring_t *ring, boolean_t heed_signals)
 			}
 		}
 	}
+	viona_ring_lease_drop(ring);
 	mutex_exit(&ring->vr_lock);
 	return (0);
 }
 
+static boolean_t
+viona_ring_map(viona_vring_t *ring)
+{
+	uint64_t pos = ring->vr_pa;
+	const uint16_t qsz = ring->vr_size;
+
+	ASSERT3U(qsz, !=, 0);
+	ASSERT3U(pos, !=, 0);
+	ASSERT(MUTEX_HELD(&ring->vr_lock));
+
+	const size_t desc_sz = qsz * sizeof (struct virtio_desc);
+	ring->vr_descr = viona_gpa2kva(ring, pos, desc_sz);
+	if (ring->vr_descr == NULL) {
+		goto fail;
+	}
+	pos += desc_sz;
+
+	const size_t avail_sz = (qsz + 3) * sizeof (uint16_t);
+	ring->vr_avail_flags = viona_gpa2kva(ring, pos, avail_sz);
+	if (ring->vr_avail_flags == NULL) {
+		goto fail;
+	}
+	ring->vr_avail_idx = ring->vr_avail_flags + 1;
+	ring->vr_avail_ring = ring->vr_avail_flags + 2;
+	ring->vr_avail_used_event = ring->vr_avail_ring + qsz;
+	pos += avail_sz;
+
+	const size_t used_sz = (qsz * sizeof (struct virtio_used)) +
+	    (sizeof (uint16_t) * 3);
+	pos = P2ROUNDUP(pos, VRING_ALIGN);
+	ring->vr_used_flags = viona_gpa2kva(ring, pos, used_sz);
+	if (ring->vr_used_flags == NULL) {
+		goto fail;
+	}
+	ring->vr_used_idx = ring->vr_used_flags + 1;
+	ring->vr_used_ring = (struct virtio_used *)(ring->vr_used_flags + 2);
+	ring->vr_used_avail_event = (uint16_t *)(ring->vr_used_ring + qsz);
+
+	return (B_TRUE);
+
+fail:
+	viona_ring_unmap(ring);
+	return (B_FALSE);
+}
+
+static void
+viona_ring_unmap(viona_vring_t *ring)
+{
+	ASSERT(MUTEX_HELD(&ring->vr_lock));
+
+	ring->vr_descr = NULL;
+	ring->vr_avail_flags = NULL;
+	ring->vr_avail_idx = NULL;
+	ring->vr_avail_ring = NULL;
+	ring->vr_avail_used_event = NULL;
+	ring->vr_used_flags = NULL;
+	ring->vr_used_idx = NULL;
+	ring->vr_used_ring = NULL;
+	ring->vr_used_avail_event = NULL;
+}
+
 static int
 viona_ioc_ring_init(viona_link_t *link, void *udata, int md)
 {
 	vioc_ring_init_t kri;
 	viona_vring_t *ring;
 	kthread_t *t;
-	uintptr_t pos;
-	size_t desc_sz, avail_sz, used_sz;
-	uint16_t cnt;
 	int err = 0;
 
 	if (ddi_copyin(udata, &kri, sizeof (kri), md) != 0) {
@@ -1268,8 +1395,8 @@ viona_ioc_ring_init(viona_link_t *link, void *udata, int md)
 	if (kri.ri_index >= VIONA_VQ_MAX) {
 		return (EINVAL);
 	}
-	cnt = kri.ri_qsize;
-	if (cnt == 0 || cnt > VRING_MAX_LEN || (1 << (ffs(cnt) - 1)) != cnt) {
+	const uint16_t qsz = kri.ri_qsize;
+	if (qsz == 0 || qsz > VRING_MAX_LEN || (1 << (ffs(qsz) - 1)) != qsz) {
 		return (EINVAL);
 	}
 
@@ -1281,39 +1408,19 @@ viona_ioc_ring_init(viona_link_t *link, void *udata, int md)
 	}
 	VERIFY(ring->vr_state_flags == 0);
 
-	pos = kri.ri_qaddr;
-	desc_sz = cnt * sizeof (struct virtio_desc);
-	avail_sz = (cnt + 3) * sizeof (uint16_t);
-	used_sz = (cnt * sizeof (struct virtio_used)) + (sizeof (uint16_t) * 3);
-
-	ring->vr_size = kri.ri_qsize;
-	ring->vr_mask = (ring->vr_size - 1);
-	ring->vr_descr = viona_gpa2kva(link, pos, desc_sz);
-	if (ring->vr_descr == NULL) {
-		err = EINVAL;
+	ring->vr_lease = NULL;
+	if (!viona_ring_lease_renew(ring)) {
+		err = EBUSY;
 		goto fail;
 	}
-	pos += desc_sz;
 
-	ring->vr_avail_flags = viona_gpa2kva(link, pos, avail_sz);
-	if (ring->vr_avail_flags == NULL) {
-		err = EINVAL;
-		goto fail;
-	}
-	ring->vr_avail_idx = ring->vr_avail_flags + 1;
-	ring->vr_avail_ring = ring->vr_avail_flags + 2;
-	ring->vr_avail_used_event = ring->vr_avail_ring + cnt;
-	pos += avail_sz;
-
-	pos = P2ROUNDUP(pos, VRING_ALIGN);
-	ring->vr_used_flags = viona_gpa2kva(link, pos, used_sz);
-	if (ring->vr_used_flags == NULL) {
+	ring->vr_size = qsz;
+	ring->vr_mask = (ring->vr_size - 1);
+	ring->vr_pa = kri.ri_qaddr;
+	if (!viona_ring_map(ring)) {
 		err = EINVAL;
 		goto fail;
 	}
-	ring->vr_used_idx = ring->vr_used_flags + 1;
-	ring->vr_used_ring = (struct virtio_used *)(ring->vr_used_flags + 2);
-	ring->vr_used_avail_event = (uint16_t *)(ring->vr_used_ring + cnt);
 
 	/* Initialize queue indexes */
 	ring->vr_cur_aidx = 0;
@@ -1322,9 +1429,9 @@ viona_ioc_ring_init(viona_link_t *link, void *udata, int md)
 	if (kri.ri_index == VIONA_VQ_TX && !viona_force_copy_tx_mblks) {
 		viona_desb_t *dp;
 
-		dp = kmem_zalloc(sizeof (viona_desb_t) * cnt, KM_SLEEP);
+		dp = kmem_zalloc(sizeof (viona_desb_t) * qsz, KM_SLEEP);
 		ring->vr_desb = dp;
-		for (uint_t i = 0; i < cnt; i++, dp++) {
+		for (uint_t i = 0; i < qsz; i++, dp++) {
 			dp->d_frtn.free_func = viona_desb_release;
 			dp->d_frtn.free_arg = (void *)dp;
 			dp->d_ring = ring;
@@ -1352,20 +1459,12 @@ viona_ioc_ring_init(viona_link_t *link, void *udata, int md)
 	return (0);
 
 fail:
+	viona_ring_lease_drop(ring);
 	if (ring->vr_desb != NULL) {
 		viona_ring_desb_free(ring);
 	}
 	ring->vr_size = 0;
 	ring->vr_mask = 0;
-	ring->vr_descr = NULL;
-	ring->vr_avail_flags = NULL;
-	ring->vr_avail_idx = NULL;
-	ring->vr_avail_ring = NULL;
-	ring->vr_avail_used_event = NULL;
-	ring->vr_used_flags = NULL;
-	ring->vr_used_idx = NULL;
-	ring->vr_used_ring = NULL;
-	ring->vr_used_avail_event = NULL;
 	mutex_exit(&ring->vr_lock);
 	return (err);
 }
@@ -1498,10 +1597,40 @@ viona_vr_num_avail(viona_vring_t *ring)
 	return (ndesc);
 }
 
+static void
+viona_rx_set(viona_vring_t *ring)
+{
+	viona_link_t *link = ring->vr_link;
+
+	ASSERT(MUTEX_HELD(&ring->vr_lock));
+
+	mac_rx_set(link->l_mch, viona_rx, link);
+}
+
+static void
+viona_rx_clear(viona_vring_t *ring)
+{
+	viona_link_t *link = ring->vr_link;
+
+	ASSERT(MUTEX_HELD(&ring->vr_lock));
+
+	/*
+	 * Clearing the RX function involves MAC quiescing any flows on that
+	 * client.  If MAC happens to be delivering packets to this ring via
+	 * viona_rx() at the time of worker clean-up, that thread may need to
+	 * acquire vr_lock for tasks such as delivering an interrupt.  In order
+	 * to avoid such deadlocks, vr_lock must temporarily be dropped here.
+	 */
+	mutex_exit(&ring->vr_lock);
+	mac_rx_clear(link->l_mch);
+	mutex_enter(&ring->vr_lock);
+}
+
 static void
 viona_worker_rx(viona_vring_t *ring, viona_link_t *link)
 {
 	proc_t *p = ttoproc(curthread);
+	boolean_t already_cleared = B_FALSE;
 
 	thread_vsetname(curthread, "viona_rx_%p", ring);
 
@@ -1509,28 +1638,28 @@ viona_worker_rx(viona_vring_t *ring, viona_link_t *link)
 	ASSERT3U(ring->vr_state, ==, VRS_RUN);
 
 	atomic_or_16(ring->vr_used_flags, VRING_USED_F_NO_NOTIFY);
-	mac_rx_set(link->l_mch, viona_rx, link);
+	viona_rx_set(ring);
 
 	do {
+		if (vmm_drv_lease_expired(ring->vr_lease)) {
+			viona_rx_clear(ring);
+			if (!viona_ring_lease_renew(ring)) {
+				already_cleared = B_TRUE;
+				break;
+			}
+			viona_rx_set(ring);
+		}
+
 		/*
 		 * For now, there is little to do in the RX worker as inbound
 		 * data is delivered by MAC via the viona_rx callback.
-		 * If tap-like functionality is added later, this would be a
-		 * convenient place to inject frames into the guest.
 		 */
 		(void) cv_wait_sig(&ring->vr_cv, &ring->vr_lock);
 	} while (!VRING_NEED_BAIL(ring, p));
 
-	mutex_exit(&ring->vr_lock);
-	/*
-	 * Clearing the RX function involves MAC quiescing any flows on that
-	 * client.  If MAC happens to be delivering packets to this ring via
-	 * viona_rx() at the time of worker clean-up, that thread may need to
-	 * acquire vr_lock for tasks such as delivering an interrupt.  In order
-	 * to avoid such deadlocks, vr_lock must temporarily be dropped here.
-	 */
-	mac_rx_clear(link->l_mch);
-	mutex_enter(&ring->vr_lock);
+	if (!already_cleared) {
+		viona_rx_clear(ring);
+	}
 }
 
 static void
@@ -1547,6 +1676,7 @@ viona_worker_tx(viona_vring_t *ring, viona_link_t *link)
 
 	for (;;) {
 		boolean_t bail = B_FALSE;
+		boolean_t renew = B_FALSE;
 		uint_t ntx = 0;
 
 		atomic_or_16(ring->vr_used_flags, VRING_USED_F_NO_NOTIFY);
@@ -1570,7 +1700,8 @@ viona_worker_tx(viona_vring_t *ring, viona_link_t *link)
 		 * case a late addition raced with the NO_NOTIFY flag toggle.
 		 */
 		bail = VRING_NEED_BAIL(ring, p);
-		if (!bail && viona_vr_num_avail(ring)) {
+		renew = vmm_drv_lease_expired(ring->vr_lease);
+		if (!bail && !renew && viona_vr_num_avail(ring)) {
 			continue;
 		}
 
@@ -1579,26 +1710,33 @@ viona_worker_tx(viona_vring_t *ring, viona_link_t *link)
 		}
 
 		mutex_enter(&ring->vr_lock);
-		while (!bail && !viona_vr_num_avail(ring)) {
+
+		while (!bail && !renew && !viona_vr_num_avail(ring)) {
 			(void) cv_wait_sig(&ring->vr_cv, &ring->vr_lock);
 			bail = VRING_NEED_BAIL(ring, p);
+			renew = vmm_drv_lease_expired(ring->vr_lease);
 		}
+
 		if (bail) {
 			break;
+		} else if (renew) {
+			/*
+			 * When renewing the lease for the ring, no TX
+			 * frames may be outstanding, as they contain
+			 * references to guest memory.
+			 */
+			viona_tx_wait_outstanding(ring);
+
+			if (!viona_ring_lease_renew(ring)) {
+				break;
+			}
 		}
 		mutex_exit(&ring->vr_lock);
 	}
 
 	ASSERT(MUTEX_HELD(&ring->vr_lock));
 
-	while (ring->vr_xfer_outstanding != 0) {
-		/*
-		 * Paying heed to signals is counterproductive here.  This is a
-		 * very tight loop if pending transfers take an extended amount
-		 * of time to be reclaimed while the host process is exiting.
-		 */
-		cv_wait(&ring->vr_cv, &ring->vr_lock);
-	}
+	viona_tx_wait_outstanding(ring);
 
 	/* Free any desb resources before the ring is completely stopped */
 	if (ring->vr_desb != NULL) {
@@ -1653,6 +1791,7 @@ cleanup:
 		viona_ring_desb_free(ring);
 	}
 
+	viona_ring_lease_drop(ring);
 	ring->vr_cur_aidx = 0;
 	ring->vr_state = VRS_RESET;
 	ring->vr_state_flags = 0;
@@ -1727,7 +1866,6 @@ viona_ioc_intr_poll(viona_link_t *link, void *udata, int md, int *rv)
 static int
 vq_popchain(viona_vring_t *ring, struct iovec *iov, int niov, uint16_t *cookie)
 {
-	viona_link_t *link = ring->vr_link;
 	uint_t i, ndesc, idx, head, next;
 	struct virtio_desc vdir;
 	void *buf;
@@ -1776,7 +1914,7 @@ vq_popchain(viona_vring_t *ring, struct iovec *iov, int niov, uint16_t *cookie)
 				VIONA_RING_STAT_INCR(ring, desc_bad_len);
 				goto bail;
 			}
-			buf = viona_gpa2kva(link, vdir.vd_addr, vdir.vd_len);
+			buf = viona_gpa2kva(ring, vdir.vd_addr, vdir.vd_len);
 			if (buf == NULL) {
 				VIONA_PROBE_BAD_RING_ADDR(ring, vdir.vd_addr);
 				VIONA_RING_STAT_INCR(ring, bad_ring_addr);
@@ -1796,7 +1934,7 @@ vq_popchain(viona_vring_t *ring, struct iovec *iov, int niov, uint16_t *cookie)
 				VIONA_RING_STAT_INCR(ring, indir_bad_len);
 				goto bail;
 			}
-			vindir = viona_gpa2kva(link, vdir.vd_addr, vdir.vd_len);
+			vindir = viona_gpa2kva(ring, vdir.vd_addr, vdir.vd_len);
 			if (vindir == NULL) {
 				VIONA_PROBE_BAD_RING_ADDR(ring, vdir.vd_addr);
 				VIONA_RING_STAT_INCR(ring, bad_ring_addr);
@@ -1829,7 +1967,7 @@ vq_popchain(viona_vring_t *ring, struct iovec *iov, int niov, uint16_t *cookie)
 					    desc_bad_len);
 					goto bail;
 				}
-				buf = viona_gpa2kva(link, vp.vd_addr,
+				buf = viona_gpa2kva(ring, vp.vd_addr,
 				    vp.vd_len);
 				if (buf == NULL) {
 					VIONA_PROBE_BAD_RING_ADDR(ring,
@@ -1932,7 +2070,7 @@ viona_intr_ring(viona_vring_t *ring)
 		uint64_t msg = ring->vr_msi_msg;
 
 		mutex_exit(&ring->vr_lock);
-		(void) vmm_drv_msi(ring->vr_link->l_vm_hold, addr, msg);
+		(void) vmm_drv_msi(ring->vr_lease, addr, msg);
 		return;
 	}
 	mutex_exit(&ring->vr_lock);
@@ -2492,6 +2630,21 @@ viona_desb_release(viona_desb_t *dp)
 	mutex_exit(&ring->vr_lock);
 }
 
+static void
+viona_tx_wait_outstanding(viona_vring_t *ring)
+{
+	ASSERT(MUTEX_HELD(&ring->vr_lock));
+
+	while (ring->vr_xfer_outstanding != 0) {
+		/*
+		 * Paying heed to signals is counterproductive here.  This is a
+		 * very tight loop if pending transfers take an extended amount
+		 * of time to be reclaimed while the host process is exiting.
+		 */
+		cv_wait(&ring->vr_cv, &ring->vr_lock);
+	}
+}
+
 static boolean_t
 viona_tx_csum(viona_vring_t *ring, const struct virtio_net_hdr *hdr,
     mblk_t *mp, uint32_t len)
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c b/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c
index 62ee5e4c51..63a24688ff 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c
+++ b/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c
@@ -11,7 +11,7 @@
 
 /*
  * Copyright 2015 Pluribus Networks Inc.
- * Copyright 2018 Joyent, Inc.
+ * Copyright 2019 Joyent, Inc.
  */
 
 #include <sys/types.h>
@@ -84,10 +84,20 @@ extern int vmx_x86_supported(const char **);
 struct vmm_hold {
 	list_node_t	vmh_node;
 	vmm_softc_t	*vmh_sc;
-	boolean_t	vmh_expired;
+	boolean_t	vmh_release_req;
 	uint_t		vmh_ioport_hook_cnt;
 };
 
+struct vmm_lease {
+	list_node_t		vml_node;
+	struct vm		*vml_vm;
+	boolean_t		vml_expired;
+	void			(*vml_expire_func)(void *);
+	void			*vml_expire_arg;
+	list_node_t		vml_expire_node;
+	struct vmm_hold		*vml_hold;
+};
+
 static int vmm_drv_block_hook(vmm_softc_t *, boolean_t);
 
 static int
@@ -223,72 +233,161 @@ vmmdev_alloc_memseg(vmm_softc_t *sc, struct vm_memseg *mseg)
 	return (error);
 }
 
-
 static int
-vcpu_lock_one(vmm_softc_t *sc, int vcpu)
+vmmdev_copyin_vcpu(const void *datap, const int mode, int *vcpup,
+    boolean_t allow_all)
 {
-	int error;
+	int vcpu;
 
-	if (vcpu < 0 || vcpu >= VM_MAXCPU)
+	if (ddi_copyin(datap, &vcpu, sizeof (vcpu), mode)) {
+		return (EFAULT);
+	}
+	/*
+	 * The vcpu ID must be valid (0 <= vcpu < VM_MAXCPU) or, if allow_all
+	 * is set, be -1 to indicate all vCPUs.
+	 */
+	if (vcpu >= VM_MAXCPU || (vcpu < 0 && !allow_all) || (vcpu < -1)) {
 		return (EINVAL);
+	}
 
-	error = vcpu_set_state(sc->vmm_vm, vcpu, VCPU_FROZEN, true);
-	return (error);
+	*vcpup = vcpu;
+	return (0);
+}
+
+/*
+ * Resource Locking and Exclusion
+ *
+ * Much of bhyve depends on key portions of VM state, such as the guest memory
+ * map, to remain unchanged while the guest is running.  As ported from
+ * FreeBSD, the initial strategy for this resource exclusion hinged on gating
+ * access to the instance vCPUs.  Threads acting on a single vCPU, like those
+ * performing the work of actually running the guest in VMX/SVM, would lock
+ * only that vCPU during ioctl() entry.  For ioctls which would change VM-wide
+ * state, all of the vCPUs would be first locked, ensuring that the
+ * operation(s) could complete without any other threads stumbling into
+ * intermediate states.
+ *
+ * This approach is largely effective for bhyve as it exists in FreeBSD.
+ * Common operations, such as running the vCPUs, steer clear of lock
+ * contention.  Less common ioctls like LAPIC MSI delivery, which does not run
+ * in the context of a specific vCPU, may see contention since they use
+ * VM_MAXCPU-1 as their solution for "any vCPU".
+ */
+
+static int
+vcpu_lock_one(vmm_softc_t *sc, int vcpu)
+{
+	ASSERT(vcpu >= 0 && vcpu < VM_MAXCPU);
+
+	return (vcpu_set_state(sc->vmm_vm, vcpu, VCPU_FROZEN, true));
 }
 
 static void
 vcpu_unlock_one(vmm_softc_t *sc, int vcpu)
 {
-	enum vcpu_state state;
-
-	state = vcpu_get_state(sc->vmm_vm, vcpu, NULL);
-	if (state != VCPU_FROZEN) {
-		panic("vcpu %s(%d) has invalid state %d", vm_name(sc->vmm_vm),
-		    vcpu, state);
-	}
+	ASSERT(vcpu >= 0 && vcpu < VM_MAXCPU);
 
+	VERIFY3U(vcpu_get_state(sc->vmm_vm, vcpu, NULL), ==, VCPU_FROZEN);
 	vcpu_set_state(sc->vmm_vm, vcpu, VCPU_IDLE, false);
 }
 
+static void
+vmm_read_lock(vmm_softc_t *sc)
+{
+	rw_enter(&sc->vmm_rwlock, RW_READER);
+}
+
+static void
+vmm_read_unlock(vmm_softc_t *sc)
+{
+	rw_exit(&sc->vmm_rwlock);
+}
+
 static int
-vcpu_lock_all(vmm_softc_t *sc)
+vmm_write_lock(vmm_softc_t *sc)
 {
-	int error, vcpu;
+	list_t expire_list;
+	vmm_lease_t *lease;
 
-	for (vcpu = 0; vcpu < VM_MAXCPU; vcpu++) {
-		error = vcpu_lock_one(sc, vcpu);
-		if (error)
-			break;
+	/* First attempt to lock all the vCPUs */
+	for (int vcpu = 0; vcpu < VM_MAXCPU; vcpu++) {
+		int error = vcpu_lock_one(sc, vcpu);
+		if (error) {
+			/*
+			 * Unlock any vCPUs which were successfully locked
+			 * prior to encountering an error
+			 */
+			while (--vcpu >= 0) {
+				vcpu_unlock_one(sc, vcpu);
+			}
+			return (error);
+		}
 	}
 
-	if (error) {
-		while (--vcpu >= 0)
-			vcpu_unlock_one(sc, vcpu);
+	list_create(&expire_list, sizeof (vmm_lease_t),
+	    offsetof(vmm_lease_t, vml_expire_node));
+
+	mutex_enter(&sc->vmm_lease_lock);
+	VERIFY3U(sc->vmm_lease_blocker, !=, UINT_MAX);
+	sc->vmm_lease_blocker++;
+	if (sc->vmm_lease_blocker == 1) {
+		list_t *list = &sc->vmm_lease_list;
+
+		for (lease = list_head(list); lease != NULL;
+		    lease = list_next(list, lease)) {
+			lease->vml_expired = B_TRUE;
+			list_insert_tail(&expire_list, lease);
+		}
 	}
+	mutex_exit(&sc->vmm_lease_lock);
 
-	return (error);
+	/*
+	 * Trigger lease expiration callbacks outside vmm_lease_lock so that
+	 * consumers that choose to synchronously drop their lease can do so
+	 * without incurring a deadlock.
+	 */
+	for (lease = list_remove_head(&expire_list); lease != NULL;
+	    lease = list_remove_head(&expire_list)) {
+		lease->vml_expire_func(lease->vml_expire_arg);
+	}
+
+	rw_enter(&sc->vmm_rwlock, RW_WRITER);
+	return (0);
 }
 
 static void
-vcpu_unlock_all(vmm_softc_t *sc)
+vmm_write_unlock(vmm_softc_t *sc)
 {
-	int vcpu;
+	mutex_enter(&sc->vmm_lease_lock);
+	VERIFY3U(sc->vmm_lease_blocker, !=, 0);
+	sc->vmm_lease_blocker--;
+	if (sc->vmm_lease_blocker == 0) {
+		cv_broadcast(&sc->vmm_lease_cv);
+	}
+	mutex_exit(&sc->vmm_lease_lock);
+	rw_exit(&sc->vmm_rwlock);
 
-	for (vcpu = 0; vcpu < VM_MAXCPU; vcpu++)
+	/* Unlock all the vCPUs */
+	for (int vcpu = 0; vcpu < VM_MAXCPU; vcpu++) {
 		vcpu_unlock_one(sc, vcpu);
+	}
 }
 
+
 static int
 vmmdev_do_ioctl(vmm_softc_t *sc, int cmd, intptr_t arg, int md,
     cred_t *credp, int *rvalp)
 {
 	int error = 0, vcpu = -1;
 	void *datap = (void *)arg;
-	boolean_t locked_one = B_FALSE, locked_all = B_FALSE;
-
-	/*
-	 * Some VMM ioctls can operate only on vcpus that are not running.
-	 */
+	enum vm_lock_type {
+		LOCK_NONE = 0,
+		LOCK_VCPU,
+		LOCK_READ_HOLD,
+		LOCK_WRITE_HOLD
+	} lock_type = LOCK_NONE;
+
+	/* Acquire any exclusion resources needed for the operation */
 	switch (cmd) {
 	case VM_RUN:
 	case VM_GET_REGISTER:
@@ -316,56 +415,62 @@ vmmdev_do_ioctl(vmm_softc_t *sc, int cmd, intptr_t arg, int md,
 		 * in, it is _critical_ that this local 'vcpu' variable be used
 		 * rather than the in-struct one when performing the ioctl.
 		 */
-		if (ddi_copyin(datap, &vcpu, sizeof (vcpu), md)) {
-			return (EFAULT);
-		}
-		if (vcpu < 0 || vcpu >= VM_MAXCPU) {
-			error = EINVAL;
-			goto done;
+		error = vmmdev_copyin_vcpu(datap, md, &vcpu, B_FALSE);
+		if (error != 0) {
+			return (error);
 		}
-
 		error = vcpu_lock_one(sc, vcpu);
-		if (error)
-			goto done;
-		locked_one = B_TRUE;
+		if (error != 0) {
+			return (error);
+		}
+		lock_type = LOCK_VCPU;
 		break;
 
-	case VM_MAP_PPTDEV_MMIO:
+	case VM_REINIT:
 	case VM_BIND_PPTDEV:
 	case VM_UNBIND_PPTDEV:
+	case VM_MAP_PPTDEV_MMIO:
 	case VM_ALLOC_MEMSEG:
 	case VM_MMAP_MEMSEG:
-	case VM_REINIT:
 		/*
 		 * All vCPUs must be prevented from running when performing
 		 * operations which act upon the entire VM.
 		 */
-		error = vcpu_lock_all(sc);
-		if (error)
-			goto done;
-		locked_all = B_TRUE;
+		error = vmm_write_lock(sc);
+		if (error != 0) {
+			return (error);
+		}
+		lock_type = LOCK_WRITE_HOLD;
 		break;
 
+	case VM_GET_GPA_PMAP:
 	case VM_GET_MEMSEG:
 	case VM_MMAP_GETNEXT:
+	case VM_LAPIC_IRQ:
+	case VM_INJECT_NMI:
+	case VM_IOAPIC_ASSERT_IRQ:
+	case VM_IOAPIC_DEASSERT_IRQ:
+	case VM_IOAPIC_PULSE_IRQ:
+	case VM_LAPIC_MSI:
+	case VM_LAPIC_LOCAL_IRQ:
+	case VM_GET_X2APIC_STATE:
+	case VM_RTC_READ:
+	case VM_RTC_WRITE:
+	case VM_RTC_SETTIME:
+	case VM_RTC_GETTIME:
 #ifndef __FreeBSD__
 	case VM_DEVMEM_GETOFFSET:
 #endif
-		/*
-		 * Lock a vcpu to make sure that the memory map cannot be
-		 * modified while it is being inspected.
-		 */
-		vcpu = VM_MAXCPU - 1;
-		error = vcpu_lock_one(sc, vcpu);
-		if (error)
-			goto done;
-		locked_one = B_TRUE;
+		vmm_read_lock(sc);
+		lock_type = LOCK_READ_HOLD;
 		break;
 
+	case VM_IOAPIC_PINCOUNT:
 	default:
 		break;
 	}
 
+	/* Execute the primary logic for the ioctl. */
 	switch (cmd) {
 	case VM_RUN: {
 		struct vm_run vmrun;
@@ -971,29 +1076,17 @@ vmmdev_do_ioctl(vmm_softc_t *sc, int cmd, intptr_t arg, int md,
 		break;
 
 	case VM_SUSPEND_CPU:
-		if (ddi_copyin(datap, &vcpu, sizeof (vcpu), md)) {
-			error = EFAULT;
-			break;
+		error = vmmdev_copyin_vcpu(datap, md, &vcpu, B_TRUE);
+		if (error == 0) {
+			error = vm_suspend_cpu(sc->vmm_vm, vcpu);
 		}
-		if (vcpu < -1 || vcpu >= VM_MAXCPU) {
-			error = EINVAL;
-			break;
-		}
-
-		error = vm_suspend_cpu(sc->vmm_vm, vcpu);
 		break;
 
 	case VM_RESUME_CPU:
-		if (ddi_copyin(datap, &vcpu, sizeof (vcpu), md)) {
-			error = EFAULT;
-			break;
+		error = vmmdev_copyin_vcpu(datap, md, &vcpu, B_TRUE);
+		if (error == 0) {
+			error = vm_resume_cpu(sc->vmm_vm, vcpu);
 		}
-		if (vcpu < -1 || vcpu >= VM_MAXCPU) {
-			error = EINVAL;
-			break;
-		}
-
-		error = vm_resume_cpu(sc->vmm_vm, vcpu);
 		break;
 
 	case VM_GET_CPUS: {
@@ -1168,16 +1261,24 @@ vmmdev_do_ioctl(vmm_softc_t *sc, int cmd, intptr_t arg, int md,
 		break;
 	}
 
-	/* Release any vCPUs that were locked for the operation */
-	if (locked_one) {
+	/* Release exclusion resources */
+	switch (lock_type) {
+	case LOCK_NONE:
+		break;
+	case LOCK_VCPU:
 		vcpu_unlock_one(sc, vcpu);
-	} else if (locked_all) {
-		vcpu_unlock_all(sc);
+		break;
+	case LOCK_READ_HOLD:
+		vmm_read_unlock(sc);
+		break;
+	case LOCK_WRITE_HOLD:
+		vmm_write_unlock(sc);
+		break;
+	default:
+		panic("unexpected lock type");
+		break;
 	}
 
-done:
-	/* Make sure that no handler returns a bogus value like ERESTART */
-	KASSERT(error >= 0, ("vmmdev_ioctl: invalid error return %d", error));
 	return (error);
 }
 
@@ -1246,10 +1347,17 @@ vmmdev_do_vm_create(char *name, cred_t *cr)
 		sc->vmm_minor = minor;
 		list_create(&sc->vmm_devmem_list, sizeof (vmm_devmem_entry_t),
 		    offsetof(vmm_devmem_entry_t, vde_node));
+
 		list_create(&sc->vmm_holds, sizeof (vmm_hold_t),
 		    offsetof(vmm_hold_t, vmh_node));
 		cv_init(&sc->vmm_cv, NULL, CV_DEFAULT, NULL);
 
+		mutex_init(&sc->vmm_lease_lock, NULL, MUTEX_DEFAULT, NULL);
+		list_create(&sc->vmm_lease_list, sizeof (vmm_lease_t),
+		    offsetof(vmm_lease_t, vml_node));
+		cv_init(&sc->vmm_lease_cv, NULL, CV_DEFAULT, NULL);
+		rw_init(&sc->vmm_rwlock, NULL, RW_DEFAULT, NULL);
+
 		sc->vmm_zone = crgetzone(cr);
 		zone_hold(sc->vmm_zone);
 		vmm_zsd_add_vm(sc);
@@ -1270,6 +1378,23 @@ fail:
 	return (error);
 }
 
+/*
+ * Bhyve 'Driver' Interface
+ *
+ * While many devices are emulated in the bhyve userspace process, there are
+ * others with performance constraints which require that they run mostly or
+ * entirely in-kernel.  For those not integrated directly into bhyve, an API is
+ * needed so they can query/manipulate the portions of VM state needed to
+ * fulfill their purpose.
+ *
+ * This includes:
+ * - Translating guest-physical addresses to host-virtual pointers
+ * - Injecting MSIs
+ * - Hooking IO port addresses
+ *
+ * The vmm_drv interface exists to provide that functionality to its consumers.
+ * (At this time, 'viona' is the only user)
+ */
 int
 vmm_drv_hold(file_t *fp, cred_t *cr, vmm_hold_t **holdp)
 {
@@ -1306,7 +1431,8 @@ vmm_drv_hold(file_t *fp, cred_t *cr, vmm_hold_t **holdp)
 
 	hold = kmem_zalloc(sizeof (*hold), KM_SLEEP);
 	hold->vmh_sc = sc;
-	hold->vmh_expired = B_FALSE;
+	hold->vmh_release_req = B_FALSE;
+
 	list_insert_tail(&sc->vmm_holds, hold);
 	sc->vmm_flags |= VMM_HELD;
 	*holdp = hold;
@@ -1337,25 +1463,79 @@ vmm_drv_rele(vmm_hold_t *hold)
 }
 
 boolean_t
-vmm_drv_expired(vmm_hold_t *hold)
+vmm_drv_release_reqd(vmm_hold_t *hold)
 {
 	ASSERT(hold != NULL);
 
-	return (hold->vmh_expired);
+	return (hold->vmh_release_req);
+}
+
+vmm_lease_t *
+vmm_drv_lease_sign(vmm_hold_t *hold, void (*expiref)(void *), void *arg)
+{
+	vmm_softc_t *sc = hold->vmh_sc;
+	vmm_lease_t *lease;
+
+	ASSERT3P(expiref, !=, NULL);
+
+	if (hold->vmh_release_req) {
+		return (NULL);
+	}
+
+	lease = kmem_alloc(sizeof (*lease), KM_SLEEP);
+	list_link_init(&lease->vml_node);
+	lease->vml_expire_func = expiref;
+	lease->vml_expire_arg = arg;
+	lease->vml_expired = B_FALSE;
+	lease->vml_hold = hold;
+	/* cache the VM pointer for one less pointer chase */
+	lease->vml_vm = sc->vmm_vm;
+
+	mutex_enter(&sc->vmm_lease_lock);
+	while (sc->vmm_lease_blocker != 0) {
+		cv_wait(&sc->vmm_lease_cv, &sc->vmm_lease_lock);
+	}
+	list_insert_tail(&sc->vmm_lease_list, lease);
+	rw_enter(&sc->vmm_rwlock, RW_READER);
+	mutex_exit(&sc->vmm_lease_lock);
+
+	return (lease);
+}
+
+void
+vmm_drv_lease_break(vmm_hold_t *hold, vmm_lease_t *lease)
+{
+	vmm_softc_t *sc = hold->vmh_sc;
+
+	VERIFY3P(hold, ==, lease->vml_hold);
+
+	mutex_enter(&sc->vmm_lease_lock);
+	list_remove(&sc->vmm_lease_list, lease);
+	mutex_exit(&sc->vmm_lease_lock);
+	rw_exit(&sc->vmm_rwlock);
+	kmem_free(lease, sizeof (*lease));
+}
+
+boolean_t
+vmm_drv_lease_expired(vmm_lease_t *lease)
+{
+	return (lease->vml_expired);
 }
 
 void *
-vmm_drv_gpa2kva(vmm_hold_t *hold, uintptr_t gpa, size_t sz)
+vmm_drv_gpa2kva(vmm_lease_t *lease, uintptr_t gpa, size_t sz)
 {
-	struct vm *vm;
-	struct vmspace *vmspace;
+	ASSERT(lease != NULL);
 
-	ASSERT(hold != NULL);
+	return (vmspace_find_kva(vm_get_vmspace(lease->vml_vm), gpa, sz));
+}
 
-	vm = hold->vmh_sc->vmm_vm;
-	vmspace = vm_get_vmspace(vm);
+int
+vmm_drv_msi(vmm_lease_t *lease, uint64_t addr, uint64_t msg)
+{
+	ASSERT(lease != NULL);
 
-	return (vmspace_find_kva(vmspace, gpa, sz));
+	return (lapic_intr_msi(lease->vml_vm, addr, msg));
 }
 
 int
@@ -1411,17 +1591,6 @@ vmm_drv_ioport_unhook(vmm_hold_t *hold, void **cookie)
 	mutex_exit(&vmm_mtx);
 }
 
-int
-vmm_drv_msi(vmm_hold_t *hold, uint64_t addr, uint64_t msg)
-{
-	struct vm *vm;
-
-	ASSERT(hold != NULL);
-
-	vm = hold->vmh_sc->vmm_vm;
-	return (lapic_intr_msi(vm, addr, msg));
-}
-
 static int
 vmm_drv_purge(vmm_softc_t *sc)
 {
@@ -1433,7 +1602,7 @@ vmm_drv_purge(vmm_softc_t *sc)
 		sc->vmm_flags |= VMM_CLEANUP;
 		for (hold = list_head(&sc->vmm_holds); hold != NULL;
 		    hold = list_next(&sc->vmm_holds, hold)) {
-			hold->vmh_expired = B_TRUE;
+			hold->vmh_release_req = B_TRUE;
 		}
 		while ((sc->vmm_flags & VMM_HELD) != 0) {
 			if (cv_wait_sig(&sc->vmm_cv, &vmm_mtx) <= 0) {
@@ -1714,10 +1883,8 @@ vmm_segmap(dev_t dev, off_t off, struct as *as, caddr_t *addrp, off_t len,
 	if (sc->vmm_flags & VMM_DESTROY)
 		return (ENXIO);
 
-	/* Get a read lock on the guest memory map by freezing any vcpu. */
-	if ((err = vcpu_lock_all(sc)) != 0) {
-		return (err);
-	}
+	/* Grab read lock on the VM to prevent any changes to the memory map */
+	vmm_read_lock(sc);
 
 	vm = sc->vmm_vm;
 	vms = vm_get_vmspace(vm);
@@ -1742,7 +1909,7 @@ vmm_segmap(dev_t dev, off_t off, struct as *as, caddr_t *addrp, off_t len,
 
 
 out:
-	vcpu_unlock_all(sc);
+	vmm_read_unlock(sc);
 	return (err);
 }
 
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c b/usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c
index 58c10d9da0..74275769b1 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c
+++ b/usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c
@@ -10,7 +10,7 @@
  */
 
 /*
- * Copyright 2018 Joyent, Inc.
+ * Copyright 2019 Joyent, Inc.
  */
 
 #include <sys/param.h>
@@ -230,8 +230,13 @@ vmspace_find_kva(struct vmspace *vms, uintptr_t addr, size_t size)
 	vmspace_mapping_t *vmsm;
 	void *result = NULL;
 
-	mutex_enter(&vms->vms_lock);
-	vmsm = vm_mapping_find(vms, addr, size, B_FALSE);
+	/*
+	 * Since vmspace_find_kva is provided so that vmm_drv consumers can do
+	 * GPA2KVA translations, it is expected to be called when there is a
+	 * read lock preventing vmspace alterations.  As such, it can do the
+	 * lockless vm_mapping_find() lookup.
+	 */
+	vmsm = vm_mapping_find(vms, addr, size, B_TRUE);
 	if (vmsm != NULL) {
 		struct vm_object *vmo = vmsm->vmsm_object;
 
@@ -244,7 +249,6 @@ vmspace_find_kva(struct vmspace *vms, uintptr_t addr, size_t size)
 			break;
 		}
 	}
-	mutex_exit(&vms->vms_lock);
 
 	return (result);
 }
diff --git a/usr/src/uts/i86pc/sys/vmm_drv.h b/usr/src/uts/i86pc/sys/vmm_drv.h
index b883070abf..c48fcf936d 100644
--- a/usr/src/uts/i86pc/sys/vmm_drv.h
+++ b/usr/src/uts/i86pc/sys/vmm_drv.h
@@ -10,7 +10,7 @@
  */
 
 /*
- * Copyright 2017 Joyent, Inc.
+ * Copyright 2019 Joyent, Inc.
  */
 
 #ifndef _VMM_DRV_H_
@@ -20,6 +20,9 @@
 struct vmm_hold;
 typedef struct vmm_hold vmm_hold_t;
 
+struct vmm_lease;
+typedef struct vmm_lease vmm_lease_t;
+
 /*
  * Because of tangled headers, these definitions mirror their vmm_[rw]mem_cb_t
  * counterparts in vmm.h.
@@ -29,12 +32,18 @@ typedef int (*vmm_drv_wmem_cb_t)(void *, uintptr_t, uint_t, uint64_t);
 
 extern int vmm_drv_hold(file_t *, cred_t *, vmm_hold_t **);
 extern void vmm_drv_rele(vmm_hold_t *);
-extern boolean_t vmm_drv_expired(vmm_hold_t *);
-extern void *vmm_drv_gpa2kva(vmm_hold_t *, uintptr_t, size_t);
+extern boolean_t vmm_drv_release_reqd(vmm_hold_t *);
+
+extern vmm_lease_t *vmm_drv_lease_sign(vmm_hold_t *, void (*)(void *), void *);
+extern void vmm_drv_lease_break(vmm_hold_t *, vmm_lease_t *);
+extern boolean_t vmm_drv_lease_expired(vmm_lease_t *);
+
+extern void *vmm_drv_gpa2kva(vmm_lease_t *, uintptr_t, size_t);
+extern int vmm_drv_msi(vmm_lease_t *, uint64_t, uint64_t);
+
 extern int vmm_drv_ioport_hook(vmm_hold_t *, uint_t, vmm_drv_rmem_cb_t,
     vmm_drv_wmem_cb_t, void *, void **);
 extern void vmm_drv_ioport_unhook(vmm_hold_t *, void **);
-extern int vmm_drv_msi(vmm_hold_t *, uint64_t, uint64_t);
 #endif /* _KERNEL */
 
 #endif /* _VMM_DRV_H_ */
diff --git a/usr/src/uts/i86pc/sys/vmm_impl.h b/usr/src/uts/i86pc/sys/vmm_impl.h
index 8fa19c8247..cdc56cc464 100644
--- a/usr/src/uts/i86pc/sys/vmm_impl.h
+++ b/usr/src/uts/i86pc/sys/vmm_impl.h
@@ -11,7 +11,7 @@
 
 /*
  * Copyright 2014 Pluribus Networks Inc.
- * Copyright 2018 Joyent, Inc.
+ * Copyright 2019 Joyent, Inc.
  */
 
 #ifndef _VMM_IMPL_H_
@@ -46,7 +46,7 @@ typedef struct vmm_devmem_entry vmm_devmem_entry_t;
 typedef struct vmm_zsd vmm_zsd_t;
 
 enum vmm_softc_state {
-	VMM_HELD	= 1,	/* external driver(s) possess hold on VM */
+	VMM_HELD	= 1,	/* external driver(s) possess hold on the VM */
 	VMM_CLEANUP	= 2,	/* request that holds are released */
 	VMM_PURGED	= 4,	/* all hold have been released */
 	VMM_BLOCK_HOOK	= 8,	/* mem hook install temporarily blocked */
@@ -58,11 +58,18 @@ struct vmm_softc {
 	struct vm	*vmm_vm;
 	minor_t		vmm_minor;
 	char		vmm_name[VM_MAX_NAMELEN];
-	uint_t		vmm_flags;
-	boolean_t	vmm_is_open;
 	list_t		vmm_devmem_list;
-	list_t		vmm_holds;
+
 	kcondvar_t	vmm_cv;
+	list_t		vmm_holds;
+	uint_t		vmm_flags;
+	boolean_t	vmm_is_open;
+
+	kmutex_t	vmm_lease_lock;
+	list_t		vmm_lease_list;
+	uint_t		vmm_lease_blocker;
+	kcondvar_t	vmm_lease_cv;
+	krwlock_t	vmm_rwlock;
 
 	/* For zone specific data */
 	list_node_t	vmm_zsd_linkage;
-- 
2.21.0

