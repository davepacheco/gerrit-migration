From d1ed9208ef653173e3fdb2fb480b0a0601189da0 Mon Sep 17 00:00:00 2001
From: Patrick Mooney <pmooney@pfmooney.com>
Date: Tue, 12 Jun 2018 22:52:58 +0000
Subject: [PATCH] OS-7012 bhyve wedged in vlapic cyclics OS-7016 vmx ctx ops
 should inform VMRESUME

---
 usr/src/compat/freebsd/sys/mutex.h      |  2 +-
 usr/src/uts/i86pc/io/vmm/intel/vmx.c    | 27 +++++++++++++++++++------
 usr/src/uts/i86pc/io/vmm/intel/vmx.h    | 10 ++++++++-
 usr/src/uts/i86pc/io/vmm/vmm_sol_glue.c | 15 ++++++++------
 4 files changed, 40 insertions(+), 14 deletions(-)

diff --git a/usr/src/compat/freebsd/sys/mutex.h b/usr/src/compat/freebsd/sys/mutex.h
index 57ebfee901..9e588cb98a 100644
--- a/usr/src/compat/freebsd/sys/mutex.h
+++ b/usr/src/compat/freebsd/sys/mutex.h
@@ -11,6 +11,7 @@
 
 /*
  * Copyright 2014 Pluribus Networks Inc.
+ * Copyright 2018 Joyent, Inc.
  */
 
 #ifndef _COMPAT_FREEBSD_SYS_MUTEX_H_
@@ -33,7 +34,6 @@ void mtx_destroy(struct mtx *);
 #ifdef	_KERNEL
 
 struct mtx {
-	kmutex_type_t	t;
 	kmutex_t	m;
 };
 
diff --git a/usr/src/uts/i86pc/io/vmm/intel/vmx.c b/usr/src/uts/i86pc/io/vmm/intel/vmx.c
index eaad8864c2..3373b9191f 100644
--- a/usr/src/uts/i86pc/io/vmm/intel/vmx.c
+++ b/usr/src/uts/i86pc/io/vmm/intel/vmx.c
@@ -2972,8 +2972,8 @@ vmx_run(void *arg, int vcpu, register_t rip, pmap_t pmap,
 	VMPTRLD(vmcs);
 
 #ifndef __FreeBSD__
-	VERIFY(!vmx->ctx_loaded[vcpu] && curthread->t_preempt != 0);
-	vmx->ctx_loaded[vcpu] = B_TRUE;
+	VERIFY(vmx->vmcs_state[vcpu] == VS_NONE && curthread->t_preempt != 0);
+	vmx->vmcs_state[vcpu] = VS_LOADED;
 #endif
 
 	/*
@@ -3052,10 +3052,16 @@ vmx_run(void *arg, int vcpu, register_t rip, pmap_t pmap,
 			break;
 		}
 
+#ifndef	__FreeBSD__
+		launched = (vmx->vmcs_state[vcpu] & VS_LAUNCHED) != 0;
+#endif
 		vmx_run_trace(vmx, vcpu);
 		vmx_dr_enter_guest(vmxctx);
 		rc = vmx_enter_guest(vmxctx, vmx, launched);
 		vmx_dr_leave_guest(vmxctx);
+#ifndef	__FreeBSD__
+		vmx->vmcs_state[vcpu] |= VS_LAUNCHED;
+#endif
 
 		/* Collect some information for VM exit processing */
 		vmexit->rip = rip = vmcs_guest_rip();
@@ -3074,7 +3080,9 @@ vmx_run(void *arg, int vcpu, register_t rip, pmap_t pmap,
 			enable_intr();
 			vmx_exit_inst_error(vmxctx, rc, vmexit);
 		}
+#ifdef	__FreeBSD__
 		launched = 1;
+#endif
 		vmx_exit_trace(vmx, vcpu, rip, exit_reason, handled);
 		rip = vmexit->rip;
 	} while (handled);
@@ -3103,8 +3111,8 @@ vmx_run(void *arg, int vcpu, register_t rip, pmap_t pmap,
 	vmx_msr_guest_exit(vmx, vcpu);
 
 #ifndef __FreeBSD__
-	VERIFY(vmx->ctx_loaded[vcpu] && curthread->t_preempt != 0);
-	vmx->ctx_loaded[vcpu] = B_FALSE;
+	VERIFY(vmx->vmcs_state != VS_NONE && curthread->t_preempt != 0);
+	vmx->vmcs_state[vcpu] = VS_NONE;
 #endif
 
 	return (0);
@@ -3938,9 +3946,14 @@ vmx_savectx(void *arg, int vcpu)
 	struct vmx *vmx = arg;
 	struct vmcs *vmcs = &vmx->vmcs[vcpu];
 
-	if (vmx->ctx_loaded[vcpu]) {
+	if ((vmx->vmcs_state[vcpu] & VS_LOADED) != 0) {
 		VERIFY3U(vmclear(vmcs), ==, 0);
 		vmx_msr_guest_exit(vmx, vcpu);
+		/*
+		 * Having VMCLEARed the VMCS, it can no longer be re-entered
+		 * with VMRESUME, but must be VMLAUNCHed again.
+		 */
+		vmx->vmcs_state[vcpu] &= ~VS_LAUNCHED;
 	}
 }
 
@@ -3950,7 +3963,9 @@ vmx_restorectx(void *arg, int vcpu)
 	struct vmx *vmx = arg;
 	struct vmcs *vmcs = &vmx->vmcs[vcpu];
 
-	if (vmx->ctx_loaded[vcpu]) {
+	ASSERT0(vmx->vmcs_state[vcpu] & VS_LAUNCHED);
+
+	if ((vmx->vmcs_state[vcpu] & VS_LOADED) != 0) {
 		vmx_msr_guest_enter(vmx, vcpu);
 		VERIFY3U(vmptrld(vmcs), ==, 0);
 	}
diff --git a/usr/src/uts/i86pc/io/vmm/intel/vmx.h b/usr/src/uts/i86pc/io/vmm/intel/vmx.h
index 0ee83fcc81..2d16799bdd 100644
--- a/usr/src/uts/i86pc/io/vmm/intel/vmx.h
+++ b/usr/src/uts/i86pc/io/vmm/intel/vmx.h
@@ -126,6 +126,14 @@ enum {
 	GUEST_MSR_NUM		/* must be the last enumeration */
 };
 
+#ifndef	__FreeBSD__
+typedef enum {
+	VS_NONE		= 0x0,
+	VS_LAUNCHED	= 0x1,
+	VS_LOADED	= 0x2
+} vmcs_state_t;
+#endif /* __FreeBSD__ */
+
 /* virtual machine softc */
 struct vmx {
 	struct vmcs	vmcs[VM_MAXCPU];	/* one vmcs per virtual cpu */
@@ -136,7 +144,7 @@ struct vmx {
 #ifndef	__FreeBSD__
 	uint64_t	host_msrs[VM_MAXCPU][GUEST_MSR_NUM];
 	uint64_t	tsc_offset_active[VM_MAXCPU];
-	boolean_t	ctx_loaded[VM_MAXCPU];
+	vmcs_state_t	vmcs_state[VM_MAXCPU];
 #endif
 	struct vmxctx	ctx[VM_MAXCPU];
 	struct vmxcap	cap[VM_MAXCPU];
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_sol_glue.c b/usr/src/uts/i86pc/io/vmm/vmm_sol_glue.c
index b523b69bd9..4643bb9c41 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_sol_glue.c
+++ b/usr/src/uts/i86pc/io/vmm/vmm_sol_glue.c
@@ -294,12 +294,15 @@ contigfree(void *addr, unsigned long size, struct malloc_type *type)
 void
 mtx_init(struct mtx *mtx, char *name, const char *type_name, int opts)
 {
-	if (opts & MTX_SPIN) {
-		mutex_init(&mtx->m, name, MUTEX_SPIN,
-		    (ddi_iblock_cookie_t)ipltospl(DISP_LEVEL));
-	} else {
-		mutex_init(&mtx->m, name, MUTEX_DRIVER, NULL);
-	}
+	/*
+	 * Requests that a mutex be initialized to the MTX_SPIN type are
+	 * ignored.  The limitations which may have required spinlocks on
+	 * FreeBSD do not apply to how bhyve has been structured here.
+	 *
+	 * Adaptive mutexes are required to avoid deadlocks when certain
+	 * cyclics behavior interacts with interrupts and contended locks.
+	 */
+	mutex_init(&mtx->m, name, MUTEX_ADAPTIVE, NULL);
 }
 
 void
-- 
2.21.0

