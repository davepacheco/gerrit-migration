commit ccb05bd751ee5973a740fc8bbe1eeccf43e01822 (refs/changes/71/3971/2)
Author: Richard Bradley <richard.bradley@joyent.com>
Date:   2018-12-17T15:57:59+00:00 (10 months ago)
    
    MANATEE-375 Update dev zone scripts for MANATEE-335

diff --git a/README.md b/README.md
index d931f35..7af05d4 100644
--- a/README.md
+++ b/README.md
@@ -38,6 +38,9 @@ Problems? Check out the [Troubleshooting guide](docs/trouble-shooting.md).
 Migrating from Manatee 1.0 to 2.0?  Check out the [migration
 guide](docs/migrate-1-to-2.md).
 
+Working on Manatee? Check out the [Working on Manatee
+guide](docs/working-on-manatee.md).
+
 # Features
 
 * Automated liveliness detection, failover, and recovery. Reads are always
@@ -84,194 +87,3 @@ client.on('error', function (err) {
     console.error({err: err}, 'got client error');
 });
 ```
-
-# Working on Manatee
-
-When working on Manatee, it's convenient to be able to run multiple instances in
-the same environment.  This won't allow you to test all possible failure modes,
-but it works for most basic functionality.
-
-This process involves:
-
-* Deploying a zone with the right version of postgres available and with the
-  ability to manage ZFS datasets.  Manatee needs to be able to run as root
-  inside this zone.
-* Installing postgres, git, gcc, and other tools required to build.
-* Creating a ZFS dataset for each Manatee peer you want to run.  (We'll assume
-  three instances in this guide.)
-* Creating a configuration file for each Manatee peer you want to run.
-* Starting each peer by hand.
-
-
-## Summary
-
-These steps assume you've already got ZooKeeper running somewhere.  In
-the steps below, $ZK_CONN_STR is a connection string, or a comma-separated list
-of IP:PORT pairs for the zookeeper cluster.
-
-Run all of the following as root:
-
-1. Provision a SmartOS zone using multiarch 13.3.1.  This is image
-   4aec529c-55f9-11e3-868e-a37707fcbe86.  Be sure to provision the zone with a
-   delegated ZFS dataset.
-1. Log into the zone and run the following steps as root (or with sudo or
-   as another privileged user).
-1. Install packages:
-
-        # pkgin -y in gmake scmgit gcc47 postgresql92-server-9.2.4nb1 \
-            postgresql92-adminpack postgresql92-replicationtools \
-            postgresql92-upgrade lz4-120
-
-1. Get and build a local copy of this repo:
-
-        # git clone https://github.com/joyent/manatee
-        # cd manatee
-        # git checkout MANATEE-188
-        # make
-
-1. Pick an IP address from "ifconfig -a".  We'll call this $SELF_IP.  The IP to
-   use will depend on your configuration.  The Manatee services will bind to
-   this IP, so don't pick a public IP unless that's really what you want.
-
-1. Run the setup script
-
-        # ./tools/mkdevsitters $SELF_IP $ZK_CONN_STR
-
-1. For each peer ("1", "2", "3"), open up two terminals.  In the first, start
-   the sitter:
-
-        # node sitter.js -f devconfs/sitter1/sitter.json | bunyan
-
-   In the second terminal, start the backup server:
-
-        # node backupserver.js -f devconfs/sitter1/backupserver.json | bunyan
-
-If you want to clean everything up (**note: this will destroy all data stored
-in these peers!)**, run:
-
-    # for peer in 1 2 3; do zfs destroy -R zones/$(zonename)/data/peer$peer; done
-
-**This command is very destructive!  Be sure you're okay with destroying the
-datasets, snapshots, and clones of all of the peers you created before you run
-this command.**
-
-Then run:
-
-    # rm -rf devconfs
-
-## Details
-
-This section has more details about the above procedure.
-
-### Provisioning a development zone
-
-We develop Manatee in SmartOS zones running under SDC.  You should be able to
-run on standalone SmartOS (i.e., not running under SDC), or even other systems
-with ZFS and Postgres installed (e.g., BSD).  Manatee requires access to ZFS
-datasets to create snapshots, send streams, and the like, and it also must
-run as root.  The former currently rules out the Joyent Public Cloud as a
-deployment option.
-
-We deploy Manatee using the multiarch 13.3.1 image (equivalent to image
-4aec529c-55f9-11e3-868e-a37707fcbe86).  For development, we recommend using a
-zone based on that image, deployed on a network with a ZooKeeper instance
-running.  On SDC, be sure to set `delegate_dataset=true` when provisioning.  On
-standalone SmartOS, set `delegate_dataset=true` when you invoke "vmadm create".
-
-### Installing packages
-
-You'll need git, GNU make, a compiler toolchain, lz4, and the postgres client,
-server, and tools.  On the above multiarch SmartOS zone, you can install these
-with:
-
-    # pkgin -y in gmake scmgit gcc47 postgresql92-server-9.2.4nb1 \
-        postgresql92-adminpack postgresql92-replicationtools \
-        postgresql92-upgrade lz4-120
-
-### Creating ZFS datasets and configurations
-
-There's a tool inside the repo called "mkdevsitters" which configures the local
-system to run three Manatee peers.  You'll have to run the three peers by hand.
-The script just creates configuration files and ZFS datasets.  The script must
-be run as root.
-
-To use the script, you'll need to know:
-
-* The local IP address you intend to use for these Manatee peers.  If you don't
-  know, you can run "ifconfig -a" and pick one.  The tool does not do this
-  automatically because common develompent environments have multiple addresses,
-  only one of which is correct for this purpose, and it's impossible for the
-  script to know which to use.
-* The IP address and port of a remote ZooKeeper server.  The port is usually
-  2181.  The value you use here is actually a comma-separated list of IP:PORT
-  pairs.
-
-To use this script, as the root user, run:
-
-    # ./tools/mkdevsitters MY_IP ZK_IPS
-
-For example, if my local IP is 172.21.1.74 and there's a ZooKeeper server at
-172.21.1.11, I might run this as root:
-
-    # ./tools/mkdevsitters 172.21.1.74 172.21.1.11:2181
-
-This does several things:
-
-* Creates a directory called "devconfs" in the current directory.  "devconfs"
-  will contain the configuration and data for each of the three test peers.
-* Creates three ZFS datasets under zones/$(zonename)/data, called "peer1",
-  "peer2", and "peer3".  The mountpoints for these datasets are in
-  "devconfs/datasets".
-* Creates configuration files for the Manatee sitter and Manatee backup server
-  in "devconfs/sitterN".  Also creates a template postgres configuration file
-  in the same directory.
-
-The various services associated with each peer (postgres itself, the sitter's
-status API, the backup server, and so on) are all configured to run on different
-ports.  The first peer runs on the default ports; subsequent peers run on ports
-numbered 10 more than the previous port.  The default postgres port is 5432, so
-the first peer runs postgres on port 5432, the second peer runs postgres on port
-5442, and the third peer runs postgres on port 5452.
-
-
-### Running each peer
-
-There are currently two components to run for each peer: the sitter (which also
-starts postgres) and the backup server (which is used for bootstrapping
-replication for new downstream peers).  These commands are all intended to be
-run with "root" user privileges.  To start the first peer, use:
-
-    # node sitter.js -f devconfs/sitter1/sitter.json
-
-You'll probably want to pipe this to bunyan.  Be sure to run this as root.  To
-run other peers, replace "sitter1" with "sitter2" or "sitter3".
-
-Similarly, to run the backupserver, use:
-
-    # node backupserver.js -f devconfs/sitter1/backupserver.json
-
-There's also a snapshotter, but you will likely want to create a custom
-configuration file for running it for development:
-
-    1. Create a file of this format, i.e. `etc/snapshotter_test_config.json`
-
-    {
-      "//": "The ZFS dataset used by Manatee."
-      "dataset": "zones/$ZONE_UUID/data/manatee",
-      "//" : "Snapshot period in ms",
-      "pollInterval": 36000,
-      "//" : "Number of snapshots to keep.",
-      "snapshotNumber": 20
-    }
-
-    2. Run the snapshotter with the config file:
-
-    # node snapshotter.js -f etc/snapshotter_test_config.json 2>&1 | bunyan
-
-### Running tests
-
-Before you can run a clean `make prepush`, you will need to set these
-environmental variables:
-
-    # export SHARD=1.moray.$YOUR_LAB_OR_VM.joyent.us
-    # export ZK_IPS=$NAMESERVICE_INSTANCE_IP
\ No newline at end of file
diff --git a/docs/working-on-manatee.md b/docs/working-on-manatee.md
new file mode 100644
index 0000000..af3bb53
--- /dev/null
+++ b/docs/working-on-manatee.md
@@ -0,0 +1,249 @@
+# Working on Manatee
+
+When working on Manatee, it's convenient to be able to run multiple instances in
+the same environment.  This won't allow you to test all possible failure modes,
+but it works for most basic functionality.
+
+This process involves:
+
+* Deploying a zone with the right versions of postgres available and with the
+  ability to manage ZFS datasets.  Manatee needs to be able to run as root
+  inside this zone.
+* Installing postgres, git, gcc, and other tools required to build.
+* Creating a ZFS dataset for each Manatee peer you want to run.  (We'll assume
+  three instances in this guide.)
+* Creating a configuration file for each Manatee peer you want to run.
+* Starting the cluster.
+
+
+## Summary
+
+These steps assume you've already got ZooKeeper running somewhere.  In
+the steps below, $ZK_CONN_STR is a connection string, or a comma-separated list
+of IP:PORT pairs for the zookeeper cluster.  If you're provisioning inside a
+non-production Triton installation that you own, it's generally acceptable to
+provision your zone onto the admin and external networks and make use of
+Triton's `binder` instance.
+
+The following steps should run as roon inside a SmartOS zone provisioned onto
+the multiarch 13.3.1 image (image_uuid=4aec529c-55f9-11e3-868e-a37707fcbe86).
+This zone will also need to be provisioned with a delegated dataset.  See
+"Provisioning a development zone" for an example.
+
+1. Install packages:
+
+        # pkgin in git gmake gcc47 bison flex
+
+1. Get and build a local copy of this repo:
+
+        # git clone https://github.com/joyent/manatee
+        # cd manatee
+        # make
+
+1. Get the following details about the cluster:
+    1. An IP address from "ifconfig -a".  We'll call this $SELF_IP.  The IP to
+        use will depend on your configuration.  The Manatee services will bind to
+        this IP, so don't pick a public IP unless that's really what you want.
+    1. The $ZK_CONN_STRING mentioned earlier.
+    1. A name for this cluster as $SHARD_NAME, which will define the location
+        that the cluster's state will be stored in ZooKeeper, so ensure it doesn't
+        violate ZooKeeper's conventions.  Either the full path (e.g.
+        "/my/test/cluster"), or the top level directory name (e.g. "testing123",
+        in which case we'll prefix "/manatee/" to this choice, making the full
+        path "/manatee/testing123").
+    1. The version of PostgreSQL you want to use as $PG_VERSION.  Supported
+        options for this are "9.2" or "9.6".  If no value is supplied, the
+        default value of "9.6" is used.
+
+1. Run the setup script
+
+        # ./tools/mkdevsitters $SELF_IP $ZK_CONN_STR $SHARD_NAME $PG_VERSION
+
+1. Start the cluster using the methods described in the "Running the cluster"
+    section
+
+## Details
+
+This section has more details about the above procedure.
+
+### Provisioning a development zone
+
+We develop Manatee in SmartOS zones running under Triton.  You should be able to
+run on standalone SmartOS (i.e., not running under Triton), or even other systems
+with ZFS and Postgres installed (e.g., BSD).  Manatee requires access to ZFS
+datasets to create snapshots, send streams, and the like, and it also must
+run as root.  The former currently rules out the Joyent Public Cloud as a
+deployment option.
+
+We deploy Manatee using the multiarch 13.3.1 image (equivalent to image
+4aec529c-55f9-11e3-868e-a37707fcbe86).  For development, we recommend using a
+zone based on that image, deployed on a network with a ZooKeeper instance
+running.  On Triton, be sure to set `delegate_dataset=true` when provisioning.  On
+standalone SmartOS, set `delegate_dataset=true` when you invoke "vmadm create".
+
+An example of using Triton to provision this zone (run from and provisioned to
+the headnode):
+
+    # sdc-vmapi /vms -X POST -d '{
+        "owner_uuid": "'$(sdc-useradm get admin | json uuid)'",
+        "server_uuid": "'$(sysinfo | json UUID)'",
+        "image_uuid": "4aec529c-55f9-11e3-868e-a37707fcbe86",
+        "networks": [ {
+            "ipv4_uuid": "'$(sdc-napi /networks?name=external | json -H 0.uuid)'",
+            "primary": true
+        }, {
+            "ipv4_uuid": "'$(sdc-napi /networks?name=admin | json -H 0.uuid)'"
+        } ],
+        "brand": "joyent",
+        "ram": 2048,
+        "delegate_dataset": true }'
+
+If you don't have the image installed, you can run the following (again, from
+the headnode):
+
+    # sdc-imgadm import 4aec529c-55f9-11e3-868e-a37707fcbe86 \
+        -S https://updates.joyent.com
+
+Once this zone is provisioned you can `zlogin` to the resulting zone and
+continue with setting up Manatee.
+
+### Installing packages/dependencies
+
+You'll need git, GNU make, a compiler toolchain, and some libraries that are
+required for building PostgreSQL.  On the above multiarch SmartOS zone, you can
+install these with:
+
+    # pkgin in git gmake gcc47 bison flex
+
+PostgreSQL is built from source, which we pull down as a submodule and checkout
+at a certain commit to define the version.  Once built in a temporary location,
+we move it to a location on the filesystem that our Manatee configs expect.
+This is done by the `./tool/mkdevsitters` script (detailed in "Using
+`mkdevsitters`", but is described below in case you should need to install
+PostgreSQL directly.)
+
+Checkout is done like so (in this example for 9.6):
+
+    # git submodule add https://github.com/postgres/postgres.git \
+        deps/postgresql96 && cd deps/postgresql96 && \
+        git checkout ca9cfe && cd -
+
+Building PostgreSQL like so:
+
+    # make -f Makefile.postgres RELSTAGEDIR=/tmp/test \
+        DEPSDIR=/root/manatee/deps pg96
+
+And installing like so:
+
+    # cp -R /tmp/test/root/opt/postgresql /opt/.
+
+Note: Manatee will create a symlink under "/opt/postgresql" to the current
+version of PostgreSQL that it expects as "/opt/postgresql/current".  This will
+not be available until Manatee has been started for the first time.
+
+### Using `mkdevsitters`
+
+There's a tool inside the repo called "mkdevsitters" which configures the local
+system to run three Manatee peers.  You'll have to run the three peers by hand.
+The script builds/installs PostgreSQL, creates configuration files, and creates
+ZFS datasets.  The script must be run as root.
+
+To use the script, you'll need to know:
+
+* The local IP address you intend to use for these Manatee peers.  If you don't
+  know, you can run "ifconfig -a" and pick one.  The tool does not do this
+  automatically because common develompent environments have multiple addresses,
+  only one of which is correct for this purpose, and it's impossible for the
+  script to know which to use.
+* The IP address and port of a remote ZooKeeper server.  The port is usually
+  2181.  The value you use here is actually a comma-separated list of IP:PORT
+  pairs.
+* The name of the cluster you're creating.  This is used to identify your
+  cluster, and will define the location in ZooKeeper that your cluster's state
+  will be stored.  Either choose a full path or an identifier, that latter of
+  which will be prefixed with "/manatee/".  Ensure this is unique to your test
+  cluster.
+* The version of PostgreSQL that you're using.  Either "9.2" or "9.6".
+
+To use this script, run:
+
+    # ./tools/mkdevsitters MY_IP ZK_IPS SHARD_NAME PG_VERSION
+
+For example, if my local IP is 172.21.1.74 and there's a ZooKeeper server at
+172.21.1.11, with the cluster name "testing123" and version 9.6 of PostgreSQL,
+I might run this as root:
+
+    # ./tools/mkdevsitters 172.21.1.74 172.21.1.11:2181 testing123 9.6
+
+This does several things:
+
+* Builds and installs all versions of PostgreSQL currently supported by Manatee
+  (which is currently 9.2 and 9.6).
+* Creates a directory called "devconfs" in the current directory.  "devconfs"
+  will contain the configuration and data for each of the three test peers.
+* Creates three ZFS datasets under zones/$(zonename)/data, called "peer1",
+  "peer2", and "peer3".  The mountpoints for these datasets are in
+  "devconfs/datasets".
+* Creates configuration files for the Manatee sitter and Manatee backup server
+  in "devconfs/sitterN".  Also creates a template postgres configuration file
+  in the same directory.
+
+The various services associated with each peer (postgres itself, the sitter's
+status API, the backup server, and so on) are all configured to run on different
+ports.  The first peer runs on the default ports; subsequent peers run on ports
+numbered 10 more than the previous port.  The default postgres port is 5432, so
+the first peer runs postgres on port 5432, the second peer runs postgres on port
+5442, and the third peer runs postgres on port 5452.
+
+
+### Running the cluster
+
+There are currently two components to run for each peer: the sitter (which also
+starts postgres) and the backup server (which is used for bootstrapping
+replication for new downstream peers).  The following is an example of starting
+the cluster and ensuring each peer's output is directed to a file:
+
+    # seq 1 3 | while read peer; do node --abort-on-uncaught-exception \
+        sitter.js -vvv -f devconfs/sitter$peer/sitter.json \
+        > /var/tmp/sitter$peer.log 2>&1 & done
+
+Be sure to run this as root.  Logs can be tailed under `/var/tmp/sitter*.log`.  To
+kill all running instances of sitter and postgres, run:
+
+    # pgrep -f sitter | while read pid; do pkill -9 -P $pid; done
+
+Because sitter spawns PostgreSQL as a child process, care must be taken to
+ensure that PostgreSQL is also stopped when sitter is.  If in doubt,
+`pkill postgres` will ensure that all instances of PostgreSQL are stopped.
+
+Similarly, to run a backupserver for each peer in the cluster, use:
+
+    # seq 1 3 | while read peer; do node --abort-on-uncaught-exception \
+        backupserver.js -f devconfs/sitter$peer/backupserver.json \
+        > /var/tmp/backupserver$peer.log 2>&1 & done
+
+### Clearing the cluster
+
+**Note: This section will destroy data!  Be very sure that you no longer need
+this cluster's data before proceeding.**
+
+If you want to clean everything up and start with an empty cluster, run the
+following commands:
+
+1. In your dev zone:
+
+    1. Destroy all ZFS datasets:
+
+            # seq 1 3 | while read peer; do \
+                zfs destroy -R zones/$(zonename)/data/peer$peer; done
+
+    1. Delete all development configs:
+
+            # rm -rf devconfs
+
+1. In your ZooKeeper instance:
+
+    1. Note: this step is only required if you plan on re-using this cluster's
+    name again in a new cluster.
+
+            # zkCli.sh rmr /manatee/testing123
diff --git a/tools/mkdevsitters b/tools/mkdevsitters
index 92b23aa..e72acad 100755
--- a/tools/mkdevsitters
+++ b/tools/mkdevsitters
@@ -6,11 +6,19 @@
 #
 
 mds_arg0="$(basename ${BASH_SOURCE[0]})"
-mds_zonename=
+mds_zonename="$(zonename)"
 mds_basedir="$(dirname ${BASH_SOURCE[0]})/.."
 mds_mksitterconfig="$mds_basedir/tools/mksitterconfig"
+declare -A mds_pgsupported
+mds_pgsupported=(
+    ["9.2"]="73c122"
+    ["9.6"]="ca9cfe"
+)
+mds_pgbuilddir="/tmp/mds_pgbuild"
 mds_ip="$1"
 mds_zkconnstr="$2"
+mds_shard="$3"
+mds_pgversion="$4"
 
 function fail
 {
@@ -27,6 +35,8 @@ function mksitter
 	which="$1"
 	ip="$2"
 	zkconnstr="$3"
+	shard="$4"
+	pgversion="$5"
 	dataset="zones/$mds_zonename/data/peer$which"
 	sitterdir="$PWD/devconfs/sitter$which"
 	mountpoint="$PWD/devconfs/datasets/manatee$which"
@@ -60,13 +70,18 @@ function mksitter
 	#    o the backup server configuration file
 	#
 	mkdir -p $sitterdir
+	mkdir -p $sitterdir/9.2
+	mkdir -p $sitterdir/9.6
 	mkdir -p $sitterdir/log
 	chown postgres:postgres $sitterdir/log || \
 	    fail "failed to chown postgres log directory"
 
 	echo -n "peer $which: creating configuration ... "
 	$mds_mksitterconfig "$PWD/devconfs" "$ip" "$zkconnstr" "$which" \
-	    > "$sitterdir/sitter.json" || fail "failed"
+	    "$shard" "$pgversion" > "$sitterdir/sitter.json" || \
+	    fail "failed to generate sitter config"
+	echo "{}" > "$sitterdir/pg_overrides.json" || \
+	    fail "failed to write pg_overrides.json"
 	echo "done."
 
 	echo -n "peer $which: fetching port from generated config ... "
@@ -75,11 +90,16 @@ function mksitter
 	echo "$port."
 
 	echo -n "peer $which: creating template postgresql.conf ... "
-	egrep -v '^\s*port\s*=' $mds_basedir/etc/postgresql.conf |
-	    egrep -v '^\s*log_directory\s*=' > "$sitterdir/postgres.conf"
+	egrep -v '^\s*port\s*=' "$mds_basedir/etc/postgresql.conf" |
+	    egrep -v '^\s*log_directory\s*=' > \
+	    "$sitterdir/$pgversion/postgresql.conf"
 	echo "port = $port # (change requires restart)" >> \
-	    "$sitterdir/postgres.conf"
-	echo "log_directory = '$sitterdir/log'" >> "$sitterdir/postgres.conf"
+	    "$sitterdir/$pgversion/postgresql.conf"
+	echo "log_directory = '$sitterdir/log'" >> \
+	    "$sitterdir/$pgversion/postgresql.conf"
+	for conf in recovery pg_hba; do
+		cp "$mds_basedir/etc/$conf.conf" "$sitterdir/$pgversion/."
+	done
 	echo "done."
 
 	echo -n "peer $which: creating backupserver config ... "
@@ -95,16 +115,72 @@ function mksitter
 	echo "done."
 }
 
+# a fair amount of this function is making naive assumptions, but for
+# development purposes it's likely enough
+function ensurepostgres {
+	local vshort pgdir
+
+	if ! json -f package.json > /dev/null 2>&1; then
+		fail "must be in manatee project directory"
+	fi
+
+	if ! ls .git > /dev/null 2>&1; then
+		fail "pwd doesn't appear to be a git repository"
+	fi
+
+	groupadd -g 907 postgres > /dev/null 2>&1
+	useradd -u 907 -g postgres postgres > /dev/null 2>&1
+
+	if ! grep ^postgres /opt/local/etc/sudoers > /dev/null 2>&1; then
+	echo "postgres    ALL=(ALL) NOPASSWD: /usr/bin/chown, /usr/bin/chmod," \
+	    "/opt/local/bin/chown, /opt/local/bin/chmod" \
+	    >> /opt/local/etc/sudoers
+	fi
+
+	echo "ensuring supported postgres versions are installed"
+	git submodule add https://github.com/reorg/pg_repack.git deps/pg_repack
+	for version in ${!mds_pgsupported[@]}; do
+		vshort=$(echo "$version" | sed 's/\.//')
+		pgdir="$mds_pgbuilddir/root/opt/postgresql/"
+
+		git submodule add https://github.com/postgres/postgres.git \
+		    "deps/postgresql$vshort"
+		cd "deps/postgresql$vshort"
+		git checkout "${mds_pgsupported[$version]}"
+		cd - > /dev/null 2>&1
+
+		if ! ls "$pgdir/$version"*"/bin/postgres" > /dev/null 2>&1; then
+			echo "building postgres $version" \
+			    "(log at $mds_pgbuilddir/build.log)"
+			mkdir -p "$mds_pgbuilddir"
+			make -f Makefile.postgres \
+			    RELSTAGEDIR="$mds_pgbuilddir" \
+			    DEPSDIR="$PWD/deps" "pg$vshort" \
+			    >> "$mds_pgbuilddir/build.log" 2>&1
+		fi
+	done
+	cp -R "$mds_pgbuilddir/root/opt/postgresql" /opt/.
+}
+
 if ! type zonename > /dev/null 2>&1; then
 	fail "cannot determine dataset root: zonename(1M) command not found"
 fi
 
-if [[ $# -ne 2 ]]; then
-	echo "usage: $mds_arg0 LOCAL_IP ZK_IP:ZK_PORT" >&2
+if [[ $# -lt 3 ]]; then
+	echo "usage: $mds_arg0 LOCAL_IP ZK_IP:ZK_PORT SHARD PG_VERSION" >&2
 	exit 2
 fi
 
-mds_zonename="$(zonename)"
+if [[ -z "$mds_pgversion" ]]; then
+	mds_pgversion="9.6"
+fi
+
+if [[ ! "${!mds_pgsupported[@]}" =~ "$mds_pgversion" ]]; then
+	fail "supported version are \"9.2\" or \"9.6\""
+fi
+
+ensurepostgres
+
 for (( i = 1; i <= 3; i++ )) {
-	mksitter $i "$mds_ip" "$mds_zkconnstr"
+	mksitter $i "$mds_ip" "$mds_zkconnstr" "$mds_shard" "$mds_pgversion"
 }
diff --git a/tools/mksitterconfig b/tools/mksitterconfig
index 5d0e75b..8e5ac58 100755
--- a/tools/mksitterconfig
+++ b/tools/mksitterconfig
@@ -26,6 +26,17 @@ var configTemplate = {
     'backupPort': 10002,
     'ip': null,
     'postgresMgrCfg': {
+        'dataConfig': null,
+        'postgresConfDir': null,
+        'defaultVersion': null,
+        'pgBaseDir': '/opt/postgresql/',
+        'postgresConfFile': 'postgresql.conf',
+        'recoveryConfFile': 'recovery.conf',
+        'hbaConfFile': 'pg_hba.conf',
+        'versions': {
+            '9.2': '9.2.4',
+            '9.6': '9.6.3'
+        },
         'dataDir': null,
         'dbUser': 'postgres',
         'hbaConf': './etc/pg_hba.conf',
@@ -72,7 +83,8 @@ function usage(message)
 {
     if (message)
         console.error('%s: %s', arg0, message);
-    console.error('usage: %s SELF_IP ZK_IP:ZK_PORT[...] WHICH', arg0);
+    console.error('usage: %s SELF_IP ZK_IP:ZK_PORT[...] WHICH SHARD ' +
+        'PG_VERSION', arg0);
     process.exit(2);
 }
 
@@ -84,9 +96,9 @@ function fatal(message)
 
 function main()
 {
-    var root, ip, shardpath, zkconnstr, which;
+    var root, ip, shardpath, zkconnstr, which, shardname, pgversion;
 
-    if (process.argv.length < 6)
+    if (process.argv.length < 7)
         usage();
 
     root = process.argv[2];
@@ -109,7 +121,24 @@ function main()
     which = parseInt(process.argv[5], 10);
     if (isNaN(which) || which <= 0)
         usage('WHICH argument should be a positive integer');
-    shardpath = '/manatee_test/1';
+
+    shardname = process.argv[6];
+    if (typeof (shardname) !== 'string')
+        usage('SHARD does not apear to be a string');
+
+    /*
+     * Basic checks on whether we have a full shard path starting at the root,
+     * or a shard name that we'll append to the "/manatee" path.
+     */
+    if (shardname.substr('/', 1) == '/') {
+        shardpath = shardname;
+    } else {
+        shardpath = '/manatee/' + shardname;
+    }
+
+    pgversion = process.argv[7];
+    if ([ '9.2', '9.6' ].indexOf(pgversion) === -1)
+        usage('PG_VERSION should be either "9.2" or "9.6"');
 
     child_process.exec('zonename', function (err, stdout, stderr) {
         var config, reason;
@@ -126,7 +155,8 @@ function main()
             'zkconnstr': zkconnstr,
             'zonename': stdout.trim(),
             'which': which,
-            'shardpath': shardpath
+            'shardpath': shardpath,
+            'pgversion': pgversion
         });
 
         console.log(JSON.stringify(config, false, '\t'));
@@ -136,7 +166,8 @@ function main()
 function mkconfig(template, params)
 {
     var rv, key, which, portdelta;
-    var dataset, mountpoint, datadir, cookie, pgurl, pgconf;
+    var dataset, mountpoint, datadir, cookie, pgurl, pgconf, pgconfdir,
+        dataconfig;
 
     /* XXX should be a deep copy */
     rv = {};
@@ -164,11 +195,16 @@ function mkconfig(template, params)
     datadir = util.format('%s/data', mountpoint);
     cookie = util.format('/var/tmp/manatee%s_sync_cookie', which);
     pgconf = util.format('%s/sitter%s/postgres.conf', params.root, which);
+    pgconfdir = util.format('%s/sitter%s', params.root, which);
+    dataconfig = util.format('%s/manatee-config.json', mountpoint);
 
     /*
      * Construct the final configuration and return it.
      */
     rv.ip = params.ip;
+
+    rv.postgresMgrCfg.postgresConfDir = pgconfdir;
+    rv.postgresMgrCfg.dataConfig = dataconfig;
     rv.postgresMgrCfg.dataDir = datadir;
     rv.postgresMgrCfg.postgresConf = pgconf;
     rv.postgresMgrCfg.syncStateCheckerCfg.cookieLocation = cookie;
@@ -177,9 +213,10 @@ function mkconfig(template, params)
     rv.postgresMgrCfg.zfsClientCfg.dataset = dataset;
     rv.postgresMgrCfg.zfsClientCfg.mountpoint = mountpoint;
     rv.postgresMgrCfg.zfsClientCfg.zfsHost = params.ip;
+    rv.postgresMgrCfg.defaultVersion = params.pgversion;
     rv.zkCfg.connStr = params.zkconnstr;
     rv.shardPath = params.shardpath;
-    rv.zoneId = params.zonename;
+    rv.zoneId = util.format('peer%d-%s', params.which, params.zonename);
     return (rv);
 }
 
