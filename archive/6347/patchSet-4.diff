From 84e0765ecda75bbe3e75ccc6a3bd64c9d4a82f0f Mon Sep 17 00:00:00 2001
From: David Pacheco <dap@joyent.com>
Date: Sat, 25 May 2019 03:40:41 +0000
Subject: [PATCH] MANTA-4295 add basic documentation for key Manta metrics to
 debugging guide Reviewed by: Alex Wilson <alex.wilson@joyent.com> Approved
 by: Alex Wilson <alex.wilson@joyent.com>

---
 0001-incident-response.adoc |  14 ++
 0005-metrics.adoc           | 303 ++++++++++++++++++++--
 docs/index.html             | 485 +++++++++++++++++++++++++++++++++++-
 3 files changed, 777 insertions(+), 25 deletions(-)

diff --git a/0001-incident-response.adoc b/0001-incident-response.adoc
index 4f562fa..3a4bea9 100644
--- a/0001-incident-response.adoc
+++ b/0001-incident-response.adoc
@@ -252,6 +252,20 @@ There are a couple of major kinds of error.
 - If there's a networking issue that causes the client or server to abandon the
   connection, both sides will generally report an explicit socket error.
 
+Errors that manifest as explicit HTTP responses (i.e., 400-level and 500-level
+responses) are visible to operators via logs and metrics provided by Manta.
+
+There are less common failures as well:
+
+- A client may find no servers in DNS.  This may or may not be visible to
+  operators.
+- A client may time out attempting to resolve DNS.  This is unlikely to be
+  visible to Manta operators.
+- A client may time out attempting to establish a TCP connection to Manta.  This
+  is not likely to be visible to Manta operators.
+
+The rest of this section discusses specific response codes for errors and what
+they mean.
 
 ==== Response code 507
 
diff --git a/0005-metrics.adoc b/0005-metrics.adoc
index 3f0bac2..2c119cb 100644
--- a/0005-metrics.adoc
+++ b/0005-metrics.adoc
@@ -1,23 +1,298 @@
 == Metrics
 
+// XXX add a section on Characterizing current behavior
+
 Real-time metrics provided by Manta form the basis of situational awareness,
-particularly during incident response.  Metrics are available for most data path
-components.  Understanding these metrics requires a basic understanding of these
-components and how they work together.  For more on these components, see
-http://joyent.github.io/manta/#components-of-manta[Components of Manta] in the
-Operator Guide.
+particularly during incident response.  Understanding the metrics provided by
+the various components internal to Manta requires
+http://joyent.github.io/manta/#components-of-manta[a basic understanding of these
+components] and how they work together.
 
-Manta components expose metrics, but within any given Manta deployment, it's up
-to operators to set up systems for collecting, presenting, and alerting on
-metrics.  For more, see <<_deployment_specific_details>>.
+This section discusses a number of useful metrics exposed by Manta.  When we say
+a metric is exposed by Manta, we usually mean that Triton (via CMON) and/or
+individual Manta components collect these metrics and serve them over HTTP in
+Prometheus format.  Within a Manta deployment, it's up to operators to set up
+systems for collecting, presenting, and alerting on these metrics.  For more on
+this, see <<_deployment_specific_details>>.  There are screenshots from existing
+deployments below, but the specific metrics available and the appearance of
+graphs may vary in your deployment.
 
-This section uses screenshots from existing deployments to discuss metrics
-provided by Manta and how to interpret them.  The specific metrics available and
-the appearance of graphs may vary in your deployment.
+NOTE: This section covers _operational_ metrics used to understand Manta's
+runtime behavior.  These are not end-user-visible metrics.
+
+NOTE: This section covers what information is available and how to interpret it.
+Unfortunately, there is no single place today that documents the list of metrics
+available.
+
+=== Key service metrics
+
+Key metrics for assessing the health of a service are driven by whatever the
+customer cares about.  For Manta, that's typically:
+
+- the *error rate*
+- *throughput* of data in and out of Manta
+- *latency*
+
+These are closely related to the
+https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/#xref_monitoring_golden-signals[the
+Four Golden Signals] described in the _Site Reliability Engineering_ book from
+Google.  That section provides a good summary of these metrics and why they're
+so important.
+
+A Manta deployment typically seeks to achieve a particular level of performance,
+usually expressed in terms of throughput (objects per second read or written
+_or_ bytes per second read or written) or latency (the time required to complete
+each request) while maintaining an acceptable error rate.  Errors must be
+considered when looking at performance, since many types of error responses can
+be served very quickly -- that doesn't mean the service is working as expected!
+
+NOTE: We use the term **error rate** to refer to the fraction of requests that
+failed with an error (e.g., "3% of responses were 500-level errors" would be a
+3% error rate).  The same term is sometimes used to refer to the count of errors
+per unit time (e.g., "3 errors per second"), but that's usually less useful for
+describing the service's health.
+
+Since there may be thousands of requests completed per second at any given time,
+when understanding latency, we almost always use some aggregated form of
+latency:
+
+- **Average latency** is useful for use-cases that primarily care about
+  throughput because the average latency is closely related to throughput:
+  throughput (in requests per second) equals the total number of clients divided
+  by the average latency per request.  When all you know is that throughput has
+  gone down, it's hard to tell whether there's a problem because it could
+  reflect a server issue or a lack of client activity.  Average latency resolves
+  this ambiguity: if average latency has gone up, there's likely a server
+  problem.  If not, clients have likely stopped trying to do as much work.
+- **Tail latency** refers to the latency of the the slowest requests (the _tail_
+  of the latency distribution).  Tail latency is often a better indicator of
+  general service health than average latency because small or occasional
+  problems often affect tail latency significantly even when they don't affect
+  the average that much.  Tail latency is often expressed as a percentile: for
+  example, the shorthand "p90" refers to the 90th percentile, which is the
+  minimum latency of the slowest 10% of requests.  Similarly, p99 is the minimum
+  latency of the slowest 1% of requests; and so on.  When we say that the p99 is
+  300ms, we mean that 99% of requests completed within 300ms.
+
+Not only do we use these metrics (error rate, throughput, and latency) to
+describe Manta's overall health, but we also use them to describe the health of
+the various components within Manta.
+
+=== Top-level Manta metrics
+
+To understand Manta's overall health, we typically look at the error rate,
+throughput, and latency for Muskie (webapi) since Muskie most directly handles
+the HTTP requests being issued by end users.
+
+NOTE: **If you're looking for help understanding a Manta problem, see the
+<<_incident_response_decision_tree>>.**  This section provides background on the
+metrics referenced in that section.
+
+**Errors at webapi:** For background on errors in Manta, see
+<<_investigating_an_increase_in_error_rate>>.  That section also discusses how
+to debug specific errors.  At this level, we're typically talking about explicit
+500-level errors.  When evaluating an error rate against expectations, we
+usually use its inverse -- the _success rate_: the fraction of requests that
+completed successfully.  This is often measured in
+https://en.wikipedia.org/wiki/High_availability#Percentage_calculation["nines"].
+Generally, if a deployment seeks a particular level of availability (e.g.,
+99.9%), an incident may be raised if the error rate exceeds a target percentage
+(e.g., 0.1%) for some period of time (e.g., 5 minutes); an incident may be
+considered resolved once the error rate is below this threshold.  The specific
+thresholds vary by deployment, but are usually at least 99%.  Generally, errors
+indicate a software failure, a significant server-side failure, or unexpected
+overload -- all of which are not supposed to be common in production
+deployments.
+
+**Throughput at webapi:** Manta throughput can be measured in
+
+- objects created per second
+- objects fetched per second
+- bytes written per second
+- bytes read per second
+
+Any of these might be significant to end users.  The target throughput in any of
+these dimensions is deployment-specific.  As mentioned above, it's very
+difficult to use throughput directly to assess the server's behavior because
+it's significantly affected by client behavior.  Average latency may be more
+useful.
+
+**Latency at webapi:** For webapi, we usually define latency as the
+time-to-first-byte for each request.  For uploads, this is the time between when
+the request is received at the server to the time when the server tells the
+client to proceed with the upload.  For downloads, this is the time between when
+the request is received and client receives the first bytes of data.  For other
+requests (which do not involve object data transfer), we look at the whole
+latency of the request.  As we've defined it, latency includes the time used by
+the server to parse the request, validate it, authenticate it, authorize it,
+load metadata, and initiate any data transfer, but it does _not_ include the
+time to transfer data.  This is useful, since transfer time depends on object
+size, and we usually want to factor that out.
+
+For general system health, we typically monitor Muskie error rate and tail
+latency (p90, p99).  When throughput is important, average latency is also
+useful for the reasons mentioned above.
+
+
+=== Key metrics internal to Manta
+
+When understanding problems with Manta, we use the same key metrics -- error
+rate, throughput, and latency -- measured by various other components.
+
+NOTE: **If you're looking for help understanding a Manta problem, see the
+<<_incident_response_decision_tree>>.**  This section provides background on the
+metrics referenced in that section.
+
+**Metadata tier:** We generally consider the metadata tier to include:
+
+- Electric-Moray, which is the gateway for nearly all requests to the metadata
+  tier that come from the data path
+- Moray, each instance of which handles requests for a particular database shard
+- PostgreSQL (deployed under Manatee), which ultimately services requests to the
+  metadata tier
+
+Electric-Moray and Moray both operate in terms of RPCs.  They both expose
+metrics that count the number of RPCs completed and failed, as well as
+histograms that can be used to calculate average latency and estimate tail
+latency.
+
+PostgreSQL operates in terms of transactions.  Manta exposes metrics collected
+from PostgreSQL about transactions completed and aborted, but not latency.  We
+typically use Moray latency as a proxy for PostgreSQL latency.
+
+**Storage tier:** We do not currently record request throughput, latency, or
+error rate from the storage tier.  However, Triton (via CMON) collects network
+bytes transferred and ZFS bytes read and written, which are useful proxies for
+inbound and outbound data transfer.
+
+
+=== Other operational metrics
+
+Manta exposes a number of other useful metrics:
+
+- CPU utilization, broken out by zone (and filterable by type of component).
+  For stateless services (i.e., most services _within_ Manta), this is a useful
+  way to determine if instances (or a whole service) is overloaded.  For
+  example, webapi instances are typically deployed using 16 processes that are
+  effectively single-threaded.  If any webapi instances are using close to 1600%
+  of one CPU (i.e., 16 CPUs), they're likely overloaded, and end users are
+  likely to experience elevated latency as a consequence.  In order to interpret
+  these values, you generally have to know how many CPUs a particular component
+  can typically use.
+- Disk utilization, broken out by zone (and filterable by type of component).
+  This is useful for understanding disk capacity at both the metadata tier and
+  storage tier.
+- PostgreSQL active connections, broken out by shard.  This roughly reflects how
+  much concurrent work is happening on each PostgreSQL shard.  This can be
+  useful for identifying busy or slow shards (though it can be hard to tell if a
+  shard is slow because it's busy or if it's busy because it's slow).
+- PostgreSQL vacuum activity, <<_predicting_autovacuum_activity, described
+  below>>.
+- TCP errors (collected via Triton's CMON), including failed connection
+  attempts, listen drops, and retransmitted packets.  These can reflect various
+  types of network issues.
+- OS anonymous allocation failures (collected via Triton's CMON).  This
+  particular event indicates that a process attempted to allocated memory but
+  failed because it has reached a memory cap.  Many programs do not handle
+  running out of memory well, so these allocation failures can sometimes result
+  in cascading failures.
+
+
+=== Summary of metrics
+
+Below is a rough summary of the metrics exposed by Manta and which components
+expose them.  There are several caveats:
+
+- **This information is subject to change without notice as the underlying
+  software evolves!**
+- This table does not describe which Prometheus instances collect, aggregate, and serve each metric.  See <<_deployment_specific_details>>.
+- Relatedly, in large deployments, Prometheus **recording rules** may be used to precalculate important metrics.  These are not documented here.
+- Many metrics provided a number of breakdowns using Prometheus labels.
+
+[cols="4*",options="header"]
+|===
+|Component being measured
+|Where the metric is collected
+|Metric name
+|Notes
+
+|Manta itself
+|webapi
+|`muskie_inbound_streamed_bytes`
+|count of bytes uploaded to Muskie (all uploads, including in-progress and failures).  This is a primary metric for end users.
+
+|Manta itself
+|webapi
+|`muskie_outbound_streamed_bytes`
+|count of bytes downloaded from Muskie (all downloads, including in-progress and failures).  This is a primary metric for end users.
+
+|Manta itself
+|webapi
+|`http_requests_completed`
+|count of requests completed, with labels for individual HTTP response codes.  This can be used to calculate the error rate as well.  This is the basis for several primary metrics for end users.
+
+|Manta itself
+|webapi
+|`http_request_latency_ms`
+|histogram of request latency, used to calculate average latency and to estimate percentiles.  This is a primary metric for end users.
+
+|Electric-Moray
+|electric-moray
+|`fast_requests_completed`
+|count of requests completed, with a label for RPC method name.  This is useful for measuring overall throughput at the metadata tier.
+
+|Electric-Moray
+|electric-moray
+|`fast_server_request_time_seconds`
+|histogram of RPC latency, with a label for RPC method name.  This is useful for calculating average latency and estimating tail latency at the metadata tier.
+
+|Moray
+|moray
+|`fast_requests_completed`
+|Same as for electric-moray, but this is measured for a particular Moray instance (and so a particular database shard).
+
+|Moray
+|moray
+|`fast_server_request_time_seconds`
+|Same as for electric-moray, but this is measured for a particular Moray instance (and so a particular database shard).
+
+|PostgreSQL
+|pgstatsmon
+|Various
+|A number of https://www.postgresql.org/docs/9.6/monitoring-stats.html[stats exposed by PostgreSQL] are collected and exposed by pgstatsmon.  For the authoritative set, see https://github.com/joyent/pgstatsmon/blob/master/lib/queries.js[the pgstatsmon source].  These stats are named according to their PostgreSQL names, so for example the `xact_commit` stat in the `pg_stat_database` view is exposed as `pg_stat_database_xact_commit`.  Labels are used to identify the PostgreSQL instance, which can often be used to break out by shard.
+
+|TCP stack
+|CMON (in Triton)
+|`tcp_listen_drop_count`, `tcp_listen_drop_Qzero_count`
+|count of the number of times a TCP connect attempt was dropped on the server side, often due to overload.  This is useful for identifying TCP server problems.
+
+|TCP stack
+|CMON (in Triton)
+|`tcp_failed_connection_attempt_count`
+|count of the number of times a TCP connect attempt failed on the client side.  This is useful for identifying when clients are having issues, even if you can't see corresponding server-side failures.
+
+|TCP stack
+|CMON (in Triton)
+|`tcp_retransmitted_segment_count`
+|count of the number of times a TCP packet was retransmitted.  This can indicate a network problem or a software problem on either end of the TCP connection, but interpreting this stat is difficult because there are many non-failure cases where packets may be retransmitted.
+
+|OS
+|CMON (in Triton)
+|`mem_anon_alloc_fail`
+|count of the number of times an operating system process attempted to allocate memory but failed because the container would exceeds its cap.  This often indicates a type of memory exhaustion.
+
+|OS
+|CMON (in Triton)
+|`cpu_user_usage`, `cpu_sys_usage`
+|count of the number nanoseconds of CPU time (user and system time, respectively) used by a given container, with labels for the container being measured.  This is useful for understanding CPU usage, including problems of CPU saturation.
+
+|ZFS (filesystem)
+|CMON (in Triton)
+|`zfs_used`, `zfs_available`
+|gauge of the number of bytes used and available, with labels for the container being measured.  This is useful for identifying containers that are low on disk space and for understanding overall system storage capacity.
+
+|===
 
-// XXX add a section on Characterizing current behavior
-// XXX concurrency, latency, throughput, errors
-// XXX resource utilization, saturation, errors
 
 
 === Predicting autovacuum activity
diff --git a/docs/index.html b/docs/index.html
index 4e3d74a..fab45fd 100644
--- a/docs/index.html
+++ b/docs/index.html
@@ -540,6 +540,11 @@ body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-b
 <li><a href="#_deployment_specific_details">Deployment-specific details</a></li>
 <li><a href="#_metrics">Metrics</a>
 <ul class="sectlevel2">
+<li><a href="#_key_service_metrics">Key service metrics</a></li>
+<li><a href="#_top_level_manta_metrics">Top-level Manta metrics</a></li>
+<li><a href="#_key_metrics_internal_to_manta">Key metrics internal to Manta</a></li>
+<li><a href="#_other_operational_metrics">Other operational metrics</a></li>
+<li><a href="#_summary_of_metrics">Summary of metrics</a></li>
 <li><a href="#_predicting_autovacuum_activity">Predicting autovacuum activity</a>
 <ul class="sectlevel3">
 <li><a href="#_background_on_vacuum_in_manta">Background on vacuum in Manta</a></li>
@@ -979,6 +984,33 @@ connection, both sides will generally report an explicit socket error.</p>
 </li>
 </ul>
 </div>
+<div class="paragraph">
+<p>Errors that manifest as explicit HTTP responses (i.e., 400-level and 500-level
+responses) are visible to operators via logs and metrics provided by Manta.</p>
+</div>
+<div class="paragraph">
+<p>There are less common failures as well:</p>
+</div>
+<div class="ulist">
+<ul>
+<li>
+<p>A client may find no servers in DNS.  This may or may not be visible to
+operators.</p>
+</li>
+<li>
+<p>A client may time out attempting to resolve DNS.  This is unlikely to be
+visible to Manta operators.</p>
+</li>
+<li>
+<p>A client may time out attempting to establish a TCP connection to Manta.  This
+is not likely to be visible to Manta operators.</p>
+</li>
+</ul>
+</div>
+<div class="paragraph">
+<p>The rest of this section discusses specific response codes for errors and what
+they mean.</p>
+</div>
 <div class="sect3">
 <h4 id="_response_code_507">Response code 507</h4>
 <div class="paragraph">
@@ -4495,21 +4527,452 @@ document or distributed to responders.</p>
 <div class="sectionbody">
 <div class="paragraph">
 <p>Real-time metrics provided by Manta form the basis of situational awareness,
-particularly during incident response.  Metrics are available for most data path
-components.  Understanding these metrics requires a basic understanding of these
-components and how they work together.  For more on these components, see
-<a href="http://joyent.github.io/manta/#components-of-manta">Components of Manta</a> in the
-Operator Guide.</p>
+particularly during incident response.  Understanding the metrics provided by
+the various components internal to Manta requires
+<a href="http://joyent.github.io/manta/#components-of-manta">a basic understanding of these
+components</a> and how they work together.</p>
+</div>
+<div class="paragraph">
+<p>This section discusses a number of useful metrics exposed by Manta.  When we say
+a metric is exposed by Manta, we usually mean that Triton (via CMON) and/or
+individual Manta components collect these metrics and serve them over HTTP in
+Prometheus format.  Within a Manta deployment, it&#8217;s up to operators to set up
+systems for collecting, presenting, and alerting on these metrics.  For more on
+this, see <a href="#_deployment_specific_details">Deployment-specific details</a>.  There are screenshots from existing
+deployments below, but the specific metrics available and the appearance of
+graphs may vary in your deployment.</p>
+</div>
+<div class="admonitionblock note">
+<table>
+<tr>
+<td class="icon">
+<div class="title">Note</div>
+</td>
+<td class="content">
+This section covers <em>operational</em> metrics used to understand Manta&#8217;s
+runtime behavior.  These are not end-user-visible metrics.
+</td>
+</tr>
+</table>
+</div>
+<div class="admonitionblock note">
+<table>
+<tr>
+<td class="icon">
+<div class="title">Note</div>
+</td>
+<td class="content">
+This section covers what information is available and how to interpret it.
+Unfortunately, there is no single place today that documents the list of metrics
+available.
+</td>
+</tr>
+</table>
+</div>
+<div class="sect2">
+<h3 id="_key_service_metrics">Key service metrics</h3>
+<div class="paragraph">
+<p>Key metrics for assessing the health of a service are driven by whatever the
+customer cares about.  For Manta, that&#8217;s typically:</p>
+</div>
+<div class="ulist">
+<ul>
+<li>
+<p>the <strong>error rate</strong></p>
+</li>
+<li>
+<p><strong>throughput</strong> of data in and out of Manta</p>
+</li>
+<li>
+<p><strong>latency</strong></p>
+</li>
+</ul>
+</div>
+<div class="paragraph">
+<p>These are closely related to the
+<a href="https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/#xref_monitoring_golden-signals">the
+Four Golden Signals</a> described in the <em>Site Reliability Engineering</em> book from
+Google.  That section provides a good summary of these metrics and why they&#8217;re
+so important.</p>
+</div>
+<div class="paragraph">
+<p>A Manta deployment typically seeks to achieve a particular level of performance,
+usually expressed in terms of throughput (objects per second read or written
+<em>or</em> bytes per second read or written) or latency (the time required to complete
+each request) while maintaining an acceptable error rate.  Errors must be
+considered when looking at performance, since many types of error responses can
+be served very quickly&#8201;&#8212;&#8201;that doesn&#8217;t mean the service is working as expected!</p>
+</div>
+<div class="admonitionblock note">
+<table>
+<tr>
+<td class="icon">
+<div class="title">Note</div>
+</td>
+<td class="content">
+We use the term <strong>error rate</strong> to refer to the fraction of requests that
+failed with an error (e.g., "3% of responses were 500-level errors" would be a
+3% error rate).  The same term is sometimes used to refer to the count of errors
+per unit time (e.g., "3 errors per second"), but that&#8217;s usually less useful for
+describing the service&#8217;s health.
+</td>
+</tr>
+</table>
+</div>
+<div class="paragraph">
+<p>Since there may be thousands of requests completed per second at any given time,
+when understanding latency, we almost always use some aggregated form of
+latency:</p>
+</div>
+<div class="ulist">
+<ul>
+<li>
+<p><strong>Average latency</strong> is useful for use-cases that primarily care about
+throughput because the average latency is closely related to throughput:
+throughput (in requests per second) equals the total number of clients divided
+by the average latency per request.  When all you know is that throughput has
+gone down, it&#8217;s hard to tell whether there&#8217;s a problem because it could
+reflect a server issue or a lack of client activity.  Average latency resolves
+this ambiguity: if average latency has gone up, there&#8217;s likely a server
+problem.  If not, clients have likely stopped trying to do as much work.</p>
+</li>
+<li>
+<p><strong>Tail latency</strong> refers to the latency of the the slowest requests (the <em>tail</em>
+of the latency distribution).  Tail latency is often a better indicator of
+general service health than average latency because small or occasional
+problems often affect tail latency significantly even when they don&#8217;t affect
+the average that much.  Tail latency is often expressed as a percentile: for
+example, the shorthand "p90" refers to the 90th percentile, which is the
+minimum latency of the slowest 10% of requests.  Similarly, p99 is the minimum
+latency of the slowest 1% of requests; and so on.  When we say that the p99 is
+300ms, we mean that 99% of requests completed within 300ms.</p>
+</li>
+</ul>
+</div>
+<div class="paragraph">
+<p>Not only do we use these metrics (error rate, throughput, and latency) to
+describe Manta&#8217;s overall health, but we also use them to describe the health of
+the various components within Manta.</p>
+</div>
+</div>
+<div class="sect2">
+<h3 id="_top_level_manta_metrics">Top-level Manta metrics</h3>
+<div class="paragraph">
+<p>To understand Manta&#8217;s overall health, we typically look at the error rate,
+throughput, and latency for Muskie (webapi) since Muskie most directly handles
+the HTTP requests being issued by end users.</p>
+</div>
+<div class="admonitionblock note">
+<table>
+<tr>
+<td class="icon">
+<div class="title">Note</div>
+</td>
+<td class="content">
+<strong>If you&#8217;re looking for help understanding a Manta problem, see the
+<a href="#_incident_response_decision_tree">Incident Response Decision Tree</a>.</strong>  This section provides background on the
+metrics referenced in that section.
+</td>
+</tr>
+</table>
+</div>
+<div class="paragraph">
+<p><strong>Errors at webapi:</strong> For background on errors in Manta, see
+<a href="#_investigating_an_increase_in_error_rate">Investigating an increase in error rate</a>.  That section also discusses how
+to debug specific errors.  At this level, we&#8217;re typically talking about explicit
+500-level errors.  When evaluating an error rate against expectations, we
+usually use its inverse&#8201;&#8212;&#8201;the <em>success rate</em>: the fraction of requests that
+completed successfully.  This is often measured in
+<a href="https://en.wikipedia.org/wiki/High_availability#Percentage_calculation">"nines"</a>.
+Generally, if a deployment seeks a particular level of availability (e.g.,
+99.9%), an incident may be raised if the error rate exceeds a target percentage
+(e.g., 0.1%) for some period of time (e.g., 5 minutes); an incident may be
+considered resolved once the error rate is below this threshold.  The specific
+thresholds vary by deployment, but are usually at least 99%.  Generally, errors
+indicate a software failure, a significant server-side failure, or unexpected
+overload&#8201;&#8212;&#8201;all of which are not supposed to be common in production
+deployments.</p>
+</div>
+<div class="paragraph">
+<p><strong>Throughput at webapi:</strong> Manta throughput can be measured in</p>
+</div>
+<div class="ulist">
+<ul>
+<li>
+<p>objects created per second</p>
+</li>
+<li>
+<p>objects fetched per second</p>
+</li>
+<li>
+<p>bytes written per second</p>
+</li>
+<li>
+<p>bytes read per second</p>
+</li>
+</ul>
+</div>
+<div class="paragraph">
+<p>Any of these might be significant to end users.  The target throughput in any of
+these dimensions is deployment-specific.  As mentioned above, it&#8217;s very
+difficult to use throughput directly to assess the server&#8217;s behavior because
+it&#8217;s significantly affected by client behavior.  Average latency may be more
+useful.</p>
+</div>
+<div class="paragraph">
+<p><strong>Latency at webapi:</strong> For webapi, we usually define latency as the
+time-to-first-byte for each request.  For uploads, this is the time between when
+the request is received at the server to the time when the server tells the
+client to proceed with the upload.  For downloads, this is the time between when
+the request is received and client receives the first bytes of data.  For other
+requests (which do not involve object data transfer), we look at the whole
+latency of the request.  As we&#8217;ve defined it, latency includes the time used by
+the server to parse the request, validate it, authenticate it, authorize it,
+load metadata, and initiate any data transfer, but it does <em>not</em> include the
+time to transfer data.  This is useful, since transfer time depends on object
+size, and we usually want to factor that out.</p>
+</div>
+<div class="paragraph">
+<p>For general system health, we typically monitor Muskie error rate and tail
+latency (p90, p99).  When throughput is important, average latency is also
+useful for the reasons mentioned above.</p>
+</div>
+</div>
+<div class="sect2">
+<h3 id="_key_metrics_internal_to_manta">Key metrics internal to Manta</h3>
+<div class="paragraph">
+<p>When understanding problems with Manta, we use the same key metrics&#8201;&#8212;&#8201;error
+rate, throughput, and latency&#8201;&#8212;&#8201;measured by various other components.</p>
+</div>
+<div class="admonitionblock note">
+<table>
+<tr>
+<td class="icon">
+<div class="title">Note</div>
+</td>
+<td class="content">
+<strong>If you&#8217;re looking for help understanding a Manta problem, see the
+<a href="#_incident_response_decision_tree">Incident Response Decision Tree</a>.</strong>  This section provides background on the
+metrics referenced in that section.
+</td>
+</tr>
+</table>
+</div>
+<div class="paragraph">
+<p><strong>Metadata tier:</strong> We generally consider the metadata tier to include:</p>
+</div>
+<div class="ulist">
+<ul>
+<li>
+<p>Electric-Moray, which is the gateway for nearly all requests to the metadata
+tier that come from the data path</p>
+</li>
+<li>
+<p>Moray, each instance of which handles requests for a particular database shard</p>
+</li>
+<li>
+<p>PostgreSQL (deployed under Manatee), which ultimately services requests to the
+metadata tier</p>
+</li>
+</ul>
+</div>
+<div class="paragraph">
+<p>Electric-Moray and Moray both operate in terms of RPCs.  They both expose
+metrics that count the number of RPCs completed and failed, as well as
+histograms that can be used to calculate average latency and estimate tail
+latency.</p>
 </div>
 <div class="paragraph">
-<p>Manta components expose metrics, but within any given Manta deployment, it&#8217;s up
-to operators to set up systems for collecting, presenting, and alerting on
-metrics.  For more, see <a href="#_deployment_specific_details">Deployment-specific details</a>.</p>
+<p>PostgreSQL operates in terms of transactions.  Manta exposes metrics collected
+from PostgreSQL about transactions completed and aborted, but not latency.  We
+typically use Moray latency as a proxy for PostgreSQL latency.</p>
 </div>
 <div class="paragraph">
-<p>This section uses screenshots from existing deployments to discuss metrics
-provided by Manta and how to interpret them.  The specific metrics available and
-the appearance of graphs may vary in your deployment.</p>
+<p><strong>Storage tier:</strong> We do not currently record request throughput, latency, or
+error rate from the storage tier.  However, Triton (via CMON) collects network
+bytes transferred and ZFS bytes read and written, which are useful proxies for
+inbound and outbound data transfer.</p>
+</div>
+</div>
+<div class="sect2">
+<h3 id="_other_operational_metrics">Other operational metrics</h3>
+<div class="paragraph">
+<p>Manta exposes a number of other useful metrics:</p>
+</div>
+<div class="ulist">
+<ul>
+<li>
+<p>CPU utilization, broken out by zone (and filterable by type of component).
+For stateless services (i.e., most services <em>within</em> Manta), this is a useful
+way to determine if instances (or a whole service) is overloaded.  For
+example, webapi instances are typically deployed using 16 processes that are
+effectively single-threaded.  If any webapi instances are using close to 1600%
+of one CPU (i.e., 16 CPUs), they&#8217;re likely overloaded, and end users are
+likely to experience elevated latency as a consequence.  In order to interpret
+these values, you generally have to know how many CPUs a particular component
+can typically use.</p>
+</li>
+<li>
+<p>Disk utilization, broken out by zone (and filterable by type of component).
+This is useful for understanding disk capacity at both the metadata tier and
+storage tier.</p>
+</li>
+<li>
+<p>PostgreSQL active connections, broken out by shard.  This roughly reflects how
+much concurrent work is happening on each PostgreSQL shard.  This can be
+useful for identifying busy or slow shards (though it can be hard to tell if a
+shard is slow because it&#8217;s busy or if it&#8217;s busy because it&#8217;s slow).</p>
+</li>
+<li>
+<p>PostgreSQL vacuum activity, <a href="#_predicting_autovacuum_activity">described
+below</a>.</p>
+</li>
+<li>
+<p>TCP errors (collected via Triton&#8217;s CMON), including failed connection
+attempts, listen drops, and retransmitted packets.  These can reflect various
+types of network issues.</p>
+</li>
+<li>
+<p>OS anonymous allocation failures (collected via Triton&#8217;s CMON).  This
+particular event indicates that a process attempted to allocated memory but
+failed because it has reached a memory cap.  Many programs do not handle
+running out of memory well, so these allocation failures can sometimes result
+in cascading failures.</p>
+</li>
+</ul>
+</div>
+</div>
+<div class="sect2">
+<h3 id="_summary_of_metrics">Summary of metrics</h3>
+<div class="paragraph">
+<p>Below is a rough summary of the metrics exposed by Manta and which components
+expose them.  There are several caveats:</p>
+</div>
+<div class="ulist">
+<ul>
+<li>
+<p><strong>This information is subject to change without notice as the underlying
+software evolves!</strong></p>
+</li>
+<li>
+<p>This table does not describe which Prometheus instances collect, aggregate, and serve each metric.  See <a href="#_deployment_specific_details">Deployment-specific details</a>.</p>
+</li>
+<li>
+<p>Relatedly, in large deployments, Prometheus <strong>recording rules</strong> may be used to precalculate important metrics.  These are not documented here.</p>
+</li>
+<li>
+<p>Many metrics provided a number of breakdowns using Prometheus labels.</p>
+</li>
+</ul>
+</div>
+<table class="tableblock frame-all grid-all stretch">
+<colgroup>
+<col style="width: 25%;">
+<col style="width: 25%;">
+<col style="width: 25%;">
+<col style="width: 25%;">
+</colgroup>
+<thead>
+<tr>
+<th class="tableblock halign-left valign-top">Component being measured</th>
+<th class="tableblock halign-left valign-top">Where the metric is collected</th>
+<th class="tableblock halign-left valign-top">Metric name</th>
+<th class="tableblock halign-left valign-top">Notes</th>
+</tr>
+</thead>
+<tbody>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Manta itself</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">webapi</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock"><code>muskie_inbound_streamed_bytes</code></p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">count of bytes uploaded to Muskie (all uploads, including in-progress and failures).  This is a primary metric for end users.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Manta itself</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">webapi</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock"><code>muskie_outbound_streamed_bytes</code></p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">count of bytes downloaded from Muskie (all downloads, including in-progress and failures).  This is a primary metric for end users.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Manta itself</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">webapi</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock"><code>http_requests_completed</code></p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">count of requests completed, with labels for individual HTTP response codes.  This can be used to calculate the error rate as well.  This is the basis for several primary metrics for end users.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Manta itself</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">webapi</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock"><code>http_request_latency_ms</code></p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">histogram of request latency, used to calculate average latency and to estimate percentiles.  This is a primary metric for end users.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Electric-Moray</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">electric-moray</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock"><code>fast_requests_completed</code></p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">count of requests completed, with a label for RPC method name.  This is useful for measuring overall throughput at the metadata tier.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Electric-Moray</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">electric-moray</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock"><code>fast_server_request_time_seconds</code></p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">histogram of RPC latency, with a label for RPC method name.  This is useful for calculating average latency and estimating tail latency at the metadata tier.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Moray</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">moray</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock"><code>fast_requests_completed</code></p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Same as for electric-moray, but this is measured for a particular Moray instance (and so a particular database shard).</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Moray</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">moray</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock"><code>fast_server_request_time_seconds</code></p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Same as for electric-moray, but this is measured for a particular Moray instance (and so a particular database shard).</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">PostgreSQL</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">pgstatsmon</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">Various</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">A number of <a href="https://www.postgresql.org/docs/9.6/monitoring-stats.html">stats exposed by PostgreSQL</a> are collected and exposed by pgstatsmon.  For the authoritative set, see <a href="https://github.com/joyent/pgstatsmon/blob/master/lib/queries.js">the pgstatsmon source</a>.  These stats are named according to their PostgreSQL names, so for example the <code>xact_commit</code> stat in the <code>pg_stat_database</code> view is exposed as <code>pg_stat_database_xact_commit</code>.  Labels are used to identify the PostgreSQL instance, which can often be used to break out by shard.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">TCP stack</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">CMON (in Triton)</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock"><code>tcp_listen_drop_count</code>, <code>tcp_listen_drop_Qzero_count</code></p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">count of the number of times a TCP connect attempt was dropped on the server side, often due to overload.  This is useful for identifying TCP server problems.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">TCP stack</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">CMON (in Triton)</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock"><code>tcp_failed_connection_attempt_count</code></p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">count of the number of times a TCP connect attempt failed on the client side.  This is useful for identifying when clients are having issues, even if you can&#8217;t see corresponding server-side failures.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">TCP stack</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">CMON (in Triton)</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock"><code>tcp_retransmitted_segment_count</code></p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">count of the number of times a TCP packet was retransmitted.  This can indicate a network problem or a software problem on either end of the TCP connection, but interpreting this stat is difficult because there are many non-failure cases where packets may be retransmitted.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">OS</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">CMON (in Triton)</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock"><code>mem_anon_alloc_fail</code></p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">count of the number of times an operating system process attempted to allocate memory but failed because the container would exceeds its cap.  This often indicates a type of memory exhaustion.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">OS</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">CMON (in Triton)</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock"><code>cpu_user_usage</code>, <code>cpu_sys_usage</code></p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">count of the number nanoseconds of CPU time (user and system time, respectively) used by a given container, with labels for the container being measured.  This is useful for understanding CPU usage, including problems of CPU saturation.</p></td>
+</tr>
+<tr>
+<td class="tableblock halign-left valign-top"><p class="tableblock">ZFS (filesystem)</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">CMON (in Triton)</p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock"><code>zfs_used</code>, <code>zfs_available</code></p></td>
+<td class="tableblock halign-left valign-top"><p class="tableblock">gauge of the number of bytes used and available, with labels for the container being measured.  This is useful for identifying containers that are low on disk space and for understanding overall system storage capacity.</p></td>
+</tr>
+</tbody>
+</table>
 </div>
 <div class="sect2">
 <h3 id="_predicting_autovacuum_activity">Predicting autovacuum activity</h3>
-- 
2.21.0

