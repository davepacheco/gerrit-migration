commit 5546a1a2b2790a6a9e58c69891727812c98b14b9 (refs/changes/27/2927/3)
Author: Tim Kordas <tim.kordas@joyent.com>
Date:   2017-11-09T09:26:15-08:00 (1 year, 11 months ago)
    
    MANATEE-387 JPC manatee pg_dump.sh duration impedes month-end billing cycle

diff --git a/pg_dump/pg_backup_common.sh b/pg_dump/pg_backup_common.sh
index c329a00..877fb8f 100644
--- a/pg_dump/pg_backup_common.sh
+++ b/pg_dump/pg_backup_common.sh
@@ -111,7 +111,18 @@ function mount_data_set
     # remove postmaster.pid
     rm -f $PG_DIR/postmaster.pid
 
-    ctrun -o noorphan sudo -u postgres postgres -D $PG_DIR -p 23456 -c logging_collector=off &
+    # Port over the performance startup options from manta-manatee
+    # Versions of PG after 9.5 removed the checkpoint_segments parameter
+    # if we're running on 9.2, we'll tune it, otherwise leave it alone.
+    PG_STARTUP_OPTIONS="-c logging_collector=off -c fsync=off \
+        -c synchronous_commit=off -c checkpoint_timeout=1h"
+    PG_SERVER_VERSION=$(postgres --version | cut -d' ' -f3)
+    if [[ $PG_SERVER_VERSION == 9.2* ]]; then
+        PG_STARTUP_OPTIONS+=" -c checkpoint_segments=100"
+    fi
+
+    ctrun -o noorphan sudo -u postgres postgres -D $PG_DIR -p 23456 \
+         $PG_STARTUP_OPTIONS &
     PG_PID=$!
     [[ $? -eq 0 ]] || fatal "unable to start postgres"
 
@@ -154,21 +165,31 @@ function backup ()
         # trim the first 3 lines of the schema dump
         sudo -u postgres psql -p 23456 moray -c '\dt' | sed -e '1,3d' > $schema
         [[ $? -eq 0 ]] || (rm $schema; fatal "unable to read db schema")
-        for i in `sed 'N;$!P;$!D;$d' $schema | tr -d ' '| cut -d '|' -f2 | grep -v ^napi_ips_`
+        # We walk the schema in reverse order because there happen to
+        # be some tables with names which happen to collate late in
+        # the \dt output which we'd like to finish early in the
+        # backup.
+        for i in `sed 'N;$!P;$!D;$d' $schema | tr -d ' '| cut -d '|' -f2 | grep -v ^napi_ips_ | sort -r`
         do
             local time=$(date -u +%F-%H-%M-%S)
             local dump_file=$DUMP_DIR/$date'_'$i-$time.gz
             sudo -u postgres pg_dump -p 23456 moray -a -t $i | gsed 's/\\\\/\\/g' | sqlToJson.js | gzip -1 > $dump_file
             [[ $? -eq 0 ]] || fatal "Unable to dump table $i"
+            # move each dump to the archive location as we complete it.
+            move_pg_dumps
         done
         #
-        # If we have napi_ips_* tables, dump them to separate files based on the
-        # first two hex characters of their uuid, since there will be many
-        # thousands of these tables and otherwise pg_dump runs postgres out of
-        # shared memory. (See NET-307)
+        # If we have napi_ips_* tables, dump them to separate files
+        # based on the first two hex characters of their uuid, since
+        # there will be many thousands of these tables and otherwise
+        # pg_dump runs postgres out of shared memory. (See NET-307)
         #
-        # We don't run these through sqlToJson.js because the table information
-        # is critical to restoring here.
+        # We don't run these through sqlToJson.js because the table
+        # information is critical to restoring here.
+        #
+        # Note: We don't reverse the order for napi_ips_* as we do the
+        # non-napi tables (that is, we dump them in collated order,
+        # not reverse collated order).
         #
         if [[ -n $(sed 'N;$!P;$!D;$d' $schema | tr -d ' '| cut -d '|' -f2 | grep ^napi_ips_ | head -1) ]]; then
             for idx in $(seq 0 255); do
@@ -181,7 +202,10 @@ function backup ()
                     [[ $? -eq 0 ]] || fatal "Unable to dump napi_ips_${prefix}* tables"
                 fi
             done
+            # move each dump to the archive location as we complete it.
+            move_pg_dumps
         fi
+
         rm $schema
         [[ $? -eq 0 ]] || fatal "unable to remove schema"
     fi
diff --git a/pg_dump/pg_dump.sh b/pg_dump/pg_dump.sh
index 0f063ac..b888050 100755
--- a/pg_dump/pg_dump.sh
+++ b/pg_dump/pg_dump.sh
@@ -52,9 +52,11 @@ if [[ $? = '1' ]]; then
     take_zfs_snapshot
     check_lock
     mount_data_set
+
+    # Note: backup() will move each dump file into the
+    # archive location automatically.
     backup 'JSON'
-    move_pg_dumps
-else
+    else
     echo "not performing backup, not lowest peer in shard"
     exit 0
 fi
