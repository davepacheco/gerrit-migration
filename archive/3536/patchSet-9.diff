commit 62a7afcb02af1e1ef75863208e5cb29bca65415e (refs/changes/36/3536/9)
Author: John Levon <john.levon@joyent.com>
Date:   2018-03-29T21:58:41+00:00 (1 year, 6 months ago)
    
    OS-6606 want memory arena for vmm applications
    OS-6835 memory DR should be disabled

diff --git a/usr/src/cmd/mdb/common/modules/genunix/memory.c b/usr/src/cmd/mdb/common/modules/genunix/memory.c
index fa4918b9b8..f127adb850 100644
--- a/usr/src/cmd/mdb/common/modules/genunix/memory.c
+++ b/usr/src/cmd/mdb/common/modules/genunix/memory.c
@@ -20,7 +20,7 @@
  */
 /*
  * Copyright (c) 2001, 2010, Oracle and/or its affiliates. All rights reserved.
- * Copyright 2017 Joyent, Inc.
+ * Copyright 2018 Joyent, Inc.
  */
 
 #include <mdb/mdb_param.h>
@@ -443,11 +443,11 @@ vn_get(vn_htable_t *hp, struct vnode *vp, uintptr_t ptr)
 
 /* Summary statistics of pages */
 typedef struct memstat {
-	struct vnode    *ms_kvp;	/* Cached address of kernel vnode */
 	struct vnode    *ms_unused_vp;	/* Unused pages vnode pointer	  */
-	struct vnode    *ms_zvp;	/* Cached address of zio vnode    */
+	struct vnode    *ms_kvps;	/* Cached address of vnode array  */
 	uint64_t	ms_kmem;	/* Pages of kernel memory	  */
 	uint64_t	ms_zfs_data;	/* Pages of zfs data		  */
+	uint64_t	ms_vmm_mem;	/* Pages of VMM mem		  */
 	uint64_t	ms_anon;	/* Pages of anonymous memory	  */
 	uint64_t	ms_vnode;	/* Pages of named (vnode) memory  */
 	uint64_t	ms_exec;	/* Pages of exec/library memory	  */
@@ -458,11 +458,8 @@ typedef struct memstat {
 	struct vnode	ms_vn;		/* vnode buffer			  */
 } memstat_t;
 
-#define	MS_PP_ISKAS(pp, stats)				\
-	((pp)->p_vnode == (stats)->ms_kvp)
-
-#define	MS_PP_ISZFS_DATA(pp, stats)			\
-	(((stats)->ms_zvp != NULL) && ((pp)->p_vnode == (stats)->ms_zvp))
+#define	MS_PP_ISTYPE(pp, stats, index) \
+	((pp)->p_vnode == &(stats->ms_kvps[index]))
 
 /*
  * Summarize pages by type and update stat information
@@ -478,10 +475,12 @@ memstat_callback(page_t *page, page_t *pp, memstat_t *stats)
 		stats->ms_bootpages++;
 	else if (pp->p_vnode == NULL || pp->p_vnode == stats->ms_unused_vp)
 		return (WALK_NEXT);
-	else if (MS_PP_ISKAS(pp, stats))
+	else if (MS_PP_ISTYPE(pp, stats, KV_KVP))
 		stats->ms_kmem++;
-	else if (MS_PP_ISZFS_DATA(pp, stats))
+	else if (MS_PP_ISTYPE(pp, stats, KV_ZVP))
 		stats->ms_zfs_data++;
+	else if (MS_PP_ISTYPE(pp, stats, KV_VVP))
+		stats->ms_vmm_mem++;
 	else if (PP_ISFREE(pp))
 		stats->ms_cachelist++;
 	else if (vn_get(stats->ms_vn_htable, vp, (uintptr_t)pp->p_vnode))
@@ -507,7 +506,6 @@ memstat(uintptr_t addr, uint_t flags, int argc, const mdb_arg_t *argv)
 	memstat_t stats;
 	GElf_Sym sym;
 	vn_htable_t ht;
-	struct vnode *kvps;
 	uintptr_t vn_size = 0;
 #if defined(__i386) || defined(__amd64)
 	bln_stats_t bln_stats;
@@ -548,16 +546,10 @@ memstat(uintptr_t addr, uint_t flags, int argc, const mdb_arg_t *argv)
 	/* read kernel vnode array pointer */
 	if (mdb_lookup_by_obj(MDB_OBJ_EXEC, "kvps",
 	    (GElf_Sym *)&sym) == -1) {
-		mdb_warn("unable to read kvps");
+		mdb_warn("unable to look up kvps");
 		return (DCMD_ERR);
 	}
-	kvps = (struct vnode *)(uintptr_t)sym.st_value;
-	stats.ms_kvp =  &kvps[KV_KVP];
-
-	/*
-	 * Read the zio vnode pointer.
-	 */
-	stats.ms_zvp = &kvps[KV_ZVP];
+	stats.ms_kvps = (struct vnode *)(uintptr_t)sym.st_value;
 
 	/*
 	 * If physmem != total_pages, then the administrator has limited the
@@ -605,6 +597,13 @@ memstat(uintptr_t addr, uint_t flags, int argc, const mdb_arg_t *argv)
 		    MS_PCT_TOTAL(stats.ms_zfs_data));
 	}
 
+	if (stats.ms_vmm_mem != 0) {
+		mdb_printf("VMM Memory       %16llu  %16llu  %3lu%%\n",
+		    stats.ms_vmm_mem,
+		    (uint64_t)stats.ms_vmm_mem * PAGESIZE / (1024 * 1024),
+		    MS_PCT_TOTAL(stats.ms_vmm_mem));
+	}
+
 	mdb_printf("Anon             %16llu  %16llu  %3lu%%\n",
 	    stats.ms_anon,
 	    (uint64_t)stats.ms_anon * PAGESIZE / (1024 * 1024),
diff --git a/usr/src/uts/common/sys/vnode.h b/usr/src/uts/common/sys/vnode.h
index 8dfd36353b..b527558895 100644
--- a/usr/src/uts/common/sys/vnode.h
+++ b/usr/src/uts/common/sys/vnode.h
@@ -21,7 +21,7 @@
 
 /*
  * Copyright (c) 1988, 2010, Oracle and/or its affiliates. All rights reserved.
- * Copyright (c) 2017, Joyent, Inc.
+ * Copyright (c) 2018, Joyent, Inc.
  * Copyright (c) 2011, 2017 by Delphix. All rights reserved.
  * Copyright 2017 RackTop Systems.
  */
@@ -1486,6 +1486,7 @@ extern struct vnode kvps[];
 typedef enum {
 	KV_KVP,		/* vnode for all segkmem pages */
 	KV_ZVP,		/* vnode for all ZFS pages */
+	KV_VVP,		/* vnode for all VMM pages */
 #if defined(__sparc)
 	KV_MPVP,	/* vnode for all page_t meta-pages */
 	KV_PROMVP,	/* vnode for all PROM pages */
diff --git a/usr/src/uts/common/vm/page_lock.c b/usr/src/uts/common/vm/page_lock.c
index 7e48602189..7305c9c85a 100644
--- a/usr/src/uts/common/vm/page_lock.c
+++ b/usr/src/uts/common/vm/page_lock.c
@@ -20,6 +20,7 @@
  */
 /*
  * Copyright (c) 1991, 2010, Oracle and/or its affiliates. All rights reserved.
+ * Copyright 2018 Joyent, Inc.
  */
 
 
@@ -140,9 +141,8 @@ static pad_mutex_t	pszc_mutex[PSZC_MTX_TABLE_SIZE];
 	    & (VPH_TABLE_SIZE - 1))
 
 /*
- * Two slots after VPH_TABLE_SIZE are reserved in vph_mutex for kernel vnodes.
- * The lock for kvp is VPH_TABLE_SIZE + 0, and the lock for zvp is
- * VPH_TABLE_SIZE + 1.
+ * Two slots after VPH_TABLE_SIZE are reserved in vph_mutex for kernel vnodes,
+ * one for kvps[KV_ZVP], and one for other kvps[] users.
  */
 
 kmutex_t	vph_mutex[VPH_TABLE_SIZE + 2];
@@ -888,10 +888,10 @@ static int page_vnode_mutex_stress = 0;
 kmutex_t *
 page_vnode_mutex(vnode_t *vp)
 {
-	if (vp == &kvp)
+	if (vp == &kvp || vp == &kvps[KV_VVP])
 		return (&vph_mutex[VPH_TABLE_SIZE + 0]);
 
-	if (vp == &zvp)
+	if (vp == &kvps[KV_ZVP])
 		return (&vph_mutex[VPH_TABLE_SIZE + 1]);
 #ifdef DEBUG
 	if (page_vnode_mutex_stress != 0)
diff --git a/usr/src/uts/common/vm/page_retire.c b/usr/src/uts/common/vm/page_retire.c
index 76be970a45..f4e8d0737f 100644
--- a/usr/src/uts/common/vm/page_retire.c
+++ b/usr/src/uts/common/vm/page_retire.c
@@ -22,6 +22,7 @@
  * Copyright 2010 Sun Microsystems, Inc.  All rights reserved.
  * Use is subject to license terms.
  * Copyright (c) 2016 by Delphix. All rights reserved.
+ * Copyright 2018 Joyent, Inc.
  */
 
 /*
@@ -851,9 +852,8 @@ page_retire_incr_pend_count(void *datap)
 {
 	PR_INCR_KSTAT(pr_pending);
 
-	if ((datap == &kvp) || (datap == &zvp)) {
+	if (datap == &kvp || datap == &kvps[KV_ZVP] || datap == &kvps[KV_VVP])
 		PR_INCR_KSTAT(pr_pending_kas);
-	}
 }
 
 void
@@ -861,9 +861,8 @@ page_retire_decr_pend_count(void *datap)
 {
 	PR_DECR_KSTAT(pr_pending);
 
-	if ((datap == &kvp) || (datap == &zvp)) {
+	if (datap == &kvp || datap == &kvps[KV_ZVP] || datap == &kvps[KV_VVP])
 		PR_DECR_KSTAT(pr_pending_kas);
-	}
 }
 
 /*
diff --git a/usr/src/uts/common/vm/seg_kmem.c b/usr/src/uts/common/vm/seg_kmem.c
index 439c859d96..0b116d6eba 100644
--- a/usr/src/uts/common/vm/seg_kmem.c
+++ b/usr/src/uts/common/vm/seg_kmem.c
@@ -20,7 +20,7 @@
  */
 /*
  * Copyright (c) 1998, 2010, Oracle and/or its affiliates. All rights reserved.
- * Copyright 2016 Joyent, Inc.
+ * Copyright 2018 Joyent, Inc.
  */
 
 #include <sys/types.h>
@@ -122,6 +122,11 @@ vmem_t *static_alloc_arena;	/* arena for allocating static memory */
 vmem_t *zio_arena = NULL;	/* arena for allocating zio memory */
 vmem_t *zio_alloc_arena = NULL;	/* arena for allocating zio memory */
 
+#if defined(__amd64)
+vmem_t *kvmm_arena;		/* arena for vmm VA */
+struct seg kvmmseg;		/* Segment for vmm memory */
+#endif
+
 /*
  * seg_kmem driver can map part of the kernel heap with large pages.
  * Currently this functionality is implemented for sparc platforms only.
@@ -440,7 +445,7 @@ segkmem_badop()
 /*ARGSUSED*/
 static faultcode_t
 segkmem_fault(struct hat *hat, struct seg *seg, caddr_t addr, size_t size,
-	enum fault_type type, enum seg_rw rw)
+    enum fault_type type, enum seg_rw rw)
 {
 	pgcnt_t npages;
 	spgcnt_t pg;
@@ -655,13 +660,19 @@ segkmem_dump(struct seg *seg)
 		    segkmem_dump_range, seg->s_as);
 		vmem_walk(heaptext_arena, VMEM_ALLOC | VMEM_REENTRANT,
 		    segkmem_dump_range, seg->s_as);
+	/*
+	 * We don't want to dump pages attached to kzioseg since they
+	 * contain file data from ZFS.  If this page's segment is
+	 * kzioseg return instead of writing it to the dump device.
+	 *
+	 * Same applies to VM memory allocations.
+	 */
 	} else if (seg == &kzioseg) {
-		/*
-		 * We don't want to dump pages attached to kzioseg since they
-		 * contain file data from ZFS.  If this page's segment is
-		 * kzioseg return instead of writing it to the dump device.
-		 */
 		return;
+#if defined(__amd64)
+	} else if (seg == &kvmmseg) {
+		return;
+#endif
 	} else {
 		segkmem_dump_range(seg->s_as, seg->s_base, seg->s_size);
 	}
@@ -677,7 +688,7 @@ segkmem_dump(struct seg *seg)
 /*ARGSUSED*/
 static int
 segkmem_pagelock(struct seg *seg, caddr_t addr, size_t len,
-	page_t ***ppp, enum lock_type type, enum seg_rw rw)
+    page_t ***ppp, enum lock_type type, enum seg_rw rw)
 {
 	page_t **pplist, *pp;
 	pgcnt_t npages;
@@ -801,22 +812,19 @@ struct seg_ops segkmem_ops = {
 	seg_inherit_notsup		/* inherit */
 };
 
-int
-segkmem_zio_create(struct seg *seg)
-{
-	ASSERT(seg->s_as == &kas && RW_WRITE_HELD(&kas.a_lock));
-	seg->s_ops = &segkmem_ops;
-	seg->s_data = &zvp;
-	kas.a_size += seg->s_size;
-	return (0);
-}
-
 int
 segkmem_create(struct seg *seg)
 {
 	ASSERT(seg->s_as == &kas && RW_WRITE_HELD(&kas.a_lock));
 	seg->s_ops = &segkmem_ops;
-	seg->s_data = &kvp;
+	if (seg == &kzioseg)
+		seg->s_data = &kvps[KV_ZVP];
+#if defined(__amd64)
+	else if (seg == &kvmmseg)
+		seg->s_data = &kvps[KV_VVP];
+#endif
+	else
+		seg->s_data = &kvps[KV_KVP];
 	kas.a_size += seg->s_size;
 	return (0);
 }
@@ -858,7 +866,7 @@ segkmem_page_create(void *addr, size_t size, int vmflag, void *arg)
  */
 void *
 segkmem_xalloc(vmem_t *vmp, void *inaddr, size_t size, int vmflag, uint_t attr,
-	page_t *(*page_create_func)(void *, size_t, int, void *), void *pcarg)
+    page_t *(*page_create_func)(void *, size_t, int, void *), void *pcarg)
 {
 	page_t *ppl;
 	caddr_t addr = inaddr;
@@ -968,10 +976,10 @@ segkmem_alloc(vmem_t *vmp, size_t size, int vmflag)
 	return (segkmem_alloc_vn(vmp, size, vmflag, &kvp));
 }
 
-void *
+static void *
 segkmem_zio_alloc(vmem_t *vmp, size_t size, int vmflag)
 {
-	return (segkmem_alloc_vn(vmp, size, vmflag, &zvp));
+	return (segkmem_alloc_vn(vmp, size, vmflag, &kvps[KV_ZVP]));
 }
 
 /*
@@ -980,8 +988,8 @@ segkmem_zio_alloc(vmem_t *vmp, size_t size, int vmflag)
  * we currently don't have a special kernel segment for non-paged
  * kernel memory that is exported by drivers to user space.
  */
-static void
-segkmem_free_vn(vmem_t *vmp, void *inaddr, size_t size, struct vnode *vp,
+void
+segkmem_xfree(vmem_t *vmp, void *inaddr, size_t size, struct vnode *vp,
     void (*func)(page_t *))
 {
 	page_t *pp;
@@ -1037,22 +1045,16 @@ segkmem_free_vn(vmem_t *vmp, void *inaddr, size_t size, struct vnode *vp,
 
 }
 
-void
-segkmem_xfree(vmem_t *vmp, void *inaddr, size_t size, void (*func)(page_t *))
-{
-	segkmem_free_vn(vmp, inaddr, size, &kvp, func);
-}
-
 void
 segkmem_free(vmem_t *vmp, void *inaddr, size_t size)
 {
-	segkmem_free_vn(vmp, inaddr, size, &kvp, NULL);
+	segkmem_xfree(vmp, inaddr, size, &kvp, NULL);
 }
 
-void
+static void
 segkmem_zio_free(vmem_t *vmp, void *inaddr, size_t size)
 {
-	segkmem_free_vn(vmp, inaddr, size, &zvp, NULL);
+	segkmem_xfree(vmp, inaddr, size, &kvps[KV_ZVP], NULL);
 }
 
 void
@@ -1534,8 +1536,21 @@ segkmem_zio_init(void *zio_mem_base, size_t zio_mem_size)
 	ASSERT(zio_alloc_arena != NULL);
 }
 
-#ifdef __sparc
+#if defined(__amd64)
+
+void
+segkmem_kvmm_init(void *base, size_t size)
+{
+	ASSERT(base != NULL);
+	ASSERT(size != 0);
+
+	kvmm_arena = vmem_create("kvmm_arena", base, size, 1024 * 1024,
+	    NULL, NULL, NULL, 0, VM_SLEEP);
+
+	ASSERT(kvmm_arena != NULL);
+}
 
+#elif defined(__sparc)
 
 static void *
 segkmem_alloc_ppa(vmem_t *vmp, size_t size, int vmflag)
diff --git a/usr/src/uts/common/vm/seg_kmem.h b/usr/src/uts/common/vm/seg_kmem.h
index 1db85826b1..9a20101670 100644
--- a/usr/src/uts/common/vm/seg_kmem.h
+++ b/usr/src/uts/common/vm/seg_kmem.h
@@ -21,7 +21,7 @@
 /*
  * Copyright 2009 Sun Microsystems, Inc.  All rights reserved.
  * Use is subject to license terms.
- * Copyright 2016 Joyent, Inc.
+ * Copyright 2018 Joyent, Inc.
  * Copyright 2017 RackTop Systems.
  */
 
@@ -65,12 +65,18 @@ extern vmem_t *static_arena;	/* arena for caches to import static memory */
 extern vmem_t *static_alloc_arena;	/* arena for allocating static memory */
 extern vmem_t *zio_arena;	/* arena for zio caches */
 extern vmem_t *zio_alloc_arena;	/* arena for zio caches */
+
+#if defined(__amd64)
+extern struct seg kvmmseg;	/* Segment for vmm mappings */
+extern vmem_t *kvmm_arena;	/* arena for vmm VA */
+extern void segkmem_kvmm_init(void *, size_t);
+#endif
+
 extern struct vnode kvps[];
 /*
- * segkmem page vnodes
+ * segkmem page vnodes (please don't add more defines here...)
  */
 #define	kvp		(kvps[KV_KVP])
-#define	zvp		(kvps[KV_ZVP])
 #if defined(__sparc)
 #define	mpvp		(kvps[KV_MPVP])
 #define	promvp		(kvps[KV_PROMVP])
@@ -83,16 +89,14 @@ extern void *segkmem_xalloc(vmem_t *, void *, size_t, int, uint_t,
 extern void *segkmem_alloc(vmem_t *, size_t, int);
 extern void *segkmem_alloc_permanent(vmem_t *, size_t, int);
 extern void segkmem_free(vmem_t *, void *, size_t);
-extern void segkmem_xfree(vmem_t *, void *, size_t, void (*)(page_t *));
+extern void segkmem_xfree(vmem_t *, void *, size_t,
+    struct vnode *, void (*)(page_t *));
 
 extern void *boot_alloc(void *, size_t, uint_t);
 extern void boot_mapin(caddr_t addr, size_t size);
 extern void kernelheap_init(void *, void *, char *, void *, void *);
 extern void segkmem_gc(void);
 
-extern void *segkmem_zio_alloc(vmem_t *, size_t, int);
-extern int segkmem_zio_create(struct seg *);
-extern void segkmem_zio_free(vmem_t *, void *, size_t);
 extern void segkmem_zio_init(void *, size_t);
 
 /*
diff --git a/usr/src/uts/i86pc/dboot/dboot_startkern.c b/usr/src/uts/i86pc/dboot/dboot_startkern.c
index 2eef2eb97a..b9c4bc9051 100644
--- a/usr/src/uts/i86pc/dboot/dboot_startkern.c
+++ b/usr/src/uts/i86pc/dboot/dboot_startkern.c
@@ -23,7 +23,7 @@
  * Copyright 2009 Sun Microsystems, Inc.  All rights reserved.
  * Use is subject to license terms.
  *
- * Copyright 2013 Joyent, Inc.  All rights reserved.
+ * Copyright 2018 Joyent, Inc.  All rights reserved.
  */
 
 
@@ -2314,11 +2314,7 @@ startup_kernel(void)
 	 * Need correct target_kernel_text value
 	 */
 #if defined(_BOOT_TARGET_amd64)
-	target_kernel_text = KERNEL_TEXT_amd64;
-#elif defined(__xpv)
-	target_kernel_text = KERNEL_TEXT_i386_xpv;
-#else
-	target_kernel_text = KERNEL_TEXT_i386;
+	target_kernel_text = KERNEL_TEXT;
 #endif
 	DBG(target_kernel_text);
 
diff --git a/usr/src/uts/i86pc/io/vmm/vm/vm_glue.h b/usr/src/uts/i86pc/io/vmm/vm/vm_glue.h
index cf1808c799..cb6d4c358e 100644
--- a/usr/src/uts/i86pc/io/vmm/vm/vm_glue.h
+++ b/usr/src/uts/i86pc/io/vmm/vm/vm_glue.h
@@ -10,7 +10,7 @@
  */
 
 /*
- * Copyright 2017 Joyent, Inc.
+ * Copyright 2018 Joyent, Inc.
  */
 
 #ifndef	_VM_GLUE_
@@ -71,10 +71,10 @@ struct vm_page {
 /* Illumos-specific functions for setup and operation */
 int vm_segmap_obj(struct vmspace *, vm_object_t, struct as *, caddr_t *,
     uint_t, uint_t, uint_t);
-int vm_segmap_space(struct vmspace *, off_t , struct as *, caddr_t *, off_t,
+int vm_segmap_space(struct vmspace *, off_t, struct as *, caddr_t *, off_t,
     uint_t, uint_t, uint_t);
 void *vmspace_find_kva(struct vmspace *, uintptr_t, size_t);
 void vmm_arena_init(void);
-boolean_t vmm_arena_fini(void);
+void vmm_arena_fini(void);
 
 #endif /* _VM_GLUE_ */
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c b/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c
index b6f3e40e0a..7841735ef2 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c
+++ b/usr/src/uts/i86pc/io/vmm/vmm_sol_dev.c
@@ -1945,7 +1945,6 @@ vmm_attach(dev_info_t *dip, ddi_attach_cmd_t cmd)
 
 	ddi_report_dev(dip);
 
-	/* XXX: This needs updating */
 	vmm_arena_init();
 
 	vmmdev_load_failure = B_FALSE;
@@ -1986,19 +1985,14 @@ vmm_detach(dev_info_t *dip, ddi_detach_cmd_t cmd)
 		mutex_exit(&vmmdev_mtx);
 		return (DDI_FAILURE);
 	}
-
 	vmm_sdev_hdl = NULL;
 
-	/* XXX: This needs updating */
-	if (!vmm_arena_fini()) {
-		mutex_exit(&vmmdev_mtx);
-		return (DDI_FAILURE);
-	}
-
 	/* Remove the control node. */
 	ddi_remove_minor_node(dip, "ctl");
 	vmm_dip = NULL;
 	vmm_sol_glue_cleanup();
+	vmm_arena_fini();
+
 	mutex_exit(&vmmdev_mtx);
 
 	return (DDI_SUCCESS);
diff --git a/usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c b/usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c
index 1b074d2f92..d8a44ddf37 100644
--- a/usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c
+++ b/usr/src/uts/i86pc/io/vmm/vmm_sol_vm.c
@@ -10,7 +10,7 @@
  */
 
 /*
- * Copyright 2017 Joyent, Inc.
+ * Copyright 2018 Joyent, Inc.
  */
 
 #include <sys/param.h>
@@ -141,28 +141,36 @@ static void vm_mapping_remove(struct vmspace *, vmspace_mapping_t *);
 static kmutex_t eptable_map_lock;
 static struct eptable_map *eptable_map_head = NULL;
 
-static vmem_t	*vmm_arena = NULL;
+static vmem_t *vmm_alloc_arena = NULL;
 
+static void *
+vmm_arena_alloc(vmem_t *vmp, size_t size, int vmflag)
+{
+	return (segkmem_xalloc(vmp, NULL, size, vmflag, 0,
+	    segkmem_page_create, &kvps[KV_VVP]));
+}
+
+static void
+vmm_arena_free(vmem_t *vmp, void *inaddr, size_t size)
+{
+	segkmem_xfree(vmp, inaddr, size, &kvps[KV_VVP], NULL);
+}
 
 void
 vmm_arena_init(void)
 {
-	/*
-	 * XXXJOY: Hahaha, this is terrible, pls fix, prototype only
-	 */
-	vmm_arena = vmem_create("vmm_arena", NULL, 0, PAGESIZE,
-	    segkmem_zio_alloc, segkmem_zio_free, zio_arena, 0, VM_SLEEP);
+	vmm_alloc_arena = vmem_create("vmm_alloc_arena", NULL, 0, 1024 * 1024,
+	    vmm_arena_alloc, vmm_arena_free, kvmm_arena, 0, VM_SLEEP);
+
+	ASSERT(vmm_alloc_arena != NULL);
 }
 
-boolean_t
+void
 vmm_arena_fini(void)
 {
-	if (vmem_size(vmm_arena, VMEM_ALLOC) != 0) {
-		return (B_FALSE);
-	}
-	vmem_destroy(vmm_arena);
-	vmm_arena = NULL;
-	return (B_TRUE);
+	VERIFY(vmem_size(vmm_alloc_arena, VMEM_ALLOC) == 0);
+	vmem_destroy(vmm_alloc_arena);
+	vmm_alloc_arena = NULL;
 }
 
 struct vmspace *
@@ -941,7 +949,7 @@ vm_object_allocate(objtype_t type, vm_pindex_t psize)
 	switch (type) {
 	case OBJT_DEFAULT: {
 		/* XXXJOY: opt-in to larger pages? */
-		vmo->vmo_data = vmem_alloc(vmm_arena, size, KM_NOSLEEP);
+		vmo->vmo_data = vmem_alloc(vmm_alloc_arena, size, KM_NOSLEEP);
 		if (vmo->vmo_data == NULL) {
 			mutex_destroy(&vmo->vmo_lock);
 			kmem_free(vmo, sizeof (*vmo));
@@ -1001,7 +1009,7 @@ vm_object_deallocate(vm_object_t vmo)
 
 	switch (vmo->vmo_type) {
 	case OBJT_DEFAULT:
-		vmem_free(vmm_arena, vmo->vmo_data, vmo->vmo_size);
+		vmem_free(vmm_alloc_arena, vmo->vmo_data, vmo->vmo_size);
 		break;
 	case OBJT_SG:
 		sglist_free((struct sglist *)vmo->vmo_data);
diff --git a/usr/src/uts/i86pc/os/ddi_impl.c b/usr/src/uts/i86pc/os/ddi_impl.c
index 0143b20922..b8d7ec952f 100644
--- a/usr/src/uts/i86pc/os/ddi_impl.c
+++ b/usr/src/uts/i86pc/os/ddi_impl.c
@@ -24,6 +24,7 @@
  * Copyright 2012 Garrett D'Amore <garrett@damore.org>
  * Copyright 2014 Pluribus Networks, Inc.
  * Copyright 2016 Nexenta Systems, Inc.
+ * Copyright 2018 Joyent, Inc.
  */
 
 /*
@@ -1009,10 +1010,10 @@ page_create_io_wrapper(void *addr, size_t len, int vmflag, void *arg)
 
 #ifdef __xpv
 static void
-segkmem_free_io(vmem_t *vmp, void * ptr, size_t size)
+segkmem_free_io(vmem_t *vmp, void *ptr, size_t size)
 {
 	extern void page_destroy_io(page_t *);
-	segkmem_xfree(vmp, ptr, size, page_destroy_io);
+	segkmem_xfree(vmp, ptr, size, &kvp, page_destroy_io);
 }
 #endif
 
diff --git a/usr/src/uts/i86pc/os/startup.c b/usr/src/uts/i86pc/os/startup.c
index a3026f0eb4..f63973f092 100644
--- a/usr/src/uts/i86pc/os/startup.c
+++ b/usr/src/uts/i86pc/os/startup.c
@@ -319,22 +319,16 @@ static struct seg *segmap = &kmapseg;	/* easier to use name for in here */
 
 struct seg *segkp = &kpseg;	/* Pageable kernel virtual memory segment */
 
-#if defined(__amd64)
 struct seg kvseg_core;		/* Segment used for the core heap */
 struct seg kpmseg;		/* Segment used for physical mapping */
 struct seg *segkpm = &kpmseg;	/* 64bit kernel physical mapping segment */
-#else
-struct seg *segkpm = NULL;	/* Unused on IA32 */
-#endif
 
 caddr_t segkp_base;		/* Base address of segkp */
 caddr_t segzio_base;		/* Base address of segzio */
-#if defined(__amd64)
 pgcnt_t segkpsize = btop(SEGKPDEFSIZE);	/* size of segkp segment in pages */
-#else
-pgcnt_t segkpsize = 0;
-#endif
-pgcnt_t segziosize = 0;		/* size of zio segment in pages */
+caddr_t segkvmm_base;
+pgcnt_t segkvmmsize;
+pgcnt_t segziosize;
 
 /*
  * A static DR page_t VA map is reserved that can map the page structures
@@ -455,23 +449,32 @@ static pgcnt_t kphysm_init(page_t *, pgcnt_t);
  * 0xFFFFFFFF.C0000000  |-----------------------|- core_base / ekernelheap
  *			|	 Kernel		|
  *			|	  heap		|
+ *			|			|
+ *			|			|
  * 0xFFFFFXXX.XXX00000  |-----------------------|- kernelheap (floating)
  *			|	 segmap		|
  * 0xFFFFFXXX.XXX00000  |-----------------------|- segmap_start (floating)
  *			|    device mappings	|
  * 0xFFFFFXXX.XXX00000  |-----------------------|- toxic_addr (floating)
- *			|	  segzio	|
+ *			|	 segzio		|
  * 0xFFFFFXXX.XXX00000  |-----------------------|- segzio_base (floating)
- *			|	  segkp		|
- * ---                  |-----------------------|- segkp_base (floating)
+ *			|        segkvmm	|
+ *			|			|
+ *			|			|
+ *			|			|
+ * 0xFFFFFXXX.XXX00000  |-----------------------|- segkvmm_base (floating)
+ *			|	 segkp		|
+ * 			|-----------------------|- segkp_base (floating)
  *			|   page_t structures	|  valloc_base + valloc_sz
  *			|   memsegs, memlists,	|
  *			|   page hash, etc.	|
- * 0xFFFFFF00.00000000  |-----------------------|- valloc_base (lower if >256GB)
+ * 0xFFFFFE00.00000000  |-----------------------|- valloc_base (lower if >256GB)
  *			|	 segkpm		|
- * 0xFFFFFE00.00000000  |-----------------------|
+ *			|			|
+ * 0xFFFFFD00.00000000  |-----------------------|- SEGKPM_BASE (lower if >256GB)
  *			|	Red Zone	|
- * 0xFFFFFD80.00000000  |-----------------------|- KERNELBASE (lower if >256GB)
+ * 0xFFFFFC80.00000000  |-----------------------|- KERNELBASE (lower if >256GB)
+ * 0xFFFFFC7F.FFE00000  |-----------------------|- USERLIMIT (lower if >256GB)
  *			|     User stack	|- User space memory
  *			|			|
  *			| shared objects, etc	|	(grows downwards)
@@ -1084,22 +1087,9 @@ startup_memlist(void)
 	PRM_DEBUG(memblocks);
 
 	/*
-	 * Compute maximum physical address for memory DR operations.
-	 * Memory DR operations are unsupported on xpv or 32bit OSes.
+	 * We no longer support any form of memory DR.
 	 */
-#ifdef	__amd64
-	if (plat_dr_support_memory()) {
-		if (plat_dr_physmax == 0) {
-			uint_t pabits = UINT_MAX;
-
-			cpuid_get_addrsize(CPU, &pabits, NULL);
-			plat_dr_physmax = btop(1ULL << pabits);
-		}
-		if (plat_dr_physmax > PHYSMEM_MAX64)
-			plat_dr_physmax = PHYSMEM_MAX64;
-	} else
-#endif
-		plat_dr_physmax = 0;
+	plat_dr_physmax = 0;
 
 	/*
 	 * Examine the bios reserved memory to find out:
@@ -1260,68 +1250,55 @@ startup_memlist(void)
 	pse_table_alloc_size = pse_table_size * sizeof (pad_mutex_t);
 	ADD_TO_ALLOCATIONS(pse_mutex, pse_table_alloc_size);
 
-#if defined(__amd64)
 	valloc_sz = ROUND_UP_LPAGE(valloc_sz);
 	valloc_base = VALLOC_BASE;
 
 	/*
-	 * The default values of VALLOC_BASE and SEGKPM_BASE should work
-	 * for values of physmax up to 256GB (1/4 TB). They need adjusting when
-	 * memory is at addresses above 256GB. When adjusted, segkpm_base must
-	 * be aligned on KERNEL_REDZONE_SIZE boundary (span of top level pte).
+	 * The signicant memory-sized regions are roughly sized as follows in
+	 * the default layout with max physmem:
+	 *  segkpm: 1x physmem allocated (but 1Tb room, below VALLOC_BASE)
+	 *  segzio: 1.5x physmem
+	 *  segkvmm: 4x physmem
+	 *  heap: whatever's left up to COREHEAP_BASE, at least 1.5x physmem
+	 *
+	 * The idea is that we leave enough room to avoid fragmentation issues,
+	 * so we would like the VA arenas to have some extra.
+	 *
+	 * Ignoring the loose change of segkp, valloc, and such, this means that
+	 * as COREHEAP_BASE-VALLOC_BASE=2Tb, we can accommodate a physmem up to
+	 * about (2Tb / 7.0), rounded down to 256Gb in the check below.
 	 *
-	 * In the general case (>256GB), we use (4 * physmem) for the
-	 * kernel's virtual addresses, which is divided approximately
-	 * as follows:
-	 *  - 1 * physmem for segkpm
-	 *  - 1.5 * physmem for segzio
-	 *  - 1.5 * physmem for heap
-	 * Total: 4.0 * physmem
+	 * Note that KPM lives below VALLOC_BASE, but we want to include it in
+	 * adjustments, hence the 8 below.
 	 *
-	 * Note that the segzio and heap sizes are more than physmem so that
-	 * VA fragmentation does not prevent either of them from being
-	 * able to use nearly all of physmem.  The value of 1.5x is determined
-	 * experimentally and may need to change if the workload changes.
+	 * Beyond 256Gb, we push segkpm_base (and hence kernelbase and
+	 * _userlimit) down to accommodate the VA requirements above.
 	 */
-	if (physmax + 1 > mmu_btop(TERABYTE / 4) ||
-	    plat_dr_physmax > mmu_btop(TERABYTE / 4)) {
-		uint64_t kpm_resv_amount = mmu_ptob(physmax + 1);
+	if (physmax + 1 > mmu_btop(TERABYTE / 4)) {
+		uint64_t physmem_bytes = mmu_ptob(physmax + 1);
+		uint64_t adjustment = 8 * (physmem_bytes - (TERABYTE / 4));
 
-		if (kpm_resv_amount < mmu_ptob(plat_dr_physmax)) {
-			kpm_resv_amount = mmu_ptob(plat_dr_physmax);
-		}
+		PRM_DEBUG(adjustment);
 
 		/*
-		 * This is what actually controls the KVA : UVA split.
-		 * The kernel uses high VA, and this is lowering the
-		 * boundary, thus increasing the amount of VA for the kernel.
-		 * This gives the kernel 4 * (amount of physical memory) VA.
-		 *
-		 * The maximum VA is UINT64_MAX and we are using
-		 * 64-bit 2's complement math, so e.g. if you have 512GB
-		 * of memory, segkpm_base = -(4 * 512GB) == -2TB ==
-		 * UINT64_MAX - 2TB (approximately).  So the kernel's
-		 * VA is [UINT64_MAX-2TB to UINT64_MAX].
+		 * segkpm_base is always aligned on a L3 PTE boundary.
 		 */
-		segkpm_base = -(P2ROUNDUP((4 * kpm_resv_amount),
-		    KERNEL_REDZONE_SIZE));
+		segkpm_base -= P2ROUNDUP(adjustment, KERNEL_REDZONE_SIZE);
 
-		/* make sure we leave some space for user apps above hole */
+		/*
+		 * But make sure we leave some space for user apps above hole.
+		 */
 		segkpm_base = MAX(segkpm_base, AMD64_VA_HOLE_END + TERABYTE);
-		if (segkpm_base > SEGKPM_BASE)
-			segkpm_base = SEGKPM_BASE;
-		PRM_DEBUG(segkpm_base);
 
-		valloc_base = segkpm_base + P2ROUNDUP(kpm_resv_amount, ONE_GIG);
+		ASSERT(segkpm_base <= SEGKPM_BASE);
+
+		valloc_base = segkpm_base + P2ROUNDUP(physmem_bytes, ONE_GIG);
 		if (valloc_base < segkpm_base)
 			panic("not enough kernel VA to support memory size");
-		PRM_DEBUG(valloc_base);
 	}
-#else	/* __i386 */
-	valloc_base = (uintptr_t)(MISC_VA_BASE - valloc_sz);
-	valloc_base = P2ALIGN(valloc_base, mmu.level_size[1]);
+
+	PRM_DEBUG(segkpm_base);
 	PRM_DEBUG(valloc_base);
-#endif	/* __i386 */
 
 	/*
 	 * do all the initial allocations
@@ -1909,73 +1886,70 @@ protect_boot_range(uintptr_t low, uintptr_t high, int setaside)
 }
 
 /*
- *
+ * Establish the final size of the kernel's heap, size of segmap, segkp, etc.
  */
 static void
 layout_kernel_va(void)
 {
-	PRM_POINT("layout_kernel_va() starting...");
-	/*
-	 * Establish the final size of the kernel's heap, size of segmap,
-	 * segkp, etc.
-	 */
+	const size_t physmem_size = mmu_ptob(physmem);
+	size_t size;
 
-#if defined(__amd64)
+	PRM_POINT("layout_kernel_va() starting...");
 
 	kpm_vbase = (caddr_t)segkpm_base;
-	if (physmax + 1 < plat_dr_physmax) {
-		kpm_size = ROUND_UP_LPAGE(mmu_ptob(plat_dr_physmax));
-	} else {
-		kpm_size = ROUND_UP_LPAGE(mmu_ptob(physmax + 1));
-	}
+	kpm_size = ROUND_UP_LPAGE(mmu_ptob(physmax + 1));
 	if ((uintptr_t)kpm_vbase + kpm_size > (uintptr_t)valloc_base)
 		panic("not enough room for kpm!");
 	PRM_DEBUG(kpm_size);
 	PRM_DEBUG(kpm_vbase);
 
-	/*
-	 * By default we create a seg_kp in 64 bit kernels, it's a little
-	 * faster to access than embedding it in the heap.
-	 */
 	segkp_base = (caddr_t)valloc_base + valloc_sz;
 	if (!segkp_fromheap) {
-		size_t sz = mmu_ptob(segkpsize);
+		size = mmu_ptob(segkpsize);
 
 		/*
 		 * determine size of segkp
 		 */
-		if (sz < SEGKPMINSIZE || sz > SEGKPMAXSIZE) {
-			sz = SEGKPDEFSIZE;
+		if (size < SEGKPMINSIZE || size > SEGKPMAXSIZE) {
+			size = SEGKPDEFSIZE;
 			cmn_err(CE_WARN, "!Illegal value for segkpsize. "
 			    "segkpsize has been reset to %ld pages",
-			    mmu_btop(sz));
+			    mmu_btop(size));
 		}
-		sz = MIN(sz, MAX(SEGKPMINSIZE, mmu_ptob(physmem)));
+		size = MIN(size, MAX(SEGKPMINSIZE, physmem_size));
 
-		segkpsize = mmu_btop(ROUND_UP_LPAGE(sz));
+		segkpsize = mmu_btop(ROUND_UP_LPAGE(size));
 	}
 	PRM_DEBUG(segkp_base);
 	PRM_DEBUG(segkpsize);
 
 	/*
-	 * segzio is used for ZFS cached data. It uses a distinct VA
-	 * segment (from kernel heap) so that we can easily tell not to
-	 * include it in kernel crash dumps on 64 bit kernels. The trick is
-	 * to give it lots of VA, but not constrain the kernel heap.
-	 * We can use 1.5x physmem for segzio, leaving approximately
-	 * another 1.5x physmem for heap.  See also the comment in
-	 * startup_memlist().
+	 * segkvmm: backing for vmm guest memory. Like segzio, we have a
+	 * separate segment for two reasons: it makes it easy to skip our pages
+	 * on kernel crash dumps, and it helps avoid fragmentation.  With this
+	 * segment, we're expecting significantly-sized allocations only; we'll
+	 * default to 4x the size of physmem.
+	 */
+	segkvmm_base = segkp_base + mmu_ptob(segkpsize);
+	size = segkvmmsize != 0 ? mmu_ptob(segkvmmsize) : (physmem_size * 4);
+
+	size = MAX(size, SEGVMMMINSIZE);
+	segkvmmsize = mmu_btop(ROUND_UP_LPAGE(size));
+
+	PRM_DEBUG(segkvmmsize);
+	PRM_DEBUG(segkvmm_base);
+
+	/*
+	 * segzio is used for ZFS cached data.  For segzio, we use 1.5x physmem.
 	 */
-	segzio_base = segkp_base + mmu_ptob(segkpsize);
+	segzio_base = segkvmm_base + mmu_ptob(segkvmmsize);
 	if (segzio_fromheap) {
 		segziosize = 0;
 	} else {
-		size_t physmem_size = mmu_ptob(physmem);
-		size_t size = (segziosize == 0) ?
-		    physmem_size * 3 / 2 : mmu_ptob(segziosize);
+		size = (segziosize != 0) ? mmu_ptob(segziosize) :
+		    (physmem_size * 3) / 2;
 
-		if (size < SEGZIOMINSIZE)
-			size = SEGZIOMINSIZE;
+		size = MAX(size, SEGZIOMINSIZE);
 		segziosize = mmu_btop(ROUND_UP_LPAGE(size));
 	}
 	PRM_DEBUG(segziosize);
@@ -1989,10 +1963,6 @@ layout_kernel_va(void)
 	    ROUND_UP_LPAGE((uintptr_t)segzio_base + mmu_ptob(segziosize));
 	PRM_DEBUG(toxic_addr);
 	segmap_start = ROUND_UP_LPAGE(toxic_addr + toxic_size);
-#else /* __i386 */
-	segmap_start = ROUND_UP_LPAGE(kernelbase);
-#endif /* __i386 */
-	PRM_DEBUG(segmap_start);
 
 	/*
 	 * Users can change segmapsize through eeprom. If the variable
@@ -2001,16 +1971,6 @@ layout_kernel_va(void)
 	 */
 	segmapsize = MAX(ROUND_UP_LPAGE(segmapsize), SEGMAPDEFAULT);
 
-#if defined(__i386)
-	/*
-	 * 32-bit systems don't have segkpm or segkp, so segmap appears at
-	 * the bottom of the kernel's address range.  Set aside space for a
-	 * small red zone just below the start of segmap.
-	 */
-	segmap_start += KERNEL_REDZONE_SIZE;
-	segmapsize -= KERNEL_REDZONE_SIZE;
-#endif
-
 	PRM_DEBUG(segmap_start);
 	PRM_DEBUG(segmapsize);
 	kernelheap = (caddr_t)ROUND_UP_LPAGE(segmap_start + segmapsize);
@@ -2799,11 +2759,16 @@ kvm_init(void)
 		(void) segkmem_create(&kvseg_core);
 	}
 
+	PRM_POINT("attaching segkvmm");
+	(void) seg_attach(&kas, segkvmm_base, mmu_ptob(segkvmmsize), &kvmmseg);
+	(void) segkmem_create(&kvmmseg);
+	segkmem_kvmm_init(segkvmm_base, mmu_ptob(segkvmmsize));
+
 	if (segziosize > 0) {
 		PRM_POINT("attaching segzio");
 		(void) seg_attach(&kas, segzio_base, mmu_ptob(segziosize),
 		    &kzioseg);
-		(void) segkmem_zio_create(&kzioseg);
+		(void) segkmem_create(&kzioseg);
 
 		/* create zio area covering new segment */
 		segkmem_zio_init(segzio_base, mmu_ptob(segziosize));
diff --git a/usr/src/uts/i86pc/sys/machparam.h b/usr/src/uts/i86pc/sys/machparam.h
index 51d7559483..3728f30ca6 100644
--- a/usr/src/uts/i86pc/sys/machparam.h
+++ b/usr/src/uts/i86pc/sys/machparam.h
@@ -31,14 +31,15 @@
 #ifndef _SYS_MACHPARAM_H
 #define	_SYS_MACHPARAM_H
 
-#if !defined(_ASM)
+#ifndef _ASM
+
 #include <sys/types.h>
 
 #if defined(__xpv)
 #include <sys/xpv_impl.h>
 #endif
 
-#endif
+#endif /* !_ASM */
 
 #ifdef	__cplusplus
 extern "C" {
@@ -54,17 +55,12 @@ extern "C" {
  * Machine dependent parameters and limits.
  */
 
-#if defined(__amd64)
 /*
  * If NCPU grows beyond 256, sizing for the x86 comm page will require
  * adjustment.
  */
 #define	NCPU	256
 #define	NCPU_LOG2	8
-#elif defined(__i386)
-#define	NCPU	32
-#define	NCPU_LOG2	5
-#endif
 
 /* NCPU_P2 is NCPU rounded to a power of 2 */
 #define	NCPU_P2	(1 << NCPU_LOG2)
@@ -116,11 +112,7 @@ extern "C" {
 /*
  * DEFAULT KERNEL THREAD stack size (in pages).
  */
-#if defined(__amd64)
 #define	DEFAULTSTKSZ_NPGS	5
-#elif defined(__i386)
-#define	DEFAULTSTKSZ_NPGS	3
-#endif
 
 #if !defined(_ASM)
 #define	DEFAULTSTKSZ	(DEFAULTSTKSZ_NPGS * PAGESIZE)
@@ -129,43 +121,42 @@ extern "C" {
 #endif	/* !_ASM */
 
 /*
- * KERNELBASE is the virtual address at which the kernel segments start in
- * all contexts.
- *
- * KERNELBASE is not fixed.  The value of KERNELBASE can change with
- * installed memory or on 32 bit systems the eprom variable 'eprom_kernelbase'.
- *
- * common/conf/param.c requires a compile time defined value for KERNELBASE.
- * This value is save in the variable _kernelbase.  _kernelbase may then be
- * modified with to a different value in i86pc/os/startup.c.
- *
- * Most code should be using kernelbase, which resolves to a reference to
- * _kernelbase.
+ * During intial boot we limit heap to the top 4Gig.
  */
-#define	KERNEL_TEXT_amd64	UINT64_C(0xfffffffffb800000)
-
-#ifdef __i386
-
-#define	KERNEL_TEXT_i386	ADDRESS_C(0xfe800000)
+#define	BOOT_KERNELHEAP_BASE	ADDRESS_C(0xffffffff00000000)
 
 /*
- * We don't use HYPERVISOR_VIRT_START, as we need both the PAE and non-PAE
- * versions in our code. We always compile based on the lower PAE address.
+ * VMWare works best if we don't use the top 64Meg of memory for amd64.
+ * Set KERNEL_TEXT to top_o_memory - 64Meg - 8 Meg for 8Meg of nucleus pages.
  */
-#define	KERNEL_TEXT_i386_xpv	\
-	(HYPERVISOR_VIRT_START_PAE - 3 * ADDRESS_C(0x400000))
-
-#endif /* __i386 */
+#define	PROMSTART	ADDRESS_C(0xffc00000)
 
-#if defined(__amd64)
+/*
+ * Virtual address range available to the debugger
+ */
+#define	SEGDEBUGBASE	ADDRESS_C(0xffffffffff800000)
+#define	SEGDEBUGSIZE	ADDRESS_C(0x400000)
 
-#define	KERNELBASE	ADDRESS_C(0xfffffd8000000000)
+#define	KERNEL_TEXT	UINT64_C(0xfffffffffb800000)
 
 /*
- * Size of the unmapped "red zone" at the very bottom of the kernel's
- * address space.  Corresponds to 1 slot in the toplevel pagetable.
+ * Reserve pages just below KERNEL_TEXT for the GDT, IDT, LDT, TSS and debug
+ * info.
+ *
+ * For now, DEBUG_INFO_VA must be first in this list for "xm" initiated dumps
+ * of solaris domUs to be usable with mdb. Relying on a fixed VA is not viable
+ * long term, but it's the best we've got for now.
  */
-#define	KERNEL_REDZONE_SIZE   ((uintptr_t)1 << 39)
+#if !defined(_ASM)
+#define	DEBUG_INFO_VA	(KERNEL_TEXT - MMU_PAGESIZE)
+#define	GDT_VA		(DEBUG_INFO_VA - MMU_PAGESIZE)
+#define	IDT_VA		(GDT_VA - MMU_PAGESIZE)
+#define	LDT_VA		(IDT_VA - (16 * MMU_PAGESIZE))
+#define	KTSS_VA		(IDT_VA - MMU_PAGESIZE)
+#define	DFTSS_VA	(KTSS_VA - MMU_PAGESIZE)
+#define	MISC_VA_BASE	(DFTSS_VA)
+#define	MISC_VA_SIZE	(KERNEL_TEXT - MISC_VA_BASE)
+#endif /* !_ASM */
 
 /*
  * Base of 'core' heap area, which is used for kernel and module text/data
@@ -173,53 +164,48 @@ extern "C" {
  */
 #define	COREHEAP_BASE	ADDRESS_C(0xffffffffc0000000)
 
-/*
- * Beginning of the segkpm window. A lower value than this is used if
- * physical addresses exceed 1TB. See i86pc/os/startup.c
- */
-#define	SEGKPM_BASE	ADDRESS_C(0xfffffe0000000000)
-
 /*
  * This is valloc_base, above seg_kpm, but below everything else.
  * A lower value than this may be used if SEGKPM_BASE is adjusted.
  * See i86pc/os/startup.c
  */
-#define	VALLOC_BASE	ADDRESS_C(0xffffff0000000000)
+#define	VALLOC_BASE	ADDRESS_C(0xfffffe0000000000)
+
+#define	SEGZIOMINSIZE	(400L * 1024 * 1024L)			/* 400M */
+#define	SEGVMMMINSIZE	(4096L * 1024 * 1024L)			/* 4G */
 
-/*
- * default and boundary sizes for segkp
- */
 #define	SEGKPDEFSIZE	(2L * 1024L * 1024L * 1024L)		/*   2G */
 #define	SEGKPMAXSIZE	(8L * 1024L * 1024L * 1024L)		/*   8G */
 #define	SEGKPMINSIZE	(200L * 1024 * 1024L)			/* 200M */
 
-/*
- * minimum size for segzio
- */
-#define	SEGZIOMINSIZE	(400L * 1024 * 1024L)			/* 400M */
-
-/*
- * During intial boot we limit heap to the top 4Gig.
- */
-#define	BOOT_KERNELHEAP_BASE	ADDRESS_C(0xffffffff00000000)
+#define	SEGKPM_BASE	ADDRESS_C(0xfffffd0000000000)
 
 /*
- * VMWare works best if we don't use the top 64Meg of memory for amd64.
- * Set KERNEL_TEXT to top_o_memory - 64Meg - 8 Meg for 8Meg of nucleus pages.
+ * KERNELBASE is the virtual address at which the kernel segments start in
+ * all contexts.
+ *
+ * KERNELBASE is not fixed.  The value of KERNELBASE can change with
+ * installed memory size.
+ *
+ * common/conf/param.c requires a compile time defined value for KERNELBASE.
+ * This value is save in the variable _kernelbase.  _kernelbase may then be
+ * modified with to a different value in i86pc/os/startup.c.
+ *
+ * Most code should be using kernelbase, which resolves to a reference to
+ * _kernelbase.
  */
-#define	PROMSTART	ADDRESS_C(0xffc00000)
-#define	KERNEL_TEXT	KERNEL_TEXT_amd64
+#define	KERNELBASE	ADDRESS_C(0xfffffc8000000000)
 
 /*
- * Virtual address range available to the debugger
+ * Size of the unmapped "red zone" at the very bottom of the kernel's
+ * address space.  Corresponds to 1 slot in the toplevel pagetable.
  */
-#define	SEGDEBUGBASE	ADDRESS_C(0xffffffffff800000)
-#define	SEGDEBUGSIZE	ADDRESS_C(0x400000)
+#define	KERNEL_REDZONE_SIZE   ((uintptr_t)1 << 39)
 
 /*
  * Define upper limit on user address space
  *
- * In amd64, the upper limit on a 64-bit user address space is 1 large page
+ * The upper limit on a 64-bit user address space is 1 large page
  * (2MB) below kernelbase.  The upper limit for a 32-bit user address space
  * is 1 small page (4KB) below the top of the 32-bit range.  The 64-bit
  * limit give dtrace the red zone it needs below kernelbase.  The 32-bit
@@ -232,7 +218,7 @@ extern "C" {
 #if defined(__xpv)
 #define	USERLIMIT	ADDRESS_C(0x00007fffffe00000)
 #else
-#define	USERLIMIT	ADDRESS_C(0xfffffd7fffe00000)
+#define	USERLIMIT	ADDRESS_C(0xfffffc7fffe00000)
 #endif
 
 #ifdef bug_5074717_is_fixed
@@ -241,76 +227,6 @@ extern "C" {
 #define	USERLIMIT32	ADDRESS_C(0xfefff000)
 #endif
 
-#elif defined(__i386)
-
-#ifdef DEBUG
-#define	KERNELBASE	ADDRESS_C(0xc8000000)
-#else
-#define	KERNELBASE	ADDRESS_C(0xd4000000)
-#endif
-
-#define	KERNELBASE_MAX	ADDRESS_C(0xe0000000)
-
-/*
- * The i386 ABI requires that the user address space be at least 3Gb
- * in size.  KERNELBASE_ABI_MIN is used as the default KERNELBASE for
- * physical memory configurations > 4gb.
- */
-#define	KERNELBASE_ABI_MIN	ADDRESS_C(0xc0000000)
-
-/*
- * Size of the unmapped "red zone" at the very bottom of the kernel's
- * address space.  Since segmap start immediately above the red zone, this
- * needs to be MAXBSIZE aligned.
- */
-#define	KERNEL_REDZONE_SIZE   MAXBSIZE
-
-/*
- * This is the last 4MB of the 4G address space. Some psm modules
- * need this region of virtual address space mapped 1-1
- * The top 64MB of the address space is reserved for the hypervisor.
- */
-#define	PROMSTART	ADDRESS_C(0xffc00000)
-#ifdef __xpv
-#define	KERNEL_TEXT	KERNEL_TEXT_i386_xpv
-#else
-#define	KERNEL_TEXT	KERNEL_TEXT_i386
-#endif
-
-/*
- * Virtual address range available to the debugger
- * We place it just above the kernel text (4M) and kernel data (4M).
- */
-#define	SEGDEBUGBASE	(KERNEL_TEXT + ADDRESS_C(0x800000))
-#define	SEGDEBUGSIZE	ADDRESS_C(0x400000)
-
-/*
- * Define upper limit on user address space
- */
-#define	USERLIMIT	KERNELBASE
-#define	USERLIMIT32	USERLIMIT
-
-#endif	/* __i386 */
-
-/*
- * Reserve pages just below KERNEL_TEXT for the GDT, IDT, LDT, TSS and debug
- * info.
- *
- * For now, DEBUG_INFO_VA must be first in this list for "xm" initiated dumps
- * of solaris domUs to be usable with mdb. Relying on a fixed VA is not viable
- * long term, but it's the best we've got for now.
- */
-#if !defined(_ASM)
-#define	DEBUG_INFO_VA	(KERNEL_TEXT - MMU_PAGESIZE)
-#define	GDT_VA		(DEBUG_INFO_VA - MMU_PAGESIZE)
-#define	IDT_VA		(GDT_VA - MMU_PAGESIZE)
-#define	LDT_VA		(IDT_VA - (16 * MMU_PAGESIZE))
-#define	KTSS_VA		(LDT_VA - MMU_PAGESIZE)
-#define	DFTSS_VA	(KTSS_VA - MMU_PAGESIZE)
-#define	MISC_VA_BASE	(DFTSS_VA)
-#define	MISC_VA_SIZE	(KERNEL_TEXT - MISC_VA_BASE)
-#endif /* !_ASM */
-
 #if !defined(_ASM) && !defined(_KMDB)
 extern uintptr_t kernelbase, segmap_start, segmapsize;
 #endif
diff --git a/usr/src/uts/i86pc/vm/seg_vmm.c b/usr/src/uts/i86pc/vm/seg_vmm.c
index faebf9ac36..beb5e81d53 100644
--- a/usr/src/uts/i86pc/vm/seg_vmm.c
+++ b/usr/src/uts/i86pc/vm/seg_vmm.c
@@ -14,12 +14,15 @@
  */
 
 /*
- * VM - Virtual-Machine-Memory segment
+ * segvmm - Virtual-Machine-Memory segment
  *
  * The vmm segment driver was designed for mapping regions of kernel memory
  * allocated to an HVM instance into userspace for manipulation there.  It
  * draws direct lineage from the umap segment driver, but meant for larger
  * mappings with fewer restrictions.
+ *
+ * seg*k*vmm, in contrast, has mappings for every VMM into kas.  We use its
+ * mappings here only to find the relevant PFNs in segvmm_fault_in().
  */
 
 
@@ -93,7 +96,7 @@ static struct seg_ops segvmm_ops = {
 
 
 /*
- * Create a kernel/user-mapped segment.
+ * Create a kernel/user-mapped segment.  ->kaddr is the segkvmm mapping.
  */
 int
 segvmm_create(struct seg **segpp, void *argsp)
diff --git a/usr/src/uts/sun4/os/startup.c b/usr/src/uts/sun4/os/startup.c
index 7cc7074c10..6ebb07412f 100644
--- a/usr/src/uts/sun4/os/startup.c
+++ b/usr/src/uts/sun4/os/startup.c
@@ -22,6 +22,7 @@
 /*
  * Copyright (c) 2003, 2010, Oracle and/or its affiliates. All rights reserved.
  * Copyright (c) 2016 by Delphix. All rights reserved.
+ * Copyright 2018 Joyent, Inc.
  */
 
 #include <sys/machsystm.h>
@@ -2054,7 +2055,7 @@ startup_vm(void)
 
 		(void) seg_attach(&kas, segzio_base, mmu_ptob(segziosize),
 		    &kzioseg);
-		(void) segkmem_zio_create(&kzioseg);
+		(void) segkmem_create(&kzioseg);
 
 		/* create zio area covering new segment */
 		segkmem_zio_init(segzio_base, mmu_ptob(segziosize));
@@ -2438,7 +2439,7 @@ memlist_new(uint64_t start, uint64_t len, struct memlist **memlistp)
  */
 static void
 memlist_add(uint64_t start, uint64_t len, struct memlist **memlistp,
-	struct memlist **curmemlistp)
+    struct memlist **curmemlistp)
 {
 	struct memlist *new = *memlistp;
 
