From 438a667114bbd463639c91412e2933a603b4522e Mon Sep 17 00:00:00 2001
From: Alex Wilson <alex.wilson@joyent.com>
Date: Thu, 10 Aug 2017 15:41:36 -0700
Subject: [PATCH] joyent/node-cueball#120 Add a section about the shuffle timer
 to internals doc

---
 docs/internals.adoc | 113 ++++++++++++++++++++++++++++++++++++++++++++
 1 file changed, 113 insertions(+)

diff --git a/docs/internals.adoc b/docs/internals.adoc
index 2896d2b..7fd76c5 100644
--- a/docs/internals.adoc
+++ b/docs/internals.adoc
@@ -239,6 +239,119 @@ cleared out, calling their callbacks with errors.
 The pool will remain in the "failed" state until one of its "monitor" slots
 manages to connect to a backend again.
 
+### Coherence and decoherence
+
+In a large distributed system with many clients on different machines attempting
+to use the same logical service, a phenomenon we will refer to as "coherence"
+can emerge.
+
+For example, let us think about a situation where 5 clients all want to make
+2 connections to a logical service with 4 backends. Let's suppose all the
+backends are currently running, and each client picks 2 of the 4 at random. We
+might see the following distribution:
+
+.Initial state
+|===
+|          | Slot 1    | Slot 2
+
+| Client 1 | Backend 1 | Backend 2
+
+| Client 2 | Backend 3 | Backend 4
+
+| Client 3 | Backend 3 | Backend 1
+
+| Client 4 | Backend 2 | Backend 4
+
+| Client 5 | Backend 1 | Backend 2
+
+|===
+
+This is fairly even loading (3 on #1, 3 on #2, 2 on #3, 2 on #4). But we may not
+produce such an even distribution in reality (the random number generator does
+not always produce such perfect results).
+
+The first kind of coherence that can occur is when the *initial* random choices
+of the whole group of clients result in them "ganging up" or concentrating their
+connections upon some subset of the available backends. We call this "static"
+coherence.
+
+There is a second kind of coherence which can occur as well: start by supposing
+that backend #2 goes offline. Using the mechanisms above, we retry until we
+exhaust our retry policy and mark backend #2 as dead on all clients. Then we
+seek a replacement for it from the remaining entries on our list of backends.
+Our final configuration might end up looking like this:
+
+.State after losing backend #2
+|===
+|          | Slot 1    | Slot 2
+
+| Client 1 | Backend 1 | Backend 4
+
+| Client 2 | Backend 3 | Backend 4
+
+| Client 3 | Backend 3 | Backend 1
+
+| Client 4 | Backend 1 | Backend 4
+
+| Client 5 | Backend 1 | Backend 4
+
+|===
+
+Now we have 4 on #1, 2 on #3 and 3 on #3. Backend #1 is no longer fairly loaded.
+Additionally, let us suppose backend #2 comes back online. Without the "monitor"
+mode we discussed above, a simplistic pool implementation would just continue
+to use this set of slots and have no connections made at all to backend #2.
+
+If we restarted #2 as the first step in a "rolling restart" of all 4 backends,
+now what we will see is that the clients concentrate all their connections onto
+the exact backend we're about to restart next as we go around (because it's the
+backend that has been up and running the longest!). This means that we are
+guaranteed to produce the maximum possible disruption to these clients by doing
+such a rolling restart -- we would disrupt their workload for less time if we
+just restarted everything at once. This is clearly not a good result.
+
+These are both examples of the second kind of coherence: "dynamic" coherence,
+caused by the pool's reaction to changes in the environment (as opposed to being
+caused by its static configuration).
+
+As we've just observed, the slot monitor mode (discussed above) mitigates
+against the most common form of dynamic coherence -- the monitor slots will
+notice that backend #2 is back again, and the pools will change back to their
+original configuration, removing the coherence.
+
+The way this is implemented in cueball is by using a "preference list". This is
+a randomly ordered list of all the backends available in the logical service.
+Being higher up this list (closer to index 0) means that backend is "preferred"
+for being used by this pool. The pool will attempt to get to its configured
+number of slots by working its way down this list from most to least preferred
+(taking into account dead markings as it goes).
+
+To mitigate against static coherence, and the other more subtle kinds of dynamic
+coherence, cueball makes use of a "decoherence" or "shuffle" timer. This timer
+goes off every 60 seconds, and triggers the pool to take the least preferred
+backend on its preference list and inject it back into the list at a random
+higher index.
+
+This means it has an N/(M - 1) probability (N being the number of slots active
+in the pool and M being the number of backends in the logical service) of
+changing the active set of slots footnoteref:[,Since the decoherence timer's
+primary objective is protection against static coherence, it makes sense to have
+the expected time between it causing changes in the active set of slots go up as
+the number of logical backends in the service goes up -- with a larger number of
+logical backends available it is less likely for static coherence to occur as
+the probability of choosing the same small set from the larger set many times
+goes down. To our knowledge this does not make it less effective at guarding
+against the other kinds of dynamic coherence, such as during a partial outage,
+because these seem to have a similar property.]. Such a change is carried out by
+the rebalancer as part of its normal operation, by marking slots for the old
+backend as unwanted and creating new slots.
+
+Having this shuffling of preference lists take place periodically creates a
+maximum expected time bound on how long a coherence event can last -- when one
+does occur at random, the decoherence timer will eventually cause the clients to
+go their separate ways and it will not persist indefinitely. It also lowers the
+probability of severe coherence events occuring.
+
 ### ConnectionSet logical connections
 
 In the `ConnectionSet` implementation, there is one more additional FSM in use:
-- 
2.21.0

