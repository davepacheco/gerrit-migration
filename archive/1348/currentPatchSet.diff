commit 0cfc60b4d3775607f697452cf4c08761f95876b0 (refs/changes/48/1348/16)
Author: Todd Whiteman <todd.whiteman@joyent.com>
Date:   2017-07-24T10:49:59-07:00 (2 years, 2 months ago)
    
    DOCKER-929 Support the docker v2.2 manifest format
    Reviewed by: Trent Mick <trentm@gmail.com>
    Approved by: Trent Mick <trentm@gmail.com>

diff --git a/CHANGES.md b/CHANGES.md
index a6d9e38..766c04b 100644
--- a/CHANGES.md
+++ b/CHANGES.md
@@ -1,5 +1,12 @@
 # IMGAPI changelog
 
+## 4.0.0
+
+- DOCKER-929 Support the docker v2.2 manifest format.
+
+This is a major version bump because of the significant changes to the docker
+image pull handling, which is incompatible with previous versions of IMGAPI.
+
 ## 3.3.1
 
 - IMGAPI-632 Fix potential crash from in static served URLs.
diff --git a/docs/index.md b/docs/index.md
index d3d1f4a..6624ef4 100644
--- a/docs/index.md
+++ b/docs/index.md
@@ -359,6 +359,8 @@ uploaded. A "file" entry has the following fields:
 | compression  | The type of file compression used by the file. One of 'bzip2', 'gzip', 'none'.                                                                                                                                                                                                                                                                    |
 | dataset_guid | Optional. The ZFS internal unique identifier for this dataset's snapshot (available via `zfs get guid SNAPSHOT`, e.g. `zfs get guid zones/f669428c-a939-11e2-a485-b790efc0f0c1@final`). If available, this is used to ensure a common base snapshot for incremental images (via `imgadm create -i`) and VM migrations (via `vmadm send/receive`). |
 | stor         | Only included if `?inclAdminFields=true` is passed to GetImage/ListImages. The IMGAPI storage type used to store this file. |
+| digest       | Optional. Docker digest of the file contents. Only used when manifest.type is 'docker'. This field gets set automatically by the AdminImportDockerImage call. |
+| uncompressedDigest | Optional. Docker digest of the uncompressed file contents. Only used when manifest.type is 'docker'. This field gets set automatically by the AdminImportDockerImage call. Note that this field will be removed in a future version of IMGAPI. |
 
 Example:
 
@@ -2061,8 +2063,29 @@ Headers:
 
 ### Returns
 
-A stream of progress messages.
-<!-- TODO: spec the messages -->
+A stream of messages in JSON format. These messages will be a mix of status and
+action messages, some of which are used by sdc-docker and some of which are
+sent back to the docker client. The messages will always have a 'type' (string)
+field, which must be one the following:
+
+* **status** - for status information regarding the pull. Example:
+
+    {"type":"status","payload":{"status":"latest: Pulling from busybox (req f3b0c95c-461f-4ace-843b-f34cd21af3c3)"},"id":"docker.io/busybox"}
+
+* **head** - information for which image layer is the top "head" layer. Example:
+
+    {"type":"head","head":"bbed08f07a6bccc8aca4f6053dd1b5bdf1050f830e0989738e6532dd4a703a58","id":"docker.io/busybox"}
+
+* **progress** - like status, but may also contain a payload.progressDetail field which shows the progress for downloading an image layer. Example:
+
+    {"type":"progress","payload":{"id":"27144aa8f1b9","status":"Downloading","progressDetail":{"current":539527,"total":699243,"start":1497634554}},"id":"docker.io/busybox"}
+
+* **create-docker-image** - message for sdc-docker to create an entry in the
+    docker_images_v2 bucket. Example:
+
+    {"type":"create-docker-image","config_digest":"sha256:c30178c5239f2937c21c261b0365efcda25be4921ccb95acd63beeeb78786f27", ...}
+
+#### Error Event
 
 ### Errors
 
@@ -2082,17 +2105,17 @@ Raw `curl`:
     x-server-name: ec0cd67d-7731-422a-a6c2-f91eb98c6c52
     Connection: keep-alive
     Transfer-Encoding: chunked
-
-    {"type":"status","payload":{"status":"Pulling repository busybox"},"id":"docker.io/busybox"}
-    {"type":"head","head":"8c2e06607696bd4afb3d03b687e361cc43cf8ec1a4a725bc96e39f05ba97dd55","id":"docker.io/busybox"}
-    {"type":"progress","payload":{"id":"8c2e06607696","status":"Pulling dependent layers"},"id":"docker.io/busybox"}
-    {"type":"data","id":"docker.io/busybox","payload":{...}}
-    {"type":"progress","id":"docker.io/busybox","payload":{"id":"8c2e06607696","status":"Pulling metadata."}}
-    {"type":"progress","id":"docker.io/busybox","payload":{"id":"cf2616975b4a","status":"Pulling metadata."}}
-    ...
-    {"type":"progress","id":"docker.io/busybox","payload":{"id":"8c2e06607696","status":"Download complete."}}
-    {"type":"status","id":"docker.io/busybox","payload":{"status":"Status: Downloaded newer image for busybox:latest"}}
-
+    
+    {"type":"status","payload":{"status":"latest: Pulling from busybox (req 7734c61d-ad6b-40f5-ac93-ecacd53e4387)"},"id":"docker.io/busybox"}
+    {"type":"status","payload":{"id":"27144aa8f1b9","progressDetail":{},"status":"Pulling fs layer"},"id":"docker.io/busybox"}
+    {"type":"progress","payload":{"id":"27144aa8f1b9","status":"Pulling fs layer"},"id":"docker.io/busybox"}
+    {"type":"progress","payload":{"id":"27144aa8f1b9","status":"Downloading","progressDetail":{"current":539527,"total":699243,"start":1497634554}},"id":"docker.io/busybox"}
+    {"type":"progress","payload":{"id":"27144aa8f1b9","status":"Download complete"},"id":"docker.io/busybox"}
+    {"type":"create-docker-image","config_digest":"sha256:c30178c5239f2937c21c261b0365efcda25be4921ccb95acd63beeeb78786f27","head":true,...}
+    {"type":"progress","payload":{"id":"27144aa8f1b9","status":"Activating image"},"id":"docker.io/busybox"}
+    {"type":"progress","payload":{"id":"27144aa8f1b9","status":"Pull complete"},"id":"docker.io/busybox"}
+    {"type":"status","payload":{"status":"Digest: sha256:be3c11fdba7cfe299214e46edc642e09514dbb9bbefcd0d3836c05a1e0cd0642"},"id":"docker.io/busybox"}
+    {"type":"status","payload":{"status":"Status: Downloaded newer image for busybox:latest"},"id":"docker.io/busybox"}
 
 
 ## AdminChangeImageStor (POST /images/$uuid?action=change-stor&stor=$newstor)
diff --git a/lib/constants.js b/lib/constants.js
index 1b7c84c..ba8913d 100644
--- a/lib/constants.js
+++ b/lib/constants.js
@@ -44,6 +44,12 @@ module.exports = {
      */
     DATABASE_LOCAL_DIR: LOCAL_BASE_DIR + '/manifests',
 
+    MAX_ICON_SIZE: 128*1024, // 128KiB
+    MAX_ICON_SIZE_STR: '128 KiB',
+
+    MAX_IMAGE_SIZE: 20*1024*1024*1024, // 20GiB
+    MAX_IMAGE_SIZE_STR: '20 GiB',
+
     /*
      * Dir used by the 'local' storage backend to store images and archive
      * files.
@@ -51,6 +57,12 @@ module.exports = {
     STORAGE_LOCAL_IMAGES_DIR: LOCAL_BASE_DIR + '/images',
     STORAGE_LOCAL_ARCHIVE_DIR: LOCAL_BASE_DIR + '/archive',
 
+    UNSET_OWNER_UUID: '00000000-0000-0000-0000-000000000000',
+
+    // TODO: should use constant from node-imgmanifest
+    VALID_FILE_COMPRESSIONS: ['gzip', 'bzip2', 'xz', 'none'],
+    VALID_STORAGES: ['local', 'manta'],
+
     AUTHKEYS_BASE_DIR: LOCAL_BASE_DIR + '/etc/authkeys'
 };
 
diff --git a/lib/docker.js b/lib/docker.js
new file mode 100644
index 0000000..82d9538
--- /dev/null
+++ b/lib/docker.js
@@ -0,0 +1,1679 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright 2017 Joyent, Inc.
+ */
+
+/*
+ * IMGAPI docker specific code.
+ */
+
+var crypto = require('crypto');
+var util = require('util'),
+    format = util.format;
+var zlib = require('zlib');
+
+var assert = require('assert-plus');
+var drc = require('docker-registry-client');
+var imgmanifest = require('imgmanifest');
+var once = require('once');
+var streampeek = require('buffer-peek-stream');
+var vasync = require('vasync');
+
+var constants = require('./constants');
+var errors = require('./errors');
+var magic = require('./magic');
+var utils = require('./utils');
+
+
+//---- globals
+
+/*
+ * DOCKER-893: Maintain an in-memory image cache (just the image/layer metadata)
+ * for the docker layers that have been downloaded, but not yet registered with
+ * an image. This is used when a docker image download fails, we keep all the
+ * successfully downloaded layers from that image in this cache. If the image is
+ * attempted to be downloaded again, this cache is checked to avoid
+ * re-downloading those layers.
+ */
+var DOCKER_IMAGE_CACHE = {};
+
+
+//---- helpers
+
+/**
+ * Check if the cmd is a metadata command - i.e. doesn't modify the filesystem.
+ */
+function isMetadataCmd(cmd) {
+    assert.string(cmd, 'cmd');
+    var marker = ' #(nop) ';
+    var idx = cmd.indexOf(marker);
+    if (idx === -1) {
+        // Some older manifests don't include the #nop marker, e.g. for run
+        // commands.
+        return false;
+    }
+    var name = cmd.substr(idx + marker.length).split(' ')[0];
+    return ['ADD', 'COPY', 'RUN'].indexOf(name) === -1;
+}
+
+
+
+
+/*
+ * Called during `AdminImportDockerImage` to create (unactivated) and
+ * download the file for a single type=docker image.
+ *
+ * Note that if this function returns an error that is a DownloadError instance,
+ * the caller can schedule a retry of the download, whilst any other error
+ * indicates that the caller should not retry the download.
+ *
+ * @param opts {Object}
+ *      - @param ctx {Object} The run context for the
+ *        `apiAdminImportRemoteImage` call.
+ *      - @param imgJson {Object} Docker image object (config, rootfs, layers).
+ *      - @param digest {String} This is the layer sha256 digest.
+ *      - @param layerDigests {Array} All digests in the chain (including the
+ *        current digest as the last entry).
+ *      - @param uncompressedDigest {String} Digest of the uncompressed layer.
+ * @param callback {Function}
+ */
+function dockerDownloadAndImportLayer(opts, callback) {
+    assert.object(opts, 'opts');
+    assert.optionalString(opts.compression, 'opts.compression');
+    assert.object(opts.ctx, 'opts.ctx');
+    assert.object(opts.ctx.digestFromUuid, 'opts.ctx.digestFromUuid');
+    assert.func(opts.ctx.Image, 'opts.ctx.Image');
+    assert.object(opts.ctx.rat, 'opts.ctx.rat');
+    assert.object(opts.ctx.regClientV2, 'opts.ctx.regClientV2');
+    assert.object(opts.ctx.req, 'opts.ctx.req');
+    assert.func(opts.ctx.resMessage, 'opts.ctx.resMessage');
+    assert.string(opts.digest, 'opts.digest');
+    assert.object(opts.imgJson, 'opts.imgJson');
+    assert.arrayOfString(opts.layerDigests, 'opts.layerDigests');
+    assert.optionalString(opts.uncompressedDigest, 'opts.uncompressedDigest');
+    assert.func(callback, 'callback');
+
+    var compression = opts.compression;
+    var ctx = opts.ctx;
+    var digest = opts.digest;
+    var imgJson = opts.imgJson;
+    var req = ctx.req;
+    var app = req._app;
+    var log = req.log;
+    var rat = ctx.rat;
+
+    var fileSize = -1; // The same value used in Docker-docker for "don't know".
+    var manifest;
+    var newImage;
+    var shortId = imgmanifest.shortDockerId(
+        imgmanifest.dockerIdFromDigest(digest));
+    var uncompressedDigest = opts.uncompressedDigest;
+    var uuid = imgmanifest.imgUuidFromDockerDigests(opts.layerDigests);
+
+    function progressStatus(msg, progressDetail) {
+        var payload = {
+            id: shortId,
+            status: msg
+        };
+        if (progressDetail) {
+            payload.progressDetail = progressDetail;
+        }
+        ctx.resMessage({
+            type: 'progress',
+            payload: payload
+        });
+    }
+
+    // Remember the uuid -> digest relationship.
+    ctx.digestFromUuid[uuid] = digest;
+
+    log.debug({uuid: uuid, digest: digest},
+        'dockerDownloadAndImportLayer: check if image already exists');
+    ctx.Image.get(app, uuid, log, function (gErr, image) {
+        if (!gErr) {
+            assert.object(image, 'image');
+
+            if (image.state !== 'unactivated') {
+                // When the image already exists, just return the image as is.
+                log.debug({uuid: uuid, repo: rat.canonicalName, digest: digest},
+                    'dockerDownloadAndImportLayer: layer already exists');
+                progressStatus('Already exists');
+                callback(null, image);
+                return;
+            }
+
+            // Mark this Image as existing in the database
+            log.debug({uuid: uuid, repo: rat.canonicalName, digest: digest},
+                'dockerDownloadAndImportLayer: '
+                + 'layer exists, but is unactivated');
+            newImage = image;
+
+        } else if (gErr.restCode !== 'ResourceNotFound') {
+            return callback(gErr);
+        }
+
+        // Check if this image layer has already been downloaded before.
+        if (DOCKER_IMAGE_CACHE.hasOwnProperty(uuid)) {
+            var cachedItem = DOCKER_IMAGE_CACHE[uuid];
+            ctx.newFileInfoFromUuid[uuid] = cachedItem;
+            newImage = cachedItem.image;
+
+            log.debug({digest: digest, uuid: uuid},
+                'dockerDownloadAndImportImage: image layer already cached');
+
+            progressStatus('Download complete (cached)');
+            callback(null, newImage);
+            return;
+        }
+
+        log.debug({uuid: uuid, repo: rat.canonicalName, digest: digest},
+            'dockerDownloadAndImportLayer: start import');
+
+        vasync.pipeline({ arg: {}, funcs: [
+            genImgapiManifest,
+            handleOwner,
+            handleChannels,
+            createReadStream,
+            detectCompression,
+            uncompressAndSha256Contents,
+            createImageFromManifest,
+            addImageFile,
+            addUncompressedDigest
+        ]}, function afterPipe(pipeErr) {
+            if (pipeErr) {
+                callback(pipeErr);
+                return;
+            }
+            progressStatus('Download complete');
+            callback(null, newImage);
+        });
+    });
+
+    function genImgapiManifest(_, next) {
+        try {
+            manifest = imgmanifest.imgManifestFromDockerInfo({
+                uuid: uuid,
+                layerDigests: opts.layerDigests,
+                imgJson: imgJson,
+                repo: rat,
+                public: ctx.public_
+            });
+        } catch (e) {
+            return next(new errors.InternalError(e,
+                'could not convert Docker image JSON to a manifest'));
+        }
+
+        if (!manifest.os) {
+           manifest.os = 'other';  // some docker layers have no `.os`
+        }
+        next();
+    }
+
+    function handleOwner(_, next) {
+        if (newImage) {
+            next();
+            return;
+        }
+
+        /**
+         * In 'dc' mode (i.e. with a UFDS user database) change owner from
+         * UNSET_OWNER_UUID -> admin. In other modes (i.e. not user
+         * database), change owner from anything -> UNSET_OWNER_UUID.
+         *
+         * This means that the cycle of publishing an image to a public
+         * repo and importing into a DC makes the image cleanly owned by
+         * the 'admin' user. See IMGAPI-408.
+         */
+        if (app.mode === 'dc') {
+            if (manifest.owner === constants.UNSET_OWNER_UUID) {
+                manifest.owner = app.config.adminUuid;
+                return next();
+            }
+        } else {
+            manifest.owner = constants.UNSET_OWNER_UUID;
+        }
+
+        return next();
+    }
+
+    function handleChannels(_, next) {
+        if (newImage) {
+            next();
+            return;
+        }
+
+        delete manifest.channels;
+        if (req.channel) {
+            manifest.channels = [req.channel.name];
+        }
+        next();
+    }
+
+    function createReadStream(arg, next) {
+        ctx.regClientV2.createBlobReadStream({digest: digest},
+                function onStreamCb(err, blobStream) {
+            if (err) {
+                next(errors.wrapErrorFromDrc(err));
+                return;
+            }
+            arg.httpResponse = blobStream;
+            arg.stream = blobStream;
+            next();
+        });
+    }
+
+    function detectCompression(arg, next) {
+        if (compression) {
+            next();
+            return;
+        }
+
+        streampeek(arg.stream, magic.maxMagicLen,
+                function onpeek(err, buf, stream) {
+            if (err) {
+                next(err);
+                return;
+            }
+
+            // Update stream reference.
+            arg.stream = stream;
+
+            if (buf.length < magic.maxMagicLen) {
+                // Not a compressed file.
+                compression = 'none';
+                next();
+                return;
+            }
+
+            compression = magic.compressionTypeFromBufSync(buf) || 'none';
+            next();
+        });
+    }
+
+    function uncompressAndSha256Contents(arg, next) {
+        if (compression === 'none' || uncompressedDigest) {
+            next();
+            return;
+        }
+
+        // Need to uncompress the data to get the uncompressed sha256 digest.
+        var uncompressStream;
+        if (compression === 'gzip') {
+            uncompressStream = zlib.createGunzip();
+        } else if (compression === 'bzip2') {
+            uncompressStream = zlib.createBunzip2();
+        } else {
+            // Unsupported compression stream.
+            next(new errors.InternalError(format(
+                'Unsupported layer compression: %s', compression)));
+            return;
+        }
+
+        var sha256sum = crypto.createHash('sha256');
+        sha256sum.on('readable', function () {
+            var hash = sha256sum.read();
+            if (hash) {
+                uncompressedDigest = 'sha256:' + hash.toString('hex');
+                // Check if the final callback is waiting for this hash.
+                if (arg.uncompressedDigestCallback) {
+                    arg.uncompressedDigestCallback(null, uncompressedDigest);
+                }
+            }
+        });
+        uncompressStream.pipe(sha256sum);
+
+        // Pipe contents, but ensure stream is put back into paused mode.
+        arg.stream.pipe(uncompressStream);
+        arg.stream.pause();
+
+        next();
+    }
+
+    function createImageFromManifest(_, next) {
+        if (newImage) {
+            next();
+            return;
+        }
+
+        log.debug({ data: manifest },
+            'dockerDownloadAndImportLayer: create it');
+        ctx.Image.create(app, manifest, true, false, function (cErr, img) {
+            if (cErr) {
+                return next(cErr);
+            }
+            newImage = img;
+            next();
+        });
+    }
+
+    function addImageFile(arg, next) {
+        log.debug({digest: digest}, 'AddImageFile: start');
+
+        var connectionTimeoutHandler;
+        var DOCKER_READ_STREAM_TIMEOUT = 15 * 1000;
+        var md5sum = crypto.createHash('md5');
+        // Send a progress message whenever we've downloaded at least
+        // `progUpdateEvery` data (i.e. every 1/2 MiB).
+        var progLastUpdateSize = 0;
+        var progUpdateEvery = 512 * 1024;
+        var resp = arg.httpResponse;
+        var shasum = crypto.createHash('sha1');
+        var sha256sum = crypto.createHash('sha256');
+        var size = 0;
+        var startTs = Math.floor(new Date().getTime() / 1000);
+        var stor;  // the storage class
+        var stream = arg.stream;
+
+        progressStatus('Pulling fs layer');
+
+        if (resp.headers['content-length'] !== undefined) {
+            fileSize = Number(resp.headers['content-length']);
+            if (fileSize > constants.MAX_IMAGE_SIZE) {
+                // Using ImageFileTooBigError instead of DownloadError so the
+                // caller doesn't retry to download.
+                return next(new errors.ImageFileTooBigError(format(
+                    'Image file size, %s, exceeds the maximum allowed ' +
+                    'file size, %s', fileSize, constants.MAX_IMAGE_SIZE_STR)));
+            }
+        }
+
+        // Setup a response timeout listener to handle connection timeout.
+        assert.object(resp.connection, 'resp.connection');
+
+        resp.connection.setTimeout(DOCKER_READ_STREAM_TIMEOUT);
+        connectionTimeoutHandler = function onDockerConnectionTimeout() {
+            log.info({digest: digest, size: size, fileSize: fileSize},
+                'dockerDownloadAndImportImage: '
+                + 'createBlobReadStream connection timed out');
+            progressStatus('Connection timed out');
+            // Note that by destroying the stream this will result in a
+            // call to finish() with an error, as the drc
+            // createBlobReadStream handler has an 'end' handler that
+            // validates the size and digest of downloaded data and
+            // emits an error event when all the data wasn't downloaded.
+            resp.destroy();
+        };
+        resp.connection.on('timeout', connectionTimeoutHandler);
+
+        function finish_(fErr, tmpFilename, filename) {
+            // Remove connection timeout handler.
+            resp.connection.removeListener('timeout', connectionTimeoutHandler);
+            connectionTimeoutHandler = null;
+
+            if (fErr) {
+                log.info({digest: digest, err: fErr},
+                    'dockerDownloadAndImportImage: error');
+                return next(fErr);
+            } else if (ctx.downloadsCanceled) {
+                return next(new errors.DownloadError('Download canceled'));
+            } else if (size > constants.MAX_IMAGE_SIZE) {
+                // Using ImageFileTooBigError instead of DownloadError so the
+                // caller doesn't retry to download.
+                return next(new errors.ImageFileTooBigError(format(
+                    'Image file size, %s, exceeds the maximum allowed ' +
+                    'file size, %s', size, constants.MAX_IMAGE_SIZE_STR)));
+            } else if (fileSize >= 0 && size !== fileSize) {
+                return next(new errors.DownloadError(format(
+                    'Download error: "Content-Length" header, %s, does ' +
+                    'not match downloaded size, %d', fileSize, size)));
+            }
+
+            var sha1 = shasum.digest('hex');
+            var fileDigest = 'sha256:' + sha256sum.digest('hex');
+
+            // Validate the sha256 of the downloaded bits matches the
+            // digest, if they don't match there is a corruption.
+            if (fileDigest !== digest) {
+                // Note that we don't cleanup the failed image file download,
+                // that is handled by IMGAPI-616.
+                log.warn({expectedDigest: digest, fileDigest: fileDigest},
+                    'Downloaded layer digest does not match');
+                next(new errors.DownloadError(format(
+                    'layer digest does not match, got %s, expected %s',
+                    fileDigest, digest)));
+                return;
+            }
+
+            var file = {
+                sha1: sha1,
+                digest: fileDigest,
+                size: size,
+                contentMD5: md5sum.digest('base64'),
+                mtime: (new Date()).toISOString(),
+                stor: stor.type,
+                compression: compression
+            };
+
+            ctx.newFileInfoFromUuid[uuid] = {
+                file: file,
+                image: newImage,
+                storage: stor.type,
+                tmpFilename: tmpFilename,
+                filename: filename
+            };
+
+            log.info({digest: digest, fileSize: fileSize},
+                'dockerDownloadAndImportImage: Download successful');
+            return next();
+        }
+        var finish = once(finish_);
+
+        stream.on('data', function (chunk) {
+            if (ctx.downloadsCanceled) {
+                stream.destroy();
+                progressStatus('Aborted');
+                return;
+            }
+
+            size += chunk.length;
+            if (size > constants.MAX_IMAGE_SIZE) {
+                finish(new errors.DownloadError(format(
+                    'Download error: image file size exceeds the ' +
+                    'maximum allowed size, %s', constants.MAX_IMAGE_SIZE_STR)));
+            }
+            shasum.update(chunk, 'binary');
+            sha256sum.update(chunk, 'binary');
+            md5sum.update(chunk, 'binary');
+
+            if ((size - progLastUpdateSize) > progUpdateEvery) {
+                progressStatus('Downloading', {
+                    current: size,
+                    total: fileSize,
+                    start: startTs
+                });
+                progLastUpdateSize = size;
+            }
+        });
+
+        stream.on('error', function (streamErr) {
+            finish(errors.wrapErrorFromDrc(streamErr));
+        });
+
+        stor = app.chooseStor(newImage);
+        stor.storeFileFromStream({
+            image: newImage,
+            stream: stream,
+            reqId: resp.id(),
+            filename: 'file0',
+            noStreamErrorHandler: true
+        }, function (sErr, tmpFilename, filename) {
+            if (sErr) {
+                log.error({err: sErr, digest: digest},
+                    'error storing image file');
+                finish(errors.parseErrorFromStorage(
+                    sErr, 'error receiving image file'));
+            } else {
+                finish(null, tmpFilename, filename);
+            }
+        });
+    }
+
+    function addUncompressedDigest(arg, next) {
+        var newFileInfo = ctx.newFileInfoFromUuid[uuid];
+        assert.object(newFileInfo, 'newFileInfo');
+        assert.object(newFileInfo.file, 'newFileInfo.file');
+
+        if (compression === 'none') {
+            // Same sha256 - as there is no compression.
+            newFileInfo.file.uncompressedDigest = 'sha256:'
+                + newFileInfo.sha256;
+            next();
+            return;
+        }
+
+        if (uncompressedDigest) {
+            newFileInfo.file.uncompressedDigest = uncompressedDigest;
+            next();
+            return;
+        }
+
+        // Data is still piping to the uncompress/sha256 function, set and wait
+        // for it's callback.
+        arg.uncompressedDigestCallback = function (err, uDigest) {
+            if (!err) {
+                newFileInfo.file.uncompressedDigest = uDigest;
+                progressStatus('Uncompression completed');
+            }
+            next(err);
+        };
+
+        progressStatus('Uncompressing layer');
+    }
+}
+
+
+function _dockerDownloadAndImportLayerWithRetries(opts, callback) {
+    assert.object(opts, 'opts');
+    assert.optionalNumber(opts.addImageFileAttempt, 'opts.addImageFileAttempt');
+    assert.object(opts.ctx, 'opts.ctx');
+    assert.object(opts.ctx.req, 'opts.ctx.req');
+    assert.object(opts.ctx.req.log, 'opts.ctx.req.log');
+    assert.func(opts.ctx.resMessage, 'opts.ctx.resMessage');
+    assert.string(opts.digest, 'opts.digest');
+    assert.func(callback, 'callback');
+
+    var MAX_IMAGE_FILE_DOWNLOAD_ATTEMPTS = 5;
+    var addImageFileAttempt = opts.addImageFileAttempt || 0;
+    var ctx = opts.ctx;
+    var digest = opts.digest;
+    var log = ctx.req.log;
+
+    function retryDownload(err) {
+        addImageFileAttempt += 1;
+
+        // Abort if we've exceeded the maximum retry attempts.
+        if (addImageFileAttempt >= MAX_IMAGE_FILE_DOWNLOAD_ATTEMPTS) {
+            log.info({digest: digest},
+                'dockerDownloadAndImportImage: download failed after '
+                + '%d attempts', addImageFileAttempt);
+            callback(err);
+            return;
+        }
+
+        // Give a short respite and then go again.
+        setTimeout(function _retryDockerImgDownload() {
+            if (ctx.downloadsCanceled) {
+                log.info({err: err, digest: digest,
+                    addImageFileAttempt: addImageFileAttempt},
+                    'dockerDownloadAndImportImage: not retrying, ' +
+                    'download already canceled');
+                callback(new errors.DownloadError(err, 'Download canceled'));
+                return;
+            }
+
+            log.info({digest: digest, addImageFileAttempt: addImageFileAttempt},
+                'dockerDownloadAndImportImage: retrying blob download');
+            opts.addImageFileAttempt = addImageFileAttempt;
+            _dockerDownloadAndImportLayerWithRetries(opts, callback);
+        }, 1000);
+    }
+
+    dockerDownloadAndImportLayer(opts, function _dockerDlImgCb(err, image) {
+        // Return if no error, or the error is not a DownloadError.
+        if (err) {
+            if (err.name === 'DownloadError') {
+                retryDownload(err);
+                return;
+            }
+
+            callback(err);
+            return;
+        }
+
+        callback(null, image);
+    });
+}
+
+
+/*
+ * This function is run as a forEach in adminImportDockerImage. We need
+ * to ensure image objects are added serially into the database.
+ * The function 'this' is bound to be { req: req, res: res }
+ */
+function _dockerActivateImage(newImage, ctx, callback) {
+    var req = ctx.req;
+    var resMessage = ctx.resMessage;
+    var app = req._app;
+    var log = req.log;
+    var digest = ctx.digestFromUuid[newImage.uuid];
+    // If newFileInfo exists, it means a new file was downloaded for this image.
+    var newFileInfo = ctx.newFileInfoFromUuid[newImage.uuid];
+    var shortId = imgmanifest.shortDockerId(
+        imgmanifest.dockerIdFromDigest(digest));
+
+    vasync.pipeline({ funcs: [
+        archiveManifest,
+        addManifestToDb,
+        finishMoveImageLayer,
+        activateImage
+    ]}, function afterPipe(pipeErr, results) {
+        if (pipeErr) {
+            callback(pipeErr);
+            return;
+        }
+
+        if (newFileInfo) {
+            resMessage({
+                type: 'progress',
+                payload: {
+                    id: shortId,
+                    status: 'Pull complete'
+                }
+            });
+        }
+
+        callback();
+    });
+
+    function archiveManifest(_, next) {
+        if (!newFileInfo) {
+            next();
+            return;
+        }
+
+        var local = app.storage.local;
+        var serialized = newImage.serialize(app.mode, '*');
+
+        local.archiveImageManifest(serialized, function (archErr) {
+            if (archErr) {
+                log.error({uuid: newImage.uuid},
+                    'error archiving image manifest:', serialized);
+                return next(archErr);
+            }
+            next();
+        });
+    }
+
+    function addManifestToDb(_, next) {
+        if (!newFileInfo) {
+            next();
+            return;
+        }
+
+        app.db.add(newImage.uuid, newImage.raw, function (addErr) {
+            if (addErr) {
+                log.error({uuid: newImage.uuid},
+                    'error saving to database: raw data:',
+                    newImage.raw);
+                return next(new errors.InternalError(addErr,
+                    'could not create local image'));
+            }
+            app.cacheInvalidateWrite('Image', newImage);
+            next();
+        });
+    }
+
+    function finishMoveImageLayer(_, next) {
+        if (newImage.activated) {
+            next();
+            return;
+        }
+
+        log.debug({digest: digest}, 'MoveImageLayer: start');
+        var stor = app.getStor(newFileInfo.storage);
+
+        stor.moveImageFile(newImage, newFileInfo.tmpFilename,
+                newFileInfo.filename,
+                function (mErr) {
+            if (mErr) {
+                return next(mErr);
+            }
+
+            newImage.addFile(app, newFileInfo.file, req.log, function (err2) {
+                if (err2) {
+                    req.log.error(err2, 'error adding file info to Image');
+                    return next(new errors.InternalError(err2,
+                        'could not save image'));
+                }
+
+                next();
+            });
+        });
+    }
+
+    function activateImage(_, next) {
+        if (newImage.activated) {
+            next();
+            return;
+        }
+
+        resMessage({
+            type: 'progress',
+            payload: {
+                id: shortId,
+                status: 'Activating image'
+            }
+        });
+
+        newImage.activate(app, req.log, next);
+    }
+}
+
+
+/**
+ * Add image history entries.
+ *
+ *  [
+ *    {
+ *      "created": "2016-05-05T18:13:29.963947682Z",
+ *      "author": "Me Now <me@now.com>",
+ *      "created_by": "/bin/sh -c #(nop) MAINTAINER Me Now <me@now.com>",
+ *      "empty_layer": true
+ *    }, {
+ *      "created": "2016-05-05T18:13:30.218788521Z",
+ *      "author": "Me Now <me@now.com>",
+ *      "created_by": "/bin/sh -c #(nop) ADD file:c59222783...364a in /"
+ *    }, {
+ *      "created": "2016-05-05T18:13:30.456465331Z",
+ *      "author": "Me Now <me@now.com>",
+ *      "created_by": "/bin/touch /odd.txt"
+ *    }
+ *  ]
+ */
+function historyEntryFromImageJson(imgJson) {
+    assert.object(imgJson.container_config, 'imgJson.container_config');
+    assert.arrayOfString(imgJson.container_config.Cmd,
+        'imgJson.container_config.Cmd');
+
+    var entry = {
+        created: imgJson.created,
+        created_by: imgJson.container_config.Cmd.join(' ')
+    };
+
+    if (isMetadataCmd(entry.created_by)) {
+        entry.empty_layer = true;
+    }
+    if (imgJson.author) {
+        entry.author = imgJson.author;
+    }
+    if (imgJson.comment) {
+        entry.comment = imgJson.comment;
+    }
+
+    return entry;
+}
+
+/**
+ * Create a docker config object from the given arguments.
+ *
+ * @param layers {Array} Info on each layer (digest, imgFile, etc...).
+ * @param fakeIt {Boolean} Create placeholder rootfs information.
+ *
+ * @returns {Object} The docker config object.
+ */
+function createImgJsonFromLayers(layers, fakeIt) {
+    assert.arrayOfObject(layers, 'layers');
+    assert.optionalBool(fakeIt, 'fakeIt');
+
+    var imgJson = utils.objCopy(layers.slice(-1)[0].imgJson);
+    if (imgJson.hasOwnProperty('id')) {
+        delete imgJson.id;   // No longer needed.
+    }
+    imgJson.history = layers.map(function (layer) {
+        return historyEntryFromImageJson(layer.imgJson);
+    });
+
+    assert.equal(layers.length, imgJson.history.length,
+        'Layers and image history must be the same length');
+
+    /**
+     * Add RootFS layers.
+     *
+     * {
+     *   "type": "layers",
+     *   "diff_ids": [
+     *       "sha256:3f69a7949970fe2d62a5...c65003d01ac3bbe8645d574b",
+     *       "sha256:f980315eda5e9265282c...41b30de83027a2077651b465",
+     *       "sha256:30785cd7f84479984348...533457f3a5dcf677d0b0c51e"
+     *   ]
+     * }
+     */
+    var nonEmptyLayers = layers.filter(function _filterEmpty(layer, idx) {
+        return !imgJson.history[idx].empty_layer;
+    });
+    imgJson.rootfs = {
+        type: 'layers',
+        diff_ids: nonEmptyLayers.map(function _getRootfsDiffId(layer) {
+            if (!layer.uncompressedDigest && fakeIt) {
+                return '';
+            }
+            assert.string(layer.uncompressedDigest);
+            return layer.uncompressedDigest;
+        })
+    };
+
+    return imgJson;
+}
+
+
+/**
+ * Create a docker manifest object (schemaVersion 2) from the given arguments.
+ *
+ * @param imgJson {Object} The docker image config.
+ * @param layers {Array} Info on each layer (digest, imgFile, etc...).
+ * @param fakeIt {Boolean} Create placeholder layer information.
+ *
+ * @returns {Object} The docker manifest object.
+ */
+function createV2Manifest(imgJson, layers, fakeIt) {
+    assert.object(imgJson, 'imgJson');
+    assert.arrayOfObject(imgJson.history, 'imgJson.history');
+    assert.arrayOfObject(layers, 'layers');
+    assert.optionalBool(fakeIt, 'fakeIt');
+
+    assert.equal(imgJson.history.length, layers.length,
+        'history length should equal layers length');
+
+    var imageStr = JSON.stringify(imgJson);
+    var imageDigest = 'sha256:' + crypto.createHash('sha256')
+        .update(imageStr, 'binary').digest('hex');
+
+    var manifest = {
+        schemaVersion: 2,
+        mediaType: 'application/vnd.docker.distribution.manifest.v2+json',
+        config: {
+            'mediaType': 'application/vnd.docker.container.image.v1+json',
+            'size': imageStr.length,
+            'digest': imageDigest
+        },
+        layers: layers.filter(function _filterLayers(layer, idx) {
+            return !(imgJson.history[idx].empty_layer);
+        }).map(function _mapLayers(layer) {
+            assert.string(layer.digest, 'layer.digest');
+            // If we have an imgManifest, then we have already downloaded the
+            // file/layer and thus we don't need to fake it, as we have the
+            // information we need to create the docker layer information.
+            if (!layer.imgManifest && fakeIt) {
+                // Fake it until you make it.
+                return {
+                    digest: layer.digest,
+                    mediaType: '(unknown)'
+                };
+            }
+            assert.object(layer.imgFile, 'layer.imgFile');
+            assert.string(layer.compression, 'layer.compression');
+            var compressionSuffix = '';
+            if (layer.compression && layer.compression !== 'none') {
+                compressionSuffix = '.' + layer.compression;
+            }
+            return {
+                digest: layer.digest,
+                mediaType: 'application/vnd.docker.image.rootfs.diff.tar' +
+                    compressionSuffix,
+                size: layer.imgFile.size
+            };
+        })
+    };
+
+    return manifest;
+}
+
+
+function compressionFromMediaType(mediaType) {
+    switch (mediaType) {
+        case 'application/vnd.docker.image.rootfs.diff.tar.bzip2':
+            return 'bzip2';
+        case 'application/vnd.docker.image.rootfs.diff.tar.gzip':
+            return 'gzip';
+        case 'application/vnd.docker.image.rootfs.diff.tar.xz':
+            return 'xz';
+        case 'application/vnd.docker.image.rootfs.diff.tar':
+        case 'application/vnd.docker.image.rootfs.diff':
+            return 'none';
+        default:
+            return undefined;
+    }
+    return undefined;
+}
+
+
+/* BEGIN JSSTYLED */
+/*
+ * Pull a docker image (with schemaVersion === 2) using v2 registry.
+ *
+ * Example Docker-docker pull:
+ *
+ *  $ docker pull alpine@sha256:fb9f16730ac6316afa4d97caa5130219927bfcecf0b0ce35c01dcb612f449739
+ *  sha256:fb9f16730ac6316afa4d97caa5130219927bfcecf0b0ce35c01dcb612f449739: Pulling from library/alpine
+ *  f4fddc471ec2: Pull complete
+ *  library/alpine:sha256:fb9f16730ac6316afa4d97caa5130219927bfcecf0b0ce35c01dcb612f449739: The image you are pulling has been verified. Important: image verification is a tech preview feature and should not be relied on to provide security.
+ *  Digest: sha256:fb9f16730ac6316afa4d97caa5130219927bfcecf0b0ce35c01dcb612f449739
+ *  Status: Downloaded newer image for alpine@sha256:fb9f16730ac6316afa4d97caa5130219927bfcecf0b0ce35c01dcb612f449739
+ *
+ *  $ docker pull alpine@sha256:fb9f16730ac6316afa4d97caa5130219927bfcecf0b0ce35c01dcb612f449739
+ *  sha256:fb9f16730ac6316afa4d97caa5130219927bfcecf0b0ce35c01dcb612f449739: Pulling from library/alpine
+ *  f4fddc471ec2: Already exists
+ *  Digest: sha256:fb9f16730ac6316afa4d97caa5130219927bfcecf0b0ce35c01dcb612f449739
+ *  Status: Image is up to date for alpine@sha256:fb9f16730ac6316afa4d97caa5130219927bfcecf0b0ce35c01dcb612f449739
+ */
+/* END JSSTYLED */
+function _dockerV22Pull(ctx, cb) {
+    assert.object(ctx, 'ctx');
+    assert.string(ctx.dockerImageVersion, 'ctx.dockerImageVersion');
+    assert.string(ctx.manifestDigest, 'ctx.manifestDigest');
+    assert.object(ctx.manifestV2, 'ctx.manifestV2');
+    assert.object(ctx.manifestV2.config, 'ctx.manifest.config');
+    assert.object(ctx.rat, 'ctx.rat');
+    assert.object(ctx.regClientV2, 'ctx.regClientV2');
+    assert.func(ctx.resMessage, 'ctx.resMessage');
+    assert.optionalObject(ctx.config, 'ctx.config');
+    assert.func(cb, 'cb');
+
+    var cacheDownloadedLayers = false;
+    var configDigest = ctx.manifestV2.config.digest;
+    var imgJson = ctx.imgJson;
+    var req = ctx.req;
+    var log = req.log;
+    var resMessage = ctx.resMessage;
+    var rat = ctx.rat;
+    var tag = rat.tag;
+    var digest = rat.digest;
+
+    // Get the docker config layer id, from the manifest, then fetch the config
+    // details.
+
+    // Send initial status message.
+    resMessage({
+        type: 'status',
+        payload: {
+            /*
+             * When pulling all images in a repository is supported
+             * Docker-docker says: 'Pulling repository $localName'.
+             */
+            status: format('%s: Pulling from %s (req %s)',
+                tag || digest, rat.localName, req.getId())
+        }
+    });
+
+    vasync.pipeline({funcs: [
+        function downloadImgJson(_, next) {
+            if (imgJson) {
+                // This should only be when upconverting a v2.1 image.
+                assert.equal(ctx.dockerImageVersion, '2.1',
+                    'ctx.dockerImageVersion');
+                next();
+                return;
+            }
+            ctx.regClientV2.createBlobReadStream({digest: configDigest},
+                function (err, stream, res_) {
+                if (err) {
+                    next(errors.wrapErrorFromDrc(err));
+                    return;
+                }
+                // Read, validate and store the config.
+                log.debug({digest: configDigest},
+                    'downloadConfig:: stream started');
+
+                var configStr = '';
+                var hadErr = false;
+                var sha256sum = crypto.createHash('sha256');
+
+                stream.on('end', function _downloadConfigStreamEnd() {
+                    if (hadErr) {
+                        return;
+                    }
+                    log.debug({digest: configDigest},
+                        'downloadConfig:: stream ended, config: %s', configStr);
+                    var fileDigest = 'sha256:' + sha256sum.digest('hex');
+                    if (fileDigest !== configDigest) {
+                        log.warn({expectedDigest: configDigest,
+                            fileDigest: fileDigest},
+                            'Downloaded config digest does not match');
+                        next(new errors.DownloadError(format(
+                            'config digest does not match, got %s, expected %s',
+                            fileDigest, configDigest)));
+                        return;
+                    }
+                    // Convert into JSON.
+                    try {
+                        imgJson = JSON.parse(configStr);
+                    } catch (configErr) {
+                        next(new errors.ValidationFailedError(configErr, format(
+                            'invalid JSON for docker config, digest %s, err %s',
+                            configDigest, configErr)));
+                        return;
+                    }
+                    next();
+                });
+
+                stream.on('error', function _downloadConfigStreamError(sErr) {
+                    hadErr = true;
+                    log.warn({digest: configDigest},
+                        'downloadConfig:: error downloading config: %s', sErr);
+                    stream.destroy();
+                    next(new errors.DownloadError(format(
+                        'error downloading config with digest %s, %s',
+                        configDigest, sErr)));
+                    return;
+                });
+
+                stream.on('data', function _downloadConfigStreamData(chunk) {
+                    if (ctx.downloadsCanceled) {
+                        stream.destroy();
+                        return;
+                    }
+                    configStr += String(chunk);
+                    sha256sum.update(chunk, 'binary');
+                });
+
+                // Stream is paused, so get it moving again.
+                stream.resume();
+            });
+        },
+
+        function determineLayers(_, next) {
+            assert.arrayOfObject(imgJson.history, 'imgJson.history');
+            assert.arrayOfString(imgJson.rootfs.diff_ids,
+                'imgJson.rootfs.diff_ids');
+            assert.arrayOfObject(ctx.manifestV2.layers,
+                'ctx.manifestV2.layers');
+            // History is from oldest change (index 0) to the newest change. For
+            // each entry in history, there should be a corresponding entry in
+            // both the manifestV2.layers array and the imgJson.rootfs.diff_ids
+            // array, except in the case the history entry has a 'empty_layer'
+            // attribute set to true.
+            var layerDigests = [];
+            var layerInfos = [];
+            var idx = -1;
+            imgJson.history.forEach(function _histForEach(h, pos) {
+                // Emulate Docker's synthetic image config for each layer, so we
+                // can later generate the history entries. For reference, see:
+                // JSSTYLED
+                // https://github.com/docker/distribution/blob/docker/1.13/docs/spec/manifest-v2-2.md#backward-compatibility
+                var layerImgJson = {
+                    created: h.created,
+                    container_config: {
+                      Cmd: [ h.created_by ]
+                    }
+                };
+                if (pos === (imgJson.history.length - 1)) {
+                    // Last layer uses the original imgJson.
+                    layerImgJson = imgJson;
+                }
+
+                if (h.empty_layer) {
+                    layerInfos.push({
+                        historyEntry: h,
+                        imgJson: layerImgJson,
+                        layerDigests: layerDigests.slice()  // A copy
+                    });
+                    return;
+                }
+
+                idx += 1;
+                var compression = compressionFromMediaType(
+                    ctx.manifestV2.layers[idx].mediaType);
+                var layerDigest = ctx.manifestV2.layers[idx].digest;
+                var id = imgmanifest.dockerIdFromDigest(layerDigest);
+                layerDigests.push(layerDigest);
+                layerInfos.push({
+                    compression: compression,
+                    digest: layerDigest,
+                    historyEntry: h,
+                    imgJson: layerImgJson,
+                    layerDigests: layerDigests.slice(),  // A copy
+                    uncompressedDigest: imgJson.rootfs.diff_ids[idx]
+                });
+                resMessage({
+                    type: 'status',
+                    payload: {
+                        id: imgmanifest.shortDockerId(id),
+                        progressDetail: {},
+                        status: 'Pulling fs layer'
+                    }
+                });
+            });
+            ctx.layerInfos = layerInfos;
+            next();
+        },
+
+        /*
+         * In *parallel*, create (unactivated) and download the images.
+         */
+        function importImagesPart1(_, next) {
+            cacheDownloadedLayers = true;
+            ctx.downloadsCanceled = false;
+            var pullQueueError;
+
+            var pullQueue = vasync.queue(function (layerInfo, nextLayer) {
+                if (layerInfo.historyEntry.empty_layer) {
+                    // Nothing to download for this layer.
+                    nextLayer();
+                    return;
+                }
+
+                _dockerDownloadAndImportLayerWithRetries({
+                    compression: layerInfo.compression,
+                    digest: layerInfo.digest,
+                    layerDigests: layerInfo.layerDigests,
+                    imgJson: layerInfo.imgJson,
+                    uncompressedDigest: layerInfo.uncompressedDigest,
+                    ctx: ctx
+                }, function _dockerDownloadLayerCb(err, image) {
+                    if (err) {
+                        log.info({err: err, digest: layerInfo.digest},
+                            'dockerDownloadAndImportLayerWithRetries err');
+                        nextLayer(err);
+                        return;
+                    }
+
+                    var file0;
+                    if (ctx.newFileInfoFromUuid.hasOwnProperty(image.uuid)) {
+                        file0 = ctx.newFileInfoFromUuid[image.uuid].file;
+                    } else {
+                        // Existing image.
+                        file0 = image.files[0];
+                    }
+                    layerInfo.imgFile = file0;
+                    layerInfo.imgManifest = image;
+                    nextLayer();
+                });
+            }, 5);
+
+            pullQueue.on('end', function () {
+                next(pullQueueError);
+            });
+
+            pullQueue.push(ctx.layerInfos, function (qErr) {
+                if (qErr) {
+                    log.debug(qErr, 'dockerDownloadAndImportLayer err');
+                }
+                if (qErr && pullQueueError === undefined) {
+                    pullQueueError = qErr;
+                    ctx.downloadsCanceled = true;
+                    pullQueue.kill();
+                }
+            });
+            pullQueue.close();
+        },
+
+        function recalculateConfigAndManifest(_, next) {
+            if (ctx.dockerImageVersion !== '2.1') {
+                next();
+                return;
+            }
+            // Update compression and digest for all file layers.
+            ctx.layerInfos.forEach(function (layer) {
+                if (layer.historyEntry.empty_layer) {
+                    return;
+                }
+                assert.object(layer.imgFile, 'layer.imgFile');
+                assert.string(layer.imgFile.compression,
+                    'layer.imgFile.compression');
+                assert.string(layer.imgFile.uncompressedDigest,
+                    'layer.imgFile.uncompressedDigest');
+                layer.compression = layer.imgFile.compression;
+                layer.uncompressedDigest = layer.imgFile.uncompressedDigest;
+            });
+            imgJson = createImgJsonFromLayers(ctx.layerInfos);
+            ctx.manifestV2 = createV2Manifest(imgJson, ctx.layerInfos);
+            ctx.manifestStr = JSON.stringify(ctx.manifestV2, null, 4);
+            ctx.manifestDigest = 'sha256:' + crypto.createHash('sha256')
+                .update(ctx.manifestStr, 'binary').digest('hex');
+            configDigest = ctx.manifestV2.config.digest;
+
+            log.debug({manifest: ctx.manifestV2},
+                'recalculateConfigAndManifest');
+
+            next();
+        },
+
+        function addSdcDockerImage(_, next) {
+            // Create the sdc-docker image in the docker_images bucket.
+
+            // Calculate total size and find the last image uuid.
+            var finalUuid;
+            var size = ctx.layerInfos.map(function (layer) {
+                if (!layer.imgManifest) {
+                    return 0;
+                }
+                assert.object(layer.imgFile, 'layer.imgFile');
+                finalUuid = layer.imgManifest.uuid;
+                return layer.imgFile && layer.imgFile.size || 0;
+            }).reduce(function (a, b) { return a + b; }, 0);
+
+            ctx.resMessage({
+                type: 'create-docker-image',
+                config_digest: configDigest,
+                head: true,
+                image: imgJson,
+                image_uuid: finalUuid,
+                manifest_digest: ctx.manifestDigest,
+                manifest_str: ctx.manifestStr,
+                size: size,
+                dockerImageVersion: ctx.dockerImageVersion
+            });
+            next();
+        },
+
+        /*
+         * *Serially* complete the import of all the images:
+         * - We only ActivateImage's at this stage after the file downloading
+         *   (anticipated to be the most error-prone stage).
+         * - We activate images in layerInfos order (parent before child) for
+         *   db consistency.
+         */
+        function importImagesPart2(_, next) {
+            cacheDownloadedLayers = false;
+            vasync.forEachPipeline({
+                inputs: ctx.layerInfos,
+                func: function (layerInfo, nextLayer) {
+                    assert.object(layerInfo.historyEntry,
+                        'layerInfo.historyEntry');
+                    if (layerInfo.historyEntry.empty_layer) {
+                        nextLayer();
+                        return;
+                    }
+                    assert.object(layerInfo.imgManifest,
+                        'layerInfo.imgManifest');
+                    _dockerActivateImage(layerInfo.imgManifest, ctx, nextLayer);
+                }
+            }, function (vErr, results) {
+                next(vErr);
+            });
+        },
+
+        function finishingMessages(_, next) {
+            resMessage({
+                type: 'status',
+                payload: {
+                    status: 'Digest: ' + ctx.manifestDigest
+                }
+            });
+
+            var status = (ctx.newFileInfoFromUuid.length === 0
+                ? 'Status: Image is up to date for ' + ctx.repoAndRef
+                : 'Status: Downloaded newer image for ' + ctx.repoAndRef);
+            resMessage({
+                type: 'status',
+                payload: {status: status}
+            });
+
+            next();
+        }
+    ]}, function (err) {
+        if (cacheDownloadedLayers) {
+            // There was a failure downloading one or more image layers - keep
+            // the downloaded image metadata in memory, so we can avoid
+            // downloading it again next time.
+            Object.keys(ctx.newFileInfoFromUuid).forEach(function (u) {
+                DOCKER_IMAGE_CACHE[u] = ctx.newFileInfoFromUuid[u];
+            });
+        }
+
+        cb(err);
+    });
+}
+
+
+/*
+ * Pull a docker image using v2.1 image manifest format (schemaVersion === 1).
+ *
+ * We upconvert the manifest into a v2.2 format and then have the _dockerV22Pull
+ * function do the bulk of the layer download work. Note that we have to fake a
+ * part of the config and manifest, as we don't know the uncompressed digest or
+ * the compression of the layer at this time - luckily the _dockerV22Pull can
+ * work this out for us and then regenerates the config and manifest once this
+ * information is known.
+ */
+function _dockerV21Pull(ctx, callback) {
+    assert.object(ctx, 'ctx');
+    assert.string(ctx.manifestDigest, 'ctx.manifestDigest');
+    assert.object(ctx.manifestV2, 'ctx.manifestV2');
+    assert.object(ctx.req, 'ctx.req');
+    assert.func(callback, 'callback');
+
+    var req = ctx.req;
+    var log = req.log;
+    var layerDigest;
+    var layerDigests = [];
+
+    var layerInfos = [];  // Docker image info with the base image first.
+    for (var i = ctx.manifestV2.history.length - 1; i >= 0; i--) {
+        var imgJson;
+        try {
+            imgJson = JSON.parse(ctx.manifestV2.history[i].v1Compatibility);
+        } catch (manifestErr) {
+            return callback(
+                new errors.ValidationFailedError(manifestErr, format(
+                'invalid "v1Compatibility" JSON in docker manifest: %s (%s)',
+                manifestErr, ctx.manifestV2.history[i].v1Compatibility)));
+        }
+        layerDigest = ctx.manifestV2.fsLayers[i].blobSum;
+        layerDigests.push(layerDigest);
+        layerInfos.push({
+            digest: layerDigest,
+            imgJson: imgJson,
+            layerDigests: layerDigests.slice(),  // A copy - not a reference.
+            shortId: imgmanifest.shortDockerId(
+                imgmanifest.dockerIdFromDigest(layerDigest))
+        });
+    }
+
+    log.debug({manifestV21: ctx.manifest},
+        'Upconverting manifest from v2.1 to v2.2');
+    var fakeIt = true;
+    ctx.imgJson = createImgJsonFromLayers(layerInfos, fakeIt);
+    ctx.manifestV2 = createV2Manifest(ctx.imgJson, layerInfos, fakeIt);
+    ctx.manifestStr = JSON.stringify(ctx.manifestV2, null, 4);
+    ctx.manifestDigest = 'sha256:' + crypto.createHash('sha256')
+        .update(ctx.manifestStr, 'binary').digest('hex');
+
+    _dockerV22Pull(ctx, callback);
+}
+
+
+/* BEGIN JSSTYLED */
+/**
+ * Import a given docker repo:tag while streaming out progress messages.
+ * Typically this is called by the sdc-docker 'pull-image' workflow.
+ *
+ * Progress messages are one of the following (expanded here for clarity):
+ *
+ *      {
+ *          "type":"status",
+ *          // Used by the calling 'pull-image' workflow, and ultimately the
+ *          // sdc-docker service to know which open client response this
+ *          // belongs to.
+ *          "id":"docker.io/busybox",
+ *          // Per http://docs.docker.com/reference/api/docker_remote_api_v1.18/#create-an-image
+ *          "payload":{"status":"Pulling repository busybox"}
+ *      }
+ *
+ *      {"type":"progress","payload":{"id":"8c2e06607696","status":"Pulling dependent layers"},"id":"docker.io/busybox"}
+ *      {"type":"progress","id":"docker.io/busybox","payload":{"id":"8c2e06607696","status":"Pulling metadata."}}
+ *
+ *      {
+ *          "type":"create-docker-image",
+ *          // Docker image. TODO: How used by caller?
+ *          "image":{"container":"39e79119...
+ *      }
+ *
+ *      {
+ *          "type": "error",
+ *          "error": {
+ *              "code": "<CodeString>",
+ *              "message": <error message>
+ *          }
+ *      }
+ *
+ * Dev Note: For now we only allow a single tag to be pulled. To eventually
+ * support `docker pull -a ...` we'll likely want to support an empty `tag`
+ * here to mean all tags.
+ */
+/* END JSSTYLED */
+function adminImportDockerImage(opts, callback) {
+    assert.object(opts, 'opts');
+    assert.func(opts.Image, 'opts.Image');
+    assert.object(opts.req, 'opts.req');
+    assert.object(opts.res, 'opts.res');
+    assert.func(callback, 'callback');
+
+    var req = opts.req;
+    var res = opts.res;
+    var log = req.log;
+
+    // Validate inputs.
+    var errs = [];
+    var repo = req.query.repo;
+    var tag = req.query.tag;
+    var digest = req.query.digest;
+    if (req.query.account) {
+        return callback(new errors.OperatorOnlyError());
+    }
+    if (repo === undefined) {
+        errs.push({ field: 'repo', code: 'MissingParameter' });
+    }
+    if (!tag && !digest) {
+        errs.push({
+            field: 'tag',
+            code: 'MissingParameter',
+            message: 'one of "tag" or "digest" is required'
+        });
+    } else if (tag && digest) {
+        errs.push({
+            field: 'digest',
+            code: 'Invalid',
+            message: 'cannot specify both "tag" and "digest"'
+        });
+    }
+    if (errs.length) {
+        log.debug({errs: errs}, 'apiAdminImportDockerImage error');
+        return callback(new errors.ValidationFailedError(
+            'missing parameters', errs));
+    }
+
+    var rat;
+    try {
+        rat = drc.parseRepoAndRef(
+            repo + (tag ? ':'+tag : '@'+digest));
+    } catch (e) {
+        return callback(new errors.ValidationFailedError(
+            e,
+            e.message || e.toString(),
+            [ {field: 'repo', code: 'Invalid'}]));
+    }
+
+    var regAuth;
+    var username, password;
+    if (req.headers['x-registry-config']) {
+        /*
+         * // JSSTYLED
+         * https://github.com/docker/docker/blob/master/docs/reference/api/docker_remote_api_v1.23.md#build-image-from-a-dockerfile
+         *
+         * The 'x-registry-config' header is a map of the registry host name to
+         * the registry auth (see 'x-registry-auth') below.
+         */
+        try {
+            var regConfig = JSON.parse(new Buffer(
+                req.headers['x-registry-config'], 'base64').toString('utf8'));
+        } catch (e) {
+            log.info(e, 'invalid x-registry-config header, ignoring');
+        }
+        // Censor for audit logs.
+        req.headers['x-registry-config'] = '(censored)';
+
+        // Find registry auth from the registry hostname map. Note that Docker
+        // uses a special legacy name for the "official" docker registry, so
+        // check for that too.
+        var dockerV1CompatName = 'https://index.docker.io/v1/';
+        var indexName = rat.index.name;
+        if (regConfig.hasOwnProperty(indexName)) {
+            regAuth = regConfig[indexName];
+        } else if (indexName === 'docker.io' &&
+            regConfig.hasOwnProperty(dockerV1CompatName))
+        {
+            regAuth = regConfig[dockerV1CompatName];
+        }
+    }
+    if (req.headers['x-registry-auth']) {
+        /*
+         * // JSSTYLED
+         * https://github.com/docker/docker/blob/master/docs/reference/api/docker_remote_api_v1.23.md#create-an-image
+         *
+         * The 'x-registry-auth' header contains `username` and `password`
+         * *OR* a `identitytoken`. We don't yet support identitytoken --
+         * See DOCKER-771.
+         */
+        try {
+            regAuth = JSON.parse(new Buffer(
+                req.headers['x-registry-auth'], 'base64').toString('utf8'));
+        } catch (e) {
+            log.info(e, 'invalid x-registry-auth header, ignoring');
+        }
+        // Censor for audit logs.
+        req.headers['x-registry-auth'] = '(censored)';
+    }
+
+    if (regAuth) {
+        if (regAuth.identitytoken) {
+            callback(new errors.NotImplementedError('OAuth to Docker '
+                + 'registry is not yet supported, please "docker logout" '
+                + 'and "docker login" and try again'));
+            return;
+        } else {
+            username = regAuth.username;
+            password = regAuth.password;
+        }
+    }
+
+    try {
+        var public_ = utils.boolFromString(req.query.public, true, 'public');
+    } catch (publicErr) {
+        return callback(publicErr);
+    }
+
+    function resMessage(data) {
+        data.id = rat.canonicalName;
+        res.write(JSON.stringify(data) + '\r\n');
+    }
+
+    var context = {
+        Image: opts.Image,
+        req: req,
+        res: res,
+        resMessage: resMessage,
+
+        repoAndRef: rat.localName + (rat.tag ? ':'+rat.tag : '@'+rat.digest),
+        rat: rat,
+        regClientOpts: utils.commonHttpClientOpts({
+            name: repo,
+            log: req.log,
+            insecure: req._app.config.dockerRegistryInsecure,
+            maxSchemaVersion: 2,
+            username: username,
+            password: password
+        }, req),
+
+        digestFromUuid: {},  // <uuid> -> <docker digest>
+        newFileInfoFromUuid: {},  // <uuid> -> <file import info>
+        public_: public_
+    };
+    log.trace({rat: context.rat}, 'docker pull');
+
+    vasync.pipeline({arg: context, funcs: [
+        /*
+         * Use Docker Registry v2 to pull - v1 is no longer supported.
+         *
+         * Check v2.getManifest for the tag/digest. If it exists,
+         * then we'll be using v2 for the pull, else error.
+         */
+        function v2GetManifest(ctx, next) {
+            var ref = rat.tag || rat.digest;
+            ctx.regClientV2 = drc.createClientV2(ctx.regClientOpts);
+            ctx.regClientV2.getManifest({ref: ref},
+                    function (err, manifest, res_, manifestStr) {
+                if (err) {
+                    next(errors.wrapErrorFromDrc(err));
+                } else {
+                    log.debug({ref: ref, manifest: manifest},
+                        'v2.getManifest found');
+                    ctx.manifestV2 = manifest;
+                    ctx.manifestStr = manifestStr;
+                    ctx.manifestDigest = res_.headers['docker-content-digest'];
+                    if (!ctx.manifestDigest) {
+                        // Some registries (looking at you Amazon ECR) do not
+                        // provide the docker-content-digest header in the
+                        // response, so we have to calculate it.
+                        ctx.manifestDigest = drc.digestFromManifestStr(
+                            manifestStr);
+                    }
+                    next();
+                }
+            });
+        },
+
+        /*
+         * Determine if this is a private Docker image by trying the same
+         * without auth. The only thing this does is determine `ctx.isPrivate`
+         * so the caller (typically sdc-docker) can note that.
+         * We still used the auth'd client for doing the image pull.
+         *
+         * Note: It is debatable whether we should bother with this. Is there
+         * a need to have this 'isPrivate' boolean?
+         */
+        function determineIfPrivate(ctx, next) {
+            if (!username) {
+                ctx.isPrivate = false;
+                return next();
+            }
+
+            var regClientOpts = utils.objCopy(ctx.regClientOpts);
+            delete regClientOpts.username;
+            delete regClientOpts.password;
+            var noAuthClient;
+
+            var ref = rat.tag || rat.digest;
+            noAuthClient = drc.createClientV2(regClientOpts);
+            noAuthClient.getManifest({ref: ref}, function (err, man, res_) {
+                if (err) {
+                    if (err.statusCode === 404 ||
+                        err.statusCode === 403 ||
+                        err.statusCode === 401)
+                    {
+                        log.debug({ref: ref, code: err.code,
+                            statusCode: err.statusCode}, 'isPrivate: true');
+                        err = null;
+                        ctx.isPrivate = true;
+                    } else {
+                        log.debug({ref: ref, code: err.code,
+                            statusCode: err.statusCode},
+                            'isPrivate: unexpected err code/statusCode');
+                    }
+                } else {
+                    var noAuthDigest = res_.headers['docker-content-digest'];
+                    assert.equal(noAuthDigest, ctx.manifestDigest);
+                    ctx.isPrivate = false;
+                }
+                noAuthClient.close();
+                next(errors.wrapErrorFromDrc(err));
+            });
+        },
+
+        /*
+         * Only now do we start the streaming response. This allows us
+         * to do some sanity validation (we can talk to the registry and have
+         * found an image for the given ref) and return a non-200 status code
+         * on failure. I'm not sure this matches Docker Remote
+         * API's behaviour exactly.
+         */
+        function startStreaming(ctx, next) {
+            res.status(200);
+            res.header('Content-Type', 'application/json');
+
+            if (ctx.manifestV2.schemaVersion === 1) {
+                ctx.dockerImageVersion = '2.1';
+                _dockerV21Pull(ctx, next);
+            } else if (ctx.manifestV2.schemaVersion === 2) {
+                ctx.dockerImageVersion = '2.2';
+                _dockerV22Pull(ctx, next);
+            } else {
+                next(new errors.NotImplementedError(
+                    'unexpected manifest schemaVersion: %d',
+                    ctx.manifestV2.schemaVersion));
+                return;
+            }
+        }
+
+    ]}, function (err) {
+        if (context.regClientV2) {
+            context.regClientV2.close();
+        }
+
+        if (err) {
+            // This is a chunked transfer so we can't return a restify error.
+            log.info(err, 'error pulling image layers for %s',
+                context.repoAndRef);
+            resMessage({
+                type: 'error',
+                id: repo,
+                error: {
+                    /*
+                     * `restify.RestError` instances will have `err.code`. More
+                     * vanilla `restify.HttpError` instances may have
+                     * `err.body.code` (if the response body was JSON with a
+                     * "code"), else the contructor name is a code.
+                     */
+                    code: err.code || (err.body && err.body.code) || err.name,
+                    message: err.message
+                }
+            });
+        }
+        res.end();
+        callback(false);
+    });
+}
+
+
+
+
+//---- exports
+
+module.exports = {
+    adminImportDockerImage: adminImportDockerImage
+};
diff --git a/lib/errors.js b/lib/errors.js
index 7996779..96cd376 100644
--- a/lib/errors.js
+++ b/lib/errors.js
@@ -241,6 +241,26 @@ DownloadError.statusCode = 400;
 DownloadError.description = 'There was a problem with the download.';
 
 
+function ImageFileTooBigError(cause, message) {
+    if (message === undefined) {
+        message = cause;
+        cause = undefined;
+    }
+    RestError.call(this, {
+        restCode: this.constructor.restCode,
+        statusCode: this.constructor.statusCode,
+        message: message,
+        cause: cause
+    });
+}
+util.inherits(ImageFileTooBigError, RestError);
+ImageFileTooBigError.prototype.name = 'ImageFileTooBigError';
+ImageFileTooBigError.restCode = 'ImageFileTooBig';
+ImageFileTooBigError.statusCode = 400;
+ImageFileTooBigError.description =
+    'The image file size exceeds the max allowed size.';
+
+
 function StorageIsDownError(cause) {
     var message = 'storage is down at the moment';
     RestError.call(this, {
diff --git a/lib/images.js b/lib/images.js
index 8d2ffa6..fc6cfd4 100644
--- a/lib/images.js
+++ b/lib/images.js
@@ -22,7 +22,6 @@ var path = require('path');
 
 var assert = require('assert-plus');
 var async = require('async');
-var drc = require('docker-registry-client');
 var imgmanifest = require('imgmanifest');
 var lib_uuid = require('uuid');
 var once = require('once');
@@ -31,6 +30,8 @@ var sdcClients = require('sdc-clients');
 var vasync = require('vasync');
 
 var channels = require('./channels');
+var constants = require('./constants');
+var docker = require('./docker');
 var errors = require('./errors');
 var utils = require('./utils'),
     objCopy = utils.objCopy,
@@ -48,16 +49,7 @@ var TMPDIR = '/var/tmp';
 
 var TOP = path.resolve(__dirname, '..');
 var UUID_RE = /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/;
-var MAX_ICON_SIZE = 128*1024; // 128KiB
-var MAX_ICON_SIZE_STR = '128 KiB';
-var MAX_IMAGE_SIZE = 20*1024*1024*1024; // 20GiB
-var MAX_IMAGE_SIZE_STR = '20 GiB';
 var ICON_CONTENT_TYPES = ['image/jpeg', 'image/gif', 'image/png'];
-// TODO: should use constant from node-imgmanifest
-var VALID_FILE_COMPRESSIONS = ['gzip', 'bzip2', 'xz', 'none'];
-var VALID_STORAGES = ['local', 'manta'];
-
-var UNSET_OWNER_UUID = '00000000-0000-0000-0000-000000000000';
 
 
 /*
@@ -74,17 +66,6 @@ var IMGAPI_251_MIN_PLATFORM = {
     '6.5': '20120614T001014Z'
 };
 
-/*
- * DOCKER-893: Maintain an in-memory image cache (just the image/layer metadata)
- * for the docker layers that have been downloaded, but not yet registered with
- * an image. This is used when a docker image download fails, we keep all the
- * successfully downloaded layers from that image in this cache. If the image is
- * attempted to be downloaded again, this cache is checked to avoid
- * re-downloading those layers.
- */
-var DOCKER_IMAGE_CACHE = {};
-
-
 //---- Image model
 
 /**
@@ -303,6 +284,52 @@ Image.prototype.addFile = function addFile(app, file, log, callback) {
 };
 
 
+/**
+ * Move the image file from this, into the given Image instance.
+ *
+ * @param app {App} The IMGAPI app.
+ * @param file {Object} Describes the uploaded file, with keys:
+ *      - `sha1` {String}
+ *      - `size` {Integer}
+ *      - `stor` {String}
+ *      - `contentMD5` {String}
+ *      - `mtime` {String} ISO date string
+ * @param log {Bunyan Logger}
+ * @param callback {Function} `function (err)` where `err` is some internal
+ *      detail (i.e. it should be wrapped for the user).
+ */
+Image.prototype.moveFileToImage =
+function moveFileToImage(app, toImage, log, callback) {
+    var files = this.files;
+    assert.equal(files.length, 1, 'Expect exactly one image file');
+    var file = files[0];
+    var self = this;
+
+    var stor = app.getStor(file.stor);
+    stor.moveFileBetweenImages(self, toImage, 'file0',
+            function _moveFileCb(err) {
+        if (err) {
+            callback(err);
+            return;
+        }
+        toImage.addFile(app, file, log, function _addFileCb(addErr) {
+            if (addErr) {
+                callback(addErr);
+                return;
+            }
+            // Null out the file fields.
+            self.raw.files = [];
+            delete self._filesCache;
+
+            log.debug({fromUuid: self.uuid, toUuid: toImage.uuid},
+                'Moving file0 between images');
+
+            Image.modify(app, self, log, callback);
+        });
+    });
+};
+
+
 /**
  * Add an uploaded icon to this Image instance. The file will have already
  * be written out (to disk or to manta, depending).
@@ -2083,12 +2110,12 @@ function apiAdminImportImage(req, res, callback) {
              * the 'admin' user. See IMGAPI-408.
              */
             if (app.mode === 'dc') {
-                if (data.owner === UNSET_OWNER_UUID) {
+                if (data.owner === constants.UNSET_OWNER_UUID) {
                     data.owner = app.config.adminUuid;
                     return next();
                 }
             } else {
-                data.owner = UNSET_OWNER_UUID;
+                data.owner = constants.UNSET_OWNER_UUID;
             }
 
             if (skipOwnerCheck) {
@@ -2265,12 +2292,12 @@ function apiAdminImportImageFromSource(req, res, cb) {
              * the 'admin' user. See IMGAPI-408.
              */
             if (app.mode === 'dc') {
-                if (manifest.owner === UNSET_OWNER_UUID) {
+                if (manifest.owner === constants.UNSET_OWNER_UUID) {
                     manifest.owner = app.config.adminUuid;
                     return next();
                 }
             } else {
-                manifest.owner = UNSET_OWNER_UUID;
+                manifest.owner = constants.UNSET_OWNER_UUID;
             }
 
             if (skipOwnerCheck) {
@@ -2398,1474 +2425,184 @@ function apiAdminImportRemoteImage(req, res, callback) {
 }
 
 
-/*
- * Called during `AdminImportDockerImage` to create (unactivated) and
- * download the file for a single type=docker image.
- *
- * @param opts {Object}
- *      - @param ctx {Object} The run context for the
- *        `apiAdminImportRemoteImage` call.
- *      - @param imgId {String} The docker image id.
- *      - @param imgJson {Object} If this is a v2 import, then we'll have the
- *        imgJson already.
- *      - @param fsLayer {Object} If this is a v2 import, this is the object
- *        from `manifest.fsLayers`.
- * @param callback {Function}
- */
-function _dockerDownloadAndImportImage(opts, callback) {
-    assert.string(opts.imgId, 'opts.imgId');
-    assert.object(opts.ctx, 'opts.ctx');
-    assert.func(callback, 'callback');
-    var ctx = opts.ctx;
-    assert.finite(ctx.regV, 'ctx.regV');
-    if (ctx.regV === 1) {
-        assert.object(ctx.regClientV1, 'ctx.regClientV1');
-    } else {
-        assert.object(ctx.regClientV2, 'ctx.regClientV2');
-        assert.object(opts.imgJson, 'ctx.imgJson');
-        assert.object(opts.fsLayer, 'ctx.fsLayer');
-    }
-
-    var imgId = opts.imgId;
-    var imgJson = opts.imgJson;
-    var req = ctx.req;
-    var app = req._app;
-    var log = req.log;
-    var rat = ctx.rat;
-
-    try {
-        var uuid = imgmanifest.imgUuidFromDockerInfo({
-            id: imgId,
-            indexName: rat.index.name
-        });
-    } catch (infoErr) {
-        return callback(infoErr);
+function apiAdminImportDockerImage(req, res, next) {
+    if (req.query.action !== 'import-docker-image') {
+        return next();
     }
-    var active = false;
-    var addImageFileAttempts = 0;
-    var unactivated = false;
-    var fileSize = -1; // The same value used in Docker-docker for "don't know".
-    var manifest;
-    var newImage;
 
-    log.debug({imgId: imgId, uuid: uuid},
-        'AdminImportDockerImage: check if image already exists');
-    Image.get(app, uuid, log, function (gErr, image) {
-        if (!gErr) {
-            assert.object(image, 'image');
-            ctx.imageFromImgId[imgId] = newImage = image;
+    docker.adminImportDockerImage({Image: Image, req: req, res: res}, next);
+}
 
-            if (newImage.state === 'unactivated') {
-                unactivated = true;
-            } else {
-                // TODO: Can we `resMessage('Already exists')` and early abort?
-                active = true;
-                ctx.alreadyExistsFromImgId[imgId] = true;
-            }
 
-            // Mark this Image as existing in the database
-            newImage.exists = true;
+function apiAddImageFile(req, res, next) {
+    if (req.query.source !== undefined)
+        return next();
 
-        } else if (gErr.restCode !== 'ResourceNotFound') {
-            return callback(gErr);
-        }
+    req.log.debug({image: req._image}, 'AddImageFile: start');
 
-        // Check if this image layer has already been downloaded before.
-        if (DOCKER_IMAGE_CACHE.hasOwnProperty(imgId)) {
-            var cachedItem = DOCKER_IMAGE_CACHE[imgId];
-            ctx.fileInfoFromImgId[imgId] = cachedItem.fileInfo;
-            ctx.imageFromImgId[imgId] = cachedItem.image;
-            ctx.alreadyExistsFromImgId[imgId] = true;
+    // Can't change files on an activated image.
+    if (req._image.activated) {
+        return next(new errors.ImageFilesImmutableError(req._image.uuid));
+    }
 
-            log.debug({imgId: imgId, uuid: uuid},
-                'dockerDownloadAndImportImage: image layer already cached');
+    // Validate compression.
+    var compression = req.query.compression;
+    if (!compression) {
+        return next(new errors.InvalidParameterError('missing "compression"',
+            [ { field: 'compression', code: 'Missing' } ]));
+    } else if (constants.VALID_FILE_COMPRESSIONS.indexOf(compression) === -1) {
+        return next(new errors.InvalidParameterError(
+            format('invalid compression "%s" (must be one of %s)',
+                compression, constants.VALID_FILE_COMPRESSIONS.join(', ')),
+            [ { field: 'compression', code: 'Invalid' } ]));
+    }
 
-            callback();
-            return;
+    // Validate requested storage. Only admin requests are allowed to specify.
+    var preferredStorage = req.query.storage;
+    if (preferredStorage && req.query.account) {
+        var error = {
+            field: 'storage',
+            code: 'NotAllowed',
+            message: 'Parameter cannot be specified by non-operators'
+        };
+        return next(new errors.InvalidParameterError(
+            format('invalid storage "%s"', preferredStorage), [error]));
+    } else if (preferredStorage) {
+        if (constants.VALID_STORAGES.indexOf(preferredStorage) === -1) {
+            return next(new errors.InvalidParameterError(
+                format('invalid storage "%s" (must be one of %s)',
+                    preferredStorage, constants.VALID_STORAGES.join(', ')),
+                [ { field: 'storage', code: 'Invalid' } ]));
         }
+    }
 
-        log.debug({uuid: uuid, repo: rat.canonicalName, imgId: imgId},
-            'AdminImportDockerImage: start import');
-
-        vasync.pipeline({ funcs: [
-            getImgJson,
-            genImgapiManifest,
-            handleOwner,
-            handleChannels,
-            createImageFromManifest,
-            addImageFile
-        ]}, function afterPipe(pipeErr, results) {
-            if (pipeErr) {
-                log.info({imgId: imgId, err: pipeErr},
-                    'dockerDownloadAndImportImage: error downloading layer');
-                ctx.resMessage({
-                    type: 'progress',
-                    payload: {
-                        id: imgId.substr(0, 12),
-                        status: format('import error: %s', pipeErr.message)
-                    }
-                });
-                callback(pipeErr);
-                return;
-            }
-
-            ctx.resMessage({
-                type: 'progress',
-                payload: {
-                    id: imgId.substr(0, 12),
-                    status: (active ? 'Already exists' : 'Download complete')
-                }
-            });
-            ctx.resMessage({
-                type: 'data',
-                imgJson: imgJson,
-                image: newImage.serialize(app.mode, req.getVersion()),
-                private: ctx.isPrivate
-            });
-            callback();
-        });
-    });
-
-    function getImgJson(_, next) {
-        if (imgJson) {
-            return next();
+    var contentLength;
+    if (req.headers['content-length']) {
+        contentLength = Number(req.headers['content-length']);
+        if (isNaN(contentLength)) {
+            // TODO: error on bogus header
+            contentLength = undefined;
         }
+    }
 
-        ctx.resMessage({
-            type: 'progress',
-            payload: {
-                id: imgId.substr(0, 12),
-                status: 'Pulling metadata'
-            }
-        });
-
-        ctx.regClientV1.getImgJson({
-            imgId: imgId
-        }, function (err, imgJson_, getRes) {
-            imgJson = imgJson_;
-            next(errors.wrapErrorFromDrc(err));
-        });
+    var sha1, sha1Param;
+    if (req.query.sha1) {
+        sha1Param = req.query.sha1;
     }
 
-    function genImgapiManifest(_, next) {
-        try {
-            manifest = imgmanifest.imgManifestFromDockerInfo({
-                imgJson: imgJson,
-                repo: rat,
-                public: ctx.public_
-            });
-        } catch (e) {
-            return next(new errors.InternalError(e,
-                'could not convert Docker image JSON to a manifest'));
+    var size = 0;
+    var stor;  // the storage class
+    function finish_(err, tmpFilename, filename) {
+        if (err) {
+            return next(err);
         }
-
-        if (!manifest.os) {
-           manifest.os = 'other';  // some docker layers have no `.os`
+        if (size === 0) {
+            return next(new errors.DownloadError(
+                'image file size is 0 bytes, empty files are not allowed'));
+        }
+        if (size > constants.MAX_IMAGE_SIZE) {
+            return next(new errors.DownloadError(format(
+                'image file size, %s, exceeds the maximum allowed file ' +
+                'size, %s', size, constants.MAX_IMAGE_SIZE_STR)));
+        }
+        if (contentLength && size !== contentLength) {
+            return next(new errors.DownloadError(format(
+                '"Content-Length" header, %s, does not match uploaded ' +
+                'size, %d', contentLength, size)));
         }
-        next();
-    }
 
-    function handleOwner(_, next) {
-        if (active || unactivated) {
-            next();
-            return;
+        sha1 = shasum.digest('hex');
+        if (sha1Param && sha1Param !== sha1) {
+            return next(new errors.DownloadError(format(
+                '"sha1" hash, %s, does not match the uploaded ' +
+                'file sha1 hash, %s', sha1Param, sha1)));
         }
 
-        /**
-         * In 'dc' mode (i.e. with a UFDS user database) change owner from
-         * UNSET_OWNER_UUID -> admin. In other modes (i.e. not user
-         * database), change owner from anything -> UNSET_OWNER_UUID.
-         *
-         * This means that the cycle of publishing an image to a public
-         * repo and importing into a DC makes the image cleanly owned by
-         * the 'admin' user. See IMGAPI-408.
-         */
-        if (app.mode === 'dc') {
-            if (manifest.owner === UNSET_OWNER_UUID) {
-                manifest.owner = app.config.adminUuid;
-                return next();
-            }
-        } else {
-            manifest.owner = UNSET_OWNER_UUID;
+        var file = {
+            sha1: sha1,
+            size: size,
+            contentMD5: md5sum.digest('base64'),
+            mtime: (new Date()).toISOString(),
+            stor: stor.type,
+            compression: compression
+        };
+        if (req.query.dataset_guid) {
+            file.dataset_guid = req.query.dataset_guid;
         }
 
+        // Passing some vars onto `finishMoveImageFile`.
+        req.file = file;
+        req.storage = stor.type;
+        req.tmpFilename = tmpFilename;
+        req.filename = filename;
+
         return next();
     }
+    var finish = once(finish_);
 
-    function handleChannels(_, next) {
-        if (active || unactivated) {
-            next();
-            return;
-        }
+    if (contentLength !== undefined &&
+        contentLength > constants.MAX_IMAGE_SIZE) {
 
-        delete manifest.channels;
-        if (req.channel) {
-            manifest.channels = [req.channel.name];
-        }
-        next();
+        finish(new errors.UploadError(format(
+            'image file size %s (from Content-Length) exceeds the maximum ' +
+            'allowed size, %s', contentLength, constants.MAX_IMAGE_SIZE_STR)));
     }
 
-    function createImageFromManifest(_, next) {
-        if (active || unactivated) {
-            next();
-            return;
+    size = 0;
+    var shasum = crypto.createHash('sha1');
+    var md5sum = crypto.createHash('md5');
+    req.on('data', function (chunk) {
+        size += chunk.length;
+        if (size > constants.MAX_IMAGE_SIZE) {
+            finish(new errors.UploadError(format(
+                'image file size exceeds the maximum allowed size, %s',
+                constants.MAX_IMAGE_SIZE_STR)));
         }
+        shasum.update(chunk, 'binary');
+        md5sum.update(chunk, 'binary');
+    });
+    req.on('end', function () {
+        req.log.trace('req "end" event');
+    });
+    req.on('close', function () {
+        req.log.trace('req "close" event');
+    });
 
-        log.debug({ data: manifest }, 'AdminImportDockerImage: create it');
-        Image.create(app, manifest, true, false, function (cErr, img) {
-            if (cErr) {
-                return next(cErr);
-            }
-            ctx.imageFromImgId[imgId] = newImage = img;
-            next();
-        });
-    }
-
-    function addImageFile(_, next) {
-        if (active) {
-            next();
-            return;
+    stor = req._app.chooseStor(req._image, preferredStorage);
+    stor.storeFileFromStream({
+        image: req._image,
+        stream: req,
+        reqId: req.id(),
+        filename: 'file0'
+    }, function (sErr, tmpFilename, filename) {
+        if (sErr) {
+            req.log.error(sErr, 'error storing image file');
+            finish(errors.parseErrorFromStorage(
+                sErr, 'error receiving image file'));
+        } else {
+            finish(null, tmpFilename, filename);
         }
+    });
+}
 
-        log.debug({imgId: imgId}, 'AddImageLayer: start');
-
-        ctx.resMessage({
-            type: 'progress',
-            payload: {
-                id: imgId.substr(0, 12),
-                status: 'Pulling fs layer'
-            }
-        });
-
-        (function createLayerReadStream(cbStream) {
-            if (ctx.regV === 1) {
-                ctx.regClientV1.getImgLayerStream(
-                    {imgId: imgId},
-                    cbStream);
-            } else {
-                ctx.regClientV2.createBlobReadStream(
-                    {digest: opts.fsLayer.blobSum},
-                    cbStream);
-            }
-        })(function (err, stream) {
-            if (err) {
-                next(errors.wrapErrorFromDrc(err));
-                return;
-            }
 
-            assert.object(stream.connection, 'stream.connection');
-
-            var DOCKER_READ_STREAM_TIMEOUT = 15 * 1000;
-            var MAX_IMAGE_FILE_DOWNLOAD_ATTEMPTS = 5;
-
-            var lastUpdate = 0;
-            var updateEvery = 512 * 1024;
-            var startTs = Math.floor(new Date().getTime() / 1000);
-            var streamTimeoutHandler;
-
-            var compression = 'none';
-
-            // Setup a timeout listener and handle connection timeout.
-            stream.connection.setTimeout(DOCKER_READ_STREAM_TIMEOUT);
-            streamTimeoutHandler = function onDockerStreamTimeout() {
-                log.info({imgId: imgId, size: size, fileSize: fileSize},
-                    'dockerDownloadAndImportImage: '
-                    + 'createBlobReadStream connection timed out');
-                ctx.resMessage({
-                    type: 'progress',
-                    payload: {
-                        id: imgId.substr(0, 12),
-                        status: 'Connection timed out',
-                        progressDetail: {
-                            current: size,
-                            total: fileSize,
-                            start: startTs
-                        }
-                    }
-                });
-                // Note that by destroying the stream this will result in a
-                // call to finish() with an error, as the drc
-                // createBlobReadStream handler has an 'end' handler that
-                // validates the size and digest of downloaded data and
-                // emits an error event when all the data wasn't downloaded.
-                stream.destroy();
-            };
-            stream.connection.on('timeout', streamTimeoutHandler);
-
-            // Retry the download if there are attempts left.
-            function retryAddImageFile(rErr) {
-                addImageFileAttempts += 1;
-                if (addImageFileAttempts >= MAX_IMAGE_FILE_DOWNLOAD_ATTEMPTS) {
-                    log.info({imgId: imgId, fileSize: fileSize},
-                        'dockerDownloadAndImportImage: download failed after '
-                        + '%d attempts', addImageFileAttempts);
-                    next(rErr);
-                    return;
-                }
-                // Give a short respite and then go again.
-                setTimeout(function () {
-                    if (ctx.downloadsCanceled) {
-                        log.info({err: rErr, imgId: imgId, fileSize: fileSize,
-                            addImageFileAttempts: addImageFileAttempts},
-                            'dockerDownloadAndImportImage: not retrying, ' +
-                            'download already canceled');
-                        next(new errors.DownloadError(rErr,
-                            'Download canceled'));
-                        return;
-                    }
-                    log.info({imgId: imgId, fileSize: fileSize,
-                        addImageFileAttempts: addImageFileAttempts},
-                        'dockerDownloadAndImportImage: retrying blob download');
-                    addImageFile(null, next);
-                }, 1000);
-            }
+function apiAddImageFileFromSource(req, res, next) {
+    if (req.query.source === undefined)
+        return next();
 
-            if (stream.headers['content-length'] !== undefined) {
-                fileSize = Number(stream.headers['content-length']);
-            }
+    req.log.debug({image: req._image}, 'AddImageFileFromSource: start');
 
-            var size = 0;
-            var stor;  // the storage class
-            var sha1;
-            function finish_(fErr, tmpFilename, filename) {
-                // Remove connection timeout handler.
-                stream.connection.removeListener('timeout',
-                    streamTimeoutHandler);
-                streamTimeoutHandler = null;
-
-                if (fErr) {
-                    if (fErr.name === 'DownloadError') {
-                        retryAddImageFile(fErr);
-                        return;
-                    }
-                    log.info({imgId: imgId, err: fErr},
-                        'dockerDownloadAndImportImage: ' +
-                        'not retrying on this error');
-                    return next(fErr);
-                } else if (ctx.downloadsCanceled) {
-                    return next(new errors.DownloadError('Download canceled'));
-                } else if (size > MAX_IMAGE_SIZE) {
-                    return next(new errors.DownloadError(format(
-                        'Download error: image file size, %s, exceeds the ' +
-                        'maximum allowed file size, %s',
-                        size, MAX_IMAGE_SIZE_STR)));
-                } else if (fileSize >= 0 && size !== fileSize) {
-                    // Retry - as there was an issue downloading all the bits.
-                    retryAddImageFile(new errors.DownloadError(format(
-                        'Download error: "Content-Length" header, %s, does ' +
-                        'not match downloaded size, %d', fileSize, size)));
-                    return;
-                }
+    // Can't change files on an activated image.
+    if (req._image.activated) {
+        return next(new errors.ImageFilesImmutableError(req._image.uuid));
+    }
 
-                sha1 = shasum.digest('hex');
-
-                var file = {
-                    sha1: sha1,
-                    size: size,
-                    contentMD5: md5sum.digest('base64'),
-                    mtime: (new Date()).toISOString(),
-                    stor: stor.type,
-                    compression: compression
-                };
-
-                ctx.fileInfoFromImgId[imgId] = {
-                    file: file,
-                    storage: stor.type,
-                    tmpFilename: tmpFilename,
-                    filename: filename
-                };
-
-                log.info({imgId: imgId, fileSize: fileSize},
-                    'dockerDownloadAndImportImage: Download successful');
-                return next();
-            }
-            var finish = once(finish_);
-
-            var shasum = crypto.createHash('sha1');
-            var md5sum = crypto.createHash('md5');
-
-            stream.on('data', function (chunk) {
-                if (ctx.downloadsCanceled) {
-                    stream.destroy();
-                    ctx.resMessage({
-                        type: 'progress',
-                        payload: {
-                            id: imgId.substr(0, 12),
-                            status: 'Aborted',
-                            progressDetail: {
-                                current: size,
-                                total: fileSize,
-                                start: startTs
-                            }
-                        }
-                    });
-                    return;
-                }
-
-                size += chunk.length;
-                if (size > MAX_IMAGE_SIZE) {
-                    finish(new errors.DownloadError(format(
-                        'Download error: image file size exceeds the ' +
-                        'maximum allowed size, %s', MAX_IMAGE_SIZE_STR)));
-                }
-                shasum.update(chunk, 'binary');
-                md5sum.update(chunk, 'binary');
-
-                if ((size - lastUpdate) > updateEvery) {
-                    ctx.resMessage({
-                        type: 'progress',
-                        payload: {
-                            id: imgId.substr(0, 12),
-                            status: 'Downloading',
-                            progressDetail: {
-                                current: size,
-                                total: fileSize,
-                                start: startTs
-                            }
-                        }
-                    });
-                    lastUpdate = size;
-                }
-            });
-
-            stream.on('error', function (streamErr) {
-                finish(errors.wrapErrorFromDrc(streamErr));
-            });
-
-            stor = app.chooseStor(newImage);
-            stor.storeFileFromStream({
-                image: newImage,
-                stream: stream,
-                reqId: stream.id(),
-                filename: 'file0',
-                noStreamErrorHandler: true
-            }, function (sErr, tmpFilename, filename) {
-                if (sErr) {
-                    log.error({err: sErr, imgId: imgId},
-                        'error storing image file');
-                    finish(errors.parseErrorFromStorage(
-                        sErr, 'error receiving image file'));
-                } else {
-                    finish(null, tmpFilename, filename);
-                }
-            });
-        });
-    }
-}
-
-
-/*
- * This function is run as a forEach in apiAdminImportDockerImage. We need
- * to ensure image objects are added serially into the database.
- * The function 'this' is bound to be { req: req, res: res }
- */
-function _dockerActivateImage(imgId, ctx, callback) {
-    var req = ctx.req;
-    var resMessage = ctx.resMessage;
-    var app = req._app;
-    var log = req.log;
-    var newImage = ctx.imageFromImgId[imgId];
-    var newFile = ctx.fileInfoFromImgId[imgId];
-
-    var exists = (newImage.exists === true);
-    delete newImage.exists;
-
-    vasync.pipeline({ funcs: [
-        archiveManifest,
-        addManifestToDb,
-        finishMoveImageLayer,
-        activateImage
-    ]}, function afterPipe(pipeErr, results) {
-        if (pipeErr) {
-            callback(pipeErr);
-            return;
-        }
-
-        if (!exists) {
-            resMessage({
-                type: 'progress',
-                payload: {
-                    id: imgId.substr(0, 12),
-                    status: 'Pull complete'
-                }
-            });
-        }
-
-        callback();
-    });
-
-    function archiveManifest(_, next) {
-        if (exists) {
-            next();
-            return;
-        }
-
-        var local = app.storage.local;
-        var serialized = newImage.serialize(app.mode, '*');
-
-        local.archiveImageManifest(serialized, function (archErr) {
-            if (archErr) {
-                log.error({uuid: newImage.uuid},
-                    'error archiving image manifest:', serialized);
-                return next(archErr);
-            }
-            next();
-        });
-    }
-
-    function addManifestToDb(_, next) {
-        if (exists) {
-            next();
-            return;
-        }
-
-        app.db.add(newImage.uuid, newImage.raw, function (addErr) {
-            if (addErr) {
-                log.error({uuid: newImage.uuid},
-                    'error saving to database: raw data:',
-                    newImage.raw);
-                return next(new errors.InternalError(addErr,
-                    'could not create local image'));
-            }
-            app.cacheInvalidateWrite('Image', newImage);
-            next();
-        });
-    }
-
-    function finishMoveImageLayer(_, next) {
-        if (newImage.activated) {
-            next();
-            return;
-        }
-
-        log.debug({imgId: imgId}, 'MoveImageLayer: start');
-        var stor = app.getStor(newFile.storage);
-
-        stor.moveImageFile(newImage, newFile.tmpFilename, newFile.filename,
-          function (mErr) {
-            if (mErr) {
-                return next(mErr);
-            }
-
-            newImage.addFile(app, newFile.file, req.log, function (err2) {
-                if (err2) {
-                    req.log.error(err2, 'error adding file info to Image');
-                    return next(new errors.InternalError(err2,
-                        'could not save image'));
-                }
-
-                next();
-            });
-        });
-    }
-
-    function activateImage(_, next) {
-        if (newImage.activated) {
-            next();
-            return;
-        }
-
-        resMessage({
-            type: 'progress',
-            payload: {
-                id: imgId.substr(0, 12),
-                status: 'Activating image'
-            }
-        });
-
-        newImage.activate(app, req.log, next);
-    }
-}
-
-
-function _dockerV1Pull(ctx, cb) {
-    assert.object(ctx, 'ctx');
-    assert.func(ctx.resMessage, 'ctx.resMessage');
-    assert.object(ctx.rat, 'ctx.rat');
-    assert.string(ctx.imgId, 'ctx.imgId');
-    assert.object(ctx.regClientV1, 'ctx.regClientV1');
-    assert.func(cb, 'cb');
-
-    var req = ctx.req;
-    var log = req.log;
-    var resMessage = ctx.resMessage;
-    var rat = ctx.rat;
-    var tag = rat.tag;
-
-    var reverseAncestry;
-
-    vasync.pipeline({funcs: [
-        function starterMessages(_, next) {
-            resMessage({
-                type: 'status',
-                payload: {
-                    /*
-                     * When pulling all images in a repository is supported
-                     * Docker-docker says: 'Pulling repository $localName'.
-                     */
-                    status: format('%s: Pulling from %s (%s)',
-                        tag, rat.localName, req.getId())
-                }
-            });
-            // The `head` message tells sdc-docker to tag this Docker imgId
-            // for the pulling user.
-            resMessage({type: 'head', head: ctx.imgId});
-            resMessage({
-                type: 'progress',
-                payload: {
-                    id: ctx.imgId.substr(0, 12),
-                    status: 'Pulling dependent layers'
-                }
-            });
-
-            next();
-        },
-
-        function getAncestry(_, next) {
-            ctx.regClientV1.getImgAncestry({
-                imgId: ctx.imgId
-            }, function (err, ancestry) {
-                if (err) {
-                    return next(errors.wrapErrorFromDrc(err));
-                }
-                // Want to import oldest in ancestry first.
-                reverseAncestry = ancestry.reverse();
-                next();
-            });
-        },
-
-        /*
-         * In *parallel*, create (unactivated) and download the images.
-         */
-        function importImagesPart1(_, next) {
-            var pullQueueError;
-            ctx.downloadsCanceled = false;
-            var pullQueue = vasync.queue(function (imgId, nextImg) {
-                _dockerDownloadAndImportImage({imgId: imgId, ctx: ctx},
-                    nextImg);
-            }, 5);
-
-            pullQueue.on('end', function () {
-                next(pullQueueError);
-            });
-
-            pullQueue.push(reverseAncestry, function (qErr) {
-                if (qErr) {
-                    log.debug(qErr, '_dockerDownloadAndImportImage err');
-                }
-                if (qErr && pullQueueError === undefined) {
-                    pullQueueError = qErr;
-                    ctx.downloadsCanceled = true;
-                    pullQueue.kill();
-                }
-            });
-            pullQueue.close();
-        },
-
-        /*
-         * *Serially* complete the import of all the images:
-         * - We only ActivateImage's at this stage after the file downloading
-         *   (anticipated to be the most error-prone stage).
-         * - We activate images in ancestry order (parent before child) for
-         *   db consistency.
-         */
-        function importImagesPart2(_, next) {
-            vasync.forEachPipeline({
-                inputs: reverseAncestry,
-                func: function (imgId, nextImg) {
-                    _dockerActivateImage(imgId, ctx, nextImg);
-                }
-            }, function (vErr, results) {
-                next(vErr);
-            });
-        },
-
-        function finishingMessages(_, next) {
-            var status = (ctx.alreadyExistsFromImgId[ctx.imgId]
-                ? 'Status: Image is up to date for ' + ctx.repoAndRef
-                : 'Status: Downloaded newer image for ' + ctx.repoAndRef);
-            resMessage({
-                type: 'status',
-                payload: {status: status}
-            });
-
-            next();
-        }
-    ]}, cb);
-}
-
-
-/* BEGIN JSSTYLED */
-/*
- * Pull a docker image using v2 registry.
- *
- * Example Docker-docker pull:
- *
- *  $ docker pull alpine@sha256:fb9f16730ac6316afa4d97caa5130219927bfcecf0b0ce35c01dcb612f449739
- *  sha256:fb9f16730ac6316afa4d97caa5130219927bfcecf0b0ce35c01dcb612f449739: Pulling from library/alpine
- *  f4fddc471ec2: Pull complete
- *  library/alpine:sha256:fb9f16730ac6316afa4d97caa5130219927bfcecf0b0ce35c01dcb612f449739: The image you are pulling has been verified. Important: image verification is a tech preview feature and should not be relied on to provide security.
- *  Digest: sha256:fb9f16730ac6316afa4d97caa5130219927bfcecf0b0ce35c01dcb612f449739
- *  Status: Downloaded newer image for alpine@sha256:fb9f16730ac6316afa4d97caa5130219927bfcecf0b0ce35c01dcb612f449739
- *
- *  $ docker pull alpine@sha256:fb9f16730ac6316afa4d97caa5130219927bfcecf0b0ce35c01dcb612f449739
- *  sha256:fb9f16730ac6316afa4d97caa5130219927bfcecf0b0ce35c01dcb612f449739: Pulling from library/alpine
- *  f4fddc471ec2: Already exists
- *  Digest: sha256:fb9f16730ac6316afa4d97caa5130219927bfcecf0b0ce35c01dcb612f449739
- *  Status: Image is up to date for alpine@sha256:fb9f16730ac6316afa4d97caa5130219927bfcecf0b0ce35c01dcb612f449739
- */
-/* END JSSTYLED */
-function _dockerV2Pull(ctx, cb) {
-    assert.object(ctx, 'ctx');
-    assert.func(ctx.resMessage, 'ctx.resMessage');
-    assert.object(ctx.rat, 'ctx.rat');
-    assert.string(ctx.digestV2, 'ctx.digestV2');
-    assert.object(ctx.manifestV2, 'ctx.manifestV2');
-    assert.object(ctx.regClientV2, 'ctx.regClientV2');
-    assert.func(cb, 'cb');
-
-    var cacheDownloadedLayers = false;
-    var req = ctx.req;
-    var log = req.log;
-    var resMessage = ctx.resMessage;
-    var rat = ctx.rat;
-    var tag = rat.tag;
-    var digest = rat.digest;
-
-    var reverseAncestry = [];  // 'reversed' so that the base image is first
-    for (var i = ctx.manifestV2.history.length - 1; i >= 0; i--) {
-        try {
-            var imgJson = JSON.parse(ctx.manifestV2.history[i].v1Compatibility);
-        } catch (manifestErr) {
-            return cb(new errors.ValidationFailedError(manifestErr, format(
-                'invalid "v1Compatibility" JSON in docker manifest: %s (%s)',
-                manifestErr, ctx.manifestV2.history[i].v1Compatibility)));
-        }
-        reverseAncestry.push({
-            imgJson: imgJson,
-            fsLayer: ctx.manifestV2.fsLayers[i]
-        });
-    }
-    var imgId = reverseAncestry[reverseAncestry.length - 1].imgJson.id;
-
-    vasync.pipeline({funcs: [
-        function starterMessages(_, next) {
-            resMessage({
-                type: 'status',
-                payload: {
-                    /*
-                     * When pulling all images in a repository is supported
-                     * Docker-docker says: 'Pulling repository $localName'.
-                     */
-                    status: format('%s: Pulling from %s (req %s)',
-                        tag || digest, rat.localName, req.getId())
-                }
-            });
-            // The `head` message tells sdc-docker to tag this Docker imgId
-            // for the pulling user.
-            resMessage({type: 'head', head: imgId});
-            resMessage({
-                type: 'progress',
-                payload: {
-                    id: imgId.substr(0, 12),
-                    status: 'Pulling dependent layers'
-                }
-            });
-
-            next();
-        },
-
-        /*
-         * In *parallel*, create (unactivated) and download the images.
-         */
-        function importImagesPart1(_, next) {
-            var pullQueueError;
-            ctx.downloadsCanceled = false;
-            cacheDownloadedLayers = true;
-
-            var pullQueue = vasync.queue(function (imgInfo, nextImg) {
-                _dockerDownloadAndImportImage({
-                    imgId: imgInfo.imgJson.id,
-                    imgJson: imgInfo.imgJson,
-                    fsLayer: imgInfo.fsLayer,
-                    ctx: ctx
-                }, nextImg);
-            }, 5);
-
-            pullQueue.on('end', function () {
-                next(pullQueueError);
-            });
-
-            pullQueue.push(reverseAncestry, function (qErr) {
-                if (qErr) {
-                    log.debug(qErr, '_dockerDownloadAndImportImage err');
-                }
-                if (qErr && pullQueueError === undefined) {
-                    pullQueueError = qErr;
-                    ctx.downloadsCanceled = true;
-                    pullQueue.kill();
-                }
-            });
-            pullQueue.close();
-        },
-
-        /*
-         * *Serially* complete the import of all the images:
-         * - We only ActivateImage's at this stage after the file downloading
-         *   (anticipated to be the most error-prone stage).
-         * - We activate images in ancestry order (parent before child) for
-         *   db consistency.
-         */
-        function importImagesPart2(_, next) {
-            cacheDownloadedLayers = false;
-            vasync.forEachPipeline({
-                inputs: reverseAncestry,
-                func: function (imgInfo, nextImg) {
-                    _dockerActivateImage(imgInfo.imgJson.id, ctx, nextImg);
-                }
-            }, function (vErr, results) {
-                next(vErr);
-            });
-        },
-
-        function finishingMessages(_, next) {
-            resMessage({
-                type: 'status',
-                payload: {
-                    status: 'Digest: ' + ctx.digestV2
-                }
-            });
-
-            var status = (ctx.alreadyExistsFromImgId[imgId]
-                ? 'Status: Image is up to date for ' + ctx.repoAndRef
-                : 'Status: Downloaded newer image for ' + ctx.repoAndRef);
-            resMessage({
-                type: 'status',
-                payload: {status: status}
-            });
-
-            next();
-        }
-    ]}, function (err) {
-        if (cacheDownloadedLayers) {
-            // There was a failure downloading one or more image layers - keep
-            // the downloaded image metadata in memory, so we can avoid
-            // downloading it again next time.
-            Object.keys(ctx.fileInfoFromImgId).forEach(function (id) {
-                DOCKER_IMAGE_CACHE[id] = {
-                    fileInfo: ctx.fileInfoFromImgId[id],
-                    image: ctx.imageFromImgId[id]
-                };
-            });
-        }
-
-        cb(err);
-    });
-}
-
-
-
-/* BEGIN JSSTYLED */
-/**
- * Import a given docker repo:tag while streaming out progress messages.
- * Typically this is called by the sdc-docker 'pull-image' workflow.
- *
- * Progress messages are one of the following (expanded here for clarity):
- *
- *      {
- *          "type":"status",
- *          // Used by the calling 'pull-image' workflow, and ultimately the
- *          // sdc-docker service to know which open client response this
- *          // belongs to.
- *          "id":"docker.io/busybox",
- *          // Per http://docs.docker.com/reference/api/docker_remote_api_v1.18/#create-an-image
- *          "payload":{"status":"Pulling repository busybox"}
- *      }
- *
- *      {
- *          "type":"head",
- *          // A head Docker imgId for sdc-docker to tag.
- *          "head":"8c2e06607696bd4afb3d03b687e361cc43cf8ec1a4a725bc96e39f05ba97dd55",
- *          "id":"docker.io/busybox"
- *      }
- *
- *      {"type":"progress","payload":{"id":"8c2e06607696","status":"Pulling dependent layers"},"id":"docker.io/busybox"}
- *      {"type":"progress","id":"docker.io/busybox","payload":{"id":"8c2e06607696","status":"Pulling metadata."}}
- *
- *      {
- *          "type":"data",
- *          // Docker imgJson. TODO: How used by caller?
- *          "imgJson":{"container":"39e79119...
- *      }
- *
- *      {
- *          "type": "error",
- *          "error": {
- *              "code": "<CodeString>",
- *              "message": <error message>
- *          }
- *      }
- *
- * Dev Note: For now we only allow a single tag to be pulled. To eventually
- * support `docker pull -a ...` we'll likely want to support an empty `tag`
- * here to mean all tags.
- */
-/* END JSSTYLED */
-function apiAdminImportDockerImage(req, res, callback) {
-    if (req.query.action !== 'import-docker-image')
-        return callback();
-
-    var log = req.log;
-
-    // Validate inputs.
-    var errs = [];
-    var repo = req.query.repo;
-    var tag = req.query.tag;
-    var digest = req.query.digest;
-    if (req.query.account) {
-        return callback(new errors.OperatorOnlyError());
-    }
-    if (repo === undefined) {
-        errs.push({ field: 'repo', code: 'MissingParameter' });
-    }
-    if (!tag && !digest) {
-        errs.push({
-            field: 'tag',
-            code: 'MissingParameter',
-            message: 'one of "tag" or "digest" is required'
-        });
-    } else if (tag && digest) {
-        errs.push({
-            field: 'digest',
-            code: 'Invalid',
-            message: 'cannot specify both "tag" and "digest"'
-        });
-    }
-    if (errs.length) {
-        return callback(new errors.ValidationFailedError(
-            'missing parameters', errs));
-    }
-
-    var rat;
-    try {
-        rat = drc.parseRepoAndRef(
-            repo + (tag ? ':'+tag : '@'+digest));
-    } catch (e) {
-        return callback(new errors.ValidationFailedError(
-            e,
-            e.message || e.toString(),
-            [ {field: 'repo', code: 'Invalid'}]));
-    }
-
-    var regAuth;
-    var username, password;
-    if (req.headers['x-registry-config']) {
-        /*
-         * // JSSTYLED
-         * https://github.com/docker/docker/blob/master/docs/reference/api/docker_remote_api_v1.23.md#build-image-from-a-dockerfile
-         *
-         * The 'x-registry-config' header is a map of the registry host name to
-         * the registry auth (see 'x-registry-auth') below.
-         */
-        try {
-            var regConfig = JSON.parse(new Buffer(
-                req.headers['x-registry-config'], 'base64').toString('utf8'));
-        } catch (e) {
-            log.info(e, 'invalid x-registry-config header, ignoring');
-        }
-        // Censor for audit logs.
-        req.headers['x-registry-config'] = '(censored)';
-
-        // Find registry auth from the registry hostname map. Note that Docker
-        // uses a special legacy name for the "official" docker registry, so
-        // check for that too.
-        var dockerV1CompatName = 'https://index.docker.io/v1/';
-        var indexName = rat.index.name;
-        if (regConfig.hasOwnProperty(indexName)) {
-            regAuth = regConfig[indexName];
-        } else if (indexName === 'docker.io' &&
-            regConfig.hasOwnProperty(dockerV1CompatName))
-        {
-            regAuth = regConfig[dockerV1CompatName];
-        }
-    }
-    if (req.headers['x-registry-auth']) {
-        /*
-         * // JSSTYLED
-         * https://github.com/docker/docker/blob/master/docs/reference/api/docker_remote_api_v1.23.md#create-an-image
-         *
-         * The 'x-registry-auth' header contains `username` and `password`
-         * *OR* a `identitytoken`. We don't yet support identitytoken --
-         * See DOCKER-771.
-         */
-        try {
-            regAuth = JSON.parse(new Buffer(
-                req.headers['x-registry-auth'], 'base64').toString('utf8'));
-        } catch (e) {
-            log.info(e, 'invalid x-registry-auth header, ignoring');
-        }
-        // Censor for audit logs.
-        req.headers['x-registry-auth'] = '(censored)';
-    }
-
-    if (regAuth) {
-        if (regAuth.identitytoken) {
-            callback(new errors.NotImplementedError('OAuth to Docker '
-                + 'registry is not yet supported, please "docker logout" '
-                + 'and "docker login" and try again'));
-            return;
-        } else {
-            username = regAuth.username;
-            password = regAuth.password;
-        }
-    }
-
-    try {
-        var public_ = boolFromString(req.query.public, true, 'public');
-    } catch (publicErr) {
-        return callback(publicErr);
-    }
-
-    function resMessage(data) {
-        data.id = rat.canonicalName;
-        res.write(JSON.stringify(data) + '\r\n');
-    }
-
-    var context = {
-        req: req,
-        res: res,
-        resMessage: resMessage,
-
-        repoAndRef: rat.localName + (rat.tag ? ':'+rat.tag : '@'+rat.digest),
-        rat: rat,
-        regClientOpts: utils.commonHttpClientOpts({
-            name: repo,
-            log: req.log,
-            insecure: req._app.config.dockerRegistryInsecure,
-            username: username,
-            password: password
-        }, req),
-
-        imageFromImgId: {},  // <docker imgId> -> <IMGAPI image manifest>
-        fileInfoFromImgId: {},  // <docker imgId> -> <file import info>
-        alreadyExistsFromImgId: {},
-        public_: public_
-    };
-    log.trace({rat: context.rat}, 'docker pull');
-
-    vasync.pipeline({arg: context, funcs: [
-        /*
-         * To use Docker Registry v2 or v1 to pull? That is the question.
-         *
-         * 1. Try v2 ping to include v2 as candidate.
-         * 2. If so, check v2.getManifest for the tag/digest. If it exists,
-         *    then we'll be using v2 for the pull.
-         * 3. Else, try to resolve an imgId with v1. If 'digest', then error
-         *    because v1 doesn't support docker pull by digest. If no v1 or
-         *    the given tag not found with v1, then error out. Otherwise,
-         *    we'll be using v1 for the pull.
-         *
-         * Note: This "fallback to v1 if can't find the tag/digest in v2"
-         * behaviour is to copy `docker pull` behaviour (at least what it
-         * seems to do to Docker Hub). Basically that behaviour
-         * says that "v2" isn't a separate API version to the same underlying
-         * data, but instead is a completely disjoint registry... *some* of
-         * which Docker Hub migrated over.
-         */
-        function regSupportsV2(ctx, next) {
-            var client = drc.createClientV2(ctx.regClientOpts);
-            client.supportsV2(function (err, supportsV2) {
-                if (err) {
-                    next(errors.wrapErrorFromDrc(err));
-                } else {
-                    log.info({indexName: rat.index.name,
-                        supportsV2: supportsV2}, 'regSupportsV2');
-                    if (supportsV2) {
-                        ctx.regClientV2 = client;
-                    } else {
-                        client.close();
-                    }
-                    next();
-                }
-            });
-        },
-
-        function v2GetManifest(ctx, next) {
-            if (!ctx.regClientV2) {
-                return next();
-            }
-            var ref = rat.tag || rat.digest;
-            // TODO: Can we send in pingRes, from earlier 'supportsV2' call?
-            ctx.regClientV2.getManifest({ref: ref},
-                    function (err, man, res_, manifestStr) {
-                if (err) {
-                    if (err.statusCode === 404 || err.statusCode === 401) {
-                        /*
-                         * No workie with v2. We'll fallback to trying API v1.
-                         *
-                         * Yes a 401 is sometimes how Docker Hub v2 responds.
-                         * For example, see DOCKER-625.
-                         */
-                        log.debug({ref: ref, statusCode: err.statusCode},
-                            'v2.getManifest 404 or 401');
-                        ctx.regClientV2.close();
-                        delete ctx.regClientV2;
-                        ctx.errV2 = err; // Save it for possible use below.
-                        next();
-                    } else {
-                        next(errors.wrapErrorFromDrc(err));
-                    }
-                } else {
-                    log.debug({ref: ref, manifest: man},
-                        'v2.getManifest found');
-                    ctx.manifestV2 = man;
-                    ctx.digestV2 = res_.headers['docker-content-digest'];
-                    if (!ctx.digestV2) {
-                        // Some registries (looking at you Amazon ECR) do not
-                        // provide the docker-content-digest header in the
-                        // response, so we have to calculate it.
-                        ctx.digestV2 = drc.digestFromManifestStr(manifestStr);
-                    }
-                    ctx.regV = 2;   // The signal we'll be using v2.
-                    next();
-                }
-            });
-        },
-
-        function v1GetImgId(ctx, next) {
-            if (ctx.regV) {
-                // We've already determined we're not using v1.
-                return next();
-            } else if (ctx.digest) {
-                // V1 doesn't support pull by digest.
-                if (ctx.errV2) {
-                    next(ctx.errV2);
-                } else {
-                    next(new errors.BadRequestError(format(
-                        'Docker registry "%s" does not support v2 required ' +
-                        'to pull by digest, digest="%s"',
-                        ctx.rat.index.name, ctx.digest)));
-                }
-            }
-
-            var client = drc.createClientV1(ctx.regClientOpts);
-            client.getImgId({tag: ctx.rat.tag}, function (err, imgId) {
-                if (err) {
-                    /*
-                     * At this point we know we can't find the image.
-                     * We are going to error back to the caller. If the
-                     * repository supported v2, then we prefer to use *that*
-                     * error response.
-                     */
-                    client.close();
-                    if (ctx.errV2) {
-                        log.debug({err: err}, 'v1GetImgId error');
-                        next(errors.wrapErrorFromDrc(ctx.errV2));
-                    } else {
-                        next(errors.wrapErrorFromDrc(err));
-                    }
-                } else {
-                    ctx.regClientV1 = client;
-                    ctx.imgId = imgId;
-                    ctx.regV = 1;
-                    next();
-                }
-            });
-        },
-
-        /*
-         * At this point we know if we are using v1 or v2 (`ctx.regV`).
-         *
-         * Now determine if this is a private Docker image by trying the same
-         * without auth. The only thing this does is determine `ctx.isPrivate`
-         * so the caller (typically sdc-docker) can note that.
-         * We still used the auth'd client for doing the image pull.
-         *
-         * Note: It is debatable whether we should bother with this. Is there
-         * a need to have this 'isPrivate' boolean?
-         */
-        function determineIfPrivate(ctx, next) {
-            if (!username) {
-                ctx.isPrivate = false;
-                return next();
-            }
-
-            var opts = objCopy(ctx.regClientOpts);
-            delete opts.username;
-            delete opts.password;
-            var noAuthClient;
-
-            if (ctx.regV === 1) {
-                noAuthClient = drc.createClientV1(opts);
-                noAuthClient.getImgId({tag: ctx.rat.tag},
-                        function (err, imgId) {
-                    if (err) {
-                        if (err.statusCode === 404 ||
-                            err.statusCode === 403 ||
-                            err.statusCode === 401)
-                        {
-                            err = null;
-                            ctx.isPrivate = true;
-                        }
-                    } else {
-                        assert.equal(imgId, ctx.imgId);
-                        ctx.isPrivate = false;
-                    }
-                    noAuthClient.close();
-                    next(errors.wrapErrorFromDrc(err));
-                });
-
-            } else {
-                assert.equal(ctx.regV, 2);
-
-                var ref = rat.tag || rat.digest;
-                noAuthClient = drc.createClientV2(opts);
-                noAuthClient.getManifest({ref: ref}, function (err, man) {
-                    if (err) {
-                        if (err.statusCode === 404 ||
-                            err.statusCode === 403 ||
-                            err.statusCode === 401)
-                        {
-                            log.debug({ref: ref, code: err.code,
-                                statusCode: err.statusCode}, 'isPrivate: true');
-                            err = null;
-                            ctx.isPrivate = true;
-                        } else {
-                            log.debug({ref: ref, code: err.code,
-                                statusCode: err.statusCode},
-                                'isPrivate: unexpected err code/statusCode');
-                        }
-                    } else {
-                        // TODO: Better to assert that the Docker-Content-Digest
-                        //    is equal to above.
-                        assert.equal(man.fsLayers[0].blobSum,
-                            ctx.manifestV2.fsLayers[0].blobSum);
-                        ctx.isPrivate = false;
-                    }
-                    noAuthClient.close();
-                    next(errors.wrapErrorFromDrc(err));
-                });
-            }
-        },
-
-        /*
-         * Only now do we start the streaming response. This allows us
-         * to do some sanity validation (we can talk to the registry and have
-         * found an image for the given ref) and return a non-200 status code
-         * on failure. I'm not sure this matches Docker Remote
-         * API's behaviour exactly.
-         */
-        function startStreaming(ctx, next) {
-            res.status(200);
-            res.header('Content-Type', 'application/json');
-
-            if (ctx.regV === 1) {
-                _dockerV1Pull(ctx, next);
-            } else {
-                _dockerV2Pull(ctx, next);
-            }
-        }
-
-    ]}, function (err) {
-        if (context.regClientV1) {
-            context.regClientV1.close();
-        }
-        if (context.regClientV2) {
-            context.regClientV2.close();
-        }
-
-        if (err) {
-            // This is a chunked transfer so we can't return a restify error.
-            log.info(err, 'error pulling image layers for %s',
-                context.repoAndRef);
-            resMessage({
-                type: 'error',
-                id: repo,
-                error: {
-                    /*
-                     * `restify.RestError` instances will have `err.code`. More
-                     * vanilla `restify.HttpError` instances may have
-                     * `err.body.code` (if the response body was JSON with a
-                     * "code"), else the contructor name is a code.
-                     */
-                    code: err.code || (err.body && err.body.code) || err.name,
-                    message: err.message
-                }
-            });
-        }
-        res.end();
-        callback(false);
-    });
-}
-
-
-function apiAddImageFile(req, res, next) {
-    if (req.query.source !== undefined)
-        return next();
-
-    req.log.debug({image: req._image}, 'AddImageFile: start');
-
-    // Can't change files on an activated image.
-    if (req._image.activated) {
-        return next(new errors.ImageFilesImmutableError(req._image.uuid));
-    }
-
-    // Validate compression.
-    var compression = req.query.compression;
-    if (!compression) {
-        return next(new errors.InvalidParameterError('missing "compression"',
-            [ { field: 'compression', code: 'Missing' } ]));
-    } else if (VALID_FILE_COMPRESSIONS.indexOf(compression) === -1) {
-        return next(new errors.InvalidParameterError(
-            format('invalid compression "%s" (must be one of %s)',
-                compression, VALID_FILE_COMPRESSIONS.join(', ')),
-            [ { field: 'compression', code: 'Invalid' } ]));
-    }
-
-    // Validate requested storage. Only admin requests are allowed to specify.
-    var preferredStorage = req.query.storage;
-    if (preferredStorage && req.query.account) {
-        var error = {
-            field: 'storage',
-            code: 'NotAllowed',
-            message: 'Parameter cannot be specified by non-operators'
-        };
-        return next(new errors.InvalidParameterError(
-            format('invalid storage "%s"', preferredStorage), [error]));
-    } else if (preferredStorage) {
-        if (VALID_STORAGES.indexOf(preferredStorage) === -1) {
-            return next(new errors.InvalidParameterError(
-                format('invalid storage "%s" (must be one of %s)',
-                    preferredStorage, VALID_STORAGES.join(', ')),
-                [ { field: 'storage', code: 'Invalid' } ]));
-        }
-    }
-
-    var contentLength;
-    if (req.headers['content-length']) {
-        contentLength = Number(req.headers['content-length']);
-        if (isNaN(contentLength)) {
-            // TODO: error on bogus header
-            contentLength = undefined;
-        }
-    }
-
-    var sha1, sha1Param;
-    if (req.query.sha1) {
-        sha1Param = req.query.sha1;
-    }
-
-    var size = 0;
-    var stor;  // the storage class
-    function finish_(err, tmpFilename, filename) {
-        if (err) {
-            return next(err);
-        }
-        if (size === 0) {
-            return next(new errors.DownloadError(
-                'image file size is 0 bytes, empty files are not allowed'));
-        }
-        if (size > MAX_IMAGE_SIZE) {
-            return next(new errors.DownloadError(format(
-                'image file size, %s, exceeds the maximum allowed file ' +
-                'size, %s', size, MAX_IMAGE_SIZE_STR)));
-        }
-        if (contentLength && size !== contentLength) {
-            return next(new errors.DownloadError(format(
-                '"Content-Length" header, %s, does not match uploaded ' +
-                'size, %d', contentLength, size)));
-        }
-
-        sha1 = shasum.digest('hex');
-        if (sha1Param && sha1Param !== sha1) {
-            return next(new errors.DownloadError(format(
-                '"sha1" hash, %s, does not match the uploaded ' +
-                'file sha1 hash, %s', sha1Param, sha1)));
-        }
-
-        var file = {
-            sha1: sha1,
-            size: size,
-            contentMD5: md5sum.digest('base64'),
-            mtime: (new Date()).toISOString(),
-            stor: stor.type,
-            compression: compression
-        };
-        if (req.query.dataset_guid) {
-            file.dataset_guid = req.query.dataset_guid;
-        }
-
-        // Passing some vars onto `finishMoveImageFile`.
-        req.file = file;
-        req.storage = stor.type;
-        req.tmpFilename = tmpFilename;
-        req.filename = filename;
-
-        return next();
-    }
-    var finish = once(finish_);
-
-    if (contentLength !== undefined && contentLength > MAX_IMAGE_SIZE) {
-        finish(new errors.UploadError(format(
-            'image file size %s (from Content-Length) exceeds the maximum ' +
-            'allowed size, %s', contentLength, MAX_IMAGE_SIZE_STR)));
-    }
-
-    size = 0;
-    var shasum = crypto.createHash('sha1');
-    var md5sum = crypto.createHash('md5');
-    req.on('data', function (chunk) {
-        size += chunk.length;
-        if (size > MAX_IMAGE_SIZE) {
-            finish(new errors.UploadError(format(
-                'image file size exceeds the maximum allowed size, %s',
-                MAX_IMAGE_SIZE_STR)));
-        }
-        shasum.update(chunk, 'binary');
-        md5sum.update(chunk, 'binary');
-    });
-    req.on('end', function () {
-        req.log.trace('req "end" event');
-    });
-    req.on('close', function () {
-        req.log.trace('req "close" event');
-    });
-
-    stor = req._app.chooseStor(req._image, preferredStorage);
-    stor.storeFileFromStream({
-        image: req._image,
-        stream: req,
-        reqId: req.id(),
-        filename: 'file0'
-    }, function (sErr, tmpFilename, filename) {
-        if (sErr) {
-            req.log.error(sErr, 'error storing image file');
-            finish(errors.parseErrorFromStorage(
-                sErr, 'error receiving image file'));
-        } else {
-            finish(null, tmpFilename, filename);
-        }
-    });
-}
-
-
-function apiAddImageFileFromSource(req, res, next) {
-    if (req.query.source === undefined)
-        return next();
-
-    req.log.debug({image: req._image}, 'AddImageFileFromSource: start');
-
-    // Can't change files on an activated image.
-    if (req._image.activated) {
-        return next(new errors.ImageFilesImmutableError(req._image.uuid));
-    }
-
-    /*
-     * Node's default HTTP timeout is two minutes, and this getImageFileStream()
-     * request can take longer than that to complete.  Set this connection's
-     * timeout to an hour to avoid an abrupt close after two minutes.
-     */
-    req.connection.setTimeout(60 * 60 * 1000);
+    /*
+     * Node's default HTTP timeout is two minutes, and this getImageFileStream()
+     * request can take longer than that to complete.  Set this connection's
+     * timeout to an hour to avoid an abrupt close after two minutes.
+     */
+    req.connection.setTimeout(60 * 60 * 1000);
 
     // Validate requested storage. Only admin requests are allowed to specify.
     var preferredStorage = req.query.storage;
@@ -3878,10 +2615,10 @@ function apiAddImageFileFromSource(req, res, next) {
         return next(new errors.InvalidParameterError(
             format('invalid storage "%s"', preferredStorage), [error]));
     } else if (preferredStorage) {
-        if (VALID_STORAGES.indexOf(preferredStorage) === -1) {
+        if (constants.VALID_STORAGES.indexOf(preferredStorage) === -1) {
             return next(new errors.InvalidParameterError(
                 format('invalid storage "%s" (must be one of %s)',
-                    preferredStorage, VALID_STORAGES.join(', ')),
+                    preferredStorage, constants.VALID_STORAGES.join(', ')),
                 [ { field: 'storage', code: 'Invalid' } ]));
         }
     }
@@ -3925,10 +2662,10 @@ function apiAddImageFileFromSource(req, res, next) {
                 if (err2) {
                     return next(err2);
                 }
-                if (size > MAX_IMAGE_SIZE) {
+                if (size > constants.MAX_IMAGE_SIZE) {
                     return next(new errors.DownloadError(format(
                         'image file size, %s, exceeds the maximum allowed ' +
-                        'file size, %s', size, MAX_IMAGE_SIZE_STR)));
+                        'file size, %s', size, constants.MAX_IMAGE_SIZE_STR)));
                 }
                 if (contentLength && size !== contentLength) {
                     return next(new errors.DownloadError(format(
@@ -3970,10 +2707,10 @@ function apiAddImageFileFromSource(req, res, next) {
 
             stream.on('data', function (chunk) {
                 size += chunk.length;
-                if (size > MAX_IMAGE_SIZE) {
+                if (size > constants.MAX_IMAGE_SIZE) {
                     finish(new errors.DownloadError(format(
                         'image file size exceeds the maximum allowed size, %s',
-                        MAX_IMAGE_SIZE_STR)));
+                        constants.MAX_IMAGE_SIZE_STR)));
                 }
                 shasum.update(chunk, 'binary');
                 md5sum.update(chunk, 'binary');
@@ -4125,10 +2862,10 @@ function apiAddImageIcon(req, res, next) {
         return next(new errors.InvalidParameterError(
             format('invalid storage "%s"', preferredStorage), [error]));
     } else if (preferredStorage) {
-        if (VALID_STORAGES.indexOf(preferredStorage) === -1) {
+        if (constants.VALID_STORAGES.indexOf(preferredStorage) === -1) {
             return next(new errors.InvalidParameterError(
                 format('invalid storage "%s" (must be one of %s)',
-                    preferredStorage, VALID_STORAGES.join(', ')),
+                    preferredStorage, constants.VALID_STORAGES.join(', ')),
                 [ { field: 'storage', code: 'Invalid' } ]));
         }
     }
@@ -4153,10 +2890,10 @@ function apiAddImageIcon(req, res, next) {
         if (err) {
             return next(err);
         }
-        if (size > MAX_ICON_SIZE) {
+        if (size > constants.MAX_ICON_SIZE) {
             return next(new errors.UploadError(format(
                 'icon size, %s, exceeds the maximum allowed file ' +
-                'size, %s', size, MAX_ICON_SIZE_STR)));
+                'size, %s', size, constants.MAX_ICON_SIZE_STR)));
         }
         if (contentLength && size !== contentLength) {
             return next(new errors.UploadError(format(
@@ -4189,10 +2926,12 @@ function apiAddImageIcon(req, res, next) {
     }
     var finish = once(finish_);
 
-    if (contentLength !== undefined && contentLength > MAX_ICON_SIZE) {
+    if (contentLength !== undefined &&
+        contentLength > constants.MAX_ICON_SIZE) {
+
         finish(new errors.UploadError(format(
             'icon size %s (from Content-Length) exceeds the maximum allowed ' +
-            'size, %s', contentLength, MAX_ICON_SIZE_STR)));
+            'size, %s', contentLength, constants.MAX_ICON_SIZE_STR)));
     }
 
     size = 0;
@@ -4200,10 +2939,10 @@ function apiAddImageIcon(req, res, next) {
     var md5sum = crypto.createHash('md5');
     req.on('data', function (chunk) {
         size += chunk.length;
-        if (size > MAX_ICON_SIZE) {
+        if (size > constants.MAX_ICON_SIZE) {
             finish(new errors.UploadError(format(
                 'icon size exceeds the maximum allowed size, %s',
-                MAX_ICON_SIZE_STR)));
+                constants.MAX_ICON_SIZE_STR)));
         }
         shasum.update(chunk, 'binary');
         md5sum.update(chunk, 'binary');
@@ -4725,14 +3464,19 @@ function apiUpdateImage(req, res, cb) {
 
     req.log.debug({image: req._image}, 'UpdateImage: start');
 
+    var app = req._app;
     var image;
-    vasync.pipeline({funcs: [
-        function validateFields(_, next) {
+
+    vasync.pipeline({arg: {}, funcs: [
+        function validateFields(ctx, next) {
             var ADMIN_ONLY_ATTRS = [
                 'state',
                 'error',
                 'billing_tags',
-                'traits'
+                'traits',
+                'files',   // Restricted to digest and uncompressedDigest.
+                'origin',  // Restricted to unactivated images.
+                'uuid'     // Restricted to unactivated images.
             ];
 
             var UPDATEABLE_ATTRS = imgmanifest.fields.filter(function (field) {
@@ -4778,6 +3522,34 @@ function apiUpdateImage(req, res, cb) {
                         code: 'NotAllowed',
                         message: 'Can only be updated by operators'
                     });
+                } else if ((key === 'uuid' || key === 'origin' ||
+                        key === 'files') && data[key] !== undefined) {
+                    // These fields are a special case (used by docker build),
+                    // and it requires that the image be unactivated.
+                    if (req._image.activated) {
+                        errs.push({
+                            field: key,
+                            code: 'NotAllowed',
+                            message: 'Can only be updated on unactivated images'
+                        });
+                    } else if (key === 'files') {
+                        // Only allow changes to digest and uncompressedDigest.
+                        Object.keys(data.files[0]).forEach(function (fKey) {
+                            var file0 = req._image.files[0];
+                            if (fKey === 'digest') {
+                                return;
+                            } else if (fKey === 'uncompressedDigest') {
+                                return;
+                            } else if (data.files[0][fKey] !== file0[fKey]) {
+                                errs.push({
+                                    field: key,
+                                    code: 'NotAllowed',
+                                    message: 'Can only update "digest" or '
+                                        + '"uncompressedDigest" fields on files'
+                                });
+                            }
+                        });
+                    }
                 }
             }
 
@@ -4798,7 +3570,20 @@ function apiUpdateImage(req, res, cb) {
             var raw = objCopy(req._image.raw);
             for (i = 0; i < dataKeys.length; i++) {
                 key = dataKeys[i];
-                if (data[key] === null) {
+                if (key === 'files') {
+                    // Special handling for files, to only allow updating of
+                    // the digest and uncompressedDigest fields, whilst leaving
+                    // the other file attributes unchanged.
+                    if (data['files'] && data['files'][0]) {
+                        if (data['files'][0].digest) {
+                            raw['files'][0].digest = data['files'][0].digest;
+                        }
+                        if (data['files'][0].uncompressedDigest) {
+                            raw['files'][0].uncompressedDigest =
+                                data['files'][0].uncompressedDigest;
+                        }
+                    }
+                } else if (data[key] === null) {
                     delete raw[key];
                 } else {
                     raw[key] = data[key];
@@ -4807,23 +3592,47 @@ function apiUpdateImage(req, res, cb) {
 
             // Revalidate.
             try {
-                image = new Image(req._app, raw);
+                image = new Image(app, raw);
             } catch (cErr) {
                 return next(cErr);
             }
 
+            ctx.uuidChanged = (req._image.uuid !== image.uuid);
             next();
         },
 
         function checkOwner(_, next) {
             utils.checkOwnerExists({
-                app: req._app,
+                app: app,
                 owner: image.owner
             }, next);
         },
 
+        function moveImageFile(ctx, next) {
+            if (!ctx.uuidChanged) {
+                next();
+                return;
+            }
+            req._image.moveFileToImage(app, image, req.log, next);
+        },
+
         function doModify(_, next) {
-            Image.modify(req._app, image, req.log, next);
+            Image.modify(app, image, req.log, next);
+        },
+
+        function deleteOldImage(ctx, next) {
+            if (!ctx.uuidChanged) {
+                next();
+                return;
+            }
+            // Delete the original model.
+            app.db.del(req._image.uuid, function (delErr) {
+                if (delErr) {
+                    return next(delErr);
+                }
+                app.cacheInvalidateDelete('Image', req._image);
+                next();
+            });
         }
 
     ]}, function (err) {
@@ -4832,7 +3641,7 @@ function apiUpdateImage(req, res, cb) {
         }
 
         // Respond.
-        var serialized = image.serialize(req._app.mode, req.getVersion());
+        var serialized = image.serialize(app.mode, req.getVersion());
         resSetEtag(req, res, serialized);
         res.send(serialized);
         cb(false);
diff --git a/lib/magic.js b/lib/magic.js
index dfe8490..a3033d1 100644
--- a/lib/magic.js
+++ b/lib/magic.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright (c) 2015, Joyent, Inc.
+ * Copyright (c) 2017, Joyent, Inc.
  */
 
 /*
@@ -87,5 +87,7 @@ function compressionTypeFromPath(path, cb) {
 // ---- exports
 
 module.exports = {
+    maxMagicLen: maxMagicLen,  // Min number of bytes needed for detection.
+    compressionTypeFromBufSync: compressionTypeFromBufSync,
     compressionTypeFromPath: compressionTypeFromPath
 };
diff --git a/lib/migrations/migration-008-new-storage-layout.js b/lib/migrations/migration-008-new-storage-layout.js
index 2726731..334e22a 100644
--- a/lib/migrations/migration-008-new-storage-layout.js
+++ b/lib/migrations/migration-008-new-storage-layout.js
@@ -1,335 +1 @@
-#!/usr/bin/env node
-/*
- * This Source Code Form is subject to the terms of the Mozilla Public
- * License, v. 2.0. If a copy of the MPL was not distributed with this
- * file, You can obtain one at http://mozilla.org/MPL/2.0/.
- */
-
-/*
- * Copyright 2016 Joyent, Inc.
- */
-
-/*
- * IMGAPI db migration: Move to new storage layout for image files, from:
- *      .../images/$uuid
- * to:
- *      .../images/$uuid3charprefix/$uuid
- */
-
-var fs = require('fs');
-var path = require('path');
-var manta = require('manta');
-var assert = require('assert-plus');
-var async = require('async');
-var mkdirp = require('mkdirp');
-var passwd = require('passwd');
-
-var lib_config = require('../config');
-var constants = require('../constants');
-
-
-//---- globals
-
-var NAME = path.basename(__filename);
-var UUID_REGEX =
-    /[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/;
-
-var config;
-var mantaClient;
-var mantaDir, localDir;
-
-
-
-//---- support functions
-
-function errexit(err) {
-    console.error(NAME + ' error: ' + err);
-    process.exit(1);
-}
-
-function warn() {
-    arguments[0] = NAME + ' warn: ' + arguments[0];
-    console.warn.apply(null, arguments);
-}
-
-function info() {
-    arguments[0] = NAME + ' info: ' + arguments[0];
-    console.log.apply(null, arguments);
-}
-
-
-var _nobodyCache;
-function getNobody(callback) {
-    if (_nobodyCache !== undefined)
-        return callback(_nobodyCache);
-
-    passwd.get('nobody', function (nobody) {
-        _nobodyCache = nobody;
-        callback(_nobodyCache);
-    });
-}
-
-
-function moveMantaImages(callback) {
-    mantaClient.ls(mantaDir, function (lsErr, res) {
-        if (lsErr) {
-            if (lsErr.statusCode && lsErr.statusCode === 404) {
-              return callback();
-            } else {
-              return callback(lsErr);
-            }
-        }
-
-        var dirs = [];
-        res.on('object', function (obj) {
-            // Ignore objects
-        });
-        res.on('directory', function (dir) {
-            if (UUID_REGEX.test(dir.name)) {
-                dirs.push(dir.name);
-            }
-        });
-        // Only remove directory if it's empty
-        res.once('end', function () {
-            async.forEachSeries(dirs, moveImage, callback);
-        });
-    });
-
-    function moveImage(dir, next) {
-        var prefix = dir.slice(0, 3);
-        var dirPath = path.join(mantaDir, dir);
-        var newDirPath = path.join(mantaDir, prefix, dir);
-
-        mantaClient.mkdirp(newDirPath, function (mkErr) {
-            if (mkErr) {
-                return next(mkErr);
-            }
-
-            mantaClient.ls(dirPath, function (lsErr, res) {
-                if (lsErr) {
-                    return next(lsErr);
-                }
-
-                var files = [];
-                res.on('object', function (obj) {
-                    // $prefix, $uuid/file0
-                    // $prefix, $uuid/icon
-                    files.push([ prefix, path.join(dir, obj.name) ]);
-                });
-                res.on('directory', function (aDir) {
-                    // There should not be any directories here
-                });
-                // Only remove directory if it's empty
-                res.once('end', function () {
-                    if (files.length) {
-                        // Remove dir after move
-                        async.forEachSeries(files, moveFile, function (aErr) {
-                            if (aErr) {
-                                return next(aErr);
-                            }
-                            return removeDir(dirPath, next);
-                        });
-                    } else {
-                        return removeDir(dirPath, next);
-                    }
-                });
-            });
-        });
-    }
-
-    function removeDir(dirPath, next) {
-        mantaClient.rmr(dirPath, function (rmErr) {
-            if (rmErr) {
-                return next(rmErr);
-            }
-            info('directory "%s" has been removed',
-                dirPath);
-            return next();
-        });
-    }
-
-    function moveFile(array, next) {
-        var prefix = array[0];
-        var file = array[1];
-
-        var oldPath = path.join(mantaDir, file);
-        var newPath = path.join(mantaDir, prefix, file);
-
-        mantaClient.info(oldPath, function (infoErr, fInfo) {
-            if (infoErr) {
-                return next(infoErr);
-            }
-
-            return moveAndRemove(fInfo.md5);
-        });
-
-        function moveAndRemove(oldMd5) {
-            mantaClient.ln(oldPath, newPath, function (lnErr) {
-                if (lnErr) {
-                    return next(lnErr);
-                }
-
-                // Now, make sure both md5's match before removing the source
-                mantaClient.info(newPath, function (info2Err, info2) {
-                    if (info2Err) {
-                        return next(info2Err);
-                    }
-
-                    if (oldMd5 !== info2.md5) {
-                        warn('md5 for "%s" doesn\'t match old md5 (%s vs %s)',
-                            newPath, oldMd5, info2.md5);
-                        return next();
-                    }
-                    // If it matches, delete the old file
-                    mantaClient.unlink(oldPath, function (ulErr) {
-                        if (ulErr) {
-                            return next(ulErr);
-                        }
-
-                        info('directory "%s" successfully moved', newPath);
-                        return next();
-                    });
-                });
-            });
-        }
-    }
-}
-
-
-function mantaMigrate(callback) {
-    assert.string(config.manta.url, 'config.manta.url');
-    assert.string(config.manta.user, 'config.manta.user');
-    assert.string(config.manta.key, 'config.manta.key');
-    assert.string(config.manta.keyId, 'config.manta.keyId');
-    assert.optionalBool(config.manta.insecure, 'config.manta.insecure');
-    assert.string(config.manta.rootDir, 'config.manta.rootDir');
-
-    var insecure = config.manta.hasOwnProperty('insecure') ?
-        config.manta.insecure : false;
-    mantaDir = path.join(config.manta.rootDir, 'images');  // global
-
-    mantaClient = manta.createClient({
-        sign: {
-            key: config.manta.key,
-            keyId: config.manta.keyId,
-            user: config.manta.user
-        },
-        user: config.manta.user,
-        url: config.manta.url,
-        rejectUnauthorized: !insecure
-    });
-
-    return moveMantaImages(callback);
-}
-
-
-function moveLocalImages(callback) {
-    fs.readdir(localDir, function (dirErr, dirs) {
-        if (dirErr) {
-            warn(dirErr, 'Error reading local storage directory');
-            return callback(dirErr);
-        }
-
-        async.forEachSeries(dirs, moveDir, function (moveDirErr) {
-            if (moveDirErr) {
-                warn('Error moving local directories');
-                callback(moveDirErr);
-            } else {
-                info('Images successfully moved to new location');
-                callback();
-            }
-
-        });
-    });
-
-    function moveDir(dir, next) {
-        if (! UUID_REGEX.test(dir)) {
-            if (dir.length !== 3) {
-                warn('Directory %s is not a UUID, skipping', dir);
-            }
-            return next();
-        }
-
-        var prefix = dir.slice(0, 3);
-        var oldDir = path.join(localDir, dir);
-        var newDir = path.join(localDir, prefix, dir);
-        var prefixDir = path.join(localDir, prefix);
-
-        mkdirp(prefixDir, function (mkErr) {
-            if (mkErr) {
-                warn('Could not create %s', prefixDir);
-                return next(mkErr);
-            }
-
-            fs.rename(oldDir, newDir, function (moveErr) {
-                if (moveErr) {
-                    warn('Could not move %s', oldDir);
-                    return next(moveErr);
-                }
-
-                if (config.mode !== 'dc') {
-                    info('%s has been moved successfully', dir);
-                    return next();
-                }
-                // chmod new image directory to 'nobody' user so the imgapi
-                // service (running as 'nobody') can change it.
-                getNobody(function (nobody) {
-                    if (!nobody) {
-                        return next(new Error('could not get nobody user'));
-                    }
-                    fs.chown(prefixDir, Number(nobody.userId),
-                    Number(nobody.groupId), function (chownErr) {
-                        if (chownErr) {
-                            return next(chownErr);
-                        }
-                        info('%s has been moved successfully', dir);
-                        return next();
-                    });
-                });
-            });
-        });
-    }
-}
-
-
-function localMigrate(callback) {
-    localDir = constants.STORAGE_LOCAL_IMAGES_DIR;  // global
-    assert.ok(localDir !== '/',
-        'cannot have empty or root dir for local storage');
-
-    return moveLocalImages(callback);
-}
-
-
-
-//---- mainline
-
-function main(argv) {
-    lib_config.loadConfig({}, function (confErr, config_) {
-        if (confErr) {
-            errexit(confErr);
-            return;
-        }
-
-        config = config_;
-        assert.object(config.storageTypes, 'config.storageTypes');
-
-        // Because all IMGAPI instances have local storage
-        var functions = [ localMigrate ];
-        if (config.storageTypes.indexOf('manta') !== -1) {
-            functions.push(mantaMigrate);
-        }
-
-        async.series(functions, function (err) {
-            if (err) {
-                errexit(err);
-            } else {
-                process.exit(0);
-            }
-        });
-    });
-}
-
-if (require.main === module) {
-    main(process.argv);
-}
+// old migration file, no longer used, just here to avoid sdcadm update failures
diff --git a/lib/migrations/migration-009-backfill-archive.js b/lib/migrations/migration-009-backfill-archive.js
index 5a368cb..334e22a 100644
--- a/lib/migrations/migration-009-backfill-archive.js
+++ b/lib/migrations/migration-009-backfill-archive.js
@@ -1,312 +1 @@
-#!/usr/bin/env node
-/*
- * This Source Code Form is subject to the terms of the Mozilla Public
- * License, v. 2.0. If a copy of the MPL was not distributed with this
- * file, You can obtain one at http://mozilla.org/MPL/2.0/.
- */
-
-/*
- * Copyright 2016 Joyent, Inc.
- */
-
-/*
- * IMGAPI db migration: TODO: describe
- */
-
-var fs = require('fs');
-var path = require('path');
-var moray = require('moray');
-var bunyan = require('bunyan');
-var assert = require('assert-plus');
-var async = require('async');
-var passwd = require('passwd');
-var format = require('util').format;
-var execFile = require('child_process').execFile;
-var mkdirp = require('mkdirp');
-var rimraf = require('rimraf');
-
-var lib_config = require('../config');
-var constants = require('../constants');
-var errors = require('../errors');
-
-
-//---- globals
-
-var NAME = path.basename(__filename);
-
-var config;
-var morayClient = null;  // set in `getMorayClient()`
-
-
-//---- support functions
-
-function errexit(err) {
-    console.error(NAME + ' error: ' + err);
-    process.exit(1);
-}
-
-function warn() {
-    arguments[0] = NAME + ' warn: ' + arguments[0];
-    console.warn.apply(null, arguments);
-}
-
-function info() {
-    arguments[0] = NAME + ' info: ' + arguments[0];
-    console.log.apply(null, arguments);
-}
-
-
-function getMorayClient(callback) {
-    var client = moray.createClient({
-        connectTimeout: config.moray.connectTimeout || 200,
-        host: config.moray.host,
-        port: config.moray.port,
-        log: bunyan.createLogger({
-            name: 'moray',
-            level: 'INFO',
-            stream: process.stdout,
-            serializers: bunyan.stdSerializers
-        }),
-        reconnect: true,
-        retry: (config.moray.retry === false ? false : {
-            retries: Infinity,
-            minTimeout: 1000,
-            maxTimeout: 16000
-        })
-    });
-
-    client.on('connect', function () {
-        return callback(client);
-    });
-}
-
-function morayListImages(callback) {
-    var req = morayClient.findObjects('imgapi_images', 'uuid=*');
-    var images = [];
-
-    req.once('error', function (err) {
-        return callback(err);
-    });
-
-    req.on('record', function (object) {
-        images.push(object.value);
-    });
-
-    req.once('end', function () {
-        return callback(null, images);
-    });
-}
-
-
-var _nobodyCache;
-function getNobody(callback) {
-    if (_nobodyCache !== undefined)
-        return callback(_nobodyCache);
-
-    passwd.get('nobody', function (nobody) {
-        _nobodyCache = nobody;
-        callback(_nobodyCache);
-    });
-}
-
-
-function boolFromString(value) {
-    if (value === 'false') {
-        return false;
-    } else if (value === 'true') {
-        return true;
-    } else if (typeof (value) === 'boolean') {
-        return value;
-    }
-}
-
-
-function arrayToObject(array) {
-    if (!array) {
-        throw new TypeError('Array of key/values required');
-    } else if (typeof (array) === 'string') {
-        array = [array];
-    }
-
-    var obj = {};
-    array.forEach(function (keyvalue) {
-        var kv = keyvalue.split('=');
-
-        if (kv.length != 2) {
-            throw new TypeError('Key/value string expected');
-        }
-
-        obj[kv[0]] = kv[1];
-    });
-
-    return obj;
-}
-
-
-function toManifest(image) {
-    if (image.activated !== undefined) {
-        image.activated = boolFromString(image.activated);
-    }
-    if (image.disabled !== undefined) {
-        image.disabled = boolFromString(image.disabled);
-    }
-    if (image['public'] !== undefined) {
-        image['public'] = boolFromString(image['public']);
-    }
-    if (image.generate_passwords !== undefined) {
-        image.generate_passwords = boolFromString(image.generate_passwords);
-    }
-    if (image.image_size) {
-        image.image_size = Number(image.image_size);
-    }
-    if (image.tags) {
-        image.tags = arrayToObject(image.tags);
-    }
-    if (image.files) {
-        image.files = image.files.map(function (f) {
-            return {
-                sha1: f.sha1,
-                size: f.size,
-                compression: f.compression,
-                dataset_guid: f.dataset_guid
-            };
-        });
-    }
-
-    return image;
-}
-
-
-function migrateImage(image, callback) {
-    var archiveDir = constants.STORAGE_LOCAL_ARCHIVE_DIR;
-
-    info('migrate "%s"', image.uuid);
-    var manifest = toManifest(image);
-    var content = JSON.stringify(manifest, null, 2);
-
-    var archivePathFromImageUuid = function (uuid) {
-        return path.resolve(archiveDir, uuid.slice(0, 3), uuid + '.json');
-    };
-
-    var archPath = archivePathFromImageUuid(image.uuid);
-    var archDir = path.dirname(archPath);
-    mkdirp(archDir, function (err) {
-        if (err) {
-            return callback(err);
-        }
-        rimraf(archPath, function (err2) {
-            if (err2) {
-                return callback(err2);
-            }
-            fs.writeFile(archPath, content, 'utf8', afterWrite);
-        });
-    });
-
-    function afterWrite(err) {
-        if (err) {
-            return callback(err);
-        }
-
-        if (config.mode !== 'dc') {
-            return callback();
-        }
-        // chown both dirs
-        getNobody(function (nobody) {
-            if (!nobody) {
-                return callback(new Error('could not get nobody user'));
-            }
-            fs.chown(archPath, Number(nobody.userId), Number(nobody.groupId),
-                function (chErr) {
-                    if (chErr) {
-                        return callback(chErr);
-                    }
-                    fs.chown(archDir, Number(nobody.userId),
-                    Number(nobody.groupId), callback);
-                });
-        });
-    }
-}
-
-function morayMigrate(callback) {
-    assert.equal(config.databaseType, 'moray');
-    getMorayClient(function (mclient) {
-        morayClient = mclient;
-
-        morayListImages(function (err2, images) {
-            if (err2)
-                return callback(err2);
-            info('%d images to potentially migrate', images.length);
-            async.forEachSeries(images, migrateImage, callback);
-        });
-    });
-}
-
-
-function localListImages(callback) {
-    /*JSSTYLED*/
-    var RAW_FILE_RE = /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\.raw$/;
-    fs.readdir(constants.DATABASE_LOCAL_DIR, function (err, files) {
-        var images = [];
-        async.forEachSeries(
-            files,
-            function oneFile(file, next) {
-                if (!RAW_FILE_RE.test(file))
-                    return next();
-                var path_ = path.resolve(constants.DATABASE_LOCAL_DIR, file);
-                fs.readFile(path_, 'utf8', function (readErr, content) {
-                    if (readErr)
-                        return next(readErr);
-                    try {
-                        images.push(JSON.parse(content));
-                    } catch (ex) {
-                        return next(ex);
-                    }
-                    next();
-                });
-            },
-            function done(err2) {
-                callback(err2, images);
-            }
-        );
-    });
-}
-
-
-function localMigrate(callback) {
-    assert.equal(config.databaseType, 'local');
-    localListImages(function (err, images) {
-        if (err)
-            return callback(err);
-        async.forEachSeries(images, migrateImage, callback);
-    });
-}
-
-
-
-//---- mainline
-
-function main(argv) {
-    lib_config.loadConfig({}, function (confErr, config_) {
-        if (confErr) {
-            errexit(confErr);
-            return;
-        }
-
-        config = config_;
-        assert.string(config.databaseType, 'config.databaseType');
-
-        var migrator = (config.databaseType === 'moray'
-            ? morayMigrate : localMigrate);
-        migrator(function (err) {
-            if (err) {
-                errexit(err);
-            } else {
-                process.exit(0);
-            }
-        });
-    });
-}
-
-if (require.main === module) {
-    main(process.argv);
-}
+// old migration file, no longer used, just here to avoid sdcadm update failures
diff --git a/lib/migrations/migration-010-backfill-billing_tags.js b/lib/migrations/migration-010-backfill-billing_tags.js
index 452def9..334e22a 100644
--- a/lib/migrations/migration-010-backfill-billing_tags.js
+++ b/lib/migrations/migration-010-backfill-billing_tags.js
@@ -1,229 +1 @@
-#!/usr/bin/env node
-/*
- * This Source Code Form is subject to the terms of the Mozilla Public
- * License, v. 2.0. If a copy of the MPL was not distributed with this
- * file, You can obtain one at http://mozilla.org/MPL/2.0/.
- */
-
-/*
- * Copyright 2016 Joyent, Inc.
- */
-
-/*
- * IMGAPI db migration: TODO: describe
- */
-
-var fs = require('fs');
-var path = require('path');
-var moray = require('moray');
-var bunyan = require('bunyan');
-var errors = require('../errors');
-var assert = require('assert-plus');
-var async = require('async');
-var passwd = require('passwd');
-var format = require('util').format;
-var execFile = require('child_process').execFile;
-var mkdirp = require('mkdirp');
-var rimraf = require('rimraf');
-
-var lib_config = require('../config');
-
-
-//---- globals
-
-var NAME = path.basename(__filename);
-
-var config;
-var morayClient = null;  // set in `getMorayClient()`
-
-var BUCKET = {
-    name: 'imgapi_images',
-    indices: {
-        index: {
-            uuid: { type: 'string', unique: true},
-            name: { type: 'string' },
-            version: { type: 'string' },
-            owner: { type: 'string' },
-            origin: { type: 'string' },
-            state: { type: 'string' },
-            urn: { type: 'string', unique: true },
-            tags: { type: '[string]' },
-            billing_tags: { type: '[string]' },
-            acl: { type: '[string]' },
-            activated: { type: 'boolean' },
-            disabled: { type: 'boolean' },
-            public: { type: 'boolean' },
-            os: { type: 'string' },
-            type: { type: 'string' },
-            expires_at: { type: 'string' }
-        }
-    }
-};
-
-// Because regular findObjects will not load the "hidden" billing_tags values
-// we need to load all raw objets with sql() and then perform a single
-// getObject for each one of them
-var allBillingTags = {};
-
-
-//---- support functions
-
-function errexit(err) {
-    console.error(NAME + ' error: ' + err);
-    process.exit(1);
-}
-
-function warn() {
-    arguments[0] = NAME + ' warn: ' + arguments[0];
-    console.warn.apply(null, arguments);
-}
-
-function info() {
-    arguments[0] = NAME + ' info: ' + arguments[0];
-    console.log.apply(null, arguments);
-}
-
-
-function getMorayClient(callback) {
-    var client = moray.createClient({
-        connectTimeout: config.moray.connectTimeout || 200,
-        host: config.moray.host,
-        port: config.moray.port,
-        log: bunyan.createLogger({
-            name: 'moray',
-            level: 'INFO',
-            stream: process.stdout,
-            serializers: bunyan.stdSerializers
-        }),
-        reconnect: true,
-        retry: (config.moray.retry === false ? false : {
-            retries: Infinity,
-            minTimeout: 1000,
-            maxTimeout: 16000
-        })
-    });
-
-    client.on('connect', function () {
-        return callback(client);
-    });
-}
-
-function morayListImages(callback) {
-    // var req = morayClient.findObjects('imgapi_images', 'uuid=*');
-    var req = morayClient.sql('select * from ' + BUCKET.name);
-
-    req.once('error', function (err) {
-        return callback(err);
-    });
-
-    req.on('record', function (object) {
-        var value = JSON.parse(object._value);
-        if (value.billing_tags !== undefined) {
-            allBillingTags[object._key] = value.billing_tags;
-        }
-    });
-
-    req.once('end', function () {
-        return callback(null);
-    });
-}
-
-
-function migrateImage(uuid, callback) {
-    info('migrate "%s"', uuid);
-
-    // We just need to write the image again. billing_tags was previously inside
-    // _value and now we need to write the object again so its moray index is
-    // written correctly
-    morayClient.getObject(BUCKET.name, uuid, function (getErr, object) {
-        if (getErr) {
-            callback(getErr);
-            return;
-        }
-
-        var image = object.value;
-        image.billing_tags = allBillingTags[uuid];
-
-        morayClient.putObject(BUCKET.name, uuid, image, { noBucketCache: true },
-        function (err) {
-            if (err) {
-                callback(err);
-                return;
-            }
-
-            info('"billing_tags" for image %s have been updated', uuid);
-            callback();
-        });
-    });
-}
-
-function morayMigrate(callback) {
-    assert.equal(config.databaseType, 'moray');
-    morayListImages(function (err2) {
-        if (err2) {
-            return callback(err2);
-        }
-
-        var images = Object.keys(allBillingTags);
-
-        info('%d images to potentially migrate', images.length);
-        async.forEachSeries(images, migrateImage, callback);
-    });
-}
-
-function updateBucket(callback) {
-    getMorayClient(function (mclient) {
-        morayClient = mclient;
-        morayClient.getBucket(BUCKET.name, function (err, bck) {
-            if (err) {
-                return callback(err);
-            } else if (bck.index.billing_tags !== undefined) {
-                info('"billing_tags" index already exists, no need to add');
-                return callback();
-            }
-
-            info('adding "billing_tags" index');
-            morayClient.updateBucket(BUCKET.name, BUCKET.indices, callback);
-        });
-    });
-}
-
-
-
-//---- mainline
-
-function main(argv) {
-    lib_config.loadConfig({}, function (confErr, config_) {
-        if (confErr) {
-            errexit(confErr);
-            return;
-        }
-
-        config = config_;
-        assert.string(config.databaseType, 'config.databaseType');
-
-        if (config.databaseType !== 'moray') {
-            info('migration not needed, databases type is not moray');
-            process.exit(0);
-        }
-
-        updateBucket(function (updateErr) {
-            if (updateErr) {
-                errexit(updateErr);
-                return;
-            }
-
-            morayMigrate(function (err) {
-                if (err) {
-                    errexit(err);
-                } else {
-                    process.exit(0);
-                }
-            });
-        });
-    });
-}
-
-if (require.main === module) {
-    main(process.argv);
-}
+// old migration file, no longer used, just here to avoid sdcadm update failures
diff --git a/lib/migrations/migration-011-backfill-published_at.js b/lib/migrations/migration-011-backfill-published_at.js
index d849349..334e22a 100644
--- a/lib/migrations/migration-011-backfill-published_at.js
+++ b/lib/migrations/migration-011-backfill-published_at.js
@@ -1,241 +1 @@
-#!/usr/bin/env node
-/*
- * This Source Code Form is subject to the terms of the Mozilla Public
- * License, v. 2.0. If a copy of the MPL was not distributed with this
- * file, You can obtain one at http://mozilla.org/MPL/2.0/.
- */
-
-/*
- * Copyright 2016 Joyent, Inc.
- */
-
-/*
- * IMGAPI db migration: TODO: describe
- */
-
-var fs = require('fs');
-var path = require('path');
-var moray = require('moray');
-var bunyan = require('bunyan');
-var errors = require('../errors');
-var assert = require('assert-plus');
-var async = require('async');
-var passwd = require('passwd');
-var format = require('util').format;
-var execFile = require('child_process').execFile;
-var mkdirp = require('mkdirp');
-var rimraf = require('rimraf');
-
-var lib_config = require('../config');
-
-
-//---- globals
-
-var NAME = path.basename(__filename);
-
-var config;
-var morayClient = null;  // set in `getMorayClient()`
-
-var BUCKET = {
-    name: 'imgapi_images',
-    indices: {
-        index: {
-            uuid: { type: 'string', unique: true},
-            name: { type: 'string' },
-            version: { type: 'string' },
-            owner: { type: 'string' },
-            origin: { type: 'string' },
-            state: { type: 'string' },
-            urn: { type: 'string', unique: true },
-            tags: { type: '[string]' },
-            billing_tags: { type: '[string]' },
-            acl: { type: '[string]' },
-            activated: { type: 'boolean' },
-            disabled: { type: 'boolean' },
-            public: { type: 'boolean' },
-            os: { type: 'string' },
-            type: { type: 'string' },
-            expires_at: { type: 'string' },
-            published_at: { type: 'string' }
-        }
-    }
-};
-
-
-// Because regular findObjects will not load the "hidden" billing_tags values
-// we need to load all raw objets with sql() and then perform a single
-// getObject for each one of them
-var allPublishedAt = {};
-
-
-//---- support functions
-
-function errexit(err) {
-    console.error(NAME + ' error: ' + err);
-    process.exit(1);
-}
-
-function warn() {
-    arguments[0] = NAME + ' warn: ' + arguments[0];
-    console.warn.apply(null, arguments);
-}
-
-function info() {
-    arguments[0] = NAME + ' info: ' + arguments[0];
-    console.log.apply(null, arguments);
-}
-
-if (process.env.TRACE) {
-    function trace() {
-        arguments[0] = NAME + ' trace: ' + arguments[0];
-        console.log.apply(null, arguments);
-    }
-} else {
-    /*jsl:ignore*/
-    function trace() {}
-    /*jsl:end*/
-}
-
-
-function getMorayClient(callback) {
-    var client = moray.createClient({
-        connectTimeout: config.moray.connectTimeout || 200,
-        host: config.moray.host,
-        port: config.moray.port,
-        log: bunyan.createLogger({
-            name: 'moray',
-            level: 'INFO',
-            stream: process.stdout,
-            serializers: bunyan.stdSerializers
-        }),
-        reconnect: true,
-        retry: (config.moray.retry === false ? false : {
-            retries: Infinity,
-            minTimeout: 1000,
-            maxTimeout: 16000
-        })
-    });
-
-    client.on('connect', function () {
-        return callback(client);
-    });
-}
-
-
-function morayListImages(callback) {
-    var req = morayClient.sql('select * from ' + BUCKET.name);
-
-    req.once('error', function (err) {
-        return callback(err);
-    });
-
-    req.on('record', function (object) {
-        var value = JSON.parse(object._value);
-        if (value.published_at !== undefined) {
-            allPublishedAt[object._key] = value.published_at;
-        }
-    });
-
-    req.once('end', function () {
-        return callback(null);
-    });
-}
-
-
-function migrateImage(uuid, callback) {
-    trace('migrate "%s"', uuid);
-
-    // We just need to write the image again so its moray index is
-    // written correctly
-    morayClient.getObject(BUCKET.name, uuid, function (getErr, object) {
-        if (getErr) {
-            callback(getErr);
-            return;
-        }
-
-        var image = object.value;
-        image.published_at = allPublishedAt[uuid];
-
-        morayClient.putObject(BUCKET.name, uuid, image, { noBucketCache: true },
-        function (err) {
-            if (err) {
-                callback(err);
-                return;
-            }
-
-            info('"published_at" for image %s has been updated', uuid);
-            callback();
-        });
-    });
-}
-
-function morayMigrate(callback) {
-    assert.equal(config.databaseType, 'moray');
-    morayListImages(function (err2) {
-        if (err2) {
-            return callback(err2);
-        }
-
-        var images = Object.keys(allPublishedAt);
-
-        info('%d images to potentially migrate', images.length);
-        async.forEachSeries(images, migrateImage, callback);
-    });
-}
-
-function updateBucket(callback) {
-    getMorayClient(function (mclient) {
-        morayClient = mclient;
-        morayClient.getBucket(BUCKET.name, function (err, bck) {
-            if (err) {
-                return callback(err);
-            } else if (bck.index.published_at !== undefined) {
-                info('"published_at" index already exists, no need to add');
-                return callback();
-            }
-
-            info('adding "published_at" index');
-            morayClient.updateBucket(BUCKET.name, BUCKET.indices, callback);
-        });
-    });
-}
-
-
-
-//---- mainline
-
-function main(argv) {
-    lib_config.loadConfig({}, function (confErr, config_) {
-        if (confErr) {
-            errexit(confErr);
-            return;
-        }
-
-        config = config_;
-        assert.string(config.databaseType, 'config.databaseType');
-
-        if (config.databaseType !== 'moray') {
-            info('migration not needed, databases type is not moray');
-            process.exit(0);
-        }
-
-        updateBucket(function (updateErr) {
-            if (updateErr) {
-                errexit(updateErr);
-                return;
-            }
-
-            morayMigrate(function (err) {
-                if (err) {
-                    errexit(err);
-                } else {
-                    process.exit(0);
-                }
-            });
-        });
-    });
-}
-
-if (require.main === module) {
-    main(process.argv);
-}
+// old migration file, no longer used, just here to avoid sdcadm update failures
diff --git a/lib/migrations/migration-012-update-docker-image-uuids.js b/lib/migrations/migration-012-update-docker-image-uuids.js
index d013e21..334e22a 100644
--- a/lib/migrations/migration-012-update-docker-image-uuids.js
+++ b/lib/migrations/migration-012-update-docker-image-uuids.js
@@ -1,444 +1 @@
-#!/usr/bin/env node
-/*
- * This Source Code Form is subject to the terms of the Mozilla Public
- * License, v. 2.0. If a copy of the MPL was not distributed with this
- * file, You can obtain one at http://mozilla.org/MPL/2.0/.
- */
-
-/*
- * Copyright 2016 Joyent, Inc.
- */
-
-/*
- * Update type=docker images to the new UUID scheme. In early sdc-docker days
- * the image UUID was just the first half of the Docker ID. That changed to
- * be a v5 uuid of the Docker registry host and the Docker ID (see DOCKER-257).
- * This migration handles that UUID change:
- * - update 'manifest.uuid' in the DB
- * - update 'manifest.origin' in the DB
- * - drop tags.docker=true, this is obsolete
- * - move the files in storage as appropriate (new loc b/c new uuid)
- *
- * Limitations:
- * - Don't handle a "local" DB (i.e. only handle Moray DB).
- * - Don't handle a "manta" storage. While Docker images are intended to be
- *   stored in Manta, there was a bug before this migration that meant they
- *   weren't.
- */
-
-var fs = require('fs');
-var path = require('path');
-var moray = require('moray');
-var bunyan = require('bunyan');
-var assert = require('assert-plus');
-var async = require('async');
-var passwd = require('passwd');
-var format = require('util').format;
-var execFile = require('child_process').execFile;
-var mkdirp = require('mkdirp');
-var rimraf = require('rimraf');
-var imgmanifest = require('imgmanifest');
-var drc = require('docker-registry-client');
-var vasync = require('vasync');
-
-var lib_config = require('../config');
-var constants = require('../constants');
-var errors = require('../errors');
-var utils = require('../utils');
-
-
-//---- globals
-
-var NAME = path.basename(__filename);
-
-var config;
-var morayClient = null;  // set in `getMorayClient()`
-
-
-//---- support functions
-
-function errexit(err) {
-    arguments[0] = NAME + ' error: ' + arguments[0];
-    console.error.apply(null, arguments);
-    process.exit(1);
-}
-
-function warn() {
-    arguments[0] = NAME + ' warn: ' + arguments[0];
-    console.warn.apply(null, arguments);
-}
-
-function info() {
-    arguments[0] = NAME + ' info: ' + arguments[0];
-    console.log.apply(null, arguments);
-}
-
-var trace = function () {};
-if (process.env.TRACE) {
-    trace = function () {
-        arguments[0] = NAME + ' trace: ' + arguments[0];
-        console.log.apply(null, arguments);
-    };
-}
-
-var _nobodyCache;
-function getNobody(callback) {
-    if (_nobodyCache !== undefined)
-        return callback(_nobodyCache);
-
-    passwd.get('nobody', function (nobody) {
-        _nobodyCache = nobody;
-        callback(_nobodyCache);
-    });
-}
-
-
-var BUCKET = 'imgapi_images';
-var localDir;
-
-
-function getMorayClient(callback) {
-    var client = moray.createClient({
-        connectTimeout: config.moray.connectTimeout || 200,
-        host: config.moray.host,
-        port: config.moray.port,
-        log: bunyan.createLogger({
-            name: 'moray',
-            level: 'INFO',
-            stream: process.stdout,
-            serializers: bunyan.stdSerializers
-        }),
-        reconnect: true,
-        retry: (config.moray.retry === false ? false : {
-            retries: Infinity,
-            minTimeout: 1000,
-            maxTimeout: 16000
-        })
-    });
-
-    client.on('connect', function () {
-        return callback(client);
-    });
-}
-
-function morayFind(bucket, filter, callback) {
-    var hits = [];
-    var req = morayClient.findObjects(bucket, filter);
-    req.once('error', function (err) {
-        trace('morayFind(%s, "%s") error: %s', bucket, filter, err);
-        return callback(err);
-    });
-    req.on('record', function (object) {
-        hits.push(object);
-    });
-    req.once('end', function () {
-        trace('morayFind(%s, "%s") hits: %j', bucket, filter, hits);
-        return callback(null, hits);
-    });
-}
-
-function morayListImages(callback) {
-    var images = [];
-    var req = morayClient.sql('select * from ' + BUCKET
-        + ' where type=\'docker\' and activated');
-
-    req.once('error', function (err) {
-        return callback(err);
-    });
-
-    req.on('record', function (object) {
-        var value = JSON.parse(object._value);
-        images.push(value.uuid);
-    });
-
-    req.once('end', function () {
-        return callback(null, images);
-    });
-}
-
-
-/*
- * Images are going to be migrated only under the following conditions:
- *
- * 1. type === 'docker'
- * 2. files[0].stor === local
- * 3. tags['docker:repo'] and tags['docker:id'] are present
- * 4. newUuid !== uuid
- */
-function migrateImage(uuid, callback) {
-    trace('migrate "%s"', uuid);
-    var image;
-    var indexName;
-    var newUuid;
-    var origin;
-
-    var oldPrefix = uuid.slice(0, 3);
-    var oldFile = path.join(localDir, oldPrefix, uuid, 'file0');
-
-    function getOrigin(cb) {
-        if (!image.origin) {
-            return cb();
-        }
-
-        morayClient.getObject(BUCKET, image.origin, function (getErr, object) {
-            if (getErr && getErr.name === 'ObjectNotFoundError') {
-                /*
-                 * If the origin was already migrated, it won't be there
-                 * anymore. We have the uuid, which in the old world, is the
-                 * first half of the Docker ID. We'll make a short (12-char)
-                 * Docker ID and search by 'version=shortDockerId'. Bit of a
-                 * hack, but should have a unique hit.
-                 */
-                var shortId = image.origin.replace(/-/g, '').slice(0, 12);
-                var filter = '(&(name=docker-layer)(version=' + shortId + '))';
-                morayFind(BUCKET, filter, function (getErr2, hits) {
-                    if (getErr2) {
-                        cb(getErr2);
-                    } else {
-                        assert.equal(hits.length, 1,
-                            'unexpected number of image hits for origin: '
-                            + hits);
-                        origin = hits[0].value;
-                        cb();
-                    }
-                });
-            } else if (getErr) {
-                cb(getErr);
-            } else {
-                origin = object.value;
-                cb();
-            }
-        });
-    }
-
-    function linkFile(cb) {
-        info('migrating image %s to new UUID %s', uuid, newUuid);
-
-        var newPrefix = newUuid.slice(0, 3);
-        var newDir = path.join(localDir, newPrefix, newUuid);
-        var newFile = path.join(newDir, 'file0');
-
-        vasync.pipeline({arg: {}, funcs: [
-            function getNobodyInfo(ctx, next) {
-                getNobody(function (nobody) {
-                    if (!nobody) {
-                        next(new Error('could not get nobody user'));
-                    } else {
-                        ctx.nobody = nobody;
-                        next();
-                    }
-                });
-            },
-
-            function makeNewDir(ctx, next) {
-                mkdirp(newDir, function (err) {
-                    if (err) {
-                        warn('Could not mkdir -p %s', newDir);
-                        next(err);
-                    } else {
-                        next();
-                    }
-                });
-            },
-
-            function chownNewDir(ctx, next) {
-                // IMGAPI runs as 'nobody' and must own this dir.
-                fs.chown(newDir,
-                    Number(ctx.nobody.userId),
-                    Number(ctx.nobody.groupId),
-                    next);
-            },
-
-            function chownNewPrefixDir(ctx, next) {
-                fs.chown(path.dirname(newDir),
-                    Number(ctx.nobody.userId),
-                    Number(ctx.nobody.groupId),
-                    next);
-            },
-
-            function linkTheFile(ctx, next) {
-                fs.link(oldFile, newFile, function (err) {
-                    if (err && err.code && err.code === 'EEXIST') {
-                        warn('Destination file already exists: %s', newFile);
-                        next();
-                    } else if (err) {
-                        warn('Could not link %s', newFile);
-                        next(err);
-                    } else {
-                        // IMGAPI runs as 'nobody' and must own this file.
-                        fs.chown(newFile,
-                            Number(ctx.nobody.userId),
-                            Number(ctx.nobody.groupId),
-                            next);
-                    }
-                });
-            }
-
-        ]}, function (err) {
-            if (!err) {
-                info('%s has been linked successfully', newFile);
-            }
-            cb(err);
-        });
-    }
-
-    function recreateObject(cb) {
-        assert.ok(uuid !== newUuid);
-
-        // Updates to the manifest.
-        image.uuid = newUuid;
-        if (config.adminUuid && image.owner === config.adminUuid) {
-            // Move all sdc-docker-managed images to be private to admin.
-            image.public = false;
-        }
-        if (image.tagsObj['docker'] === true) {
-            delete image.tagsObj['docker'];
-        }
-        // Normalize tags['docker:repo'] to a repo 'localName'. Earlier code
-        // would have, e.g. 'library/busybox'.
-        if (image.tagsObj['docker:repo']) {
-            var norm = drc.parseRepo(image.tagsObj['docker:repo']).localName;
-            image.tagsObj['docker:repo'] = norm;
-        }
-        image.tags = utils.tagsSearchArrayFromObj(image.tagsObj);
-        if (image.origin) {
-            image.origin = imgmanifest.imgUuidFromDockerInfo({
-                id: origin.tagsObj['docker:id'],
-                indexName: indexName
-            });
-        }
-
-        // Save changes to the DB.
-        var batch = [ {
-            bucket: BUCKET,
-            key: newUuid,
-            value: image
-        }, {
-            bucket: BUCKET,
-            operation: 'delete',
-            key: uuid
-        }];
-        morayClient.batch(batch, cb);
-    }
-
-    function unlinkFile(cb) {
-        fs.unlink(oldFile, function (err) {
-            if (err) {
-                return cb(err);
-            }
-            info('%s has been removed successfully', oldFile);
-            return cb();
-        });
-    }
-
-    morayClient.getObject(BUCKET, uuid, function (getErr, object) {
-        if (getErr) {
-            callback(getErr);
-            return;
-        }
-
-        image = object.value;
-
-        if (!image.tagsObj || !image.tagsObj['docker:id']) {
-            errexit('cannot migrate %s. Image docker tags are incomplete',
-                uuid);
-            callback();
-            return;
-        }
-
-        var dockerId = image.tagsObj['docker:id'];
-        if (dockerId.length !== 64) {
-            // It's a different docker id to what we are expecting - skip it, as
-            // it's likely a newer (v2.2) docker id of the form 'sha256:123...'.
-            info('Skipped dockerId where length != 64: %s', dockerId);
-            callback();
-            return;
-        }
-
-        if (!image.tagsObj['docker:repo']) {
-            // Cheat. We know before this migration (which was part of
-            // private registry support) that the only index/registry from
-            // which pulls were supported was 'docker.io'.
-            indexName = 'docker.io';
-        }
-        if (!indexName) {
-            indexName = drc.parseRepo(image.tagsObj['docker:repo']).index.name;
-        }
-
-        newUuid = imgmanifest.imgUuidFromDockerInfo({
-            id: dockerId,
-            indexName: indexName
-        });
-
-        if (newUuid === uuid) {
-            trace('image %s appears to have been migrated already', uuid);
-            callback();
-            return;
-        }
-
-        if (!image.files || image.files[0].stor !== 'local') {
-            errexit('cannot migrate %s. Image file is not stored locally',
-                uuid);
-            callback();
-            return;
-        }
-
-        async.series([
-            getOrigin,
-            linkFile,
-            recreateObject,
-            unlinkFile
-        ], callback);
-    });
-}
-
-function morayMigrate(callback) {
-    getMorayClient(function (mclient) {
-        morayClient = mclient;
-
-        morayListImages(function (err, images) {
-            if (err) {
-                return callback(err);
-            }
-
-            info('%d images to potentially migrate', images.length);
-            async.forEachSeries(images, migrateImage, callback);
-        });
-    });
-}
-
-
-//---- mainline
-
-function main(argv) {
-    lib_config.loadConfig({}, function (confErr, config_) {
-        if (confErr) {
-            errexit(confErr);
-            return;
-        }
-
-        config = config_;
-        assert.string(config.databaseType, 'config.databaseType');
-
-        if (config.databaseType !== 'moray') {
-            errexit('migration not supported, database type is not moray');
-        }
-
-        localDir = constants.STORAGE_LOCAL_IMAGES_DIR;
-        assert.ok(localDir !== '/',
-            'cannot have empty or root dir for local storage');
-
-        morayMigrate(function (err) {
-            if (err) {
-                errexit(err);
-            } else {
-                info('finished sucessfully');
-                process.exit(0);
-            }
-        });
-    });
-}
-
-if (require.main === module) {
-    main(process.argv);
-}
+// old migration file, no longer used, just here to avoid sdcadm update failures
diff --git a/lib/storage.js b/lib/storage.js
index e41e308..8285872 100644
--- a/lib/storage.js
+++ b/lib/storage.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright 2016 Joyent, Inc.
+ * Copyright 2017 Joyent, Inc.
  */
 
 /*
@@ -132,6 +132,18 @@ Storage.prototype.storeFileFromStream =
  */
 Storage.prototype.moveImageFile = function (image, from, to, callback) {};
 
+/**
+ * Moves an image file from one image to another image.
+ *
+ * @param fromImage {Image}
+ * @param toImage {Image}
+ * @param filename {String} Image filename (i.e. 'file0')
+ * @param callback {Function} `function (err)`
+ */
+Storage.prototype.moveFileBetweenImages =
+    function (fromImage, toImage, filename, callback) {
+};
+
 
 /**
  * Returns the archive path for image manifests. Image manifests are archived
@@ -437,6 +449,33 @@ LocalStorage.prototype.moveImageFile = function (image, from, to, callback) {
 };
 
 
+LocalStorage.prototype.moveFileBetweenImages =
+function (fromImage, toImage, filename, callback) {
+    assert.object(fromImage, 'fromImage');
+    assert.uuid(fromImage.uuid, 'fromImage.uuid');
+    assert.object(toImage, 'toImage');
+    assert.uuid(toImage.uuid, 'toImage.uuid');
+    assert.string(filename, 'filename');
+    assert.func(callback, 'callback');
+
+    var fromPath = this.storPathFromImageUuid(fromImage.uuid, filename);
+    var toPath = this.storPathFromImageUuid(toImage.uuid, filename);
+
+    var toDir = path.dirname(toPath);
+    mkdirp(toDir, function (mkdirErr) {
+        if (mkdirErr) {
+            return callback(mkdirErr);
+        }
+        fs.rename(fromPath, toPath, function (err) {
+            if (err) {
+                return callback(err);
+            }
+            return callback(null);
+        });
+    });
+};
+
+
 LocalStorage.prototype.archiveImageManifest = function (manifest, callback) {
     assert.object(manifest, 'manifest');
     assert.string(manifest.uuid, 'manifest.uuid');
@@ -716,6 +755,40 @@ MantaStorage.prototype.moveImageFile = function (image, from, to, callback) {
     });
 };
 
+MantaStorage.prototype.moveFileBetweenImages =
+function (fromImage, toImage, filename, callback) {
+    assert.object(fromImage, 'fromImage');
+    assert.uuid(fromImage.uuid, 'fromImage.uuid');
+    assert.object(toImage, 'toImage');
+    assert.uuid(toImage.uuid, 'toImage.uuid');
+    assert.string(filename, 'filename');
+    assert.func(callback, 'callback');
+
+    var fromPath = this._storPathFromImageUuid(fromImage.uuid, filename);
+    var toPath = this._storPathFromImageUuid(toImage.uuid, filename);
+    var toDir = path.dirname(toPath);
+
+    var self = this;
+
+    self.client.mkdirp(toDir, function (mkdirErr) {
+        if (mkdirErr) {
+            return callback(mkdirErr);
+        }
+        self.client.ln(fromPath, toPath, function (err) {
+            if (err) {
+                return callback(err);
+            }
+
+            self.deleteImageFile(fromImage, filename, function (delErr) {
+                if (delErr) {
+                    return callback(delErr);
+                }
+                return callback();
+            });
+        });
+    });
+};
+
 MantaStorage.prototype.archiveImageManifest =
 function (manifest, callback) {
     assert.object(manifest, 'manifest');
diff --git a/package.json b/package.json
index c6e7847..c0c790e 100644
--- a/package.json
+++ b/package.json
@@ -1,12 +1,13 @@
 {
   "name": "imgapi",
   "description": "Image API to manage images for SDC 7",
-  "version": "3.3.1",
+  "version": "4.0.0",
   "author": "Joyent (joyent.com)",
   "private": true,
   "dependencies": {
     "assert-plus": "1.0.0",
     "async": "0.7.0",
+    "buffer-peek-stream": "1.0.1",
     "bunyan": "1.8.8",
     "cmdln": "3.2.1",
     "dashdash": "1.10.0",
@@ -17,7 +18,7 @@
     "forkexec": "1.1.0",
     "glob": "7.0.5",
     "handlebars": "4.0.5",
-    "imgmanifest": "2.3.0",
+    "imgmanifest": "3.0.0",
     "json": "9.0.4",
     "ldap-filter": "0.3.1",
     "uuid": "2.0.2",
