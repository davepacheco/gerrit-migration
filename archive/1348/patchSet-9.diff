From 72af6afd0151170f966d969f7197d38654d132b0 Mon Sep 17 00:00:00 2001
From: Todd Whiteman <todd.whiteman@joyent.com>
Date: Mon, 8 May 2017 19:46:46 -0700
Subject: [PATCH] DOCKER-929 Support the docker v2.2 manifest format

---
 lib/errors.js                                 |   19 +
 lib/images.js                                 | 1725 ++++++++++-------
 lib/magic.js                                  |    4 +-
 .../migration-008-new-storage-layout.js       |  336 +---
 .../migration-009-backfill-archive.js         |  313 +--
 .../migration-010-backfill-billing_tags.js    |  230 +--
 .../migration-011-backfill-published_at.js    |  242 +--
 ...migration-012-update-docker-image-uuids.js |  445 +----
 lib/storage.js                                |   75 +-
 package.json                                  |    5 +-
 10 files changed, 1171 insertions(+), 2223 deletions(-)

diff --git a/lib/errors.js b/lib/errors.js
index 7996779..e21a888 100644
--- a/lib/errors.js
+++ b/lib/errors.js
@@ -241,6 +241,25 @@ DownloadError.statusCode = 400;
 DownloadError.description = 'There was a problem with the download.';
 
 
+function ImageFileTooBigError(cause, message) {
+    if (message === undefined) {
+        message = cause;
+        cause = undefined;
+    }
+    RestError.call(this, {
+        restCode: this.constructor.restCode,
+        statusCode: this.constructor.statusCode,
+        message: message,
+        cause: cause
+    });
+}
+util.inherits(DownloadError, RestError);
+DownloadError.prototype.name = 'ImageFileTooBigError';
+DownloadError.restCode = 'ImageFileTooBig';
+DownloadError.statusCode = 400;
+DownloadError.description = 'The image file size exceeds the max allowed size.';
+
+
 function StorageIsDownError(cause) {
     var message = 'storage is down at the moment';
     RestError.call(this, {
diff --git a/lib/images.js b/lib/images.js
index 6673217..6b9ddd2 100644
--- a/lib/images.js
+++ b/lib/images.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright 2016 Joyent, Inc.
+ * Copyright 2017 Joyent, Inc.
  */
 
 /*
@@ -19,6 +19,7 @@ var fs = require('fs');
 var crypto = require('crypto');
 var url = require('url');
 var path = require('path');
+var zlib = require('zlib');
 
 var assert = require('assert-plus');
 var async = require('async');
@@ -28,10 +29,12 @@ var lib_uuid = require('uuid');
 var once = require('once');
 var restify = require('restify');
 var sdcClients = require('sdc-clients');
+var streampeek = require('buffer-peek-stream');
 var vasync = require('vasync');
 
 var channels = require('./channels');
 var errors = require('./errors');
+var magic = require('./magic');
 var utils = require('./utils'),
     objCopy = utils.objCopy,
     boolFromString = utils.boolFromString,
@@ -303,6 +306,52 @@ Image.prototype.addFile = function addFile(app, file, log, callback) {
 };
 
 
+/**
+ * Move the image file from this, into the given Image instance.
+ *
+ * @param app {App} The IMGAPI app.
+ * @param file {Object} Describes the uploaded file, with keys:
+ *      - `sha1` {String}
+ *      - `size` {Integer}
+ *      - `stor` {String}
+ *      - `contentMD5` {String}
+ *      - `mtime` {String} ISO date string
+ * @param log {Bunyan Logger}
+ * @param callback {Function} `function (err)` where `err` is some internal
+ *      detail (i.e. it should be wrapped for the user).
+ */
+Image.prototype.moveFileToImage =
+function moveFileToImage(app, toImage, log, callback) {
+    var files = this.files;
+    assert.equal(files.length, 1, 'Expect exactly one image file');
+    var file = files[0];
+    var self = this;
+
+    var stor = app.getStor(file.stor);
+    stor.moveFileBetweenImages(self, toImage, 'file0',
+            function _moveFileCb(err) {
+        if (err) {
+            callback(err);
+            return;
+        }
+        toImage.addFile(app, file, log, function _addFileCb(addErr) {
+            if (addErr) {
+                callback(addErr);
+                return;
+            }
+            // Null out the file fields.
+            self.raw.files = [];
+            delete self._filesCache;
+
+            log.debug({fromUuid: self.uuid, toUuid: toImage.uuid},
+                'Moving file0 between images');
+
+            Image.modify(app, self, log, callback);
+        });
+    });
+};
+
+
 /**
  * Add an uploaded icon to this Image instance. The file will have already
  * be written out (to disk or to manta, depending).
@@ -2402,154 +2451,140 @@ function apiAdminImportRemoteImage(req, res, callback) {
  * Called during `AdminImportDockerImage` to create (unactivated) and
  * download the file for a single type=docker image.
  *
+ * Note that if this function returns an error that is a DownloadError instance,
+ * the caller can schedule a retry of the download, whilst any other error
+ * indicates that the caller should not retry the download.
+ *
  * @param opts {Object}
  *      - @param ctx {Object} The run context for the
  *        `apiAdminImportRemoteImage` call.
- *      - @param imgId {String} The docker image id.
- *      - @param imgJson {Object} If this is a v2 import, then we'll have the
- *        imgJson already.
- *      - @param fsLayer {Object} If this is a v2 import, this is the object
- *        from `manifest.fsLayers`.
+ *      - @param imgJson {Object} Docker image object (config, rootfs, layers).
+ *      - @param digest {String} This is the layer sha256 digest.
+ *      - @param layerDigests {Array} All digests in the chain (including the
+ *        current digest as the last entry).
+ *      - @param uncompressedDigest {String} Sha256 of the uncompressed layer.
  * @param callback {Function}
  */
-function _dockerDownloadAndImportImage(opts, callback) {
-    assert.string(opts.imgId, 'opts.imgId');
+function _dockerDownloadAndImportLayer(opts, callback) {
+    assert.object(opts, 'opts');
+    assert.optionalString(opts.compression, 'opts.compression');
     assert.object(opts.ctx, 'opts.ctx');
+    assert.object(opts.ctx.digestFromUuid, 'opts.ctx.digestFromUuid');
+    assert.object(opts.ctx.rat, 'opts.ctx.rat');
+    assert.object(opts.ctx.regClientV2, 'opts.ctx.regClientV2');
+    assert.object(opts.ctx.req, 'opts.ctx.req');
+    assert.func(opts.ctx.resMessage, 'opts.ctx.resMessage');
+    assert.string(opts.digest, 'opts.digest');
+    assert.object(opts.imgJson, 'opts.imgJson');
+    assert.arrayOfString(opts.layerDigests, 'opts.layerDigests');
+    assert.optionalString(opts.uncompressedDigest, 'opts.uncompressedDigest');
     assert.func(callback, 'callback');
-    var ctx = opts.ctx;
-    assert.finite(ctx.regV, 'ctx.regV');
-    if (ctx.regV === 1) {
-        assert.object(ctx.regClientV1, 'ctx.regClientV1');
-    } else {
-        assert.object(ctx.regClientV2, 'ctx.regClientV2');
-        assert.object(opts.imgJson, 'ctx.imgJson');
-        assert.object(opts.fsLayer, 'ctx.fsLayer');
-    }
 
-    var imgId = opts.imgId;
+    var compression = opts.compression;
+    var ctx = opts.ctx;
+    var digest = opts.digest;
     var imgJson = opts.imgJson;
     var req = ctx.req;
     var app = req._app;
     var log = req.log;
     var rat = ctx.rat;
 
-    try {
-        var uuid = imgmanifest.imgUuidFromDockerInfo({
-            id: imgId,
-            indexName: rat.index.name
-        });
-    } catch (infoErr) {
-        return callback(infoErr);
-    }
-    var active = false;
-    var addImageFileAttempts = 0;
-    var unactivated = false;
     var fileSize = -1; // The same value used in Docker-docker for "don't know".
     var manifest;
     var newImage;
+    var shortId = imgmanifest.shortDockerId(
+        imgmanifest.dockerIdFromDigest(digest));
+    var uuid = imgmanifest.imgUuidFromDockerDigests(opts.layerDigests);
+
+    var uncompressedSha256;
+    if (opts.uncompressedDigest) {
+        uncompressedSha256 = imgmanifest.dockerIdFromDigest(
+            opts.uncompressedDigest);
+    }
+
+    function progressStatus(msg, progressDetail) {
+        var payload = {
+            id: shortId,
+            status: msg
+        };
+        if (progressDetail) {
+            payload.progressDetail = progressDetail;
+        }
+        ctx.resMessage({
+            type: 'progress',
+            payload: payload
+        });
+    }
+
+    // Remember the uuid -> digest relationship.
+    ctx.digestFromUuid[uuid] = digest;
 
-    log.debug({imgId: imgId, uuid: uuid},
+    log.debug({uuid: uuid, digest: digest},
         'AdminImportDockerImage: check if image already exists');
     Image.get(app, uuid, log, function (gErr, image) {
         if (!gErr) {
             assert.object(image, 'image');
-            ctx.imageFromImgId[imgId] = newImage = image;
 
-            if (newImage.state === 'unactivated') {
-                unactivated = true;
-            } else {
-                // TODO: Can we `resMessage('Already exists')` and early abort?
-                active = true;
-                ctx.alreadyExistsFromImgId[imgId] = true;
+            if (image.state !== 'unactivated') {
+                // When the image already exists, just return the image as is.
+                log.debug({uuid: uuid, repo: rat.canonicalName, digest: digest},
+                    'AdminImportDockerImage: layer already exists');
+                progressStatus('Already exists');
+                callback(null, image);
+                return;
             }
 
             // Mark this Image as existing in the database
-            newImage.exists = true;
+            log.debug({uuid: uuid, repo: rat.canonicalName, digest: digest},
+                'AdminImportDockerImage: layer exists, but is unactivated');
+            newImage = image;
 
         } else if (gErr.restCode !== 'ResourceNotFound') {
             return callback(gErr);
         }
 
         // Check if this image layer has already been downloaded before.
-        if (DOCKER_IMAGE_CACHE.hasOwnProperty(imgId)) {
-            var cachedItem = DOCKER_IMAGE_CACHE[imgId];
-            ctx.fileInfoFromImgId[imgId] = cachedItem.fileInfo;
-            ctx.imageFromImgId[imgId] = cachedItem.image;
-            ctx.alreadyExistsFromImgId[imgId] = true;
+        if (DOCKER_IMAGE_CACHE.hasOwnProperty(uuid)) {
+            var cachedItem = DOCKER_IMAGE_CACHE[uuid];
+            ctx.newFileInfoFromUuid[uuid] = cachedItem;
+            newImage = cachedItem.image;
 
-            log.debug({imgId: imgId, uuid: uuid},
+            log.debug({digest: digest, uuid: uuid},
                 'dockerDownloadAndImportImage: image layer already cached');
 
-            callback();
+            progressStatus('Download complete (cached)');
+            callback(null, newImage);
             return;
         }
 
-        log.debug({uuid: uuid, repo: rat.canonicalName, imgId: imgId},
+        log.debug({uuid: uuid, repo: rat.canonicalName, digest: digest},
             'AdminImportDockerImage: start import');
 
-        vasync.pipeline({ funcs: [
-            getImgJson,
+        vasync.pipeline({ arg: {}, funcs: [
             genImgapiManifest,
             handleOwner,
             handleChannels,
+            createReadStream,
+            detectCompression,
+            uncompressAndSha256Contents,
             createImageFromManifest,
-            addImageFile
-        ]}, function afterPipe(pipeErr, results) {
+            addImageFile,
+            addUncompressedSha256
+        ]}, function afterPipe(pipeErr) {
             if (pipeErr) {
-                log.info({imgId: imgId, err: pipeErr},
-                    'dockerDownloadAndImportImage: error downloading layer');
-                ctx.resMessage({
-                    type: 'progress',
-                    payload: {
-                        id: imgId.substr(0, 12),
-                        status: format('import error: %s', pipeErr.message)
-                    }
-                });
                 callback(pipeErr);
                 return;
             }
-
-            ctx.resMessage({
-                type: 'progress',
-                payload: {
-                    id: imgId.substr(0, 12),
-                    status: (active ? 'Already exists' : 'Download complete')
-                }
-            });
-            ctx.resMessage({
-                type: 'data',
-                imgJson: imgJson,
-                image: newImage.serialize(app.mode, req.getVersion()),
-                private: ctx.isPrivate
-            });
-            callback();
+            progressStatus('Download complete');
+            callback(null, newImage);
         });
     });
 
-    function getImgJson(_, next) {
-        if (imgJson) {
-            return next();
-        }
-
-        ctx.resMessage({
-            type: 'progress',
-            payload: {
-                id: imgId.substr(0, 12),
-                status: 'Pulling metadata'
-            }
-        });
-
-        ctx.regClientV1.getImgJson({
-            imgId: imgId
-        }, function (err, imgJson_, getRes) {
-            imgJson = imgJson_;
-            next(errors.wrapErrorFromDrc(err));
-        });
-    }
-
     function genImgapiManifest(_, next) {
         try {
             manifest = imgmanifest.imgManifestFromDockerInfo({
+                uuid: uuid,
+                layerDigests: opts.layerDigests,
                 imgJson: imgJson,
                 repo: rat,
                 public: ctx.public_
@@ -2566,7 +2601,7 @@ function _dockerDownloadAndImportImage(opts, callback) {
     }
 
     function handleOwner(_, next) {
-        if (active || unactivated) {
+        if (newImage) {
             next();
             return;
         }
@@ -2593,7 +2628,7 @@ function _dockerDownloadAndImportImage(opts, callback) {
     }
 
     function handleChannels(_, next) {
-        if (active || unactivated) {
+        if (newImage) {
             next();
             return;
         }
@@ -2605,252 +2640,358 @@ function _dockerDownloadAndImportImage(opts, callback) {
         next();
     }
 
-    function createImageFromManifest(_, next) {
-        if (active || unactivated) {
+    function createReadStream(arg, next) {
+        ctx.regClientV2.createBlobReadStream({digest: digest},
+                function onStreamCb(err, blobStream) {
+            if (err) {
+                next(errors.wrapErrorFromDrc(err));
+                return;
+            }
+            arg.httpResponse = blobStream;
+            arg.stream = blobStream;
+            next();
+        });
+    }
+
+    function detectCompression(arg, next) {
+        if (compression) {
             next();
             return;
         }
 
-        log.debug({ data: manifest }, 'AdminImportDockerImage: create it');
-        Image.create(app, manifest, true, false, function (cErr, img) {
-            if (cErr) {
-                return next(cErr);
+        streampeek(arg.stream, magic.maxMagicLen,
+                function onpeek(err, buf, stream) {
+            if (err) {
+                next(err);
+                return;
             }
-            ctx.imageFromImgId[imgId] = newImage = img;
+
+            // Update stream reference.
+            arg.stream = stream;
+
+            if (buf.length < magic.maxMagicLen) {
+                // Not a compressed file.
+                compression = 'none';
+                next();
+                return;
+            }
+
+            compression = magic.compressionTypeFromBufSync(buf) || 'none';
             next();
         });
     }
 
-    function addImageFile(_, next) {
-        if (active) {
+    function uncompressAndSha256Contents(arg, next) {
+        if (compression === 'none' || uncompressedSha256) {
             next();
             return;
         }
 
-        log.debug({imgId: imgId}, 'AddImageLayer: start');
+        // Need to uncompress the data to get the sha256.
+        var uncompressStream;
+        if (compression === 'gzip') {
+            uncompressStream = zlib.createGunzip();
+        } else if (compression === 'bzip2') {
+            uncompressStream = zlib.createBunzip2();
+        } else {
+            // Unsupported compression stream.
+            next(new errors.InternalError(format(
+                'Unsupported layer compression: %s', compression)));
+            return;
+        }
+
+        var sha256sum = crypto.createHash('sha256');
+        sha256sum.on('readable', function () {
+            var hash = sha256sum.read();
+            if (hash) {
+                uncompressedSha256 = hash.toString('hex');
+                // Check if the final callback is waiting for this hash.
+                if (arg.uncompressedSha256Callback) {
+                    arg.uncompressedSha256Callback(null, uncompressedSha256);
+                }
+            }
+        });
+        uncompressStream.pipe(sha256sum);
 
-        ctx.resMessage({
-            type: 'progress',
-            payload: {
-                id: imgId.substr(0, 12),
-                status: 'Pulling fs layer'
+        // Pipe contents, but ensure stream is put back into paused mode.
+        arg.stream.pipe(uncompressStream);
+        arg.stream.pause();
+
+        next();
+    }
+
+    function createImageFromManifest(_, next) {
+        if (newImage) {
+            next();
+            return;
+        }
+
+        log.debug({ data: manifest }, 'AdminImportDockerImage: create it');
+        Image.create(app, manifest, true, false, function (cErr, img) {
+            if (cErr) {
+                return next(cErr);
             }
+            newImage = img;
+            next();
         });
+    }
 
-        (function createLayerReadStream(cbStream) {
-            if (ctx.regV === 1) {
-                ctx.regClientV1.getImgLayerStream(
-                    {imgId: imgId},
-                    cbStream);
-            } else {
-                ctx.regClientV2.createBlobReadStream(
-                    {digest: opts.fsLayer.blobSum},
-                    cbStream);
+    function addImageFile(arg, next) {
+        log.debug({digest: digest}, 'AddImageFile: start');
+
+        var connectionTimeoutHandler;
+        var DOCKER_READ_STREAM_TIMEOUT = 15 * 1000;
+        var md5sum = crypto.createHash('md5');
+        // Send a progress message whenever we've downloaded at least
+        // `progUpdateEvery` data (i.e. every 1/2 MiB).
+        var progLastUpdateSize = 0;
+        var progUpdateEvery = 512 * 1024;
+        var resp = arg.httpResponse;
+        var shasum = crypto.createHash('sha1');
+        var sha256sum = crypto.createHash('sha256');
+        var size = 0;
+        var startTs = Math.floor(new Date().getTime() / 1000);
+        var stor;  // the storage class
+        var stream = arg.stream;
+
+        progressStatus('Pulling fs layer');
+
+        if (resp.headers['content-length'] !== undefined) {
+            fileSize = Number(resp.headers['content-length']);
+            if (fileSize > MAX_IMAGE_SIZE) {
+                // Using ImageFileTooBigError instead of DownloadError so the
+                // caller doesn't retry to download.
+                return next(new errors.ImageFileTooBigError(format(
+                    'Image file size, %s, exceeds the maximum allowed ' +
+                    'file size, %s', fileSize, MAX_IMAGE_SIZE_STR)));
             }
-        })(function (err, stream) {
-            if (err) {
-                next(errors.wrapErrorFromDrc(err));
-                return;
+        }
+
+        // Setup a response timeout listener to handle connection timeout.
+        assert.object(resp.connection, 'resp.connection');
+
+        resp.connection.setTimeout(DOCKER_READ_STREAM_TIMEOUT);
+        connectionTimeoutHandler = function onDockerConnectionTimeout() {
+            log.info({digest: digest, size: size, fileSize: fileSize},
+                'dockerDownloadAndImportImage: '
+                + 'createBlobReadStream connection timed out');
+            progressStatus('Connection timed out');
+            // Note that by destroying the stream this will result in a
+            // call to finish() with an error, as the drc
+            // createBlobReadStream handler has an 'end' handler that
+            // validates the size and digest of downloaded data and
+            // emits an error event when all the data wasn't downloaded.
+            resp.destroy();
+        };
+        resp.connection.on('timeout', connectionTimeoutHandler);
+
+        function finish_(fErr, tmpFilename, filename) {
+            // Remove connection timeout handler.
+            resp.connection.removeListener('timeout', connectionTimeoutHandler);
+            connectionTimeoutHandler = null;
+
+            if (fErr) {
+                log.info({digest: digest, err: fErr},
+                    'dockerDownloadAndImportImage: error');
+                return next(fErr);
+            } else if (ctx.downloadsCanceled) {
+                return next(new errors.DownloadError('Download canceled'));
+            } else if (size > MAX_IMAGE_SIZE) {
+                // Using ImageFileTooBigError instead of DownloadError so the
+                // caller doesn't retry to download.
+                return next(new errors.ImageFileTooBigError(format(
+                    'Image file size, %s, exceeds the maximum allowed ' +
+                    'file size, %s', size, MAX_IMAGE_SIZE_STR)));
+            } else if (fileSize >= 0 && size !== fileSize) {
+                return next(new errors.DownloadError(format(
+                    'Download error: "Content-Length" header, %s, does ' +
+                    'not match downloaded size, %d', fileSize, size)));
             }
 
-            assert.object(stream.connection, 'stream.connection');
+            var sha1 = shasum.digest('hex');
+            var sha256 = sha256sum.digest('hex');
+
+            // Validate the sha256 of the downloaded bits matches the
+            // digest, if they don't match there is a corruption.
+            var gotDigest = 'sha256:' + sha256;
+            if (gotDigest !== digest) {
+                // Note that we don't cleanup the failed image file download,
+                // that is handled by IMGAPI-616.
+                log.warn({expectedDigest: digest, gotDigest: gotDigest},
+                    'Downloaded layer sha256 digest does not match');
+                next(new errors.DownloadError(format(
+                    'layer digest does not match, got %s, expected %s',
+                    gotDigest, digest)));
+                return;
+            }
 
-            var DOCKER_READ_STREAM_TIMEOUT = 15 * 1000;
-            var MAX_IMAGE_FILE_DOWNLOAD_ATTEMPTS = 5;
+            var file = {
+                sha1: sha1,
+                sha256: sha256,
+                size: size,
+                contentMD5: md5sum.digest('base64'),
+                mtime: (new Date()).toISOString(),
+                stor: stor.type,
+                compression: compression
+            };
 
-            var lastUpdate = 0;
-            var updateEvery = 512 * 1024;
-            var startTs = Math.floor(new Date().getTime() / 1000);
-            var streamTimeoutHandler;
+            ctx.newFileInfoFromUuid[uuid] = {
+                file: file,
+                image: newImage,
+                storage: stor.type,
+                tmpFilename: tmpFilename,
+                filename: filename
+            };
 
-            var compression = 'none';
+            log.info({digest: digest, fileSize: fileSize},
+                'dockerDownloadAndImportImage: Download successful');
+            return next();
+        }
+        var finish = once(finish_);
 
-            // Setup a timeout listener and handle connection timeout.
-            stream.connection.setTimeout(DOCKER_READ_STREAM_TIMEOUT);
-            streamTimeoutHandler = function onDockerStreamTimeout() {
-                log.info({imgId: imgId, size: size, fileSize: fileSize},
-                    'dockerDownloadAndImportImage: '
-                    + 'createBlobReadStream connection timed out');
-                ctx.resMessage({
-                    type: 'progress',
-                    payload: {
-                        id: imgId.substr(0, 12),
-                        status: 'Connection timed out',
-                        progressDetail: {
-                            current: size,
-                            total: fileSize,
-                            start: startTs
-                        }
-                    }
-                });
-                // Note that by destroying the stream this will result in a
-                // call to finish() with an error, as the drc
-                // createBlobReadStream handler has an 'end' handler that
-                // validates the size and digest of downloaded data and
-                // emits an error event when all the data wasn't downloaded.
+        stream.on('data', function (chunk) {
+            if (ctx.downloadsCanceled) {
                 stream.destroy();
-            };
-            stream.connection.on('timeout', streamTimeoutHandler);
-
-            // Retry the download if there are attempts left.
-            function retryAddImageFile(rErr) {
-                addImageFileAttempts += 1;
-                if (addImageFileAttempts >= MAX_IMAGE_FILE_DOWNLOAD_ATTEMPTS) {
-                    log.info({imgId: imgId, fileSize: fileSize},
-                        'dockerDownloadAndImportImage: download failed after '
-                        + '%d attempts', addImageFileAttempts);
-                    next(rErr);
-                    return;
-                }
-                // Give a short respite and then go again.
-                setTimeout(function () {
-                    if (ctx.downloadsCanceled) {
-                        log.info({err: rErr, imgId: imgId, fileSize: fileSize,
-                            addImageFileAttempts: addImageFileAttempts},
-                            'dockerDownloadAndImportImage: not retrying, ' +
-                            'download already canceled');
-                        next(new errors.DownloadError(rErr,
-                            'Download canceled'));
-                        return;
-                    }
-                    log.info({imgId: imgId, fileSize: fileSize,
-                        addImageFileAttempts: addImageFileAttempts},
-                        'dockerDownloadAndImportImage: retrying blob download');
-                    addImageFile(null, next);
-                }, 1000);
+                progressStatus('Aborted');
+                return;
             }
 
-            if (stream.headers['content-length'] !== undefined) {
-                fileSize = Number(stream.headers['content-length']);
+            size += chunk.length;
+            if (size > MAX_IMAGE_SIZE) {
+                finish(new errors.DownloadError(format(
+                    'Download error: image file size exceeds the ' +
+                    'maximum allowed size, %s', MAX_IMAGE_SIZE_STR)));
+            }
+            shasum.update(chunk, 'binary');
+            sha256sum.update(chunk, 'binary');
+            md5sum.update(chunk, 'binary');
+
+            if ((size - progLastUpdateSize) > progUpdateEvery) {
+                progressStatus('Downloading', {
+                    current: size,
+                    total: fileSize,
+                    start: startTs
+                });
+                progLastUpdateSize = size;
             }
+        });
 
-            var size = 0;
-            var stor;  // the storage class
-            var sha1;
-            function finish_(fErr, tmpFilename, filename) {
-                // Remove connection timeout handler.
-                stream.connection.removeListener('timeout',
-                    streamTimeoutHandler);
-                streamTimeoutHandler = null;
-
-                if (fErr) {
-                    if (fErr.name === 'DownloadError') {
-                        retryAddImageFile(fErr);
-                        return;
-                    }
-                    log.info({imgId: imgId, err: fErr},
-                        'dockerDownloadAndImportImage: ' +
-                        'not retrying on this error');
-                    return next(fErr);
-                } else if (ctx.downloadsCanceled) {
-                    return next(new errors.DownloadError('Download canceled'));
-                } else if (size > MAX_IMAGE_SIZE) {
-                    return next(new errors.DownloadError(format(
-                        'Download error: image file size, %s, exceeds the ' +
-                        'maximum allowed file size, %s',
-                        size, MAX_IMAGE_SIZE_STR)));
-                } else if (fileSize >= 0 && size !== fileSize) {
-                    // Retry - as there was an issue downloading all the bits.
-                    retryAddImageFile(new errors.DownloadError(format(
-                        'Download error: "Content-Length" header, %s, does ' +
-                        'not match downloaded size, %d', fileSize, size)));
-                    return;
-                }
+        stream.on('error', function (streamErr) {
+            finish(errors.wrapErrorFromDrc(streamErr));
+        });
 
-                sha1 = shasum.digest('hex');
+        stor = app.chooseStor(newImage);
+        stor.storeFileFromStream({
+            image: newImage,
+            stream: stream,
+            reqId: resp.id(),
+            filename: 'file0',
+            noStreamErrorHandler: true
+        }, function (sErr, tmpFilename, filename) {
+            if (sErr) {
+                log.error({err: sErr, digest: digest},
+                    'error storing image file');
+                finish(errors.parseErrorFromStorage(
+                    sErr, 'error receiving image file'));
+            } else {
+                finish(null, tmpFilename, filename);
+            }
+        });
+    }
 
-                var file = {
-                    sha1: sha1,
-                    size: size,
-                    contentMD5: md5sum.digest('base64'),
-                    mtime: (new Date()).toISOString(),
-                    stor: stor.type,
-                    compression: compression
-                };
+    function addUncompressedSha256(arg, next) {
+        var newFileInfo = ctx.newFileInfoFromUuid[uuid];
+        assert.object(newFileInfo, 'newFileInfo');
+        assert.object(newFileInfo.file, 'newFileInfo.file');
 
-                ctx.fileInfoFromImgId[imgId] = {
-                    file: file,
-                    storage: stor.type,
-                    tmpFilename: tmpFilename,
-                    filename: filename
-                };
+        if (compression === 'none') {
+            // Same sha256 - as there is no compression.
+            newFileInfo.file.uncompressedSha256 = newFileInfo.sha256;
+            next();
+            return;
+        }
 
-                log.info({imgId: imgId, fileSize: fileSize},
-                    'dockerDownloadAndImportImage: Download successful');
-                return next();
+        if (uncompressedSha256) {
+            newFileInfo.file.uncompressedSha256 = uncompressedSha256;
+            next();
+            return;
+        }
+
+        // Data is still piping to the uncompress/sha256 function, set and wait
+        // for it's callback.
+        arg.uncompressedSha256Callback = function (err, sha256) {
+            if (!err) {
+                newFileInfo.file.uncompressedSha256 = sha256;
+                progressStatus('Uncompression completed');
             }
-            var finish = once(finish_);
+            next(err);
+        };
 
-            var shasum = crypto.createHash('sha1');
-            var md5sum = crypto.createHash('md5');
+        progressStatus('Uncompressing layer');
+    }
+}
 
-            stream.on('data', function (chunk) {
-                if (ctx.downloadsCanceled) {
-                    stream.destroy();
-                    ctx.resMessage({
-                        type: 'progress',
-                        payload: {
-                            id: imgId.substr(0, 12),
-                            status: 'Aborted',
-                            progressDetail: {
-                                current: size,
-                                total: fileSize,
-                                start: startTs
-                            }
-                        }
-                    });
-                    return;
-                }
 
-                size += chunk.length;
-                if (size > MAX_IMAGE_SIZE) {
-                    finish(new errors.DownloadError(format(
-                        'Download error: image file size exceeds the ' +
-                        'maximum allowed size, %s', MAX_IMAGE_SIZE_STR)));
-                }
-                shasum.update(chunk, 'binary');
-                md5sum.update(chunk, 'binary');
+function _dockerDownloadAndImportLayerWithRetries(opts, callback) {
+    assert.object(opts, 'opts');
+    assert.optionalNumber(opts.addImageFileAttempt, 'opts.addImageFileAttempt');
+    assert.object(opts.ctx, 'opts.ctx');
+    assert.object(opts.ctx.req, 'opts.ctx.req');
+    assert.object(opts.ctx.req.log, 'opts.ctx.req.log');
+    assert.func(opts.ctx.resMessage, 'opts.ctx.resMessage');
+    assert.string(opts.digest, 'opts.digest');
+    assert.func(callback, 'callback');
 
-                if ((size - lastUpdate) > updateEvery) {
-                    ctx.resMessage({
-                        type: 'progress',
-                        payload: {
-                            id: imgId.substr(0, 12),
-                            status: 'Downloading',
-                            progressDetail: {
-                                current: size,
-                                total: fileSize,
-                                start: startTs
-                            }
-                        }
-                    });
-                    lastUpdate = size;
-                }
-            });
+    var MAX_IMAGE_FILE_DOWNLOAD_ATTEMPTS = 5;
+    var addImageFileAttempt = opts.addImageFileAttempt || 0;
+    var ctx = opts.ctx;
+    var digest = opts.digest;
+    var log = ctx.req.log;
+
+    function retryDownload(err) {
+        addImageFileAttempt += 1;
+
+        // Abort if we've exceeded the maximum retry attempts.
+        if (addImageFileAttempt >= MAX_IMAGE_FILE_DOWNLOAD_ATTEMPTS) {
+            log.info({digest: digest},
+                'dockerDownloadAndImportImage: download failed after '
+                + '%d attempts', addImageFileAttempt);
+            callback(err);
+            return;
+        }
 
-            stream.on('error', function (streamErr) {
-                finish(errors.wrapErrorFromDrc(streamErr));
-            });
+        // Give a short respite and then go again.
+        setTimeout(function _retryDockerImgDownload() {
+            if (ctx.downloadsCanceled) {
+                log.info({err: err, digest: digest,
+                    addImageFileAttempt: addImageFileAttempt},
+                    'dockerDownloadAndImportImage: not retrying, ' +
+                    'download already canceled');
+                callback(new errors.DownloadError(err, 'Download canceled'));
+                return;
+            }
 
-            stor = app.chooseStor(newImage);
-            stor.storeFileFromStream({
-                image: newImage,
-                stream: stream,
-                reqId: stream.id(),
-                filename: 'file0',
-                noStreamErrorHandler: true
-            }, function (sErr, tmpFilename, filename) {
-                if (sErr) {
-                    log.error({err: sErr, imgId: imgId},
-                        'error storing image file');
-                    finish(errors.parseErrorFromStorage(
-                        sErr, 'error receiving image file'));
-                } else {
-                    finish(null, tmpFilename, filename);
-                }
-            });
-        });
+            log.info({digest: digest, addImageFileAttempt: addImageFileAttempt},
+                'dockerDownloadAndImportImage: retrying blob download');
+            opts.addImageFileAttempt = addImageFileAttempt;
+            _dockerDownloadAndImportLayerWithRetries(opts, callback);
+        }, 1000);
     }
+
+    _dockerDownloadAndImportLayer(opts, function _dockerDlImgCb(err, image) {
+        // Return if no error, or the error is not a DownloadError.
+        if (err) {
+            if (err.name === 'DownloadError') {
+                retryDownload(err);
+                return;
+            }
+
+            callback(err);
+            return;
+        }
+
+        callback(null, image);
+    });
 }
 
 
@@ -2859,16 +3000,16 @@ function _dockerDownloadAndImportImage(opts, callback) {
  * to ensure image objects are added serially into the database.
  * The function 'this' is bound to be { req: req, res: res }
  */
-function _dockerActivateImage(imgId, ctx, callback) {
+function _dockerActivateImage(newImage, ctx, callback) {
     var req = ctx.req;
     var resMessage = ctx.resMessage;
     var app = req._app;
     var log = req.log;
-    var newImage = ctx.imageFromImgId[imgId];
-    var newFile = ctx.fileInfoFromImgId[imgId];
-
-    var exists = (newImage.exists === true);
-    delete newImage.exists;
+    var digest = ctx.digestFromUuid[newImage.uuid];
+    // If newFileInfo exists, it means a new file was downloaded for this image.
+    var newFileInfo = ctx.newFileInfoFromUuid[newImage.uuid];
+    var shortId = imgmanifest.shortDockerId(
+        imgmanifest.dockerIdFromDigest(digest));
 
     vasync.pipeline({ funcs: [
         archiveManifest,
@@ -2881,11 +3022,11 @@ function _dockerActivateImage(imgId, ctx, callback) {
             return;
         }
 
-        if (!exists) {
+        if (newFileInfo) {
             resMessage({
                 type: 'progress',
                 payload: {
-                    id: imgId.substr(0, 12),
+                    id: shortId,
                     status: 'Pull complete'
                 }
             });
@@ -2895,7 +3036,7 @@ function _dockerActivateImage(imgId, ctx, callback) {
     });
 
     function archiveManifest(_, next) {
-        if (exists) {
+        if (!newFileInfo) {
             next();
             return;
         }
@@ -2914,7 +3055,7 @@ function _dockerActivateImage(imgId, ctx, callback) {
     }
 
     function addManifestToDb(_, next) {
-        if (exists) {
+        if (!newFileInfo) {
             next();
             return;
         }
@@ -2938,16 +3079,17 @@ function _dockerActivateImage(imgId, ctx, callback) {
             return;
         }
 
-        log.debug({imgId: imgId}, 'MoveImageLayer: start');
-        var stor = app.getStor(newFile.storage);
+        log.debug({digest: digest}, 'MoveImageLayer: start');
+        var stor = app.getStor(newFileInfo.storage);
 
-        stor.moveImageFile(newImage, newFile.tmpFilename, newFile.filename,
-          function (mErr) {
+        stor.moveImageFile(newImage, newFileInfo.tmpFilename,
+                newFileInfo.filename,
+                function (mErr) {
             if (mErr) {
                 return next(mErr);
             }
 
-            newImage.addFile(app, newFile.file, req.log, function (err2) {
+            newImage.addFile(app, newFileInfo.file, req.log, function (err2) {
                 if (err2) {
                     req.log.error(err2, 'error adding file info to Image');
                     return next(new errors.InternalError(err2,
@@ -2968,7 +3110,7 @@ function _dockerActivateImage(imgId, ctx, callback) {
         resMessage({
             type: 'progress',
             payload: {
-                id: imgId.substr(0, 12),
+                id: shortId,
                 status: 'Activating image'
             }
         });
@@ -2978,126 +3120,203 @@ function _dockerActivateImage(imgId, ctx, callback) {
 }
 
 
-function _dockerV1Pull(ctx, cb) {
-    assert.object(ctx, 'ctx');
-    assert.func(ctx.resMessage, 'ctx.resMessage');
-    assert.object(ctx.rat, 'ctx.rat');
-    assert.string(ctx.imgId, 'ctx.imgId');
-    assert.object(ctx.regClientV1, 'ctx.regClientV1');
-    assert.func(cb, 'cb');
+/**
+ * Check if the cmd is a metadata command - i.e. doesn't modify the filesystem.
+ */
+function isMetadataCmd(cmd) {
+    assert.string(cmd, 'cmd');
+    var marker = ' #(nop) ';
+    var idx = cmd.indexOf(marker);
+    if (idx === -1) {
+        // Some older manifests don't include the #nop marker, e.g. for run
+        // commands.
+        return false;
+    }
+    var name = cmd.substr(idx + marker.length).split(' ')[0];
+    return ['ADD', 'COPY', 'RUN'].indexOf(name) === -1;
+}
 
-    var req = ctx.req;
-    var log = req.log;
-    var resMessage = ctx.resMessage;
-    var rat = ctx.rat;
-    var tag = rat.tag;
 
-    var reverseAncestry;
+/**
+ * Add image history entries.
+ *
+ *  [
+ *    {
+ *      "created": "2016-05-05T18:13:29.963947682Z",
+ *      "author": "Me Now <me@now.com>",
+ *      "created_by": "/bin/sh -c #(nop) MAINTAINER Me Now <me@now.com>",
+ *      "empty_layer": true
+ *    }, {
+ *      "created": "2016-05-05T18:13:30.218788521Z",
+ *      "author": "Me Now <me@now.com>",
+ *      "created_by": "/bin/sh -c #(nop) ADD file:c59222783...364a in /"
+ *    }, {
+ *      "created": "2016-05-05T18:13:30.456465331Z",
+ *      "author": "Me Now <me@now.com>",
+ *      "created_by": "/bin/touch /odd.txt"
+ *    }
+ *  ]
+ */
+function historyEntryFromImageJson(imgJson) {
+    assert.object(imgJson.container_config, 'imgJson.container_config');
+    assert.arrayOfString(imgJson.container_config.Cmd,
+        'imgJson.container_config.Cmd');
+
+    var entry = {
+        created: imgJson.created,
+        created_by: imgJson.container_config.Cmd.join(' ')
+    };
 
-    vasync.pipeline({funcs: [
-        function starterMessages(_, next) {
-            resMessage({
-                type: 'status',
-                payload: {
-                    /*
-                     * When pulling all images in a repository is supported
-                     * Docker-docker says: 'Pulling repository $localName'.
-                     */
-                    status: format('%s: Pulling from %s (%s)',
-                        tag, rat.localName, req.getId())
-                }
-            });
-            // The `head` message tells sdc-docker to tag this Docker imgId
-            // for the pulling user.
-            resMessage({type: 'head', head: ctx.imgId});
-            resMessage({
-                type: 'progress',
-                payload: {
-                    id: ctx.imgId.substr(0, 12),
-                    status: 'Pulling dependent layers'
-                }
-            });
+    if (isMetadataCmd(entry.created_by)) {
+        entry.empty_layer = true;
+    }
+    if (imgJson.author) {
+        entry.author = imgJson.author;
+    }
+    if (imgJson.comment) {
+        entry.comment = imgJson.comment;
+    }
 
-            next();
-        },
+    return entry;
+}
 
-        function getAncestry(_, next) {
-            ctx.regClientV1.getImgAncestry({
-                imgId: ctx.imgId
-            }, function (err, ancestry) {
-                if (err) {
-                    return next(errors.wrapErrorFromDrc(err));
-                }
-                // Want to import oldest in ancestry first.
-                reverseAncestry = ancestry.reverse();
-                next();
-            });
-        },
+/**
+ * Create a docker config object from the given arguments.
+ *
+ * @param layers {Array} Info on each layer (digest, imgFile, etc...).
+ * @param fakeIt {Boolean} Create placeholder rootfs information.
+ *
+ * @returns {Object} The docker config object.
+ */
+function createImgJsonFromLayers(layers, fakeIt) {
+    assert.arrayOfObject(layers, 'layers');
+    assert.optionalBool(fakeIt, 'fakeIt');
 
-        /*
-         * In *parallel*, create (unactivated) and download the images.
-         */
-        function importImagesPart1(_, next) {
-            var pullQueueError;
-            ctx.downloadsCanceled = false;
-            var pullQueue = vasync.queue(function (imgId, nextImg) {
-                _dockerDownloadAndImportImage({imgId: imgId, ctx: ctx},
-                    nextImg);
-            }, 5);
+    var imgJson = objCopy(layers.slice(-1)[0].imgJson);
+    if (imgJson.hasOwnProperty('id')) {
+        delete imgJson.id;   // No longer needed.
+    }
+    imgJson.history = layers.map(function (layer) {
+        return historyEntryFromImageJson(layer.imgJson);
+    });
 
-            pullQueue.on('end', function () {
-                next(pullQueueError);
-            });
+    assert.equal(layers.length, imgJson.history.length,
+        'Layers and image history must be the same length');
 
-            pullQueue.push(reverseAncestry, function (qErr) {
-                if (qErr) {
-                    log.debug(qErr, '_dockerDownloadAndImportImage err');
-                }
-                if (qErr && pullQueueError === undefined) {
-                    pullQueueError = qErr;
-                    ctx.downloadsCanceled = true;
-                    pullQueue.kill();
-                }
-            });
-            pullQueue.close();
-        },
+    /**
+     * Add RootFS layers.
+     *
+     * {
+     *   "type": "layers",
+     *   "diff_ids": [
+     *       "sha256:3f69a7949970fe2d62a5...c65003d01ac3bbe8645d574b",
+     *       "sha256:f980315eda5e9265282c...41b30de83027a2077651b465",
+     *       "sha256:30785cd7f84479984348...533457f3a5dcf677d0b0c51e"
+     *   ]
+     * }
+     */
+    var nonEmptyLayers = layers.filter(function _filterEmpty(layer, idx) {
+        return !imgJson.history[idx].empty_layer;
+    });
+    imgJson.rootfs = {
+        type: 'layers',
+        diff_ids: nonEmptyLayers.map(function _getRootfsDiffId(layer) {
+            if (!layer.uncompressedDigest && fakeIt) {
+                return '';
+            }
+            assert.string(layer.uncompressedDigest);
+            return layer.uncompressedDigest;
+        })
+    };
 
-        /*
-         * *Serially* complete the import of all the images:
-         * - We only ActivateImage's at this stage after the file downloading
-         *   (anticipated to be the most error-prone stage).
-         * - We activate images in ancestry order (parent before child) for
-         *   db consistency.
-         */
-        function importImagesPart2(_, next) {
-            vasync.forEachPipeline({
-                inputs: reverseAncestry,
-                func: function (imgId, nextImg) {
-                    _dockerActivateImage(imgId, ctx, nextImg);
-                }
-            }, function (vErr, results) {
-                next(vErr);
-            });
+    return imgJson;
+}
+
+
+/**
+ * Create a docker manifest object (schemaVersion 2) from the given arguments.
+ *
+ * @param imgJson {Object} The docker image config.
+ * @param layers {Array} Info on each layer (digest, imgFile, etc...).
+ * @param fakeIt {Boolean} Create placeholder layer information.
+ *
+ * @returns {Object} The docker manifest object.
+ */
+function createV2Manifest(imgJson, layers, fakeIt) {
+    assert.object(imgJson, 'imgJson');
+    assert.arrayOfObject(imgJson.history, 'imgJson.history');
+    assert.arrayOfObject(layers, 'layers');
+    assert.optionalBool(fakeIt, 'fakeIt');
+
+    assert.equal(imgJson.history.length, layers.length,
+        'history length should equal layers length');
+
+    var imageStr = JSON.stringify(imgJson);
+    var imageDigest = 'sha256:' + crypto.createHash('sha256')
+        .update(imageStr, 'binary').digest('hex');
+
+    var manifest = {
+        schemaVersion: 2,
+        mediaType: 'application/vnd.docker.distribution.manifest.v2+json',
+        config: {
+            'mediaType': 'application/vnd.docker.container.image.v1+json',
+            'size': imageStr.length,
+            'digest': imageDigest
         },
+        layers: layers.filter(function _filterLayers(layer, idx) {
+            return !(imgJson.history[idx].empty_layer);
+        }).map(function _mapLayers(layer) {
+            assert.string(layer.digest, 'layer.digest');
+            // If we have an imgManifest, then we have already downloaded the
+            // file/layer and thus we don't need to fake it, as we have the
+            // information we need to create the docker layer information.
+            if (!layer.imgManifest && fakeIt) {
+                // Fake it until you make it.
+                return {
+                    digest: layer.digest,
+                    mediaType: '(unknown)'
+                };
+            }
+            assert.object(layer.imgFile, 'layer.imgFile');
+            assert.string(layer.compression, 'layer.compression');
+            var compressionSuffix = '';
+            if (layer.compression && layer.compression !== 'none') {
+                compressionSuffix = '.' + layer.compression;
+            }
+            return {
+                digest: layer.digest,
+                mediaType: 'application/vnd.docker.image.rootfs.diff.tar' +
+                    compressionSuffix,
+                size: layer.imgFile.size
+            };
+        })
+    };
 
-        function finishingMessages(_, next) {
-            var status = (ctx.alreadyExistsFromImgId[ctx.imgId]
-                ? 'Status: Image is up to date for ' + ctx.repoAndRef
-                : 'Status: Downloaded newer image for ' + ctx.repoAndRef);
-            resMessage({
-                type: 'status',
-                payload: {status: status}
-            });
+    return manifest;
+}
 
-            next();
-        }
-    ]}, cb);
+
+function _compressionFromMediaType(mediaType) {
+    switch (mediaType) {
+        case 'application/vnd.docker.image.rootfs.diff.tar.bzip2':
+            return 'bzip2';
+        case 'application/vnd.docker.image.rootfs.diff.tar.gzip':
+            return 'gzip';
+        case 'application/vnd.docker.image.rootfs.diff.tar.xz':
+            return 'xz';
+        case 'application/vnd.docker.image.rootfs.diff.tar':
+        case 'application/vnd.docker.image.rootfs.diff':
+            return 'none';
+        default:
+            return undefined;
+    }
+    return undefined;
 }
 
 
 /* BEGIN JSSTYLED */
 /*
- * Pull a docker image using v2 registry.
+ * Pull a docker image (with schemaVersion === 2) using v2 registry.
  *
  * Example Docker-docker pull:
  *
@@ -3115,16 +3334,21 @@ function _dockerV1Pull(ctx, cb) {
  *  Status: Image is up to date for alpine@sha256:fb9f16730ac6316afa4d97caa5130219927bfcecf0b0ce35c01dcb612f449739
  */
 /* END JSSTYLED */
-function _dockerV2Pull(ctx, cb) {
+function _dockerV22Pull(ctx, cb) {
     assert.object(ctx, 'ctx');
-    assert.func(ctx.resMessage, 'ctx.resMessage');
-    assert.object(ctx.rat, 'ctx.rat');
-    assert.string(ctx.digestV2, 'ctx.digestV2');
+    assert.string(ctx.dockerImageVersion, 'ctx.dockerImageVersion');
+    assert.string(ctx.manifestDigest, 'ctx.manifestDigest');
     assert.object(ctx.manifestV2, 'ctx.manifestV2');
+    assert.object(ctx.manifestV2.config, 'ctx.manifest.config');
+    assert.object(ctx.rat, 'ctx.rat');
     assert.object(ctx.regClientV2, 'ctx.regClientV2');
+    assert.func(ctx.resMessage, 'ctx.resMessage');
+    assert.optionalObject(ctx.config, 'ctx.config');
     assert.func(cb, 'cb');
 
     var cacheDownloadedLayers = false;
+    var configDigest = ctx.manifestV2.config.digest;
+    var imgJson = ctx.imgJson;
     var req = ctx.req;
     var log = req.log;
     var resMessage = ctx.resMessage;
@@ -3132,46 +3356,161 @@ function _dockerV2Pull(ctx, cb) {
     var tag = rat.tag;
     var digest = rat.digest;
 
-    var reverseAncestry = [];  // 'reversed' so that the base image is first
-    for (var i = ctx.manifestV2.history.length - 1; i >= 0; i--) {
-        try {
-            var imgJson = JSON.parse(ctx.manifestV2.history[i].v1Compatibility);
-        } catch (manifestErr) {
-            return cb(new errors.ValidationFailedError(manifestErr, format(
-                'invalid "v1Compatibility" JSON in docker manifest: %s (%s)',
-                manifestErr, ctx.manifestV2.history[i].v1Compatibility)));
+    // Get the docker config layer id, from the manifest, then fetch the config
+    // details.
+
+    // Send initial status message.
+    resMessage({
+        type: 'status',
+        payload: {
+            /*
+             * When pulling all images in a repository is supported
+             * Docker-docker says: 'Pulling repository $localName'.
+             */
+            status: format('%s: Pulling from %s (req %s)',
+                tag || digest, rat.localName, req.getId())
         }
-        reverseAncestry.push({
-            imgJson: imgJson,
-            fsLayer: ctx.manifestV2.fsLayers[i]
-        });
-    }
-    var imgId = reverseAncestry[reverseAncestry.length - 1].imgJson.id;
+    });
 
     vasync.pipeline({funcs: [
-        function starterMessages(_, next) {
-            resMessage({
-                type: 'status',
-                payload: {
-                    /*
-                     * When pulling all images in a repository is supported
-                     * Docker-docker says: 'Pulling repository $localName'.
-                     */
-                    status: format('%s: Pulling from %s (req %s)',
-                        tag || digest, rat.localName, req.getId())
+        function downloadImgJson(_, next) {
+            if (imgJson) {
+                // This should only be when upconverting a v2.1 image.
+                assert.equal(ctx.dockerImageVersion, '2.1',
+                    'ctx.dockerImageVersion');
+                next();
+                return;
+            }
+            ctx.regClientV2.createBlobReadStream({digest: configDigest},
+                function (err, stream, res_) {
+                if (err) {
+                    next(errors.wrapErrorFromDrc(err));
+                    return;
                 }
+                // Read, validate and store the config.
+                log.debug({digest: configDigest},
+                    'downloadConfig:: stream started');
+
+                var configStr = '';
+                var hadErr = false;
+                var sha256sum = crypto.createHash('sha256');
+
+                stream.on('end', function _downloadConfigStreamEnd() {
+                    if (hadErr) {
+                        return;
+                    }
+                    log.debug({digest: configDigest},
+                        'downloadConfig:: stream ended, config: %s', configStr);
+                    var gotDigest = 'sha256:' + sha256sum.digest('hex');
+                    if (gotDigest !== configDigest) {
+                        log.warn({expectedDigest: configDigest,
+                            gotDigest: gotDigest},
+                            'Downloaded config sha256 digest does not match');
+                        next(new errors.DownloadError(format(
+                            'config digest does not match, got %s, expected %s',
+                            gotDigest, configDigest)));
+                        return;
+                    }
+                    // Convert into JSON.
+                    try {
+                        imgJson = JSON.parse(configStr);
+                    } catch (configErr) {
+                        next(new errors.ValidationFailedError(configErr, format(
+                            'invalid JSON for docker config, digest %s, err %s',
+                            configDigest, configErr)));
+                        return;
+                    }
+                    next();
+                });
+
+                stream.on('error', function _downloadConfigStreamError(sErr) {
+                    hadErr = true;
+                    log.warn({digest: configDigest},
+                        'downloadConfig:: error downloading config: %s', sErr);
+                    stream.destroy();
+                    next(new errors.DownloadError(format(
+                        'error downloading config with digest %s, %s',
+                        configDigest, sErr)));
+                    return;
+                });
+
+                stream.on('data', function _downloadConfigStreamData(chunk) {
+                    if (ctx.downloadsCanceled) {
+                        stream.destroy();
+                        return;
+                    }
+                    configStr += String(chunk);
+                    sha256sum.update(chunk, 'binary');
+                });
+
+                // Stream is paused, so get it moving again.
+                stream.resume();
             });
-            // The `head` message tells sdc-docker to tag this Docker imgId
-            // for the pulling user.
-            resMessage({type: 'head', head: imgId});
-            resMessage({
-                type: 'progress',
-                payload: {
-                    id: imgId.substr(0, 12),
-                    status: 'Pulling dependent layers'
+        },
+
+        function determineLayers(_, next) {
+            assert.arrayOfObject(imgJson.history, 'imgJson.history');
+            assert.arrayOfString(imgJson.rootfs.diff_ids,
+                'imgJson.rootfs.diff_ids');
+            assert.arrayOfObject(ctx.manifestV2.layers,
+                'ctx.manifestV2.layers');
+            // History is from oldest change (index 0) to the newest change. For
+            // each entry in history, there should be a corresponding entry in
+            // both the manifestV2.layers array and the imgJson.rootfs.diff_ids
+            // array, except in the case the history entry has a 'empty_layer'
+            // attribute set to true.
+            var layerDigests = [];
+            var layerInfos = [];
+            var idx = -1;
+            imgJson.history.forEach(function _histForEach(h, pos) {
+                // Emulate Docker's synthetic image config for each layer, so we
+                // can later generate the history entries. For reference, see:
+                // JSSTYLED
+                // https://github.com/docker/distribution/blob/docker/1.13/docs/spec/manifest-v2-2.md#backward-compatibility
+                var layerImgJson = {
+                    created: h.created,
+                    container_config: {
+                      Cmd: [ h.created_by ]
+                    }
+                };
+                if (pos === (imgJson.history.length - 1)) {
+                    // Last layer uses the original imgJson.
+                    layerImgJson = imgJson;
+                }
+
+                if (h.empty_layer) {
+                    layerInfos.push({
+                        historyEntry: h,
+                        imgJson: layerImgJson,
+                        layerDigests: layerDigests.slice()  // A copy
+                    });
+                    return;
                 }
-            });
 
+                idx += 1;
+                var compression = _compressionFromMediaType(
+                    ctx.manifestV2.layers[idx].mediaType);
+                var layerDigest = ctx.manifestV2.layers[idx].digest;
+                var id = imgmanifest.dockerIdFromDigest(layerDigest);
+                layerDigests.push(layerDigest);
+                layerInfos.push({
+                    compression: compression,
+                    digest: layerDigest,
+                    historyEntry: h,
+                    imgJson: layerImgJson,
+                    layerDigests: layerDigests.slice(),  // A copy
+                    uncompressedDigest: imgJson.rootfs.diff_ids[idx]
+                });
+                resMessage({
+                    type: 'status',
+                    payload: {
+                        id: imgmanifest.shortDockerId(id),
+                        progressDetail: {},
+                        status: 'Pulling fs layer'
+                    }
+                });
+            });
+            ctx.layerInfos = layerInfos;
             next();
         },
 
@@ -3179,26 +3518,52 @@ function _dockerV2Pull(ctx, cb) {
          * In *parallel*, create (unactivated) and download the images.
          */
         function importImagesPart1(_, next) {
-            var pullQueueError;
-            ctx.downloadsCanceled = false;
             cacheDownloadedLayers = true;
+            ctx.downloadsCanceled = false;
+            var pullQueueError;
 
-            var pullQueue = vasync.queue(function (imgInfo, nextImg) {
-                _dockerDownloadAndImportImage({
-                    imgId: imgInfo.imgJson.id,
-                    imgJson: imgInfo.imgJson,
-                    fsLayer: imgInfo.fsLayer,
+            var pullQueue = vasync.queue(function (layerInfo, nextLayer) {
+                if (layerInfo.historyEntry.empty_layer) {
+                    // Nothing to download for this layer.
+                    nextLayer();
+                    return;
+                }
+
+                _dockerDownloadAndImportLayerWithRetries({
+                    compression: layerInfo.compression,
+                    digest: layerInfo.digest,
+                    layerDigests: layerInfo.layerDigests,
+                    imgJson: layerInfo.imgJson,
+                    uncompressedDigest: layerInfo.uncompressedDigest,
                     ctx: ctx
-                }, nextImg);
+                }, function _dockerDownloadLayerCb(err, image) {
+                    if (err) {
+                        log.info({err: err, digest: layerInfo.digest},
+                            'dockerDownloadAndImportLayerWithRetries err');
+                        nextLayer(err);
+                        return;
+                    }
+
+                    var file0;
+                    if (ctx.newFileInfoFromUuid.hasOwnProperty(image.uuid)) {
+                        file0 = ctx.newFileInfoFromUuid[image.uuid].file;
+                    } else {
+                        // Existing image.
+                        file0 = image.files[0];
+                    }
+                    layerInfo.imgFile = file0;
+                    layerInfo.imgManifest = image;
+                    nextLayer();
+                });
             }, 5);
 
             pullQueue.on('end', function () {
                 next(pullQueueError);
             });
 
-            pullQueue.push(reverseAncestry, function (qErr) {
+            pullQueue.push(ctx.layerInfos, function (qErr) {
                 if (qErr) {
-                    log.debug(qErr, '_dockerDownloadAndImportImage err');
+                    log.debug(qErr, '_dockerDownloadAndImportLayer err');
                 }
                 if (qErr && pullQueueError === undefined) {
                     pullQueueError = qErr;
@@ -3209,19 +3574,87 @@ function _dockerV2Pull(ctx, cb) {
             pullQueue.close();
         },
 
+        function recalculateConfigAndManifest(_, next) {
+            if (ctx.dockerImageVersion !== '2.1') {
+                next();
+                return;
+            }
+            // Update compression and digest for all file layers.
+            ctx.layerInfos.forEach(function (layer) {
+                if (layer.historyEntry.empty_layer) {
+                    return;
+                }
+                assert.object(layer.imgFile, 'layer.imgFile');
+                assert.string(layer.imgFile.compression,
+                    'layer.imgFile.compression');
+                assert.string(layer.imgFile.uncompressedSha256,
+                    'layer.imgFile.uncompressedSha256');
+                layer.compression = layer.imgFile.compression;
+                layer.uncompressedDigest = 'sha256:' +
+                    layer.imgFile.uncompressedSha256;
+            });
+            imgJson = createImgJsonFromLayers(ctx.layerInfos);
+            ctx.manifestV2 = createV2Manifest(imgJson, ctx.layerInfos);
+            ctx.manifestStr = JSON.stringify(ctx.manifestV2, null, 4);
+            ctx.manifestDigest = 'sha256:' + crypto.createHash('sha256')
+                .update(ctx.manifestStr, 'binary').digest('hex');
+            configDigest = ctx.manifestV2.config.digest;
+
+            log.debug({manifest: ctx.manifestV2},
+                'recalculateConfigAndManifest');
+
+            next();
+        },
+
+        function addSdcDockerImage(_, next) {
+            // Create the sdc-docker image in the docker_images bucket.
+
+            // Calculate total size and find the last image uuid.
+            var finalUuid;
+            var size = ctx.layerInfos.map(function (layer) {
+                if (!layer.imgManifest) {
+                    return 0;
+                }
+                assert.object(layer.imgFile, 'layer.imgFile');
+                finalUuid = layer.imgManifest.uuid;
+                return layer.imgFile && layer.imgFile.size || 0;
+            }).reduce(function (a, b) { return a + b; }, 0);
+
+            ctx.resMessage({
+                type: 'create-docker-image',
+                config_digest: configDigest,
+                head: true,
+                image: imgJson,
+                image_uuid: finalUuid,
+                manifest_digest: ctx.manifestDigest,
+                manifest_str: ctx.manifestStr,
+                size: size,
+                dockerImageVersion: ctx.dockerImageVersion
+            });
+            next();
+        },
+
         /*
          * *Serially* complete the import of all the images:
          * - We only ActivateImage's at this stage after the file downloading
          *   (anticipated to be the most error-prone stage).
-         * - We activate images in ancestry order (parent before child) for
+         * - We activate images in layerInfos order (parent before child) for
          *   db consistency.
          */
         function importImagesPart2(_, next) {
             cacheDownloadedLayers = false;
             vasync.forEachPipeline({
-                inputs: reverseAncestry,
-                func: function (imgInfo, nextImg) {
-                    _dockerActivateImage(imgInfo.imgJson.id, ctx, nextImg);
+                inputs: ctx.layerInfos,
+                func: function (layerInfo, nextLayer) {
+                    assert.object(layerInfo.historyEntry,
+                        'layerInfo.historyEntry');
+                    if (layerInfo.historyEntry.empty_layer) {
+                        nextLayer();
+                        return;
+                    }
+                    assert.object(layerInfo.imgManifest,
+                        'layerInfo.imgManifest');
+                    _dockerActivateImage(layerInfo.imgManifest, ctx, nextLayer);
                 }
             }, function (vErr, results) {
                 next(vErr);
@@ -3232,11 +3665,11 @@ function _dockerV2Pull(ctx, cb) {
             resMessage({
                 type: 'status',
                 payload: {
-                    status: 'Digest: ' + ctx.digestV2
+                    status: 'Digest: ' + ctx.manifestDigest
                 }
             });
 
-            var status = (ctx.alreadyExistsFromImgId[imgId]
+            var status = (ctx.newFileInfoFromUuid.length === 0
                 ? 'Status: Image is up to date for ' + ctx.repoAndRef
                 : 'Status: Downloaded newer image for ' + ctx.repoAndRef);
             resMessage({
@@ -3251,11 +3684,8 @@ function _dockerV2Pull(ctx, cb) {
             // There was a failure downloading one or more image layers - keep
             // the downloaded image metadata in memory, so we can avoid
             // downloading it again next time.
-            Object.keys(ctx.fileInfoFromImgId).forEach(function (id) {
-                DOCKER_IMAGE_CACHE[id] = {
-                    fileInfo: ctx.fileInfoFromImgId[id],
-                    image: ctx.imageFromImgId[id]
-                };
+            Object.keys(ctx.newFileInfoFromUuid).forEach(function (u) {
+                DOCKER_IMAGE_CACHE[u] = ctx.newFileInfoFromUuid[u];
             });
         }
 
@@ -3264,6 +3694,62 @@ function _dockerV2Pull(ctx, cb) {
 }
 
 
+/*
+ * Pull a docker image using v2.1 image manifest format (schemaVersion === 1).
+ *
+ * We upconvert the manifest into a v2.2 format and then have the _dockerV22Pull
+ * function do the bulk of the layer download work. Note that we have to fake a
+ * part of the config and manifest, as we don't know the uncompressed digest or
+ * the compression of the layer at this time - luckily the _dockerV22Pull can
+ * work this out for us and then regenerates the config and manifest once this
+ * information is known.
+ */
+function _dockerV21Pull(ctx, callback) {
+    assert.object(ctx, 'ctx');
+    assert.string(ctx.manifestDigest, 'ctx.manifestDigest');
+    assert.object(ctx.manifestV2, 'ctx.manifestV2');
+    assert.object(ctx.req, 'ctx.req');
+    assert.func(callback, 'callback');
+
+    var req = ctx.req;
+    var log = req.log;
+    var layerDigest;
+    var layerDigests = [];
+
+    var layerInfos = [];  // Docker image info with the base image first.
+    for (var i = ctx.manifestV2.history.length - 1; i >= 0; i--) {
+        var imgJson;
+        try {
+            imgJson = JSON.parse(ctx.manifestV2.history[i].v1Compatibility);
+        } catch (manifestErr) {
+            return callback(
+                new errors.ValidationFailedError(manifestErr, format(
+                'invalid "v1Compatibility" JSON in docker manifest: %s (%s)',
+                manifestErr, ctx.manifestV2.history[i].v1Compatibility)));
+        }
+        layerDigest = ctx.manifestV2.fsLayers[i].blobSum;
+        layerDigests.push(layerDigest);
+        layerInfos.push({
+            digest: layerDigest,
+            imgJson: imgJson,
+            layerDigests: layerDigests.slice(),  // A copy - not a reference.
+            shortId: imgmanifest.shortDockerId(
+                imgmanifest.dockerIdFromDigest(layerDigest))
+        });
+    }
+
+    log.debug({manifestV21: ctx.manifest},
+        'Upconverting manifest from v2.1 to v2.2');
+    var fakeIt = true;
+    ctx.imgJson = createImgJsonFromLayers(layerInfos, fakeIt);
+    ctx.manifestV2 = createV2Manifest(ctx.imgJson, layerInfos, fakeIt);
+    ctx.manifestStr = JSON.stringify(ctx.manifestV2, null, 4);
+    ctx.manifestDigest = 'sha256:' + crypto.createHash('sha256')
+        .update(ctx.manifestStr, 'binary').digest('hex');
+
+    _dockerV22Pull(ctx, callback);
+}
+
 
 /* BEGIN JSSTYLED */
 /**
@@ -3282,20 +3768,13 @@ function _dockerV2Pull(ctx, cb) {
  *          "payload":{"status":"Pulling repository busybox"}
  *      }
  *
- *      {
- *          "type":"head",
- *          // A head Docker imgId for sdc-docker to tag.
- *          "head":"8c2e06607696bd4afb3d03b687e361cc43cf8ec1a4a725bc96e39f05ba97dd55",
- *          "id":"docker.io/busybox"
- *      }
- *
  *      {"type":"progress","payload":{"id":"8c2e06607696","status":"Pulling dependent layers"},"id":"docker.io/busybox"}
  *      {"type":"progress","id":"docker.io/busybox","payload":{"id":"8c2e06607696","status":"Pulling metadata."}}
  *
  *      {
- *          "type":"data",
- *          // Docker imgJson. TODO: How used by caller?
- *          "imgJson":{"container":"39e79119...
+ *          "type":"create-docker-image",
+ *          // Docker image. TODO: How used by caller?
+ *          "image":{"container":"39e79119...
  *      }
  *
  *      {
@@ -3342,6 +3821,7 @@ function apiAdminImportDockerImage(req, res, callback) {
         });
     }
     if (errs.length) {
+        log.debug({errs: errs}, 'apiAdminImportDockerImage error');
         return callback(new errors.ValidationFailedError(
             'missing parameters', errs));
     }
@@ -3442,141 +3922,51 @@ function apiAdminImportDockerImage(req, res, callback) {
             name: repo,
             log: req.log,
             insecure: req._app.config.dockerRegistryInsecure,
+            maxSchemaVersion: 2,
             username: username,
             password: password
         }, req),
 
-        imageFromImgId: {},  // <docker imgId> -> <IMGAPI image manifest>
-        fileInfoFromImgId: {},  // <docker imgId> -> <file import info>
-        alreadyExistsFromImgId: {},
+        digestFromUuid: {},  // <uuid> -> <docker digest>
+        newFileInfoFromUuid: {},  // <uuid> -> <file import info>
         public_: public_
     };
     log.trace({rat: context.rat}, 'docker pull');
 
     vasync.pipeline({arg: context, funcs: [
         /*
-         * To use Docker Registry v2 or v1 to pull? That is the question.
-         *
-         * 1. Try v2 ping to include v2 as candidate.
-         * 2. If so, check v2.getManifest for the tag/digest. If it exists,
-         *    then we'll be using v2 for the pull.
-         * 3. Else, try to resolve an imgId with v1. If 'digest', then error
-         *    because v1 doesn't support docker pull by digest. If no v1 or
-         *    the given tag not found with v1, then error out. Otherwise,
-         *    we'll be using v1 for the pull.
+         * Use Docker Registry v2 to pull - v1 is no longer supported.
          *
-         * Note: This "fallback to v1 if can't find the tag/digest in v2"
-         * behaviour is to copy `docker pull` behaviour (at least what it
-         * seems to do to Docker Hub). Basically that behaviour
-         * says that "v2" isn't a separate API version to the same underlying
-         * data, but instead is a completely disjoint registry... *some* of
-         * which Docker Hub migrated over.
+         * Check v2.getManifest for the tag/digest. If it exists,
+         * then we'll be using v2 for the pull, else error.
          */
-        function regSupportsV2(ctx, next) {
-            var client = drc.createClientV2(ctx.regClientOpts);
-            client.supportsV2(function (err, supportsV2) {
-                if (err) {
-                    next(errors.wrapErrorFromDrc(err));
-                } else {
-                    log.info({indexName: rat.index.name,
-                        supportsV2: supportsV2}, 'regSupportsV2');
-                    if (supportsV2) {
-                        ctx.regClientV2 = client;
-                    } else {
-                        client.close();
-                    }
-                    next();
-                }
-            });
-        },
-
         function v2GetManifest(ctx, next) {
-            if (!ctx.regClientV2) {
-                return next();
-            }
             var ref = rat.tag || rat.digest;
-            // TODO: Can we send in pingRes, from earlier 'supportsV2' call?
+            ctx.regClientV2 = drc.createClientV2(ctx.regClientOpts);
             ctx.regClientV2.getManifest({ref: ref},
-                    function (err, man, res_, manifestStr) {
+                    function (err, manifest, res_, manifestStr) {
                 if (err) {
-                    if (err.statusCode === 404 || err.statusCode === 401) {
-                        /*
-                         * No workie with v2. We'll fallback to trying API v1.
-                         *
-                         * Yes a 401 is sometimes how Docker Hub v2 responds.
-                         * For example, see DOCKER-625.
-                         */
-                        log.debug({ref: ref, statusCode: err.statusCode},
-                            'v2.getManifest 404 or 401');
-                        ctx.regClientV2.close();
-                        delete ctx.regClientV2;
-                        ctx.errV2 = err; // Save it for possible use below.
-                        next();
-                    } else {
-                        next(errors.wrapErrorFromDrc(err));
-                    }
+                    next(errors.wrapErrorFromDrc(err));
                 } else {
-                    log.debug({ref: ref, manifest: man},
+                    log.debug({ref: ref, manifest: manifest},
                         'v2.getManifest found');
-                    ctx.manifestV2 = man;
-                    ctx.digestV2 = res_.headers['docker-content-digest'];
-                    if (!ctx.digestV2) {
+                    ctx.manifestV2 = manifest;
+                    ctx.manifestStr = manifestStr;
+                    ctx.manifestDigest = res_.headers['docker-content-digest'];
+                    if (!ctx.manifestDigest) {
                         // Some registries (looking at you Amazon ECR) do not
                         // provide the docker-content-digest header in the
                         // response, so we have to calculate it.
-                        ctx.digestV2 = drc.digestFromManifestStr(manifestStr);
+                        ctx.manifestDigest = drc.digestFromManifestStr(
+                            manifestStr);
                     }
-                    ctx.regV = 2;   // The signal we'll be using v2.
-                    next();
-                }
-            });
-        },
-
-        function v1GetImgId(ctx, next) {
-            if (ctx.regV) {
-                // We've already determined we're not using v1.
-                return next();
-            } else if (ctx.digest) {
-                // V1 doesn't support pull by digest.
-                if (ctx.errV2) {
-                    next(ctx.errV2);
-                } else {
-                    next(new errors.BadRequestError(format(
-                        'Docker registry "%s" does not support v2 required ' +
-                        'to pull by digest, digest="%s"',
-                        ctx.rat.index.name, ctx.digest)));
-                }
-            }
-
-            var client = drc.createClientV1(ctx.regClientOpts);
-            client.getImgId({tag: ctx.rat.tag}, function (err, imgId) {
-                if (err) {
-                    /*
-                     * At this point we know we can't find the image.
-                     * We are going to error back to the caller. If the
-                     * repository supported v2, then we prefer to use *that*
-                     * error response.
-                     */
-                    client.close();
-                    if (ctx.errV2) {
-                        log.debug({err: err}, 'v1GetImgId error');
-                        next(errors.wrapErrorFromDrc(ctx.errV2));
-                    } else {
-                        next(errors.wrapErrorFromDrc(err));
-                    }
-                } else {
-                    ctx.regClientV1 = client;
-                    ctx.imgId = imgId;
-                    ctx.regV = 1;
                     next();
                 }
             });
         },
 
         /*
-         * At this point we know if we are using v1 or v2 (`ctx.regV`).
-         *
-         * Now determine if this is a private Docker image by trying the same
+         * Determine if this is a private Docker image by trying the same
          * without auth. The only thing this does is determine `ctx.isPrivate`
          * so the caller (typically sdc-docker) can note that.
          * We still used the auth'd client for doing the image pull.
@@ -3595,57 +3985,31 @@ function apiAdminImportDockerImage(req, res, callback) {
             delete opts.password;
             var noAuthClient;
 
-            if (ctx.regV === 1) {
-                noAuthClient = drc.createClientV1(opts);
-                noAuthClient.getImgId({tag: ctx.rat.tag},
-                        function (err, imgId) {
-                    if (err) {
-                        if (err.statusCode === 404 ||
-                            err.statusCode === 403 ||
-                            err.statusCode === 401)
-                        {
-                            err = null;
-                            ctx.isPrivate = true;
-                        }
-                    } else {
-                        assert.equal(imgId, ctx.imgId);
-                        ctx.isPrivate = false;
-                    }
-                    noAuthClient.close();
-                    next(errors.wrapErrorFromDrc(err));
-                });
-
-            } else {
-                assert.equal(ctx.regV, 2);
-
-                var ref = rat.tag || rat.digest;
-                noAuthClient = drc.createClientV2(opts);
-                noAuthClient.getManifest({ref: ref}, function (err, man) {
-                    if (err) {
-                        if (err.statusCode === 404 ||
-                            err.statusCode === 403 ||
-                            err.statusCode === 401)
-                        {
-                            log.debug({ref: ref, code: err.code,
-                                statusCode: err.statusCode}, 'isPrivate: true');
-                            err = null;
-                            ctx.isPrivate = true;
-                        } else {
-                            log.debug({ref: ref, code: err.code,
-                                statusCode: err.statusCode},
-                                'isPrivate: unexpected err code/statusCode');
-                        }
+            var ref = rat.tag || rat.digest;
+            noAuthClient = drc.createClientV2(opts);
+            noAuthClient.getManifest({ref: ref}, function (err, man, res_) {
+                if (err) {
+                    if (err.statusCode === 404 ||
+                        err.statusCode === 403 ||
+                        err.statusCode === 401)
+                    {
+                        log.debug({ref: ref, code: err.code,
+                            statusCode: err.statusCode}, 'isPrivate: true');
+                        err = null;
+                        ctx.isPrivate = true;
                     } else {
-                        // TODO: Better to assert that the Docker-Content-Digest
-                        //    is equal to above.
-                        assert.equal(man.fsLayers[0].blobSum,
-                            ctx.manifestV2.fsLayers[0].blobSum);
-                        ctx.isPrivate = false;
+                        log.debug({ref: ref, code: err.code,
+                            statusCode: err.statusCode},
+                            'isPrivate: unexpected err code/statusCode');
                     }
-                    noAuthClient.close();
-                    next(errors.wrapErrorFromDrc(err));
-                });
-            }
+                } else {
+                    var noAuthDigest = res_.headers['docker-content-digest'];
+                    assert.equal(noAuthDigest, ctx.manifestDigest);
+                    ctx.isPrivate = false;
+                }
+                noAuthClient.close();
+                next(errors.wrapErrorFromDrc(err));
+            });
         },
 
         /*
@@ -3659,17 +4023,21 @@ function apiAdminImportDockerImage(req, res, callback) {
             res.status(200);
             res.header('Content-Type', 'application/json');
 
-            if (ctx.regV === 1) {
-                _dockerV1Pull(ctx, next);
+            if (ctx.manifestV2.schemaVersion === 1) {
+                ctx.dockerImageVersion = '2.1';
+                _dockerV21Pull(ctx, next);
+            } else if (ctx.manifestV2.schemaVersion === 2) {
+                ctx.dockerImageVersion = '2.2';
+                _dockerV22Pull(ctx, next);
             } else {
-                _dockerV2Pull(ctx, next);
+                next(new errors.NotImplementedError(
+                    'unexpected manifest schemaVersion: %d',
+                    ctx.manifestV2.schemaVersion));
+                return;
             }
         }
 
     ]}, function (err) {
-        if (context.regClientV1) {
-            context.regClientV1.close();
-        }
         if (context.regClientV2) {
             context.regClientV2.close();
         }
@@ -4728,14 +5096,19 @@ function apiUpdateImage(req, res, cb) {
 
     req.log.debug({image: req._image}, 'UpdateImage: start');
 
+    var app = req._app;
     var image;
-    vasync.pipeline({funcs: [
-        function validateFields(_, next) {
+
+    vasync.pipeline({arg: {}, funcs: [
+        function validateFields(ctx, next) {
             var ADMIN_ONLY_ATTRS = [
                 'state',
                 'error',
                 'billing_tags',
-                'traits'
+                'traits',
+                'files',   // Restricted to sha256 and uncompressedSha256.
+                'origin',  // Restricted to unactivated images.
+                'uuid'     // Restricted to unactivated images.
             ];
 
             var UPDATEABLE_ATTRS = imgmanifest.fields.filter(function (field) {
@@ -4781,6 +5154,18 @@ function apiUpdateImage(req, res, cb) {
                         code: 'NotAllowed',
                         message: 'Can only be updated by operators'
                     });
+                } else if ((key === 'uuid' || key === 'origin' ||
+                        key === 'files') && data[key] !== undefined) {
+                    // These fields are a special case (used by docker build),
+                    // and it requires that the image be unactivated.
+                    if (req._image.activated) {
+                        errs.push({
+                            field: key,
+                            code: 'NotAllowed',
+                            message: 'Can only be updated on unactivated images'
+                        });
+                        // TODO: Only allow 'sha256' and 'uncompressedSha256'?
+                    }
                 }
             }
 
@@ -4810,23 +5195,47 @@ function apiUpdateImage(req, res, cb) {
 
             // Revalidate.
             try {
-                image = new Image(req._app, raw);
+                image = new Image(app, raw);
             } catch (cErr) {
                 return next(cErr);
             }
 
+            ctx.uuidChanged = (req._image.uuid !== image.uuid);
             next();
         },
 
         function checkOwner(_, next) {
             utils.checkOwnerExists({
-                app: req._app,
+                app: app,
                 owner: image.owner
             }, next);
         },
 
+        function moveImageFile(ctx, next) {
+            if (!ctx.uuidChanged) {
+                next();
+                return;
+            }
+            req._image.moveFileToImage(app, image, req.log, next);
+        },
+
         function doModify(_, next) {
-            Image.modify(req._app, image, req.log, next);
+            Image.modify(app, image, req.log, next);
+        },
+
+        function deleteOldImage(ctx, next) {
+            if (!ctx.uuidChanged) {
+                next();
+                return;
+            }
+            // Delete the original model.
+            app.db.del(req._image.uuid, function (delErr) {
+                if (delErr) {
+                    return next(delErr);
+                }
+                app.cacheInvalidateDelete('Image', req._image);
+                next();
+            });
         }
 
     ]}, function (err) {
@@ -4835,7 +5244,7 @@ function apiUpdateImage(req, res, cb) {
         }
 
         // Respond.
-        var serialized = image.serialize(req._app.mode, req.getVersion());
+        var serialized = image.serialize(app.mode, req.getVersion());
         resSetEtag(req, res, serialized);
         res.send(serialized);
         cb(false);
diff --git a/lib/magic.js b/lib/magic.js
index dfe8490..a3033d1 100644
--- a/lib/magic.js
+++ b/lib/magic.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright (c) 2015, Joyent, Inc.
+ * Copyright (c) 2017, Joyent, Inc.
  */
 
 /*
@@ -87,5 +87,7 @@ function compressionTypeFromPath(path, cb) {
 // ---- exports
 
 module.exports = {
+    maxMagicLen: maxMagicLen,  // Min number of bytes needed for detection.
+    compressionTypeFromBufSync: compressionTypeFromBufSync,
     compressionTypeFromPath: compressionTypeFromPath
 };
diff --git a/lib/migrations/migration-008-new-storage-layout.js b/lib/migrations/migration-008-new-storage-layout.js
index 2726731..334e22a 100644
--- a/lib/migrations/migration-008-new-storage-layout.js
+++ b/lib/migrations/migration-008-new-storage-layout.js
@@ -1,335 +1 @@
-#!/usr/bin/env node
-/*
- * This Source Code Form is subject to the terms of the Mozilla Public
- * License, v. 2.0. If a copy of the MPL was not distributed with this
- * file, You can obtain one at http://mozilla.org/MPL/2.0/.
- */
-
-/*
- * Copyright 2016 Joyent, Inc.
- */
-
-/*
- * IMGAPI db migration: Move to new storage layout for image files, from:
- *      .../images/$uuid
- * to:
- *      .../images/$uuid3charprefix/$uuid
- */
-
-var fs = require('fs');
-var path = require('path');
-var manta = require('manta');
-var assert = require('assert-plus');
-var async = require('async');
-var mkdirp = require('mkdirp');
-var passwd = require('passwd');
-
-var lib_config = require('../config');
-var constants = require('../constants');
-
-
-//---- globals
-
-var NAME = path.basename(__filename);
-var UUID_REGEX =
-    /[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/;
-
-var config;
-var mantaClient;
-var mantaDir, localDir;
-
-
-
-//---- support functions
-
-function errexit(err) {
-    console.error(NAME + ' error: ' + err);
-    process.exit(1);
-}
-
-function warn() {
-    arguments[0] = NAME + ' warn: ' + arguments[0];
-    console.warn.apply(null, arguments);
-}
-
-function info() {
-    arguments[0] = NAME + ' info: ' + arguments[0];
-    console.log.apply(null, arguments);
-}
-
-
-var _nobodyCache;
-function getNobody(callback) {
-    if (_nobodyCache !== undefined)
-        return callback(_nobodyCache);
-
-    passwd.get('nobody', function (nobody) {
-        _nobodyCache = nobody;
-        callback(_nobodyCache);
-    });
-}
-
-
-function moveMantaImages(callback) {
-    mantaClient.ls(mantaDir, function (lsErr, res) {
-        if (lsErr) {
-            if (lsErr.statusCode && lsErr.statusCode === 404) {
-              return callback();
-            } else {
-              return callback(lsErr);
-            }
-        }
-
-        var dirs = [];
-        res.on('object', function (obj) {
-            // Ignore objects
-        });
-        res.on('directory', function (dir) {
-            if (UUID_REGEX.test(dir.name)) {
-                dirs.push(dir.name);
-            }
-        });
-        // Only remove directory if it's empty
-        res.once('end', function () {
-            async.forEachSeries(dirs, moveImage, callback);
-        });
-    });
-
-    function moveImage(dir, next) {
-        var prefix = dir.slice(0, 3);
-        var dirPath = path.join(mantaDir, dir);
-        var newDirPath = path.join(mantaDir, prefix, dir);
-
-        mantaClient.mkdirp(newDirPath, function (mkErr) {
-            if (mkErr) {
-                return next(mkErr);
-            }
-
-            mantaClient.ls(dirPath, function (lsErr, res) {
-                if (lsErr) {
-                    return next(lsErr);
-                }
-
-                var files = [];
-                res.on('object', function (obj) {
-                    // $prefix, $uuid/file0
-                    // $prefix, $uuid/icon
-                    files.push([ prefix, path.join(dir, obj.name) ]);
-                });
-                res.on('directory', function (aDir) {
-                    // There should not be any directories here
-                });
-                // Only remove directory if it's empty
-                res.once('end', function () {
-                    if (files.length) {
-                        // Remove dir after move
-                        async.forEachSeries(files, moveFile, function (aErr) {
-                            if (aErr) {
-                                return next(aErr);
-                            }
-                            return removeDir(dirPath, next);
-                        });
-                    } else {
-                        return removeDir(dirPath, next);
-                    }
-                });
-            });
-        });
-    }
-
-    function removeDir(dirPath, next) {
-        mantaClient.rmr(dirPath, function (rmErr) {
-            if (rmErr) {
-                return next(rmErr);
-            }
-            info('directory "%s" has been removed',
-                dirPath);
-            return next();
-        });
-    }
-
-    function moveFile(array, next) {
-        var prefix = array[0];
-        var file = array[1];
-
-        var oldPath = path.join(mantaDir, file);
-        var newPath = path.join(mantaDir, prefix, file);
-
-        mantaClient.info(oldPath, function (infoErr, fInfo) {
-            if (infoErr) {
-                return next(infoErr);
-            }
-
-            return moveAndRemove(fInfo.md5);
-        });
-
-        function moveAndRemove(oldMd5) {
-            mantaClient.ln(oldPath, newPath, function (lnErr) {
-                if (lnErr) {
-                    return next(lnErr);
-                }
-
-                // Now, make sure both md5's match before removing the source
-                mantaClient.info(newPath, function (info2Err, info2) {
-                    if (info2Err) {
-                        return next(info2Err);
-                    }
-
-                    if (oldMd5 !== info2.md5) {
-                        warn('md5 for "%s" doesn\'t match old md5 (%s vs %s)',
-                            newPath, oldMd5, info2.md5);
-                        return next();
-                    }
-                    // If it matches, delete the old file
-                    mantaClient.unlink(oldPath, function (ulErr) {
-                        if (ulErr) {
-                            return next(ulErr);
-                        }
-
-                        info('directory "%s" successfully moved', newPath);
-                        return next();
-                    });
-                });
-            });
-        }
-    }
-}
-
-
-function mantaMigrate(callback) {
-    assert.string(config.manta.url, 'config.manta.url');
-    assert.string(config.manta.user, 'config.manta.user');
-    assert.string(config.manta.key, 'config.manta.key');
-    assert.string(config.manta.keyId, 'config.manta.keyId');
-    assert.optionalBool(config.manta.insecure, 'config.manta.insecure');
-    assert.string(config.manta.rootDir, 'config.manta.rootDir');
-
-    var insecure = config.manta.hasOwnProperty('insecure') ?
-        config.manta.insecure : false;
-    mantaDir = path.join(config.manta.rootDir, 'images');  // global
-
-    mantaClient = manta.createClient({
-        sign: {
-            key: config.manta.key,
-            keyId: config.manta.keyId,
-            user: config.manta.user
-        },
-        user: config.manta.user,
-        url: config.manta.url,
-        rejectUnauthorized: !insecure
-    });
-
-    return moveMantaImages(callback);
-}
-
-
-function moveLocalImages(callback) {
-    fs.readdir(localDir, function (dirErr, dirs) {
-        if (dirErr) {
-            warn(dirErr, 'Error reading local storage directory');
-            return callback(dirErr);
-        }
-
-        async.forEachSeries(dirs, moveDir, function (moveDirErr) {
-            if (moveDirErr) {
-                warn('Error moving local directories');
-                callback(moveDirErr);
-            } else {
-                info('Images successfully moved to new location');
-                callback();
-            }
-
-        });
-    });
-
-    function moveDir(dir, next) {
-        if (! UUID_REGEX.test(dir)) {
-            if (dir.length !== 3) {
-                warn('Directory %s is not a UUID, skipping', dir);
-            }
-            return next();
-        }
-
-        var prefix = dir.slice(0, 3);
-        var oldDir = path.join(localDir, dir);
-        var newDir = path.join(localDir, prefix, dir);
-        var prefixDir = path.join(localDir, prefix);
-
-        mkdirp(prefixDir, function (mkErr) {
-            if (mkErr) {
-                warn('Could not create %s', prefixDir);
-                return next(mkErr);
-            }
-
-            fs.rename(oldDir, newDir, function (moveErr) {
-                if (moveErr) {
-                    warn('Could not move %s', oldDir);
-                    return next(moveErr);
-                }
-
-                if (config.mode !== 'dc') {
-                    info('%s has been moved successfully', dir);
-                    return next();
-                }
-                // chmod new image directory to 'nobody' user so the imgapi
-                // service (running as 'nobody') can change it.
-                getNobody(function (nobody) {
-                    if (!nobody) {
-                        return next(new Error('could not get nobody user'));
-                    }
-                    fs.chown(prefixDir, Number(nobody.userId),
-                    Number(nobody.groupId), function (chownErr) {
-                        if (chownErr) {
-                            return next(chownErr);
-                        }
-                        info('%s has been moved successfully', dir);
-                        return next();
-                    });
-                });
-            });
-        });
-    }
-}
-
-
-function localMigrate(callback) {
-    localDir = constants.STORAGE_LOCAL_IMAGES_DIR;  // global
-    assert.ok(localDir !== '/',
-        'cannot have empty or root dir for local storage');
-
-    return moveLocalImages(callback);
-}
-
-
-
-//---- mainline
-
-function main(argv) {
-    lib_config.loadConfig({}, function (confErr, config_) {
-        if (confErr) {
-            errexit(confErr);
-            return;
-        }
-
-        config = config_;
-        assert.object(config.storageTypes, 'config.storageTypes');
-
-        // Because all IMGAPI instances have local storage
-        var functions = [ localMigrate ];
-        if (config.storageTypes.indexOf('manta') !== -1) {
-            functions.push(mantaMigrate);
-        }
-
-        async.series(functions, function (err) {
-            if (err) {
-                errexit(err);
-            } else {
-                process.exit(0);
-            }
-        });
-    });
-}
-
-if (require.main === module) {
-    main(process.argv);
-}
+// old migration file, no longer used, just here to avoid sdcadm update failures
diff --git a/lib/migrations/migration-009-backfill-archive.js b/lib/migrations/migration-009-backfill-archive.js
index 5a368cb..334e22a 100644
--- a/lib/migrations/migration-009-backfill-archive.js
+++ b/lib/migrations/migration-009-backfill-archive.js
@@ -1,312 +1 @@
-#!/usr/bin/env node
-/*
- * This Source Code Form is subject to the terms of the Mozilla Public
- * License, v. 2.0. If a copy of the MPL was not distributed with this
- * file, You can obtain one at http://mozilla.org/MPL/2.0/.
- */
-
-/*
- * Copyright 2016 Joyent, Inc.
- */
-
-/*
- * IMGAPI db migration: TODO: describe
- */
-
-var fs = require('fs');
-var path = require('path');
-var moray = require('moray');
-var bunyan = require('bunyan');
-var assert = require('assert-plus');
-var async = require('async');
-var passwd = require('passwd');
-var format = require('util').format;
-var execFile = require('child_process').execFile;
-var mkdirp = require('mkdirp');
-var rimraf = require('rimraf');
-
-var lib_config = require('../config');
-var constants = require('../constants');
-var errors = require('../errors');
-
-
-//---- globals
-
-var NAME = path.basename(__filename);
-
-var config;
-var morayClient = null;  // set in `getMorayClient()`
-
-
-//---- support functions
-
-function errexit(err) {
-    console.error(NAME + ' error: ' + err);
-    process.exit(1);
-}
-
-function warn() {
-    arguments[0] = NAME + ' warn: ' + arguments[0];
-    console.warn.apply(null, arguments);
-}
-
-function info() {
-    arguments[0] = NAME + ' info: ' + arguments[0];
-    console.log.apply(null, arguments);
-}
-
-
-function getMorayClient(callback) {
-    var client = moray.createClient({
-        connectTimeout: config.moray.connectTimeout || 200,
-        host: config.moray.host,
-        port: config.moray.port,
-        log: bunyan.createLogger({
-            name: 'moray',
-            level: 'INFO',
-            stream: process.stdout,
-            serializers: bunyan.stdSerializers
-        }),
-        reconnect: true,
-        retry: (config.moray.retry === false ? false : {
-            retries: Infinity,
-            minTimeout: 1000,
-            maxTimeout: 16000
-        })
-    });
-
-    client.on('connect', function () {
-        return callback(client);
-    });
-}
-
-function morayListImages(callback) {
-    var req = morayClient.findObjects('imgapi_images', 'uuid=*');
-    var images = [];
-
-    req.once('error', function (err) {
-        return callback(err);
-    });
-
-    req.on('record', function (object) {
-        images.push(object.value);
-    });
-
-    req.once('end', function () {
-        return callback(null, images);
-    });
-}
-
-
-var _nobodyCache;
-function getNobody(callback) {
-    if (_nobodyCache !== undefined)
-        return callback(_nobodyCache);
-
-    passwd.get('nobody', function (nobody) {
-        _nobodyCache = nobody;
-        callback(_nobodyCache);
-    });
-}
-
-
-function boolFromString(value) {
-    if (value === 'false') {
-        return false;
-    } else if (value === 'true') {
-        return true;
-    } else if (typeof (value) === 'boolean') {
-        return value;
-    }
-}
-
-
-function arrayToObject(array) {
-    if (!array) {
-        throw new TypeError('Array of key/values required');
-    } else if (typeof (array) === 'string') {
-        array = [array];
-    }
-
-    var obj = {};
-    array.forEach(function (keyvalue) {
-        var kv = keyvalue.split('=');
-
-        if (kv.length != 2) {
-            throw new TypeError('Key/value string expected');
-        }
-
-        obj[kv[0]] = kv[1];
-    });
-
-    return obj;
-}
-
-
-function toManifest(image) {
-    if (image.activated !== undefined) {
-        image.activated = boolFromString(image.activated);
-    }
-    if (image.disabled !== undefined) {
-        image.disabled = boolFromString(image.disabled);
-    }
-    if (image['public'] !== undefined) {
-        image['public'] = boolFromString(image['public']);
-    }
-    if (image.generate_passwords !== undefined) {
-        image.generate_passwords = boolFromString(image.generate_passwords);
-    }
-    if (image.image_size) {
-        image.image_size = Number(image.image_size);
-    }
-    if (image.tags) {
-        image.tags = arrayToObject(image.tags);
-    }
-    if (image.files) {
-        image.files = image.files.map(function (f) {
-            return {
-                sha1: f.sha1,
-                size: f.size,
-                compression: f.compression,
-                dataset_guid: f.dataset_guid
-            };
-        });
-    }
-
-    return image;
-}
-
-
-function migrateImage(image, callback) {
-    var archiveDir = constants.STORAGE_LOCAL_ARCHIVE_DIR;
-
-    info('migrate "%s"', image.uuid);
-    var manifest = toManifest(image);
-    var content = JSON.stringify(manifest, null, 2);
-
-    var archivePathFromImageUuid = function (uuid) {
-        return path.resolve(archiveDir, uuid.slice(0, 3), uuid + '.json');
-    };
-
-    var archPath = archivePathFromImageUuid(image.uuid);
-    var archDir = path.dirname(archPath);
-    mkdirp(archDir, function (err) {
-        if (err) {
-            return callback(err);
-        }
-        rimraf(archPath, function (err2) {
-            if (err2) {
-                return callback(err2);
-            }
-            fs.writeFile(archPath, content, 'utf8', afterWrite);
-        });
-    });
-
-    function afterWrite(err) {
-        if (err) {
-            return callback(err);
-        }
-
-        if (config.mode !== 'dc') {
-            return callback();
-        }
-        // chown both dirs
-        getNobody(function (nobody) {
-            if (!nobody) {
-                return callback(new Error('could not get nobody user'));
-            }
-            fs.chown(archPath, Number(nobody.userId), Number(nobody.groupId),
-                function (chErr) {
-                    if (chErr) {
-                        return callback(chErr);
-                    }
-                    fs.chown(archDir, Number(nobody.userId),
-                    Number(nobody.groupId), callback);
-                });
-        });
-    }
-}
-
-function morayMigrate(callback) {
-    assert.equal(config.databaseType, 'moray');
-    getMorayClient(function (mclient) {
-        morayClient = mclient;
-
-        morayListImages(function (err2, images) {
-            if (err2)
-                return callback(err2);
-            info('%d images to potentially migrate', images.length);
-            async.forEachSeries(images, migrateImage, callback);
-        });
-    });
-}
-
-
-function localListImages(callback) {
-    /*JSSTYLED*/
-    var RAW_FILE_RE = /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\.raw$/;
-    fs.readdir(constants.DATABASE_LOCAL_DIR, function (err, files) {
-        var images = [];
-        async.forEachSeries(
-            files,
-            function oneFile(file, next) {
-                if (!RAW_FILE_RE.test(file))
-                    return next();
-                var path_ = path.resolve(constants.DATABASE_LOCAL_DIR, file);
-                fs.readFile(path_, 'utf8', function (readErr, content) {
-                    if (readErr)
-                        return next(readErr);
-                    try {
-                        images.push(JSON.parse(content));
-                    } catch (ex) {
-                        return next(ex);
-                    }
-                    next();
-                });
-            },
-            function done(err2) {
-                callback(err2, images);
-            }
-        );
-    });
-}
-
-
-function localMigrate(callback) {
-    assert.equal(config.databaseType, 'local');
-    localListImages(function (err, images) {
-        if (err)
-            return callback(err);
-        async.forEachSeries(images, migrateImage, callback);
-    });
-}
-
-
-
-//---- mainline
-
-function main(argv) {
-    lib_config.loadConfig({}, function (confErr, config_) {
-        if (confErr) {
-            errexit(confErr);
-            return;
-        }
-
-        config = config_;
-        assert.string(config.databaseType, 'config.databaseType');
-
-        var migrator = (config.databaseType === 'moray'
-            ? morayMigrate : localMigrate);
-        migrator(function (err) {
-            if (err) {
-                errexit(err);
-            } else {
-                process.exit(0);
-            }
-        });
-    });
-}
-
-if (require.main === module) {
-    main(process.argv);
-}
+// old migration file, no longer used, just here to avoid sdcadm update failures
diff --git a/lib/migrations/migration-010-backfill-billing_tags.js b/lib/migrations/migration-010-backfill-billing_tags.js
index 452def9..334e22a 100644
--- a/lib/migrations/migration-010-backfill-billing_tags.js
+++ b/lib/migrations/migration-010-backfill-billing_tags.js
@@ -1,229 +1 @@
-#!/usr/bin/env node
-/*
- * This Source Code Form is subject to the terms of the Mozilla Public
- * License, v. 2.0. If a copy of the MPL was not distributed with this
- * file, You can obtain one at http://mozilla.org/MPL/2.0/.
- */
-
-/*
- * Copyright 2016 Joyent, Inc.
- */
-
-/*
- * IMGAPI db migration: TODO: describe
- */
-
-var fs = require('fs');
-var path = require('path');
-var moray = require('moray');
-var bunyan = require('bunyan');
-var errors = require('../errors');
-var assert = require('assert-plus');
-var async = require('async');
-var passwd = require('passwd');
-var format = require('util').format;
-var execFile = require('child_process').execFile;
-var mkdirp = require('mkdirp');
-var rimraf = require('rimraf');
-
-var lib_config = require('../config');
-
-
-//---- globals
-
-var NAME = path.basename(__filename);
-
-var config;
-var morayClient = null;  // set in `getMorayClient()`
-
-var BUCKET = {
-    name: 'imgapi_images',
-    indices: {
-        index: {
-            uuid: { type: 'string', unique: true},
-            name: { type: 'string' },
-            version: { type: 'string' },
-            owner: { type: 'string' },
-            origin: { type: 'string' },
-            state: { type: 'string' },
-            urn: { type: 'string', unique: true },
-            tags: { type: '[string]' },
-            billing_tags: { type: '[string]' },
-            acl: { type: '[string]' },
-            activated: { type: 'boolean' },
-            disabled: { type: 'boolean' },
-            public: { type: 'boolean' },
-            os: { type: 'string' },
-            type: { type: 'string' },
-            expires_at: { type: 'string' }
-        }
-    }
-};
-
-// Because regular findObjects will not load the "hidden" billing_tags values
-// we need to load all raw objets with sql() and then perform a single
-// getObject for each one of them
-var allBillingTags = {};
-
-
-//---- support functions
-
-function errexit(err) {
-    console.error(NAME + ' error: ' + err);
-    process.exit(1);
-}
-
-function warn() {
-    arguments[0] = NAME + ' warn: ' + arguments[0];
-    console.warn.apply(null, arguments);
-}
-
-function info() {
-    arguments[0] = NAME + ' info: ' + arguments[0];
-    console.log.apply(null, arguments);
-}
-
-
-function getMorayClient(callback) {
-    var client = moray.createClient({
-        connectTimeout: config.moray.connectTimeout || 200,
-        host: config.moray.host,
-        port: config.moray.port,
-        log: bunyan.createLogger({
-            name: 'moray',
-            level: 'INFO',
-            stream: process.stdout,
-            serializers: bunyan.stdSerializers
-        }),
-        reconnect: true,
-        retry: (config.moray.retry === false ? false : {
-            retries: Infinity,
-            minTimeout: 1000,
-            maxTimeout: 16000
-        })
-    });
-
-    client.on('connect', function () {
-        return callback(client);
-    });
-}
-
-function morayListImages(callback) {
-    // var req = morayClient.findObjects('imgapi_images', 'uuid=*');
-    var req = morayClient.sql('select * from ' + BUCKET.name);
-
-    req.once('error', function (err) {
-        return callback(err);
-    });
-
-    req.on('record', function (object) {
-        var value = JSON.parse(object._value);
-        if (value.billing_tags !== undefined) {
-            allBillingTags[object._key] = value.billing_tags;
-        }
-    });
-
-    req.once('end', function () {
-        return callback(null);
-    });
-}
-
-
-function migrateImage(uuid, callback) {
-    info('migrate "%s"', uuid);
-
-    // We just need to write the image again. billing_tags was previously inside
-    // _value and now we need to write the object again so its moray index is
-    // written correctly
-    morayClient.getObject(BUCKET.name, uuid, function (getErr, object) {
-        if (getErr) {
-            callback(getErr);
-            return;
-        }
-
-        var image = object.value;
-        image.billing_tags = allBillingTags[uuid];
-
-        morayClient.putObject(BUCKET.name, uuid, image, { noBucketCache: true },
-        function (err) {
-            if (err) {
-                callback(err);
-                return;
-            }
-
-            info('"billing_tags" for image %s have been updated', uuid);
-            callback();
-        });
-    });
-}
-
-function morayMigrate(callback) {
-    assert.equal(config.databaseType, 'moray');
-    morayListImages(function (err2) {
-        if (err2) {
-            return callback(err2);
-        }
-
-        var images = Object.keys(allBillingTags);
-
-        info('%d images to potentially migrate', images.length);
-        async.forEachSeries(images, migrateImage, callback);
-    });
-}
-
-function updateBucket(callback) {
-    getMorayClient(function (mclient) {
-        morayClient = mclient;
-        morayClient.getBucket(BUCKET.name, function (err, bck) {
-            if (err) {
-                return callback(err);
-            } else if (bck.index.billing_tags !== undefined) {
-                info('"billing_tags" index already exists, no need to add');
-                return callback();
-            }
-
-            info('adding "billing_tags" index');
-            morayClient.updateBucket(BUCKET.name, BUCKET.indices, callback);
-        });
-    });
-}
-
-
-
-//---- mainline
-
-function main(argv) {
-    lib_config.loadConfig({}, function (confErr, config_) {
-        if (confErr) {
-            errexit(confErr);
-            return;
-        }
-
-        config = config_;
-        assert.string(config.databaseType, 'config.databaseType');
-
-        if (config.databaseType !== 'moray') {
-            info('migration not needed, databases type is not moray');
-            process.exit(0);
-        }
-
-        updateBucket(function (updateErr) {
-            if (updateErr) {
-                errexit(updateErr);
-                return;
-            }
-
-            morayMigrate(function (err) {
-                if (err) {
-                    errexit(err);
-                } else {
-                    process.exit(0);
-                }
-            });
-        });
-    });
-}
-
-if (require.main === module) {
-    main(process.argv);
-}
+// old migration file, no longer used, just here to avoid sdcadm update failures
diff --git a/lib/migrations/migration-011-backfill-published_at.js b/lib/migrations/migration-011-backfill-published_at.js
index d849349..334e22a 100644
--- a/lib/migrations/migration-011-backfill-published_at.js
+++ b/lib/migrations/migration-011-backfill-published_at.js
@@ -1,241 +1 @@
-#!/usr/bin/env node
-/*
- * This Source Code Form is subject to the terms of the Mozilla Public
- * License, v. 2.0. If a copy of the MPL was not distributed with this
- * file, You can obtain one at http://mozilla.org/MPL/2.0/.
- */
-
-/*
- * Copyright 2016 Joyent, Inc.
- */
-
-/*
- * IMGAPI db migration: TODO: describe
- */
-
-var fs = require('fs');
-var path = require('path');
-var moray = require('moray');
-var bunyan = require('bunyan');
-var errors = require('../errors');
-var assert = require('assert-plus');
-var async = require('async');
-var passwd = require('passwd');
-var format = require('util').format;
-var execFile = require('child_process').execFile;
-var mkdirp = require('mkdirp');
-var rimraf = require('rimraf');
-
-var lib_config = require('../config');
-
-
-//---- globals
-
-var NAME = path.basename(__filename);
-
-var config;
-var morayClient = null;  // set in `getMorayClient()`
-
-var BUCKET = {
-    name: 'imgapi_images',
-    indices: {
-        index: {
-            uuid: { type: 'string', unique: true},
-            name: { type: 'string' },
-            version: { type: 'string' },
-            owner: { type: 'string' },
-            origin: { type: 'string' },
-            state: { type: 'string' },
-            urn: { type: 'string', unique: true },
-            tags: { type: '[string]' },
-            billing_tags: { type: '[string]' },
-            acl: { type: '[string]' },
-            activated: { type: 'boolean' },
-            disabled: { type: 'boolean' },
-            public: { type: 'boolean' },
-            os: { type: 'string' },
-            type: { type: 'string' },
-            expires_at: { type: 'string' },
-            published_at: { type: 'string' }
-        }
-    }
-};
-
-
-// Because regular findObjects will not load the "hidden" billing_tags values
-// we need to load all raw objets with sql() and then perform a single
-// getObject for each one of them
-var allPublishedAt = {};
-
-
-//---- support functions
-
-function errexit(err) {
-    console.error(NAME + ' error: ' + err);
-    process.exit(1);
-}
-
-function warn() {
-    arguments[0] = NAME + ' warn: ' + arguments[0];
-    console.warn.apply(null, arguments);
-}
-
-function info() {
-    arguments[0] = NAME + ' info: ' + arguments[0];
-    console.log.apply(null, arguments);
-}
-
-if (process.env.TRACE) {
-    function trace() {
-        arguments[0] = NAME + ' trace: ' + arguments[0];
-        console.log.apply(null, arguments);
-    }
-} else {
-    /*jsl:ignore*/
-    function trace() {}
-    /*jsl:end*/
-}
-
-
-function getMorayClient(callback) {
-    var client = moray.createClient({
-        connectTimeout: config.moray.connectTimeout || 200,
-        host: config.moray.host,
-        port: config.moray.port,
-        log: bunyan.createLogger({
-            name: 'moray',
-            level: 'INFO',
-            stream: process.stdout,
-            serializers: bunyan.stdSerializers
-        }),
-        reconnect: true,
-        retry: (config.moray.retry === false ? false : {
-            retries: Infinity,
-            minTimeout: 1000,
-            maxTimeout: 16000
-        })
-    });
-
-    client.on('connect', function () {
-        return callback(client);
-    });
-}
-
-
-function morayListImages(callback) {
-    var req = morayClient.sql('select * from ' + BUCKET.name);
-
-    req.once('error', function (err) {
-        return callback(err);
-    });
-
-    req.on('record', function (object) {
-        var value = JSON.parse(object._value);
-        if (value.published_at !== undefined) {
-            allPublishedAt[object._key] = value.published_at;
-        }
-    });
-
-    req.once('end', function () {
-        return callback(null);
-    });
-}
-
-
-function migrateImage(uuid, callback) {
-    trace('migrate "%s"', uuid);
-
-    // We just need to write the image again so its moray index is
-    // written correctly
-    morayClient.getObject(BUCKET.name, uuid, function (getErr, object) {
-        if (getErr) {
-            callback(getErr);
-            return;
-        }
-
-        var image = object.value;
-        image.published_at = allPublishedAt[uuid];
-
-        morayClient.putObject(BUCKET.name, uuid, image, { noBucketCache: true },
-        function (err) {
-            if (err) {
-                callback(err);
-                return;
-            }
-
-            info('"published_at" for image %s has been updated', uuid);
-            callback();
-        });
-    });
-}
-
-function morayMigrate(callback) {
-    assert.equal(config.databaseType, 'moray');
-    morayListImages(function (err2) {
-        if (err2) {
-            return callback(err2);
-        }
-
-        var images = Object.keys(allPublishedAt);
-
-        info('%d images to potentially migrate', images.length);
-        async.forEachSeries(images, migrateImage, callback);
-    });
-}
-
-function updateBucket(callback) {
-    getMorayClient(function (mclient) {
-        morayClient = mclient;
-        morayClient.getBucket(BUCKET.name, function (err, bck) {
-            if (err) {
-                return callback(err);
-            } else if (bck.index.published_at !== undefined) {
-                info('"published_at" index already exists, no need to add');
-                return callback();
-            }
-
-            info('adding "published_at" index');
-            morayClient.updateBucket(BUCKET.name, BUCKET.indices, callback);
-        });
-    });
-}
-
-
-
-//---- mainline
-
-function main(argv) {
-    lib_config.loadConfig({}, function (confErr, config_) {
-        if (confErr) {
-            errexit(confErr);
-            return;
-        }
-
-        config = config_;
-        assert.string(config.databaseType, 'config.databaseType');
-
-        if (config.databaseType !== 'moray') {
-            info('migration not needed, databases type is not moray');
-            process.exit(0);
-        }
-
-        updateBucket(function (updateErr) {
-            if (updateErr) {
-                errexit(updateErr);
-                return;
-            }
-
-            morayMigrate(function (err) {
-                if (err) {
-                    errexit(err);
-                } else {
-                    process.exit(0);
-                }
-            });
-        });
-    });
-}
-
-if (require.main === module) {
-    main(process.argv);
-}
+// old migration file, no longer used, just here to avoid sdcadm update failures
diff --git a/lib/migrations/migration-012-update-docker-image-uuids.js b/lib/migrations/migration-012-update-docker-image-uuids.js
index d013e21..334e22a 100644
--- a/lib/migrations/migration-012-update-docker-image-uuids.js
+++ b/lib/migrations/migration-012-update-docker-image-uuids.js
@@ -1,444 +1 @@
-#!/usr/bin/env node
-/*
- * This Source Code Form is subject to the terms of the Mozilla Public
- * License, v. 2.0. If a copy of the MPL was not distributed with this
- * file, You can obtain one at http://mozilla.org/MPL/2.0/.
- */
-
-/*
- * Copyright 2016 Joyent, Inc.
- */
-
-/*
- * Update type=docker images to the new UUID scheme. In early sdc-docker days
- * the image UUID was just the first half of the Docker ID. That changed to
- * be a v5 uuid of the Docker registry host and the Docker ID (see DOCKER-257).
- * This migration handles that UUID change:
- * - update 'manifest.uuid' in the DB
- * - update 'manifest.origin' in the DB
- * - drop tags.docker=true, this is obsolete
- * - move the files in storage as appropriate (new loc b/c new uuid)
- *
- * Limitations:
- * - Don't handle a "local" DB (i.e. only handle Moray DB).
- * - Don't handle a "manta" storage. While Docker images are intended to be
- *   stored in Manta, there was a bug before this migration that meant they
- *   weren't.
- */
-
-var fs = require('fs');
-var path = require('path');
-var moray = require('moray');
-var bunyan = require('bunyan');
-var assert = require('assert-plus');
-var async = require('async');
-var passwd = require('passwd');
-var format = require('util').format;
-var execFile = require('child_process').execFile;
-var mkdirp = require('mkdirp');
-var rimraf = require('rimraf');
-var imgmanifest = require('imgmanifest');
-var drc = require('docker-registry-client');
-var vasync = require('vasync');
-
-var lib_config = require('../config');
-var constants = require('../constants');
-var errors = require('../errors');
-var utils = require('../utils');
-
-
-//---- globals
-
-var NAME = path.basename(__filename);
-
-var config;
-var morayClient = null;  // set in `getMorayClient()`
-
-
-//---- support functions
-
-function errexit(err) {
-    arguments[0] = NAME + ' error: ' + arguments[0];
-    console.error.apply(null, arguments);
-    process.exit(1);
-}
-
-function warn() {
-    arguments[0] = NAME + ' warn: ' + arguments[0];
-    console.warn.apply(null, arguments);
-}
-
-function info() {
-    arguments[0] = NAME + ' info: ' + arguments[0];
-    console.log.apply(null, arguments);
-}
-
-var trace = function () {};
-if (process.env.TRACE) {
-    trace = function () {
-        arguments[0] = NAME + ' trace: ' + arguments[0];
-        console.log.apply(null, arguments);
-    };
-}
-
-var _nobodyCache;
-function getNobody(callback) {
-    if (_nobodyCache !== undefined)
-        return callback(_nobodyCache);
-
-    passwd.get('nobody', function (nobody) {
-        _nobodyCache = nobody;
-        callback(_nobodyCache);
-    });
-}
-
-
-var BUCKET = 'imgapi_images';
-var localDir;
-
-
-function getMorayClient(callback) {
-    var client = moray.createClient({
-        connectTimeout: config.moray.connectTimeout || 200,
-        host: config.moray.host,
-        port: config.moray.port,
-        log: bunyan.createLogger({
-            name: 'moray',
-            level: 'INFO',
-            stream: process.stdout,
-            serializers: bunyan.stdSerializers
-        }),
-        reconnect: true,
-        retry: (config.moray.retry === false ? false : {
-            retries: Infinity,
-            minTimeout: 1000,
-            maxTimeout: 16000
-        })
-    });
-
-    client.on('connect', function () {
-        return callback(client);
-    });
-}
-
-function morayFind(bucket, filter, callback) {
-    var hits = [];
-    var req = morayClient.findObjects(bucket, filter);
-    req.once('error', function (err) {
-        trace('morayFind(%s, "%s") error: %s', bucket, filter, err);
-        return callback(err);
-    });
-    req.on('record', function (object) {
-        hits.push(object);
-    });
-    req.once('end', function () {
-        trace('morayFind(%s, "%s") hits: %j', bucket, filter, hits);
-        return callback(null, hits);
-    });
-}
-
-function morayListImages(callback) {
-    var images = [];
-    var req = morayClient.sql('select * from ' + BUCKET
-        + ' where type=\'docker\' and activated');
-
-    req.once('error', function (err) {
-        return callback(err);
-    });
-
-    req.on('record', function (object) {
-        var value = JSON.parse(object._value);
-        images.push(value.uuid);
-    });
-
-    req.once('end', function () {
-        return callback(null, images);
-    });
-}
-
-
-/*
- * Images are going to be migrated only under the following conditions:
- *
- * 1. type === 'docker'
- * 2. files[0].stor === local
- * 3. tags['docker:repo'] and tags['docker:id'] are present
- * 4. newUuid !== uuid
- */
-function migrateImage(uuid, callback) {
-    trace('migrate "%s"', uuid);
-    var image;
-    var indexName;
-    var newUuid;
-    var origin;
-
-    var oldPrefix = uuid.slice(0, 3);
-    var oldFile = path.join(localDir, oldPrefix, uuid, 'file0');
-
-    function getOrigin(cb) {
-        if (!image.origin) {
-            return cb();
-        }
-
-        morayClient.getObject(BUCKET, image.origin, function (getErr, object) {
-            if (getErr && getErr.name === 'ObjectNotFoundError') {
-                /*
-                 * If the origin was already migrated, it won't be there
-                 * anymore. We have the uuid, which in the old world, is the
-                 * first half of the Docker ID. We'll make a short (12-char)
-                 * Docker ID and search by 'version=shortDockerId'. Bit of a
-                 * hack, but should have a unique hit.
-                 */
-                var shortId = image.origin.replace(/-/g, '').slice(0, 12);
-                var filter = '(&(name=docker-layer)(version=' + shortId + '))';
-                morayFind(BUCKET, filter, function (getErr2, hits) {
-                    if (getErr2) {
-                        cb(getErr2);
-                    } else {
-                        assert.equal(hits.length, 1,
-                            'unexpected number of image hits for origin: '
-                            + hits);
-                        origin = hits[0].value;
-                        cb();
-                    }
-                });
-            } else if (getErr) {
-                cb(getErr);
-            } else {
-                origin = object.value;
-                cb();
-            }
-        });
-    }
-
-    function linkFile(cb) {
-        info('migrating image %s to new UUID %s', uuid, newUuid);
-
-        var newPrefix = newUuid.slice(0, 3);
-        var newDir = path.join(localDir, newPrefix, newUuid);
-        var newFile = path.join(newDir, 'file0');
-
-        vasync.pipeline({arg: {}, funcs: [
-            function getNobodyInfo(ctx, next) {
-                getNobody(function (nobody) {
-                    if (!nobody) {
-                        next(new Error('could not get nobody user'));
-                    } else {
-                        ctx.nobody = nobody;
-                        next();
-                    }
-                });
-            },
-
-            function makeNewDir(ctx, next) {
-                mkdirp(newDir, function (err) {
-                    if (err) {
-                        warn('Could not mkdir -p %s', newDir);
-                        next(err);
-                    } else {
-                        next();
-                    }
-                });
-            },
-
-            function chownNewDir(ctx, next) {
-                // IMGAPI runs as 'nobody' and must own this dir.
-                fs.chown(newDir,
-                    Number(ctx.nobody.userId),
-                    Number(ctx.nobody.groupId),
-                    next);
-            },
-
-            function chownNewPrefixDir(ctx, next) {
-                fs.chown(path.dirname(newDir),
-                    Number(ctx.nobody.userId),
-                    Number(ctx.nobody.groupId),
-                    next);
-            },
-
-            function linkTheFile(ctx, next) {
-                fs.link(oldFile, newFile, function (err) {
-                    if (err && err.code && err.code === 'EEXIST') {
-                        warn('Destination file already exists: %s', newFile);
-                        next();
-                    } else if (err) {
-                        warn('Could not link %s', newFile);
-                        next(err);
-                    } else {
-                        // IMGAPI runs as 'nobody' and must own this file.
-                        fs.chown(newFile,
-                            Number(ctx.nobody.userId),
-                            Number(ctx.nobody.groupId),
-                            next);
-                    }
-                });
-            }
-
-        ]}, function (err) {
-            if (!err) {
-                info('%s has been linked successfully', newFile);
-            }
-            cb(err);
-        });
-    }
-
-    function recreateObject(cb) {
-        assert.ok(uuid !== newUuid);
-
-        // Updates to the manifest.
-        image.uuid = newUuid;
-        if (config.adminUuid && image.owner === config.adminUuid) {
-            // Move all sdc-docker-managed images to be private to admin.
-            image.public = false;
-        }
-        if (image.tagsObj['docker'] === true) {
-            delete image.tagsObj['docker'];
-        }
-        // Normalize tags['docker:repo'] to a repo 'localName'. Earlier code
-        // would have, e.g. 'library/busybox'.
-        if (image.tagsObj['docker:repo']) {
-            var norm = drc.parseRepo(image.tagsObj['docker:repo']).localName;
-            image.tagsObj['docker:repo'] = norm;
-        }
-        image.tags = utils.tagsSearchArrayFromObj(image.tagsObj);
-        if (image.origin) {
-            image.origin = imgmanifest.imgUuidFromDockerInfo({
-                id: origin.tagsObj['docker:id'],
-                indexName: indexName
-            });
-        }
-
-        // Save changes to the DB.
-        var batch = [ {
-            bucket: BUCKET,
-            key: newUuid,
-            value: image
-        }, {
-            bucket: BUCKET,
-            operation: 'delete',
-            key: uuid
-        }];
-        morayClient.batch(batch, cb);
-    }
-
-    function unlinkFile(cb) {
-        fs.unlink(oldFile, function (err) {
-            if (err) {
-                return cb(err);
-            }
-            info('%s has been removed successfully', oldFile);
-            return cb();
-        });
-    }
-
-    morayClient.getObject(BUCKET, uuid, function (getErr, object) {
-        if (getErr) {
-            callback(getErr);
-            return;
-        }
-
-        image = object.value;
-
-        if (!image.tagsObj || !image.tagsObj['docker:id']) {
-            errexit('cannot migrate %s. Image docker tags are incomplete',
-                uuid);
-            callback();
-            return;
-        }
-
-        var dockerId = image.tagsObj['docker:id'];
-        if (dockerId.length !== 64) {
-            // It's a different docker id to what we are expecting - skip it, as
-            // it's likely a newer (v2.2) docker id of the form 'sha256:123...'.
-            info('Skipped dockerId where length != 64: %s', dockerId);
-            callback();
-            return;
-        }
-
-        if (!image.tagsObj['docker:repo']) {
-            // Cheat. We know before this migration (which was part of
-            // private registry support) that the only index/registry from
-            // which pulls were supported was 'docker.io'.
-            indexName = 'docker.io';
-        }
-        if (!indexName) {
-            indexName = drc.parseRepo(image.tagsObj['docker:repo']).index.name;
-        }
-
-        newUuid = imgmanifest.imgUuidFromDockerInfo({
-            id: dockerId,
-            indexName: indexName
-        });
-
-        if (newUuid === uuid) {
-            trace('image %s appears to have been migrated already', uuid);
-            callback();
-            return;
-        }
-
-        if (!image.files || image.files[0].stor !== 'local') {
-            errexit('cannot migrate %s. Image file is not stored locally',
-                uuid);
-            callback();
-            return;
-        }
-
-        async.series([
-            getOrigin,
-            linkFile,
-            recreateObject,
-            unlinkFile
-        ], callback);
-    });
-}
-
-function morayMigrate(callback) {
-    getMorayClient(function (mclient) {
-        morayClient = mclient;
-
-        morayListImages(function (err, images) {
-            if (err) {
-                return callback(err);
-            }
-
-            info('%d images to potentially migrate', images.length);
-            async.forEachSeries(images, migrateImage, callback);
-        });
-    });
-}
-
-
-//---- mainline
-
-function main(argv) {
-    lib_config.loadConfig({}, function (confErr, config_) {
-        if (confErr) {
-            errexit(confErr);
-            return;
-        }
-
-        config = config_;
-        assert.string(config.databaseType, 'config.databaseType');
-
-        if (config.databaseType !== 'moray') {
-            errexit('migration not supported, database type is not moray');
-        }
-
-        localDir = constants.STORAGE_LOCAL_IMAGES_DIR;
-        assert.ok(localDir !== '/',
-            'cannot have empty or root dir for local storage');
-
-        morayMigrate(function (err) {
-            if (err) {
-                errexit(err);
-            } else {
-                info('finished sucessfully');
-                process.exit(0);
-            }
-        });
-    });
-}
-
-if (require.main === module) {
-    main(process.argv);
-}
+// old migration file, no longer used, just here to avoid sdcadm update failures
diff --git a/lib/storage.js b/lib/storage.js
index e41e308..8285872 100644
--- a/lib/storage.js
+++ b/lib/storage.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright 2016 Joyent, Inc.
+ * Copyright 2017 Joyent, Inc.
  */
 
 /*
@@ -132,6 +132,18 @@ Storage.prototype.storeFileFromStream =
  */
 Storage.prototype.moveImageFile = function (image, from, to, callback) {};
 
+/**
+ * Moves an image file from one image to another image.
+ *
+ * @param fromImage {Image}
+ * @param toImage {Image}
+ * @param filename {String} Image filename (i.e. 'file0')
+ * @param callback {Function} `function (err)`
+ */
+Storage.prototype.moveFileBetweenImages =
+    function (fromImage, toImage, filename, callback) {
+};
+
 
 /**
  * Returns the archive path for image manifests. Image manifests are archived
@@ -437,6 +449,33 @@ LocalStorage.prototype.moveImageFile = function (image, from, to, callback) {
 };
 
 
+LocalStorage.prototype.moveFileBetweenImages =
+function (fromImage, toImage, filename, callback) {
+    assert.object(fromImage, 'fromImage');
+    assert.uuid(fromImage.uuid, 'fromImage.uuid');
+    assert.object(toImage, 'toImage');
+    assert.uuid(toImage.uuid, 'toImage.uuid');
+    assert.string(filename, 'filename');
+    assert.func(callback, 'callback');
+
+    var fromPath = this.storPathFromImageUuid(fromImage.uuid, filename);
+    var toPath = this.storPathFromImageUuid(toImage.uuid, filename);
+
+    var toDir = path.dirname(toPath);
+    mkdirp(toDir, function (mkdirErr) {
+        if (mkdirErr) {
+            return callback(mkdirErr);
+        }
+        fs.rename(fromPath, toPath, function (err) {
+            if (err) {
+                return callback(err);
+            }
+            return callback(null);
+        });
+    });
+};
+
+
 LocalStorage.prototype.archiveImageManifest = function (manifest, callback) {
     assert.object(manifest, 'manifest');
     assert.string(manifest.uuid, 'manifest.uuid');
@@ -716,6 +755,40 @@ MantaStorage.prototype.moveImageFile = function (image, from, to, callback) {
     });
 };
 
+MantaStorage.prototype.moveFileBetweenImages =
+function (fromImage, toImage, filename, callback) {
+    assert.object(fromImage, 'fromImage');
+    assert.uuid(fromImage.uuid, 'fromImage.uuid');
+    assert.object(toImage, 'toImage');
+    assert.uuid(toImage.uuid, 'toImage.uuid');
+    assert.string(filename, 'filename');
+    assert.func(callback, 'callback');
+
+    var fromPath = this._storPathFromImageUuid(fromImage.uuid, filename);
+    var toPath = this._storPathFromImageUuid(toImage.uuid, filename);
+    var toDir = path.dirname(toPath);
+
+    var self = this;
+
+    self.client.mkdirp(toDir, function (mkdirErr) {
+        if (mkdirErr) {
+            return callback(mkdirErr);
+        }
+        self.client.ln(fromPath, toPath, function (err) {
+            if (err) {
+                return callback(err);
+            }
+
+            self.deleteImageFile(fromImage, filename, function (delErr) {
+                if (delErr) {
+                    return callback(delErr);
+                }
+                return callback();
+            });
+        });
+    });
+};
+
 MantaStorage.prototype.archiveImageManifest =
 function (manifest, callback) {
     assert.object(manifest, 'manifest');
diff --git a/package.json b/package.json
index a00ca10..930ed0c 100644
--- a/package.json
+++ b/package.json
@@ -1,12 +1,13 @@
 {
   "name": "imgapi",
   "description": "Image API to manage images for SDC 7",
-  "version": "3.2.2",
+  "version": "4.0.0",
   "author": "Joyent (joyent.com)",
   "private": true,
   "dependencies": {
     "assert-plus": "1.0.0",
     "async": "0.7.0",
+    "buffer-peek-stream": "1.0.1",
     "bunyan": "1.8.8",
     "cmdln": "3.2.1",
     "dashdash": "1.10.0",
@@ -17,7 +18,7 @@
     "forkexec": "1.1.0",
     "glob": "7.0.5",
     "handlebars": "4.0.5",
-    "imgmanifest": "2.3.0",
+    "imgmanifest": "git+https://github.com/joyent/node-imgmanifest.git#b2e47b96e588a811ad36725fb138530efc0e4ff4",
     "json": "9.0.4",
     "ldap-filter": "0.3.1",
     "uuid": "2.0.2",
-- 
2.21.0

