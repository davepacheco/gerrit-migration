commit 640ffd583a181c028cc0ebf1c7e3405f336dc4de
Author: Robert Mustacchi <rm@joyent.com>
Date:   2019-06-14T22:30:05+00:00 (4 months ago)
    
    OS-7598 Kernel needs to be built with retpolines
    OS-7621 Kernel needs to generally use RSB stuffing

diff --git a/usr/src/uts/Makefile.uts b/usr/src/uts/Makefile.uts
index 416cf5166e..e6e8ce3861 100644
--- a/usr/src/uts/Makefile.uts
+++ b/usr/src/uts/Makefile.uts
@@ -26,7 +26,7 @@
 # Copyright (c) 2013 Andrew Stormont.  All rights reserved.
 # Copyright 2018 Joyent, Inc.
 # Copyright 2016 Hans Rosenfeld <rosenfeld@grumpf.hope-2000.org>
-# Copyright (c) 2019, Joyent, Inc.
+# Copyright 2019 Joyent, Inc.
 #
 
 #
@@ -237,6 +237,15 @@ include $(SRC)/Makefile.smatch
 #
 SMOFF += sizeof
 
+#
+# Add specific compiler options that are required based on the
+# architecture in question.
+#
+CFLAGS_uts_i386 += -_gcc7=-mindirect-branch=thunk-extern
+CFLAGS_uts_i386 += -_gcc7=-mindirect-branch-register
+CFLAGS_uts_i386 += -_gcc8=-mindirect-branch=thunk-extern
+CFLAGS_uts_i386 += -_gcc8=-mindirect-branch-register
+
 CSTD = $(CSTD_GNU99)
 
 CFLAGS_uts		=
@@ -253,6 +262,7 @@ CFLAGS_uts		+= $(CGLOBALSTATIC)
 CFLAGS_uts		+= $(EXTRA_CFLAGS)
 CFLAGS_uts		+= $(CSOURCEDEBUGFLAGS)
 CFLAGS_uts		+= $(CUSERFLAGS)
+CFLAGS_uts		+= $(CFLAGS_uts_$(MACH))
 
 #
 #	Declare that $(OBJECTS) and $(LINTS) can be compiled in parallel.
diff --git a/usr/src/uts/common/mapfiles/ddi.mapfile b/usr/src/uts/common/mapfiles/ddi.mapfile
index 1377af5857..d1de7e9862 100644
--- a/usr/src/uts/common/mapfiles/ddi.mapfile
+++ b/usr/src/uts/common/mapfiles/ddi.mapfile
@@ -10,7 +10,7 @@
 #
 
 #
-# Copyright 2018 Joyent, Inc.
+# Copyright 2019 Joyent, Inc.
 #
 
 #
@@ -40,6 +40,22 @@ SYMBOL_SCOPE {
 	__divdi3			{ FLAGS = EXTERN };
 	__stack_chk_fail		{ FLAGS = EXTERN };
 	__stack_chk_guard		{ FLAGS = EXTERN };
+	__x86_indirect_thunk		{ FLAGS = EXTERN };
+	__x86_indirect_thunk_r10	{ FLAGS = EXTERN };
+	__x86_indirect_thunk_r11	{ FLAGS = EXTERN };
+	__x86_indirect_thunk_r12	{ FLAGS = EXTERN };
+	__x86_indirect_thunk_r13	{ FLAGS = EXTERN };
+	__x86_indirect_thunk_r14	{ FLAGS = EXTERN };
+	__x86_indirect_thunk_r15	{ FLAGS = EXTERN };
+	__x86_indirect_thunk_r8		{ FLAGS = EXTERN };
+	__x86_indirect_thunk_r9		{ FLAGS = EXTERN };
+	__x86_indirect_thunk_rax	{ FLAGS = EXTERN };
+	__x86_indirect_thunk_rbp	{ FLAGS = EXTERN };
+	__x86_indirect_thunk_rbx	{ FLAGS = EXTERN };
+	__x86_indirect_thunk_rcx	{ FLAGS = EXTERN };
+	__x86_indirect_thunk_rdi	{ FLAGS = EXTERN };
+	__x86_indirect_thunk_rdx	{ FLAGS = EXTERN };
+	__x86_indirect_thunk_rsi	{ FLAGS = EXTERN };
 	allocb				{ FLAGS = EXTERN };
 	assfail				{ FLAGS = EXTERN };
 	assfail3			{ FLAGS = EXTERN };
diff --git a/usr/src/uts/i86pc/ml/cpr_wakecode.s b/usr/src/uts/i86pc/ml/cpr_wakecode.s
index 6955d5893e..0aa63dd8d4 100644
--- a/usr/src/uts/i86pc/ml/cpr_wakecode.s
+++ b/usr/src/uts/i86pc/ml/cpr_wakecode.s
@@ -20,8 +20,9 @@
  */
 /*
  * Copyright (c) 2007, 2010, Oracle and/or its affiliates. All rights reserved.
+ * Copyright 2019 Joyent, Inc.
  */
-	
+
 #include <sys/asm_linkage.h>
 #include <sys/asm_misc.h>
 #include <sys/regset.h>
@@ -339,7 +340,7 @@ pestart:
 #endif
 
 	/*
- 	 * Add any initial cr4 bits
+	 * Add any initial cr4 bits
 	 */
 	movl		%cr4, %eax
 	A16 D16 orl	CR4OFF, %eax
@@ -434,7 +435,7 @@ long_mode_active:
 
 
 	/*
- 	 * Do a far transfer to 64-bit mode.  Set the CS selector to a 64-bit
+	 * Do a far transfer to 64-bit mode.  Set the CS selector to a 64-bit
 	 * long mode selector (CS.L=1) in the temporary 32-bit GDT and jump
 	 * to the real mode platter address of wc_long_mode_64 as until the
 	 * 64-bit CS is in place we don't have access to 64-bit instructions
@@ -453,7 +454,7 @@ long_mode_active:
 	outb    (%dx)
 #endif
 
-	D16 	pushl 	$TEMP_CS64_SEL
+	D16	pushl	$TEMP_CS64_SEL
 	A16 D16 pushl	LM64OFF
 
 	D16 lret
@@ -476,7 +477,7 @@ kbdinit:
  */
 cominit:
 	/ init COM1 & COM2
-	
+
 #if     DEBUG
 /*
  * on debug kernels we need to initialize COM1 & COM2 here, so that
@@ -678,7 +679,7 @@ kernel_wc_code:
 	addq	WC_GDT+2(%rbx), %rax
 	andl	$0xfffffdff, 4(%rax)
 	movq	4(%rax), %rcx
-	ltr	WC_TR(%rbx)		
+	ltr	WC_TR(%rbx)
 
 #if     LED
 	movw        $WC_LED, %dx
@@ -701,7 +702,7 @@ kernel_wc_code:
 	movl    WC_FSBASE(%rbx), %eax
 	movl    WC_FSBASE+4(%rbx), %edx
 	wrmsr
-	
+
 	movq    WC_GS(%rbx), %rcx	/ restore gs register
 	movw    %cx, %gs
 
@@ -760,10 +761,12 @@ kernel_wc_code:
 	 */
 	cmpq	$0, ap_mlsetup
 	je	3f
-	call	*ap_mlsetup
+	leaq	ap_mlsetup, %rax
+	INDIRECT_CALL_REG(rax)
 3:
 
-	call    *cpr_start_cpu_func
+	leaq	cpr_start_cpu_func, %rax
+	INDIRECT_CALL_REG(rax)
 
 / restore %rbx to the value it ahd before we called the functions above
 	movq    rm_platter_va, %rbx
@@ -1058,7 +1061,7 @@ kernel_wc_code:
 	/ At this point we are with kernel's cs and proper eip.
 	/ We will be executing not from the copy in real mode platter,
 	/ but from the original code where boot loaded us.
-	/ By this time GDT and IDT are loaded as is cr0, cr3 and cr4. 
+	/ By this time GDT and IDT are loaded as is cr0, cr3 and cr4.
 	/ %ebx is wc_cpu
 	/ %dx is our ds
 
@@ -1106,7 +1109,7 @@ kernel_wc_code:
 
 	/*
 	 * set the stack pointer to point into the identity mapped page
-	 * temporarily, so we can make function calls 
+	 * temporarily, so we can make function calls
 	 */
 	.globl  rm_platter_va
 	movl    rm_platter_va, %eax
@@ -1139,10 +1142,10 @@ kernel_wc_code:
 	 */
 	cmpl	$0, ap_mlsetup
 	je	3f
-	call	*ap_mlsetup
+	INDIRECT_CALL(ap_mlsetup)
 3:
 
-	call    *cpr_start_cpu_func
+	INDIRECT_CALL(cpr_start_cpu_func)
 
 	pushl	WC_EFLAGS(%ebx)		/ restore flags
 	popfl
diff --git a/usr/src/uts/i86pc/ml/fast_trap_asm.s b/usr/src/uts/i86pc/ml/fast_trap_asm.s
index b762543b91..bb3d0b3686 100644
--- a/usr/src/uts/i86pc/ml/fast_trap_asm.s
+++ b/usr/src/uts/i86pc/ml/fast_trap_asm.s
@@ -21,10 +21,9 @@
 /*
  * Copyright 2007 Sun Microsystems, Inc.  All rights reserved.
  * Use is subject to license terms.
+ * Copyright 2019 Joyent, Inc.
  */
 
-#pragma ident	"%Z%%M%	%I%	%E% SMI"
-
 #include <sys/asm_linkage.h>
 #include <sys/asm_misc.h>
 #include <sys/regset.h>
@@ -101,7 +100,8 @@ getlgrp(void)
 	.globl	gethrtimef
 	ENTRY_NP(get_hrtime)
 	FAST_INTR_PUSH
-	call	*gethrtimef(%rip)
+	movq	gethrtimef(%rip), %rax
+	INDIRECT_CALL_REG(rax)
 	movq	%rax, %rdx
 	shrq	$32, %rdx			/* high 32-bit in %edx */
 	FAST_INTR_POP
@@ -127,7 +127,8 @@ getlgrp(void)
 	FAST_INTR_PUSH
 	subq	$TIMESPEC_SIZE, %rsp
 	movq	%rsp, %rdi
-	call	*gethrestimef(%rip)
+	movq	gethrestimef(%rip), %rax
+	INDIRECT_CALL_REG(rax)
 	movl	(%rsp), %eax
 	movl	CLONGSIZE(%rsp), %edx
 	addq	$TIMESPEC_SIZE, %rsp
diff --git a/usr/src/uts/i86pc/ml/interrupt.s b/usr/src/uts/i86pc/ml/interrupt.s
index 46cbf2f308..9849297ad2 100644
--- a/usr/src/uts/i86pc/ml/interrupt.s
+++ b/usr/src/uts/i86pc/ml/interrupt.s
@@ -20,6 +20,7 @@
  */
 /*
  * Copyright (c) 2004, 2010, Oracle and/or its affiliates. All rights reserved.
+ * Copyright 2019 Joyent, Inc.
  */
 
 /*	Copyright (c) 1990, 1991 UNIX System Laboratories, Inc.	*/
@@ -99,7 +100,8 @@ _interrupt(void)
 #endif
 
 	movq	%rsp, %rdi		/* pass struct regs pointer */
-	call	*do_interrupt_common
+	movq	do_interrupt_common, %rax
+	INDIRECT_CALL_REG(rax)
 
 	jmp	_sys_rtt_ints_disabled
 	/*NOTREACHED*/
diff --git a/usr/src/uts/i86pc/ml/locore.s b/usr/src/uts/i86pc/ml/locore.s
index 236f03b4ea..aad2fe89e2 100644
--- a/usr/src/uts/i86pc/ml/locore.s
+++ b/usr/src/uts/i86pc/ml/locore.s
@@ -1191,7 +1191,7 @@ cmntrap()
 	addq	%rax, %r12
 	movq	%r12, REGOFF_RIP(%rbp)
 	INTR_POP
-	call	*x86_md_clear
+	call	x86_md_clear
 	jmp	tr_iret_auto
 	/*NOTREACHED*/
 3:
@@ -1597,7 +1597,7 @@ _lwp_rtt:
 	 */
 	ALTENTRY(sys_rtt_syscall32)
 	USER32_POP
-	call	*x86_md_clear
+	call	x86_md_clear
 	jmp	tr_iret_user
 	/*NOTREACHED*/
 
@@ -1607,7 +1607,7 @@ _lwp_rtt:
 	 */
 	USER_POP
 	ALTENTRY(nopop_sys_rtt_syscall)
-	call	*x86_md_clear
+	call	x86_md_clear
 	jmp	tr_iret_user
 	/*NOTREACHED*/
 	SET_SIZE(nopop_sys_rtt_syscall)
diff --git a/usr/src/uts/i86pc/ml/md_clear.s b/usr/src/uts/i86pc/ml/md_clear.s
index 50302b43c7..03a09873b4 100644
--- a/usr/src/uts/i86pc/ml/md_clear.s
+++ b/usr/src/uts/i86pc/ml/md_clear.s
@@ -42,15 +42,13 @@
  *    this are either going to change privilege levels or halt, which makes
  *    these operations safer.
  */
-	ENTRY_NP(x86_md_clear_noop)
-	ret
-	SET_SIZE(x86_md_clear_noop)
-
 	/*
-	 * This uses the microcode based means of flushing state. VERW will
-	 * clobber flags.
+	 * By default, x86_md_clear is disabled until the system determines that
+	 * it both needs MDS related mitigations and we don't have certainty
+	 * that we're not impacted.
 	 */
-	ENTRY_NP(x86_md_clear_verw)
+	ENTRY_NP(x86_md_clear)
+	ret
 	pushfq
 	subq	$8, %rsp
 	mov	%ds, (%rsp)
@@ -58,4 +56,4 @@
 	addq	$8, %rsp
 	popfq
 	ret
-	SET_SIZE(x86_md_clear_verw)
+	SET_SIZE(x86_md_clear)
diff --git a/usr/src/uts/i86pc/ml/mpcore.s b/usr/src/uts/i86pc/ml/mpcore.s
index 2151a14b04..1bad596ba5 100644
--- a/usr/src/uts/i86pc/ml/mpcore.s
+++ b/usr/src/uts/i86pc/ml/mpcore.s
@@ -25,9 +25,9 @@
  * Copyright (c) 2010, Intel Corporation.
  * All rights reserved.
  *
- * Copyright 2018 Joyent, Inc.
+ * Copyright 2019 Joyent, Inc.
  */
-	
+
 #include <sys/asm_linkage.h>
 #include <sys/asm_misc.h>
 #include <sys/regset.h>
@@ -87,7 +87,7 @@ real_mode_stop_cpu_stage2(void)
 	 *	  prefixes need not be used on instructions EXCEPT in the case
 	 *	  of address prefixes for code for which the reference is not
 	 *	  automatically of the default operand size.
-	 */      
+	 */
 	.code16
 	cli
 	movw		%cs, %ax
@@ -116,7 +116,7 @@ real_mode_stop_cpu_stage2(void)
 
 pestart:
 	/*
- 	 * 16-bit protected mode is now active, so prepare to turn on long
+	 * 16-bit protected mode is now active, so prepare to turn on long
 	 * mode.
 	 *
 	 * Note that we currently assume that if we're attempting to run a
@@ -135,14 +135,14 @@ pestart:
 	cpuid
 	cmpl		$0x80000000, %eax	/* check if > 0x80000000 */
 	jbe		no_long_mode		/* nope, no long mode */
-	movl		$0x80000001, %eax	
+	movl		$0x80000001, %eax
 	cpuid					/* get extended feature flags */
 	btl		$29, %edx		/* check for long mode */
 	jnc		no_long_mode		/* long mode not supported */
 #endif
 
 	/*
- 	 * Add any initial cr4 bits
+	 * Add any initial cr4 bits
 	 */
 	movl		%cr4, %eax
 	addr32 orl	CR4OFF, %eax
@@ -198,13 +198,13 @@ long_mode_active:
 	addr32 lidtl	TEMPIDTOFF	/* load temporary IDT */
 
 	/*
- 	 * Do a far transfer to 64-bit mode.  Set the CS selector to a 64-bit
+	 * Do a far transfer to 64-bit mode.  Set the CS selector to a 64-bit
 	 * long mode selector (CS.L=1) in the temporary 32-bit GDT and jump
 	 * to the real mode platter address of long_mode 64 as until the 64-bit
 	 * CS is in place we don't have access to 64-bit instructions and thus
 	 * can't reference a 64-bit %rip.
 	 */
-	pushl 		$TEMP_CS64_SEL
+	pushl		$TEMP_CS64_SEL
 	addr32 pushl	LM64OFF
 	lretl
 
@@ -313,7 +313,7 @@ kernel_cs_code:
 	movq    %rax, %cr0		/* set machine status word */
 
 	/*
-	 * Before going any further, enable usage of page table NX bit if 
+	 * Before going any further, enable usage of page table NX bit if
 	 * that's how our page tables are set up.
 	 */
 	bt	$X86FSET_NX, x86_featureset(%rip)
@@ -328,7 +328,8 @@ kernel_cs_code:
 	 * Complete the rest of the setup and call mp_startup().
 	 */
 	movq	%gs:CPU_THREAD, %rax	/* get thread ptr */
-	call	*T_PC(%rax)		/* call mp_startup_boot */
+	movq	T_PC(%rax), %rax
+	INDIRECT_CALL_REG(rax)		/* call mp_startup_boot */
 	/* not reached */
 	int	$20			/* whoops, returned somehow! */
 
@@ -352,8 +353,8 @@ kernel_cs_code:
 	 */
 	D16 movl	$0xffc, %esp
 
- 	D16 A16 lgdt	%cs:GDTROFF
- 	D16 A16 lidt	%cs:IDTROFF
+	D16 A16 lgdt	%cs:GDTROFF
+	D16 A16 lidt	%cs:IDTROFF
 	D16 A16 movl	%cs:CR4OFF, %eax	/* set up CR4, if desired */
 	D16 andl	%eax, %eax
 	D16 A16 je	no_cr4
@@ -412,7 +413,7 @@ kernel_cs_code:
 	movl    %edx,%cr0		  /* set machine status word */
 
 	/*
-	 * Before going any further, enable usage of page table NX bit if 
+	 * Before going any further, enable usage of page table NX bit if
 	 * that's how our page tables are set up.
 	 */
 	bt	$X86FSET_NX, x86_featureset
@@ -503,7 +504,7 @@ kernel_cs_code:
 	mov	%edx, %cr0		/* set machine status word */
 
 	/*
-	 * Before going any farther, enable usage of page table NX bit if 
+	 * Before going any farther, enable usage of page table NX bit if
 	 * that's how our page tables are set up.  (PCIDE is enabled later on).
 	 */
 	bt	$X86FSET_NX, x86_featureset
@@ -558,7 +559,7 @@ kernel_cs_code:
 	 *	  prefixes need not be used on instructions EXCEPT in the case
 	 *	  of address prefixes for code for which the reference is not
 	 *	  automatically of the default operand size.
-	 */      
+	 */
 	.code16
 	cli
 	movw		%cs, %ax
@@ -607,6 +608,7 @@ real_mode_stop_cpu_stage1_end:
 	 * Jump to the stage 2 code in the rm_platter_va->rm_cpu_halt_code
 	 */
 	movw		$CPUHALTCODEOFF, %ax
+	/* XXX comment that this is fine */
 	jmp		*%ax
 
 #endif	/* !__GNUC_AS__ */
diff --git a/usr/src/uts/i86pc/ml/syscall_asm_amd64.s b/usr/src/uts/i86pc/ml/syscall_asm_amd64.s
index 9bf9db47bf..9ef517e2f6 100644
--- a/usr/src/uts/i86pc/ml/syscall_asm_amd64.s
+++ b/usr/src/uts/i86pc/ml/syscall_asm_amd64.s
@@ -199,7 +199,8 @@
 	je	1f							   ;\
 	movq	%r15, 16(%rsp)		/* save the callback pointer	*/ ;\
 	push_userland_ret		/* push the return address	*/ ;\
-	call	*24(%rsp)		/* call callback		*/ ;\
+	movq	24(%rsp), %r15		/* load callback pointer	*/ ;\
+	INDIRECT_CALL_REG(r15)		/* call callback		*/ ;\
 1:	movq	%gs:CPU_RTMP_R15, %r15	/* restore %r15			*/ ;\
 	movq	%gs:CPU_RTMP_RSP, %rsp	/* restore the stack pointer	*/
 
@@ -567,7 +568,7 @@ noprod_sys_syscall:
 
 	pushq	%rax
 	subq	$8, %rsp	/* align stack for call to C */
-	call	*%rdi
+	INDIRECT_CALL_REG(rdi)
 	addq	$8, %rsp
 
 	/*
@@ -607,7 +608,8 @@ _syscall_invoke:
 	shll	$SYSENT_SIZE_SHIFT, %eax
 	leaq	sysent(%rax), %rbx
 
-	call	*SY_CALLC(%rbx)
+	movq	SY_CALLC(%rbx), %rax
+	INDIRECT_CALL_REG(rax)
 
 	movq	%rax, %r12
 	movq	%rdx, %r13
@@ -685,7 +687,7 @@ _syscall_after_brand:
 	 * potentially the addresses where we stored them. Given the constraints
 	 * of sysret, that's how it has to be.
 	 */
-	call	*x86_md_clear
+	call	x86_md_clear
 
 	/*
 	 * To get back to userland, we need the return %rip in %rcx and
@@ -905,7 +907,7 @@ _syscall32_save:
 	jz	_syscall32_no_brand
 
 	movb	$LWP_SYS, LWP_STATE(%r14)
-	call	*%rax
+	INDIRECT_CALL_REG(rax)
 
 	/*
 	 * If the alternate handler returns non-zero, the normal system call
@@ -975,7 +977,8 @@ _syscall32_no_brand:
 	movl	0x30(%rsp), %r9d	/* arg5 */
 	pushq	%rax			/* arg6 saved to stack */
 
-	call	*SY_CALLC(%rbx)
+	movq	SY_CALLC(%rbx), %rax
+	INDIRECT_CALL_REG(rax)
 
 	movq	%rbp, %rsp	/* pop the args */
 
@@ -1024,7 +1027,7 @@ _syscall32_after_brand:
 	 * potentially the addresses where we stored them. Given the constraints
 	 * of sysret, that's how it has to be.
 	 */
-	call	*x86_md_clear
+	call	x86_md_clear
 
 	/*
 	 * To get back to userland, we need to put the return %rip in %rcx and
@@ -1269,7 +1272,8 @@ sys_sysenter()
 	movl	0x30(%rsp), %r9d	/* arg5 */
 	pushq	%rax			/* arg6 saved to stack */
 
-	call	*SY_CALLC(%rbx)
+	movq	SY_CALLC(%rbx), %rax
+	INDIRECT_CALL_REG(rax)
 
 	movq	%rbp, %rsp	/* pop the args */
 
@@ -1337,7 +1341,7 @@ sys_sysenter()
 	popfq
 	movl	REGOFF_RSP(%rsp), %ecx	/* sysexit: %ecx -> %esp */
         ALTENTRY(sys_sysenter_swapgs_sysexit)
-	call	*x86_md_clear
+	call	x86_md_clear
 	jmp	tr_sysexit
 	SET_SIZE(sys_sysenter_swapgs_sysexit)
 	SET_SIZE(sys_sysenter)
@@ -1462,7 +1466,7 @@ nopop_syscall_int:
 	 * tr_iret_user are done on the user gsbase.
 	 */
 	ALTENTRY(sys_sysint_swapgs_iret)
-	call	*x86_md_clear
+	call	x86_md_clear
 	SWAPGS
 	jmp	tr_iret_user
 	/*NOTREACHED*/
diff --git a/usr/src/uts/i86pc/os/cpuid.c b/usr/src/uts/i86pc/os/cpuid.c
index f53ec818d2..2125ecfaeb 100644
--- a/usr/src/uts/i86pc/os/cpuid.c
+++ b/usr/src/uts/i86pc/os/cpuid.c
@@ -897,6 +897,252 @@
  * microcode, and performance monitoring. These functions all ASSERT that the
  * CPU they're being called on has reached a certain cpuid pass. If the passes
  * are rearranged, then this needs to be adjusted.
+ *
+ * -----------------------------------------------
+ * Speculative Execution CPU Side Channel Security
+ * -----------------------------------------------
+ *
+ * With the advent of the Spectre and Meltdown attacks which exploit speculative
+ * execution in the CPU to create side channels there have been a number of
+ * different attacks and corresponding issues that the operating system needs to
+ * mitigate against. The following list is some of the common, but not
+ * exhaustive set of issues that we know about and have done some or need to do
+ * more work in the system to mitigate against:
+ *
+ *   - Spectre v1
+ *   - Spectre v2
+ *   - Meltdown (Spectre v3)
+ *   - Rogue Register Read (Spectre v3a)
+ *   - Speculative Store Bypass (Spectre v4)
+ *   - ret2spec, SpectreRSB
+ *   - L1 Terminal Fault (L1TF)
+ *   - Microarchitectural Data Sampling (MDS)
+ *
+ * Each of these requires different sets of mitigations and has different attack
+ * surfaces. For the most part, this discussion is about protecting the kernel
+ * from non-kernel executing environments such as user processes and hardware
+ * virtual machines. Unfortunately, there are a number of user vs. user
+ * scenarios that exist with these. The rest of this section will describe the
+ * overall approach that the system has taken to address these as well as their
+ * shortcomings. Unfortunately, not all of the above have been handled today.
+ *
+ * SPECTRE FAMILY (Spectre v1, v2, v4, ret2spec, SpectreRSB)
+ *
+ * The second variant of the spectre attack focuses on performing branch target
+ * injection. This generally impacts indirect call instructions in the system.
+ * There are three different ways to mitigate this issue that are commonly
+ * described today:
+ *
+ *  1. Using Indirect Branch Restricted Speculation (IBRS).
+ *  2. Using Retpolines and RSB Stuffing
+ *  3. Using Enhanced Indirect Branch Restricted Speculation (EIBRS)
+ *
+ * IBRS uses a feature added to microcode to restrict speculation, among other
+ * things. This form of mitigation has not been used as it has been generally
+ * seen as too expensive and requires reactivation upon various transitions in
+ * the system.
+ *
+ * As a less impacting alternative to IBRS, retpolines were developed by
+ * Google. These basically require one to replace indirect calls with a specific
+ * trampoline that will cause speculation to fail and break the attack.
+ * Retpolines require compiler support. We always build with retpolines in the
+ * external thunk mode. This means that a traditional indirect call is replaced
+ * with a call to one of the __x86_indirect_thunk_<reg> functions.
+ *
+ * We have to use a common external location of these so that way we can have a
+ * single place to patch these functions. As it turns out, we actually have
+ * three different forms of retpolines that exist in the system:
+ *
+ *  1. A full retpoline
+ *  2. An AMD-specific optimized retpoline
+ *  3. A no-op version
+ *
+ * The first one is used in the general case. The second one is used if we can
+ * determine that we're on an AMD system and we can successfully toggle the
+ * lfence serializing MSR that exists on the platform. Basically with this
+ * present, an lfence is sufficient and we don't need to do anywhere near as
+ * complicated a dance to successfully use retpolines.
+ *
+ * The third form described above is the most curious. It turns out that the way
+ * that retpolines are implemented is that they rely on how speculation is
+ * performed on a 'ret' instruction. Intel has continued to optimize this
+ * process (which is partly why we need to have return stack buffer stuffing,
+ * but more on that in a bit) and that in processors starting with Cascade Lake
+ * on the server side, it's dangerous to rely on retpolines. Instead, a new
+ * mechanism has been introduced called Enhanced IBRS (EIBRS).
+ *
+ * Unlike IBRS, EIBRS is designed to be enabled once at boot and left on each
+ * physical core. However, if this is the case, we don't want to use retpolines
+ * any more. Therefore if EIBRS is present, we end up turning each retpoline
+ * function (called a thunk) into a jmp instruction. This means that we're still
+ * paying the cost of an extra jump to the external thunk, but it gives us
+ * flexibility and the ability to have a single kernel image that works across a
+ * wide variety of systems and hardware features.
+ *
+ * Unfortunately, this alone is insufficient. First, Skylake systems have
+ * additional speculation for the Return Stack Buffer (RSB) which is used to
+ * implement retpolines. This means that when a user can control the RSB, we
+ * have to be more careful. This problem though is actually more pernicious. The
+ * SpectreRSB paper introduces several more problems that can arise with dealing
+ * with this. The critical parts of this are that processors other than Skylake
+ * are impacted and that the attack can be used in more contexts than just to
+ * attack the kernel.
+ *
+ * This can be mitigated by a combination of RSB stuffing, the act of creating a
+ * number of non-malicious entries in the RSB, and having supervisor mode
+ * execution protection (SMEP) enabled (a feature introduced in Ivy Bridge). We
+ * rely on SMEP being present to fully mitigate a number of these issues. If it
+ * were not present, then we would need to overwrite the RSB on every user to
+ * kernel transition, which isn't very practical right now.
+ *
+ * With SMEP present, one would need to perform RSB stuffing when switching
+ * between user processes and when returning from a VMX non-root context. If
+ * EIBRS is present, then SMEP and EIBRS are sufficient to provide the
+ * protections that we need for user to kernel and vmx to kernel attacks.
+ *
+ * To fully protect user to user and vmx to vmx attacks from these classes of
+ * issues, we would also need to allow them to opt into performing an Indirect
+ * Branch Prediction Barrier (IBPB) on switch. This is not currently wired up.
+ *
+ * By default, the system will enable RSB stuffing and the required variant of
+ * retpolines and store that information in the x86_spectrev2_mitigation value.
+ * This will be evaluated after a microcode update as well, though it is
+ * expected that microcode updates will not take away features. This may mean
+ * that a late loaded microcode may not end up in the optimal configuration
+ * (though this should be rare).
+ *
+ * The other v1 and v4 variants of spectre are not currently mitigated in the
+ * system and require other classes of changes to occur in the code.
+ *
+ * MELTDOWN
+ *
+ * Meltdown, or spectre v3, allowed a user process to read any data in their
+ * address space regardless of whether or not the page tables in question
+ * allowed the user to have the ability to read them. The solution to meltdown
+ * is kernel page table isolation. In this world, there are two page tables that
+ * are used for a process, one in user land and one in the kernel. To implement
+ * this we use per-CPU page tables and switch between the user and kernel
+ * variants when entering and exiting the kernel.  For more information about
+ * this process and how the trampolines work, please see the big theory
+ * statements and additional comments in:
+ *
+ *  - uts/i86pc/ml/kpti_trampolines.s
+ *  - uts/i86pc/vm/hat_i86.c
+ *
+ * While Meltdown only impacted Intel systems and there are also Intel systems
+ * that have Meltdown fixed (called Rogue Data Cache Load), we always have
+ * kernel page table isolation enabled. While this may at first seem weird, an
+ * important thing to remember is that you can't speculatively read an address
+ * if it's never in your page table at all. Having user processes without kernel
+ * pages present provides us with an important layer of defence in the kernel
+ * against the other side channel attacks that exist and have yet to be
+ * discovered. As such, kernel page table isolation (KPTI) is always enabled by
+ * default, no matter the x86 system.
+ *
+ * L1 TERMINAL FAULT
+ *
+ * L1 Terminal Fault (L1TF) takes advantage of an issue in how speculative
+ * execution uses page table entries. Effectively, it is two different problems.
+ * The first is that it ignores the not present bit in the page table entries
+ * when performing speculative execution. This means that something can
+ * speculatively read the listed physical address if it's present in the L1
+ * cache under certain conditions (see Intel's documentation for the full set of
+ * conditions). Secondly, this can be used to bypass hardware virtualization
+ * extended page tables (EPT) that are part of Intel's hardware virtual machine
+ * instructions.
+ *
+ * For the non-hardware virtualized case, this is relatively easy to deal with.
+ * We must make sure that all unmapped pages have an address of zero. This means
+ * that they could read the first 4k of physical memory; however, we never use
+ * that first page in the operating system and always skip putting it in our
+ * memory map, even if firmware tells us we can use it in our memory map.
+ *
+ * For hardware virtual machines things are more complicated. Because they can
+ * construct their own page tables, it isn't hard for them to perform this
+ * attack against any physical address. The one wrinkle is that this physical
+ * address must be in the L1 data cache. Thus Intel added an MSR that we can use
+ * to flush the L1 data cache. We wrap this up in the function
+ * spec_uarch_flush().  This function is also used in the mitigation of
+ * microarchitectural data sampling (MDS) discussed later on. Kenrel based
+ * hypervisors such as KVM or bhyve are responsible for performing this before
+ * entering the guest.
+ *
+ * Because this attack takes place in the L1 cache, there's another wrinkle
+ * here. The L1 cache is shared between all logical CPUs in a core in most Intel
+ * designs. This means that when a thread enters a hardware virtualized context
+ * and flushes the L1 data cache, the other thread on the processor may then go
+ * ahead and put new data in it that it can potentially target. While one
+ * solution is to disable SMT on the system, another option that is available is
+ * to use a feature for hardware virtualization called 'ht exclusion'. This goes
+ * through and makes sure that if a HVM is being scheduled on one thread, then
+ * the thing on the other thread is from the same hardware virtual machine. If
+ * an interrupt comes in or the guest exits to the broader system, then the
+ * other SMT threads will be kicked out.
+ *
+ * L1TF can be fully mitigated by hardware. If the RDCL_NO feature is set in the
+ * architecture capabilities MSR (MSR_IA32_ARCH_CAPABILITIES), then we will not
+ * perform L1TF related mitigations.
+ *
+ * MICROARCHITERTURAL DATA SAMPLING
+ *
+ * Microarchitertural data sampling (MDS) is a combination of four discrete
+ * vulnerabilities that are similar issues affecting various parts of the CPU's
+ * microarchitectural implementation around load, store, and fill buffers.
+ * Specifically it is made up of the following subcomponents:
+ *
+ *  1. Microarchitectural Store Buffer Data Sampling (MSBDS)
+ *  2. Microarchitectural Fill Buffer Data Sampling (MFBDS)
+ *  3. Microarchitectural Load Port Data Sampling (MLPDS)
+ *  4. Microarchitectural Data Sampling Uncacheable Memory (MDSUM)
+ *
+ * To begin addressing these, Intel has introduced another feature in microcode
+ * called MD_CLEAR. This changes the verw instruction to operate in a different
+ * way. This allows us to execute the verw instruction in a particular way to
+ * flush the state of the affected parts. The L1TF L1D flush mechanism is also
+ * updated when this microcode is present to flush this state.
+ *
+ * Primarily we need to flush this state whenever we transition from the kernel
+ * to a less privileged context such as user mode or an HVM guest. This is done
+ * by placing appropriate calls to x86_md_clear() in the system. MSBDS is a
+ * little bit different. Here the structures are statically sized when a logical
+ * CPU is in use and resized when it goes to sleep. We need to flush these cases
+ * before the CPU idles in by calling hlt, mwait, or another ACPI method.
+ *
+ * If hardware enumerates RDCL_NO, indicating that it is not vulnerable to L1TF,
+ * then we change the spec_uarch_flush() function to point to x86_md_clear(). If
+ * MDS_NO has been set, then this is fully mitigated and x86_md_clear() becomes
+ * a no-op.
+ *
+ * Unfortunately, with this issue hyperthreading rears its ugly head. In
+ * particular, everything we've discussed above is only valid for a single
+ * thread executing on a core. In the case where you have hyper-threading
+ * present, this attack can be performed between threads. The theoretical fix
+ * for this is to ensure that both threads are always in the same security
+ * domain. This means that they are executing in the same ring and mutually
+ * trust each other. Practically speaking, this would mean that a system call
+ * would have to issue an inter-processor interrupt (IPI) to the other thread.
+ * Rather than implement this, we recommend that one disables hyper-threading
+ * through the use of psradm -aS.
+ *
+ * SUMMARY
+ *
+ * The following table attempts to summarize the mitigations for various issues
+ * and what's done in various places:
+ *
+ *  - Spectre v1: Not currently mitigated
+ *  - Spectre v2: Retpoines/RSB Stuffing or EIBRS if HW support
+ *  - Meltdown: Kernel Page Table Isolation
+ *  - Spectre v3a: Updated CPU microcode
+ *  - Spectre v4: Not currently mitigated
+ *  - SpectreRSB: SMEP and RSB Stuffing
+ *  - L1TF: spec_uarch_flush, smt exclusion, requires microcode
+ *  - MDS: x86_md_clear, requires microcode, disabling hyper threading
+ *
+ * The following table indicates the x86 feature set bits that indicate that a
+ * given problem has been solved or a notable feature is present:
+ *
+ *  - RDCL_NO: Meltdown, L1TF, MSBDS part of MDS
+ *  - MDS_NO: All forms of MDS
  */
 
 #include <sys/types.h>
@@ -921,6 +1167,7 @@
 #include <sys/mach_mmu.h>
 #include <sys/ucode.h>
 #include <sys/tsc.h>
+#include <sys/kobj.h>
 
 #ifdef __xpv
 #include <sys/hypervisor.h>
@@ -940,6 +1187,17 @@ int x86_use_pcid = -1;
 int x86_use_invpcid = -1;
 #endif
 
+typedef enum {
+	X86_SPECTREV2_RETPOLINE,
+	X86_SPECTREV2_RETPOLINE_AMD,
+	X86_SPECTREV2_ENHANCED_IBRS,
+	X86_SPECTREV2_DISABLED
+} x86_spectrev2_mitigation_t;
+
+uint_t x86_disable_spectrev2 = 0;
+static x86_spectrev2_mitigation_t x86_spectrev2_mitigation =
+    X86_SPECTREV2_RETPOLINE;
+
 uint_t pentiumpro_bug4046376;
 
 uchar_t x86_featureset[BT_SIZEOFMAP(NUM_X86_FEATURES)];
@@ -2161,8 +2419,6 @@ spec_uarch_flush_msr(void)
  */
 void (*spec_uarch_flush)(void) = spec_uarch_flush_noop;
 
-void (*x86_md_clear)(void) = x86_md_clear_noop;
-
 static void
 cpuid_update_md_clear(cpu_t *cpu, uchar_t *featureset)
 {
@@ -2176,13 +2432,14 @@ cpuid_update_md_clear(cpu_t *cpu, uchar_t *featureset)
 	 */
 	if (cpi->cpi_vendor != X86_VENDOR_Intel ||
 	    is_x86_feature(featureset, X86FSET_MDS_NO)) {
-		x86_md_clear = x86_md_clear_noop;
-		membar_producer();
 		return;
 	}
 
 	if (is_x86_feature(featureset, X86FSET_MD_CLEAR)) {
-		x86_md_clear = x86_md_clear_verw;
+		const uint8_t nop = 0x90;
+		uint8_t *md = (uint8_t *)x86_md_clear;
+
+		*md = nop;
 	}
 
 	membar_producer();
@@ -2246,10 +2503,153 @@ cpuid_update_l1d_flush(cpu_t *cpu, uchar_t *featureset)
 	membar_producer();
 }
 
+/*
+ * We default to enabling RSB mitigations.
+ */
+static void
+cpuid_patch_rsb(x86_spectrev2_mitigation_t mit)
+{
+	const uint8_t ret = 0xc3;
+	uint8_t *stuff = (uint8_t *)x86_rsb_stuff;
+
+	switch (mit) {
+	case X86_SPECTREV2_ENHANCED_IBRS:
+	case X86_SPECTREV2_DISABLED:
+		*stuff = ret;
+		break;
+	default:
+		break;
+	}
+}
+
+/*
+ * IBPB defaults to being disabled. If the CPU has support for it, then we need
+ * to enable it. This is much more likely to come after a microcode update.
+ */
+static void
+cpuid_patch_ibpb(void)
+{
+	const uint8_t nop = 0x90;
+	uint8_t *ibpb = (uint8_t *)x86_indirect_branch_barrier;
+
+	*ibpb = nop;
+}
+
+static void
+cpuid_patch_retpolines(x86_spectrev2_mitigation_t mit)
+{
+	static uint_t npatches = 0;
+	const char *thunks[] = { "_rax", "_rbx", "_rcx", "_rdx", "_rdi",
+	    "_rsi", "_rbp", "_r8", "_r9", "_r10", "_r11", "_r12", "_r13",
+	    "_r14", "_r15" };
+	const uint_t nthunks = ARRAY_SIZE(thunks);
+	const char *type;
+	uint_t i;
+
+	if (mit == x86_spectrev2_mitigation)
+		return;
+
+	switch (mit) {
+	case X86_SPECTREV2_RETPOLINE:
+		type = "gen";
+		break;
+	case X86_SPECTREV2_RETPOLINE_AMD:
+		type = "amd";
+		break;
+	case X86_SPECTREV2_ENHANCED_IBRS:
+	case X86_SPECTREV2_DISABLED:
+		type = "jmp";
+		break;
+	default:
+		panic("asked to updated retpoline state with unknown state!");
+	}
+
+	for (i = 0; i < nthunks; i++) {
+		uintptr_t source, dest;
+		int ssize, dsize;
+		char sourcebuf[64], destbuf[64];
+		size_t len;
+
+		(void) snprintf(destbuf, sizeof (destbuf),
+		    "__x86_indirect_thunk%s", thunks[i]);
+		(void) snprintf(sourcebuf, sizeof (sourcebuf),
+		    "__x86_indirect_thunk_%s%s", type, thunks[i]);
+
+		source = kobj_getelfsym(sourcebuf, NULL, &ssize);
+		dest = kobj_getelfsym(destbuf, NULL, &dsize);
+		VERIFY3U(source, !=, 0);
+		VERIFY3U(dest, !=, 0);
+		VERIFY3S(dsize, >=, ssize);
+		bcopy((void *)source, (void *)dest, ssize);
+	}
+
+	npatches++;
+}
+
+static void
+cpuid_enable_enhanced_ibrs(void)
+{
+	uint64_t val;
+
+	val = rdmsr(MSR_IA32_SPEC_CTRL);
+	val |= IA32_SPEC_CTRL_IBRS;
+	wrmsr(MSR_IA32_SPEC_CTRL, val);
+}
+
+#ifndef __xpv
+/*
+ * Determine whether or not we can use the AMD optimized retpoline
+ * functionality. We use this when we know we're on an AMD system and we can
+ * successfully verify that lfence is dispatch serializing.
+ */
+static boolean_t
+cpuid_use_amd_retpoline(struct cpuid_info *cpi)
+{
+	uint64_t val;
+	on_trap_data_t otd;
+
+	if (cpi->cpi_vendor != X86_VENDOR_AMD)
+		return (B_FALSE);
+
+	/*
+	 * We need to determine whether or not lfence is serializing. It always
+	 * is on families 0xf and 0x11. On others, it's controlled by
+	 * MSR_AMD_DECODE_CONFIG (MSRC001_1029). If some hypervisor gives us a
+	 * crazy old family, don't try and do anything.
+	 */
+	if (cpi->cpi_family < 0xf)
+		return (B_FALSE);
+	if (cpi->cpi_family == 0xf || cpi->cpi_family == 0x11)
+		return (B_TRUE);
+
+	/*
+	 * While it may be tempting to use get_hwenv(), there are no promises
+	 * that a hypervisor will actually declare themselves to be so in a
+	 * friendly way. As such, try to read and set the MSR. If we can then
+	 * read back the value we set (it wasn't just set to zero), then we go
+	 * for it.
+	 */
+	if (!on_trap(&otd, OT_DATA_ACCESS)) {
+		val = rdmsr(MSR_AMD_DECODE_CONFIG);
+		val |= AMD_DECODE_CONFIG_LFENCE_DISPATCH;
+		wrmsr(MSR_AMD_DECODE_CONFIG, val);
+		val = rdmsr(MSR_AMD_DECODE_CONFIG);
+	} else {
+		val = 0;
+	}
+	no_trap();
+
+	if ((val & AMD_DECODE_CONFIG_LFENCE_DISPATCH) != 0)
+		return (B_TRUE);
+	return (B_FALSE);
+}
+#endif	/* !__xpv */
+
 static void
 cpuid_scan_security(cpu_t *cpu, uchar_t *featureset)
 {
 	struct cpuid_info *cpi = cpu->cpu_m.mcpu_cpi;
+	x86_spectrev2_mitigation_t v2mit;
 
 	if (cpi->cpi_vendor == X86_VENDOR_AMD &&
 	    cpi->cpi_xmaxeax >= CPUID_LEAF_EXT_8) {
@@ -2259,18 +2659,24 @@ cpuid_scan_security(cpu_t *cpu, uchar_t *featureset)
 			add_x86_feature(featureset, X86FSET_IBRS);
 		if (cpi->cpi_extd[8].cp_ebx & CPUID_AMD_EBX_STIBP)
 			add_x86_feature(featureset, X86FSET_STIBP);
-		if (cpi->cpi_extd[8].cp_ebx & CPUID_AMD_EBX_IBRS_ALL)
-			add_x86_feature(featureset, X86FSET_IBRS_ALL);
 		if (cpi->cpi_extd[8].cp_ebx & CPUID_AMD_EBX_STIBP_ALL)
 			add_x86_feature(featureset, X86FSET_STIBP_ALL);
-		if (cpi->cpi_extd[8].cp_ebx & CPUID_AMD_EBX_PREFER_IBRS)
-			add_x86_feature(featureset, X86FSET_RSBA);
 		if (cpi->cpi_extd[8].cp_ebx & CPUID_AMD_EBX_SSBD)
 			add_x86_feature(featureset, X86FSET_SSBD);
 		if (cpi->cpi_extd[8].cp_ebx & CPUID_AMD_EBX_VIRT_SSBD)
 			add_x86_feature(featureset, X86FSET_SSBD_VIRT);
 		if (cpi->cpi_extd[8].cp_ebx & CPUID_AMD_EBX_SSB_NO)
 			add_x86_feature(featureset, X86FSET_SSB_NO);
+		/*
+		 * Don't enable enhanced IBRS unless we're told that we should
+		 * prefer it and it has the same semantics as Intel. This is
+		 * split into two bits rather than a single one.
+		 */
+		if ((cpi->cpi_extd[8].cp_ebx & CPUID_AMD_EBX_PREFER_IBRS) &&
+		    (cpi->cpi_extd[8].cp_ebx & CPUID_AMD_EBX_IBRS_ALL)) {
+			add_x86_feature(featureset, X86FSET_IBRS_ALL);
+		}
+
 	} else if (cpi->cpi_vendor == X86_VENDOR_Intel &&
 	    cpi->cpi_maxeax >= 7) {
 		struct cpuid_regs *ecp;
@@ -2340,8 +2746,45 @@ cpuid_scan_security(cpu_t *cpu, uchar_t *featureset)
 			add_x86_feature(featureset, X86FSET_FLUSH_CMD);
 	}
 
-	if (cpu->cpu_id != 0)
+	if (cpu->cpu_id != 0) {
+		if (x86_spectrev2_mitigation == X86_SPECTREV2_ENHANCED_IBRS) {
+			cpuid_enable_enhanced_ibrs();
+		}
 		return;
+	}
+
+	/*
+	 * Go through and initialize various security mechanisms that we should
+	 * only do on a single CPU. This includes Spectre V2, L1TF, and MDS.
+	 */
+
+	/*
+	 * By default we've come in with retpolines enabled. Check whether we
+	 * should disable them or enable enhanced IBRS. Note, we always leave
+	 * RSB stuffing enabled. While originally it was only needed with
+	 * Spectre variant 2 mitigations on Skylake+ based systems, it turns out
+	 * that with the SpectreRSB, we might as well just always do it.
+	 */
+	if (x86_disable_spectrev2 != 0) {
+		v2mit = X86_SPECTREV2_DISABLED;
+	} else if (is_x86_feature(featureset, X86FSET_IBRS_ALL)) {
+		cpuid_enable_enhanced_ibrs();
+		v2mit = X86_SPECTREV2_ENHANCED_IBRS;
+#ifndef __xpv
+	} else if (cpuid_use_amd_retpoline(cpi)) {
+		v2mit = X86_SPECTREV2_RETPOLINE_AMD;
+#endif	/* !__xpv */
+	} else {
+		v2mit = X86_SPECTREV2_RETPOLINE;
+	}
+
+	cpuid_patch_retpolines(v2mit);
+	cpuid_patch_rsb(v2mit);
+	if (is_x86_feature(featureset, X86FSET_IBPB)) {
+		cpuid_patch_ibpb();
+	}
+	x86_spectrev2_mitigation = v2mit;
+	membar_producer();
 
 	/*
 	 * We need to determine what changes are required for mitigating L1TF
@@ -6772,8 +7215,13 @@ static int
 cpuid_post_ucodeadm_xc(xc_arg_t arg0, xc_arg_t arg1, xc_arg_t arg2)
 {
 	uchar_t *fset;
+	boolean_t first_pass = (boolean_t)arg1;
 
 	fset = (uchar_t *)(arg0 + sizeof (x86_featureset) * CPU->cpu_id);
+	if (first_pass && CPU->cpu_id != 0)
+		return (0);
+	if (!first_pass && CPU->cpu_id == 0)
+		return (0);
 	cpuid_pass_ucode(CPU, fset);
 
 	return (0);
@@ -6816,8 +7264,17 @@ cpuid_post_ucodeadm(void)
 		CPUSET_ADD(cpuset, i);
 	}
 
+	/*
+	 * We do the cross calls in two passes. The first pass is only for the
+	 * boot CPU. The second pass is for all of the other CPUs. This allows
+	 * the boot CPU to go through and change behavior related to patching or
+	 * whether or not Enhanced IBRS needs to be enabled and then allow all
+	 * other CPUs to follow suite.
+	 */
 	kpreempt_disable();
-	xc_sync((xc_arg_t)argdata, 0, 0, CPUSET2BV(cpuset),
+	xc_sync((xc_arg_t)argdata, B_TRUE, 0, CPUSET2BV(cpuset),
+	    cpuid_post_ucodeadm_xc);
+	xc_sync((xc_arg_t)argdata, B_FALSE, 0, CPUSET2BV(cpuset),
 	    cpuid_post_ucodeadm_xc);
 	kpreempt_enable();
 
diff --git a/usr/src/uts/i86pc/sys/machprivregs.h b/usr/src/uts/i86pc/sys/machprivregs.h
index 0ce3b19da4..faaecfc914 100644
--- a/usr/src/uts/i86pc/sys/machprivregs.h
+++ b/usr/src/uts/i86pc/sys/machprivregs.h
@@ -129,7 +129,7 @@ extern "C" {
 	movq	REGOFF_RDI(%rsp), %rdi;	\
 	addq	$REGOFF_RIP, %rsp
 
-#define	FAST_INTR_RETURN	call *x86_md_clear; jmp tr_iret_user
+#define	FAST_INTR_RETURN	call x86_md_clear; jmp tr_iret_user
 
 #elif defined(__i386)
 
diff --git a/usr/src/uts/intel/Makefile.files b/usr/src/uts/intel/Makefile.files
index bd10d299d7..747d61806e 100644
--- a/usr/src/uts/intel/Makefile.files
+++ b/usr/src/uts/intel/Makefile.files
@@ -53,11 +53,15 @@ CORE_OBJS +=		\
 	lock_prim.o	\
 	ovbcopy.o	\
 	polled_io.o	\
+	retpoline.o	\
 	sseblk.o	\
 	sundep.o	\
 	swtch.o		\
 	sysi86.o
 
+DBOOT_OBJS +=		\
+	retpoline.o
+
 #
 # 64-bit multiply/divide compiler helper routines
 # used only for ia32
diff --git a/usr/src/uts/intel/Makefile.rules b/usr/src/uts/intel/Makefile.rules
index 4ccc483465..5b473a9a3c 100644
--- a/usr/src/uts/intel/Makefile.rules
+++ b/usr/src/uts/intel/Makefile.rules
@@ -79,6 +79,9 @@ $(OBJS_DIR)/%.o:		$(UTSBASE)/intel/fs/proc/%.c
 $(OBJS_DIR)/%.o:		$(UTSBASE)/intel/ia32/ml/%.s
 	$(COMPILE.s) -o $@ $<
 
+$(DBOOT_OBJS_DIR)/%.o:		$(UTSBASE)/intel/ia32/ml/%.s
+	$(DBOOT_AS) -P -D_ASM $(DBOOT_DEFS) $(DBOOT_AS_INCL) -o $@ $<
+
 $(OBJS_DIR)/%.o:		$(UTSBASE)/intel/ia32/os/%.c
 	$(COMPILE.c) -o $@ $<
 	$(CTFCONVERT_O)
diff --git a/usr/src/uts/intel/amd64/krtld/kobj_crt.s b/usr/src/uts/intel/amd64/krtld/kobj_crt.s
index 16f90bd897..96025df7ea 100644
--- a/usr/src/uts/intel/amd64/krtld/kobj_crt.s
+++ b/usr/src/uts/intel/amd64/krtld/kobj_crt.s
@@ -21,10 +21,9 @@
 /*
  * Copyright 2007 Sun Microsystems, Inc.  All rights reserved.
  * Use is subject to license terms.
+ * Copyright 2019 Joyent, Inc.
  */
 
-#pragma ident	"%Z%%M%	%I%	%E% SMI"
-
 /*
  * exit routine from linker/loader to kernel
  */
@@ -64,7 +63,7 @@ exitto(caddr_t entrypoint)
 	movq    (%rax), %rdx
 
 	/ Call destination
-	call   *%r11
+	INDIRECT_CALL_REG(r11)
 
 	SET_SIZE(exitto)
 
diff --git a/usr/src/uts/intel/brand/common/brand_solaris.s b/usr/src/uts/intel/brand/common/brand_solaris.s
index 0d9b326b2f..b80b44e6c3 100644
--- a/usr/src/uts/intel/brand/common/brand_solaris.s
+++ b/usr/src/uts/intel/brand/common/brand_solaris.s
@@ -30,7 +30,7 @@
  * no easy place to save the extra parameters that would be required, so
  * each brand module needs its own copy of this code.  We #include this and
  * use brand-specific #defines to replace the XXX_brand_... definitions.
- */ 
+ */
 
 #ifdef lint
 
@@ -89,7 +89,7 @@ ENTRY(XXX_brand_syscall32_callback)
 	mov	%rcx, SYSCALL_REG; /* save orig return addr in syscall_reg */
 	mov	SCR_REG, %rcx;	/* place new return addr in %rcx */
 	mov	%gs:CPU_RTMP_R15, SCR_REG; /* restore scratch register */
-	call	*x86_md_clear		/* Flush micro-arch state */
+	call	x86_md_clear		/* Flush micro-arch state */
 	mov	V_SSP(SP_REG), SP_REG	/* restore user stack pointer */
 	jmp	nopop_sys_syscall32_swapgs_sysretl
 9:
@@ -109,7 +109,7 @@ ENTRY(XXX_brand_syscall_callback)
 	mov	%rcx, SYSCALL_REG; /* save orig return addr in syscall_reg */
 	mov	SCR_REG, %rcx;	/* place new return addr in %rcx */
 	mov	%gs:CPU_RTMP_R15, SCR_REG; /* restore scratch register */
-	call	*x86_md_clear		/* Flush micro-arch state */
+	call	x86_md_clear		/* Flush micro-arch state */
 	mov	V_SSP(SP_REG), SP_REG	/* restore user stack pointer */
 	jmp	nopop_sys_syscall_swapgs_sysretq
 9:
diff --git a/usr/src/uts/intel/ia32/ml/copy.s b/usr/src/uts/intel/ia32/ml/copy.s
index f76a8a43cb..672f7e3374 100644
--- a/usr/src/uts/intel/ia32/ml/copy.s
+++ b/usr/src/uts/intel/ia32/ml/copy.s
@@ -36,7 +36,7 @@
 /*         All Rights Reserved						*/
 
 /*
- * Copyright (c) 2018 Joyent, Inc.
+ * Copyright 2019 Joyent, Inc.
  */
 
 #include <sys/errno.h>
@@ -128,7 +128,7 @@
  * bcopy/kcopy/bzero/kzero operate on small buffers. For best performance for
  * these small sizes unrolled code is used. For medium sizes loops writing
  * 64-bytes per loop are used. Transition points were determined experimentally.
- */ 
+ */
 #define BZERO_USE_REP	(1024)
 #define BCOPY_DFLT_REP	(128)
 #define	BCOPY_NHM_REP	(768)
@@ -179,7 +179,7 @@ kcopy(const void *from, void *to, size_t count)
 	pushq	%rbp
 	movq	%rsp, %rbp
 #ifdef DEBUG
-	cmpq	postbootkernelbase(%rip), %rdi 		/* %rdi = from */
+	cmpq	postbootkernelbase(%rip), %rdi		/* %rdi = from */
 	jb	0f
 	cmpq	postbootkernelbase(%rip), %rsi		/* %rsi = to */
 	jnb	1f
@@ -231,7 +231,7 @@ _kcopy_copyerr:
 1:	popl	%ebp
 #endif
 	lea	_kcopy_copyerr, %eax	/* lofault value */
-	movl	%gs:CPU_THREAD, %edx	
+	movl	%gs:CPU_THREAD, %edx
 
 do_copy_fault:
 	pushl	%ebp
@@ -310,7 +310,7 @@ kcopy_nta(const void *from, void *to, size_t count, int copy_cached)
 	pushq	%rbp
 	movq	%rsp, %rbp
 #ifdef DEBUG
-	cmpq	postbootkernelbase(%rip), %rdi 		/* %rdi = from */
+	cmpq	postbootkernelbase(%rip), %rdi		/* %rdi = from */
 	jb	0f
 	cmpq	postbootkernelbase(%rip), %rsi		/* %rsi = to */
 	jnb	1f
@@ -406,7 +406,7 @@ _kcopy_nta_copyerr:
 	pushl	%esi
 	pushl	%edi
 
-	movl	%gs:CPU_THREAD, %edx	
+	movl	%gs:CPU_THREAD, %edx
 	movl	T_LOFAULT(%edx), %edi
 	pushl	%edi			/* save the current lofault */
 	movl	%eax, T_LOFAULT(%edx)	/* new lofault */
@@ -455,7 +455,7 @@ bcopy(const void *from, void *to, size_t count)
 	jz	1f
 	cmpq	postbootkernelbase(%rip), %rdi		/* %rdi = from */
 	jb	0f
-	cmpq	postbootkernelbase(%rip), %rsi		/* %rsi = to */		
+	cmpq	postbootkernelbase(%rip), %rsi		/* %rsi = to */
 	jnb	1f
 0:	leaq	.bcopy_panic_msg(%rip), %rdi
 	jmp	call_panic		/* setup stack and call panic */
@@ -482,7 +482,7 @@ do_copy:
 	addq	%rdx, %rsi
 	movslq	(%r10,%rdx,4), %rcx
 	leaq	(%rcx,%r10,1), %r10
-	jmpq	*%r10
+	INDIRECT_JMP_REG(r10)
 
 	.p2align 4
 L(fwdPxQx):
@@ -493,7 +493,7 @@ L(fwdPxQx):
 	.int       L(P4Q0)-L(fwdPxQx)
 	.int       L(P5Q0)-L(fwdPxQx)
 	.int       L(P6Q0)-L(fwdPxQx)
-	.int       L(P7Q0)-L(fwdPxQx) 
+	.int       L(P7Q0)-L(fwdPxQx)
 
 	.int       L(P0Q1)-L(fwdPxQx)	/* 8 */
 	.int       L(P1Q1)-L(fwdPxQx)
@@ -502,7 +502,7 @@ L(fwdPxQx):
 	.int       L(P4Q1)-L(fwdPxQx)
 	.int       L(P5Q1)-L(fwdPxQx)
 	.int       L(P6Q1)-L(fwdPxQx)
-	.int       L(P7Q1)-L(fwdPxQx) 
+	.int       L(P7Q1)-L(fwdPxQx)
 
 	.int       L(P0Q2)-L(fwdPxQx)	/* 16 */
 	.int       L(P1Q2)-L(fwdPxQx)
@@ -511,7 +511,7 @@ L(fwdPxQx):
 	.int       L(P4Q2)-L(fwdPxQx)
 	.int       L(P5Q2)-L(fwdPxQx)
 	.int       L(P6Q2)-L(fwdPxQx)
-	.int       L(P7Q2)-L(fwdPxQx) 
+	.int       L(P7Q2)-L(fwdPxQx)
 
 	.int       L(P0Q3)-L(fwdPxQx)	/* 24 */
 	.int       L(P1Q3)-L(fwdPxQx)
@@ -520,7 +520,7 @@ L(fwdPxQx):
 	.int       L(P4Q3)-L(fwdPxQx)
 	.int       L(P5Q3)-L(fwdPxQx)
 	.int       L(P6Q3)-L(fwdPxQx)
-	.int       L(P7Q3)-L(fwdPxQx) 
+	.int       L(P7Q3)-L(fwdPxQx)
 
 	.int       L(P0Q4)-L(fwdPxQx)	/* 32 */
 	.int       L(P1Q4)-L(fwdPxQx)
@@ -529,7 +529,7 @@ L(fwdPxQx):
 	.int       L(P4Q4)-L(fwdPxQx)
 	.int       L(P5Q4)-L(fwdPxQx)
 	.int       L(P6Q4)-L(fwdPxQx)
-	.int       L(P7Q4)-L(fwdPxQx) 
+	.int       L(P7Q4)-L(fwdPxQx)
 
 	.int       L(P0Q5)-L(fwdPxQx)	/* 40 */
 	.int       L(P1Q5)-L(fwdPxQx)
@@ -538,7 +538,7 @@ L(fwdPxQx):
 	.int       L(P4Q5)-L(fwdPxQx)
 	.int       L(P5Q5)-L(fwdPxQx)
 	.int       L(P6Q5)-L(fwdPxQx)
-	.int       L(P7Q5)-L(fwdPxQx) 
+	.int       L(P7Q5)-L(fwdPxQx)
 
 	.int       L(P0Q6)-L(fwdPxQx)	/* 48 */
 	.int       L(P1Q6)-L(fwdPxQx)
@@ -547,7 +547,7 @@ L(fwdPxQx):
 	.int       L(P4Q6)-L(fwdPxQx)
 	.int       L(P5Q6)-L(fwdPxQx)
 	.int       L(P6Q6)-L(fwdPxQx)
-	.int       L(P7Q6)-L(fwdPxQx) 
+	.int       L(P7Q6)-L(fwdPxQx)
 
 	.int       L(P0Q7)-L(fwdPxQx)	/* 56 */
 	.int       L(P1Q7)-L(fwdPxQx)
@@ -556,7 +556,7 @@ L(fwdPxQx):
 	.int       L(P4Q7)-L(fwdPxQx)
 	.int       L(P5Q7)-L(fwdPxQx)
 	.int       L(P6Q7)-L(fwdPxQx)
-	.int       L(P7Q7)-L(fwdPxQx) 
+	.int       L(P7Q7)-L(fwdPxQx)
 
 	.int       L(P0Q8)-L(fwdPxQx)	/* 64 */
 	.int       L(P1Q8)-L(fwdPxQx)
@@ -604,8 +604,8 @@ L(P0Q2):
 L(P0Q1):
 	mov    -0x8(%rdi), %r8
 	mov    %r8, -0x8(%rsi)
-L(P0Q0):                                   
-	ret   
+L(P0Q0):
+	ret
 
 	.p2align 4
 L(P1Q9):
@@ -638,7 +638,7 @@ L(P1Q1):
 L(P1Q0):
 	movzbq -0x1(%rdi), %r8
 	mov    %r8b, -0x1(%rsi)
-	ret   
+	ret
 
 	.p2align 4
 L(P2Q9):
@@ -671,7 +671,7 @@ L(P2Q1):
 L(P2Q0):
 	movzwq -0x2(%rdi), %r8
 	mov    %r8w, -0x2(%rsi)
-	ret   
+	ret
 
 	.p2align 4
 L(P3Q9):
@@ -702,7 +702,7 @@ L(P3Q1):
 	mov    -0xb(%rdi), %r10
 	mov    %r10, -0xb(%rsi)
 	/*
-	 * These trailing loads/stores have to do all their loads 1st, 
+	 * These trailing loads/stores have to do all their loads 1st,
 	 * then do the stores.
 	 */
 L(P3Q0):
@@ -710,7 +710,7 @@ L(P3Q0):
 	movzbq -0x1(%rdi), %r10
 	mov    %r8w, -0x3(%rsi)
 	mov    %r10b, -0x1(%rsi)
-	ret   
+	ret
 
 	.p2align 4
 L(P4Q9):
@@ -743,7 +743,7 @@ L(P4Q1):
 L(P4Q0):
 	mov    -0x4(%rdi), %r8d
 	mov    %r8d, -0x4(%rsi)
-	ret   
+	ret
 
 	.p2align 4
 L(P5Q9):
@@ -778,7 +778,7 @@ L(P5Q0):
 	movzbq -0x1(%rdi), %r10
 	mov    %r8d, -0x5(%rsi)
 	mov    %r10b, -0x1(%rsi)
-	ret   
+	ret
 
 	.p2align 4
 L(P6Q9):
@@ -813,7 +813,7 @@ L(P6Q0):
 	movzwq -0x2(%rdi), %r10
 	mov    %r8d, -0x6(%rsi)
 	mov    %r10w, -0x2(%rsi)
-	ret   
+	ret
 
 	.p2align 4
 L(P7Q9):
@@ -850,7 +850,7 @@ L(P7Q0):
 	mov    %r8d, -0x7(%rsi)
 	mov    %r10w, -0x3(%rsi)
 	mov    %cl, -0x1(%rsi)
-	ret   
+	ret
 
 	/*
 	 * For large sizes rep smovq is fastest.
@@ -938,7 +938,7 @@ L(do_remainder):
 	addq	%rdx, %rsi
 	movslq	(%r10,%rdx,4), %rcx
 	leaq	(%rcx,%r10,1), %r10
-	jmpq	*%r10
+	INDIRECT_JMP_REG(r10)
 
 	/*
 	 * Use rep smovq. Clear remainder via unrolled code
@@ -1052,7 +1052,7 @@ kzero(void *addr, size_t count)
 0:
 #endif
 	/*
-	 * pass lofault value as 3rd argument for fault return 
+	 * pass lofault value as 3rd argument for fault return
 	 */
 	leaq	_kzeroerr(%rip), %rdx
 
@@ -1095,7 +1095,7 @@ _kzeroerr:
 	movl	%esp, %ebp		/* set new stack base */
 	pushl	%edi			/* save %edi */
 
-	mov	%gs:CPU_THREAD, %edx	
+	mov	%gs:CPU_THREAD, %edx
 	movl	T_LOFAULT(%edx), %edi
 	pushl	%edi			/* save the current lofault */
 	movl	%eax, T_LOFAULT(%edx)	/* new lofault */
@@ -1170,7 +1170,7 @@ do_zero:
 	addq	%rsi, %rdi
 	movslq	(%r10,%rsi,4), %rcx
 	leaq	(%rcx,%r10,1), %r10
-	jmpq	*%r10
+	INDIRECT_JMP_REG(r10)
 
 	.p2align 4
 L(setPxQx):
@@ -1181,7 +1181,7 @@ L(setPxQx):
 	.int       L(P4Q0)-L(setPxQx)
 	.int       L(P5Q0)-L(setPxQx)
 	.int       L(P6Q0)-L(setPxQx)
-	.int       L(P7Q0)-L(setPxQx) 
+	.int       L(P7Q0)-L(setPxQx)
 
 	.int       L(P0Q1)-L(setPxQx)	/* 8 */
 	.int       L(P1Q1)-L(setPxQx)
@@ -1190,7 +1190,7 @@ L(setPxQx):
 	.int       L(P4Q1)-L(setPxQx)
 	.int       L(P5Q1)-L(setPxQx)
 	.int       L(P6Q1)-L(setPxQx)
-	.int       L(P7Q1)-L(setPxQx) 
+	.int       L(P7Q1)-L(setPxQx)
 
 	.int       L(P0Q2)-L(setPxQx)	/* 16 */
 	.int       L(P1Q2)-L(setPxQx)
@@ -1199,7 +1199,7 @@ L(setPxQx):
 	.int       L(P4Q2)-L(setPxQx)
 	.int       L(P5Q2)-L(setPxQx)
 	.int       L(P6Q2)-L(setPxQx)
-	.int       L(P7Q2)-L(setPxQx) 
+	.int       L(P7Q2)-L(setPxQx)
 
 	.int       L(P0Q3)-L(setPxQx)	/* 24 */
 	.int       L(P1Q3)-L(setPxQx)
@@ -1208,7 +1208,7 @@ L(setPxQx):
 	.int       L(P4Q3)-L(setPxQx)
 	.int       L(P5Q3)-L(setPxQx)
 	.int       L(P6Q3)-L(setPxQx)
-	.int       L(P7Q3)-L(setPxQx) 
+	.int       L(P7Q3)-L(setPxQx)
 
 	.int       L(P0Q4)-L(setPxQx)	/* 32 */
 	.int       L(P1Q4)-L(setPxQx)
@@ -1217,7 +1217,7 @@ L(setPxQx):
 	.int       L(P4Q4)-L(setPxQx)
 	.int       L(P5Q4)-L(setPxQx)
 	.int       L(P6Q4)-L(setPxQx)
-	.int       L(P7Q4)-L(setPxQx) 
+	.int       L(P7Q4)-L(setPxQx)
 
 	.int       L(P0Q5)-L(setPxQx)	/* 40 */
 	.int       L(P1Q5)-L(setPxQx)
@@ -1226,7 +1226,7 @@ L(setPxQx):
 	.int       L(P4Q5)-L(setPxQx)
 	.int       L(P5Q5)-L(setPxQx)
 	.int       L(P6Q5)-L(setPxQx)
-	.int       L(P7Q5)-L(setPxQx) 
+	.int       L(P7Q5)-L(setPxQx)
 
 	.int       L(P0Q6)-L(setPxQx)	/* 48 */
 	.int       L(P1Q6)-L(setPxQx)
@@ -1235,7 +1235,7 @@ L(setPxQx):
 	.int       L(P4Q6)-L(setPxQx)
 	.int       L(P5Q6)-L(setPxQx)
 	.int       L(P6Q6)-L(setPxQx)
-	.int       L(P7Q6)-L(setPxQx) 
+	.int       L(P7Q6)-L(setPxQx)
 
 	.int       L(P0Q7)-L(setPxQx)	/* 56 */
 	.int       L(P1Q7)-L(setPxQx)
@@ -1244,7 +1244,7 @@ L(setPxQx):
 	.int       L(P4Q7)-L(setPxQx)
 	.int       L(P5Q7)-L(setPxQx)
 	.int       L(P6Q7)-L(setPxQx)
-	.int       L(P7Q7)-L(setPxQx) 
+	.int       L(P7Q7)-L(setPxQx)
 
 	.int       L(P0Q8)-L(setPxQx)	/* 64 */
 	.int       L(P1Q8)-L(setPxQx)
@@ -1274,7 +1274,7 @@ L(P0Q4): mov    %rax, -0x20(%rdi)
 L(P0Q3): mov    %rax, -0x18(%rdi)
 L(P0Q2): mov    %rax, -0x10(%rdi)
 L(P0Q1): mov    %rax, -0x8(%rdi)
-L(P0Q0): 
+L(P0Q0):
 	 ret
 
 	.p2align 4
@@ -1422,14 +1422,14 @@ L(aligned_now):
 L(bzero_loop):
 	leaq	-0x40(%rsi), %rsi
 	cmpq	$0x40, %rsi
-	movq	%rax, (%rdi) 
-	movq	%rax, 0x8(%rdi) 
-	movq	%rax, 0x10(%rdi) 
-	movq	%rax, 0x18(%rdi) 
-	movq	%rax, 0x20(%rdi) 
-	movq	%rax, 0x28(%rdi) 
-	movq	%rax, 0x30(%rdi) 
-	movq	%rax, 0x38(%rdi) 
+	movq	%rax, (%rdi)
+	movq	%rax, 0x8(%rdi)
+	movq	%rax, 0x10(%rdi)
+	movq	%rax, 0x18(%rdi)
+	movq	%rax, 0x20(%rdi)
+	movq	%rax, 0x28(%rdi)
+	movq	%rax, 0x30(%rdi)
+	movq	%rax, 0x38(%rdi)
 	leaq	0x40(%rdi), %rdi
 	jae	L(bzero_loop)
 
@@ -1441,7 +1441,7 @@ L(bzero_loop):
 	addq	%rsi, %rdi
 	movslq	(%r10,%rsi,4), %rcx
 	leaq	(%rcx,%r10,1), %r10
-	jmpq	*%r10
+	INDIRECT_JMP_REG(r10)
 
 	/*
 	 * Use rep sstoq. Clear any remainder via unrolled code
@@ -1564,7 +1564,7 @@ copyin(const void *uaddr, void *kaddr, size_t count)
 
 _copyin_err:
 	SMAP_ENABLE_INSTR(2)
-	movq	%r11, T_LOFAULT(%r9)	/* restore original lofault */	
+	movq	%r11, T_LOFAULT(%r9)	/* restore original lofault */
 	addq	$8, %rsp		/* pop bcopy_altentry call ret addr */
 3:
 	movq	T_COPYOPS(%r9), %rax
@@ -1577,9 +1577,10 @@ _copyin_err:
 	movq	0x8(%rsp), %rsi
 	movq	0x10(%rsp), %rdx
 	leave
-	jmp	*CP_COPYIN(%rax)
+	movq	CP_COPYIN(%rax), %rax
+	INDIRECT_JMP_REG(rax)
 
-2:	movl	$-1, %eax	
+2:	movl	$-1, %eax
 	leave
 	ret
 	SET_SIZE(copyin)
@@ -1680,7 +1681,7 @@ xcopyin_nta(const void *uaddr, void *kaddr, size_t count, int copy_cached)
 6:
 	SMAP_DISABLE_INSTR(1)
 	jmp	do_copy_fault
-	
+
 	/*
 	 * Make sure src and dst are NTA_ALIGN_SIZE aligned,
 	 * count is COUNT_ALIGN_SIZE aligned.
@@ -1691,11 +1692,11 @@ xcopyin_nta(const void *uaddr, void *kaddr, size_t count, int copy_cached)
 	andq	$NTA_ALIGN_MASK, %r10
 	orq	%rdx, %r10
 	andq	$COUNT_ALIGN_MASK, %r10
-	jnz	6b	
+	jnz	6b
 	leaq	_xcopyin_nta_err(%rip), %rcx	/* doesn't set rflags */
 	SMAP_DISABLE_INSTR(2)
 	jmp	do_copy_fault_nta	/* use non-temporal access */
-	
+
 4:
 	movl	$EFAULT, %eax
 	jmp	3f
@@ -1722,7 +1723,8 @@ _xcopyin_nta_err:
 	movq	0x8(%rsp), %rsi
 	movq	0x10(%rsp), %rdx
 	leave
-	jmp	*CP_XCOPYIN(%r8)
+	movq	CP_XCOPYIN(%r8), %r8
+	INDIRECT_JMP_REG(r8)
 
 2:	leave
 	ret
@@ -1755,7 +1757,7 @@ _xcopyin_nta_err:
 	 */
 	cmpl	$XCOPY_MIN_SIZE, ARG_COUNT(%esp)
 	jb	do_copy_fault
-	
+
 	/*
 	 * Make sure src and dst are NTA_ALIGN_SIZE aligned,
 	 * count is COUNT_ALIGN_SIZE aligned.
@@ -1790,7 +1792,7 @@ _xcopyin_err:
 	movl	T_COPYOPS(%edx), %eax
 	jmp	*CP_XCOPYIN(%eax)
 
-2:	rep; 	ret	/* use 2 byte return instruction when branch target */
+2:	rep;	ret	/* use 2 byte return instruction when branch target */
 			/* AMD Software Optimization Guide - Section 6.2 */
 	SET_SIZE(xcopyin_nta)
 
@@ -1865,7 +1867,8 @@ _copyout_err:
 	movq	0x8(%rsp), %rsi
 	movq	0x10(%rsp), %rdx
 	leave
-	jmp	*CP_COPYOUT(%rax)
+	movq	CP_COPYOUT(%rax), %rax
+	INDIRECT_JMP_REG(rax)
 
 2:	movl	$-1, %eax
 	leave
@@ -1893,7 +1896,7 @@ _copyout_err:
 	cmpl	%ecx, ARG_UADDR(%esp)	/* test uaddr < kernelbase */
 	jb	do_copy_fault
 	jmp	3f
-	
+
 _copyout_err:
 	popl	%ecx
 	popl	%edi
@@ -1966,7 +1969,7 @@ xcopyout_nta(const void *kaddr, void *uaddr, size_t count, int copy_cached)
 6:
 	SMAP_DISABLE_INSTR(4)
 	jmp	do_copy_fault
-	
+
 	/*
 	 * Make sure src and dst are NTA_ALIGN_SIZE aligned,
 	 * count is COUNT_ALIGN_SIZE aligned.
@@ -1977,7 +1980,7 @@ xcopyout_nta(const void *kaddr, void *uaddr, size_t count, int copy_cached)
 	andq	$NTA_ALIGN_MASK, %r10
 	orq	%rdx, %r10
 	andq	$COUNT_ALIGN_MASK, %r10
-	jnz	6b	
+	jnz	6b
 	leaq	_xcopyout_nta_err(%rip), %rcx
 	SMAP_DISABLE_INSTR(5)
 	call	do_copy_fault_nta
@@ -2010,7 +2013,8 @@ _xcopyout_nta_err:
 	movq	0x8(%rsp), %rsi
 	movq	0x10(%rsp), %rdx
 	leave
-	jmp	*CP_XCOPYOUT(%r8)
+	movq	CP_XCOPYOUT(%r8), %r8
+	INDIRECT_JMP_REG(r8)
 
 2:	leave
 	ret
@@ -2041,7 +2045,7 @@ _xcopyout_nta_err:
 	 */
 	cmpl	$XCOPY_MIN_SIZE, %edx
 	jb	do_copy_fault
-	
+
 	/*
 	 * Make sure src and dst are NTA_ALIGN_SIZE aligned,
 	 * count is COUNT_ALIGN_SIZE aligned.
@@ -2197,7 +2201,7 @@ do_copystr:
 	pushl	%ebx			/* save registers */
 	pushl	%edi
 
-	movl	%gs:CPU_THREAD, %ebx	
+	movl	%gs:CPU_THREAD, %ebx
 	movl	T_LOFAULT(%ebx), %edi
 	pushl	%edi			/* save the current lofault */
 	movl	%eax, T_LOFAULT(%ebx)	/* new lofault */
@@ -2212,7 +2216,7 @@ do_copystr:
 copystr_loop:
 	decl	%ecx
 	movb	(%ebx), %al
-	incl	%ebx	
+	incl	%ebx
 	movb	%al, (%edx)
 	incl	%edx
 	cmpb	$0, %al
@@ -2237,13 +2241,13 @@ copystr_out:
 
 copystr_done:
 	popl	%edi
-	movl	%gs:CPU_THREAD, %ebx	
+	movl	%gs:CPU_THREAD, %ebx
 	movl	%edi, T_LOFAULT(%ebx)	/* restore the original lofault */
 
 	popl	%edi
 	popl	%ebx
 	popl	%ebp
-	ret	
+	ret
 	SET_SIZE(copystr)
 
 #undef	ARG_FROM
@@ -2324,8 +2328,9 @@ _copyinstr_error:
 	movq	0x10(%rsp), %rdx
 	movq	0x18(%rsp), %rcx
 	leave
-	jmp	*CP_COPYINSTR(%rax)
-	
+	movq	CP_COPYINSTR(%rax), %rax
+	INDIRECT_JMP_REG(rax)
+
 2:	movl	$EFAULT, %eax		/* return EFAULT */
 	leave
 	ret
@@ -2355,7 +2360,7 @@ _copyinstr_error:
 
 _copyinstr_error:
 	popl	%edi
-	movl	%gs:CPU_THREAD, %edx	
+	movl	%gs:CPU_THREAD, %edx
 	movl	%edi, T_LOFAULT(%edx)	/* original lofault */
 
 	popl	%edi
@@ -2366,7 +2371,7 @@ _copyinstr_error:
 	cmpl	$0, %eax
 	jz	2f
 	jmp	*CP_COPYINSTR(%eax)
-	
+
 2:	movl	$EFAULT, %eax		/* return EFAULT */
 	ret
 	SET_SIZE(copyinstr)
@@ -2446,13 +2451,14 @@ _copyoutstr_error:
 	movq	0x10(%rsp), %rdx
 	movq	0x18(%rsp), %rcx
 	leave
-	jmp	*CP_COPYOUTSTR(%rax)
-	
+	movq	CP_COPYOUTSTR(%rax), %rax
+	INDIRECT_JMP_REG(rax)
+
 2:	movl	$EFAULT, %eax		/* return EFAULT */
 	leave
 	ret
-	SET_SIZE(copyoutstr)	
-	
+	SET_SIZE(copyoutstr)
+
 #elif defined(__i386)
 
 #define	ARG_KADDR	4
@@ -2477,7 +2483,7 @@ _copyoutstr_error:
 
 _copyoutstr_error:
 	popl	%edi
-	movl	%gs:CPU_THREAD, %edx	
+	movl	%gs:CPU_THREAD, %edx
 	movl	%edi, T_LOFAULT(%edx)	/* restore the original lofault */
 
 	popl	%edi
@@ -2492,7 +2498,7 @@ _copyoutstr_error:
 2:	movl	$EFAULT, %eax		/* return EFAULT */
 	ret
 	SET_SIZE(copyoutstr)
-	
+
 #undef	ARG_KADDR
 #undef	ARG_UADDR
 
@@ -2503,7 +2509,7 @@ _copyoutstr_error:
  * Since all of the fuword() variants are so similar, we have a macro to spit
  * them out.  This allows us to create DTrace-unobservable functions easily.
  */
-	
+
 #if defined(__lint)
 
 #if defined(__amd64)
@@ -2562,12 +2568,13 @@ _flt_/**/NAME:					\
 	movq	T_COPYOPS(%r9), %rax;		\
 	cmpq	$0, %rax;			\
 	jz	2f;				\
-	jmp	*COPYOP(%rax);			\
+	movq	COPYOP(%rax), %rax;		\
+	INDIRECT_JMP_REG(rax);			\
 2:						\
 	movl	$-1, %eax;			\
 	ret;					\
 	SET_SIZE(NAME)
-	
+
 	FUWORD(fuword64, movq, %rax, CP_FUWORD64,8,10,11)
 	FUWORD(fuword32, movl, %eax, CP_FUWORD32,9,12,13)
 	FUWORD(fuword16, movw, %ax, CP_FUWORD16,10,14,15)
@@ -2671,7 +2678,8 @@ _flt_/**/NAME:					\
 	movq	T_COPYOPS(%r9), %rax;		\
 	cmpq	$0, %rax;			\
 	jz	3f;				\
-	jmp	*COPYOP(%rax);			\
+	movq	COPYOP(%rax), %rax;		\
+	INDIRECT_JMP_REG(rax);			\
 3:						\
 	movl	$-1, %eax;			\
 	ret;					\
@@ -3142,7 +3150,7 @@ ucopystr(const char *ufrom, char *uto, size_t umaxlength, size_t *lencopied)
 #ifndef __lint
 
 .data
-.align 	4
+.align	4
 .globl	_smap_enable_patch_count
 .type	_smap_enable_patch_count,@object
 .size	_smap_enable_patch_count, 4
diff --git a/usr/src/uts/intel/ia32/ml/ddi_i86_asm.s b/usr/src/uts/intel/ia32/ml/ddi_i86_asm.s
index f46048fadd..f90efdc922 100644
--- a/usr/src/uts/intel/ia32/ml/ddi_i86_asm.s
+++ b/usr/src/uts/intel/ia32/ml/ddi_i86_asm.s
@@ -24,7 +24,9 @@
  * Use is subject to license terms.
  */
 
-#pragma ident	"%Z%%M%	%I%	%E% SMI"
+/*
+ * Copyright 2019 Joyent, Inc.
+ */
 
 #if defined(lint) || defined(__lint)
 #include <sys/types.h>
@@ -267,7 +269,7 @@ ddi_mem_rep_put64(ddi_acc_handle_t handle, uint64_t *host_addr,
 
 #else	/* lint */
 
-	
+
 #if defined(__amd64)
 
 	ENTRY(ddi_get8)
@@ -289,7 +291,8 @@ ddi_mem_rep_put64(ddi_acc_handle_t handle, uint64_t *host_addr,
 	movzbq	(%rsi), %rax
 	ret
 2:
-	jmp	*ACC_GETB(%rdi)
+	movq	ACC_GETB(%rdi), %rax
+	INDIRECT_JMP_REG(rax)
 	SET_SIZE(ddi_get8)
 	SET_SIZE(ddi_getb)
 	SET_SIZE(ddi_mem_getb)
@@ -351,7 +354,8 @@ ddi_mem_rep_put64(ddi_acc_handle_t handle, uint64_t *host_addr,
 	movzwq	(%rsi), %rax
 	ret
 4:
-	jmp	*ACC_GETW(%rdi)
+	movq	ACC_GETW(%rdi), %rax
+	INDIRECT_JMP_REG(rax)
 	SET_SIZE(ddi_get16)
 	SET_SIZE(ddi_getw)
 	SET_SIZE(ddi_mem_getw)
@@ -412,7 +416,8 @@ ddi_mem_rep_put64(ddi_acc_handle_t handle, uint64_t *host_addr,
 	movl	(%rsi), %eax
 	ret
 6:
-	jmp	*ACC_GETL(%rdi)
+	movq	ACC_GETL(%rdi), %rax
+	INDIRECT_JMP_REG(rax)
 	SET_SIZE(ddi_get32)
 	SET_SIZE(ddi_getl)
 	SET_SIZE(ddi_mem_getl)
@@ -458,7 +463,8 @@ ddi_mem_rep_put64(ddi_acc_handle_t handle, uint64_t *host_addr,
 	ALTENTRY(ddi_getll)
 	ALTENTRY(ddi_mem_getll)
 	ALTENTRY(ddi_mem_get64)
-	jmp	*ACC_GETLL(%rdi)
+	movq	ACC_GETLL(%rdi), %rax
+	INDIRECT_JMP_REG(rax)
 	SET_SIZE(ddi_get64)
 	SET_SIZE(ddi_getll)
 	SET_SIZE(ddi_mem_getll)
@@ -500,7 +506,8 @@ ddi_mem_rep_put64(ddi_acc_handle_t handle, uint64_t *host_addr,
 	movb	%dl, (%rsi)
 	ret
 8:
-	jmp	*ACC_PUTB(%rdi)
+	movq	ACC_PUTB(%rdi), %rax
+	INDIRECT_JMP_REG(rax)
 	SET_SIZE(ddi_put8)
 	SET_SIZE(ddi_putb)
 	SET_SIZE(ddi_mem_putb)
@@ -563,7 +570,8 @@ ddi_mem_rep_put64(ddi_acc_handle_t handle, uint64_t *host_addr,
 	movw	%dx, (%rsi)
 	ret
 9:
-	jmp	*ACC_PUTW(%rdi)
+	movq	ACC_PUTW(%rdi), %rax
+	INDIRECT_JMP_REG(rax)
 	SET_SIZE(ddi_put16)
 	SET_SIZE(ddi_putw)
 	SET_SIZE(ddi_mem_putw)
@@ -626,7 +634,8 @@ ddi_mem_rep_put64(ddi_acc_handle_t handle, uint64_t *host_addr,
 	movl	%edx, (%rsi)
 	ret
 9:
-	jmp	*ACC_PUTL(%rdi)
+	movq	ACC_PUTL(%rdi), %rax
+	INDIRECT_JMP_REG(rax)
 	SET_SIZE(ddi_put32)
 	SET_SIZE(ddi_putl)
 	SET_SIZE(ddi_mem_putl)
@@ -674,7 +683,8 @@ ddi_mem_rep_put64(ddi_acc_handle_t handle, uint64_t *host_addr,
 	ALTENTRY(ddi_putll)
 	ALTENTRY(ddi_mem_putll)
 	ALTENTRY(ddi_mem_put64)
-	jmp	*ACC_PUTLL(%rdi)
+	movq	ACC_PUTLL(%rdi), %rax
+	INDIRECT_JMP_REG(rax)
 	SET_SIZE(ddi_put64)
 	SET_SIZE(ddi_putll)
 	SET_SIZE(ddi_mem_putll)
@@ -701,7 +711,8 @@ ddi_mem_rep_put64(ddi_acc_handle_t handle, uint64_t *host_addr,
 	ALTENTRY(ddi_rep_getb)
 	ALTENTRY(ddi_mem_rep_getb)
 	ALTENTRY(ddi_mem_rep_get8)
-	jmp	*ACC_REP_GETB(%rdi)
+	movq	ACC_REP_GETB(%rdi), %rax
+	INDIRECT_JMP_REG(rax)
 	SET_SIZE(ddi_rep_get8)
 	SET_SIZE(ddi_rep_getb)
 	SET_SIZE(ddi_mem_rep_getb)
@@ -728,7 +739,8 @@ ddi_mem_rep_put64(ddi_acc_handle_t handle, uint64_t *host_addr,
 	ALTENTRY(ddi_rep_getw)
 	ALTENTRY(ddi_mem_rep_getw)
 	ALTENTRY(ddi_mem_rep_get16)
-	jmp	*ACC_REP_GETW(%rdi)
+	movq	ACC_REP_GETW(%rdi), %rax
+	INDIRECT_JMP_REG(rax)
 	SET_SIZE(ddi_rep_get16)
 	SET_SIZE(ddi_rep_getw)
 	SET_SIZE(ddi_mem_rep_getw)
@@ -755,7 +767,8 @@ ddi_mem_rep_put64(ddi_acc_handle_t handle, uint64_t *host_addr,
 	ALTENTRY(ddi_rep_getl)
 	ALTENTRY(ddi_mem_rep_getl)
 	ALTENTRY(ddi_mem_rep_get32)
-	jmp	*ACC_REP_GETL(%rdi)
+	movq	ACC_REP_GETL(%rdi), %rax
+	INDIRECT_JMP_REG(rax)
 	SET_SIZE(ddi_rep_get32)
 	SET_SIZE(ddi_rep_getl)
 	SET_SIZE(ddi_mem_rep_getl)
@@ -782,7 +795,8 @@ ddi_mem_rep_put64(ddi_acc_handle_t handle, uint64_t *host_addr,
 	ALTENTRY(ddi_rep_getll)
 	ALTENTRY(ddi_mem_rep_getll)
 	ALTENTRY(ddi_mem_rep_get64)
-	jmp	*ACC_REP_GETLL(%rdi)
+	movq	ACC_REP_GETLL(%rdi), %rax
+	INDIRECT_JMP_REG(rax)
 	SET_SIZE(ddi_rep_get64)
 	SET_SIZE(ddi_rep_getll)
 	SET_SIZE(ddi_mem_rep_getll)
@@ -809,7 +823,8 @@ ddi_mem_rep_put64(ddi_acc_handle_t handle, uint64_t *host_addr,
 	ALTENTRY(ddi_rep_putb)
 	ALTENTRY(ddi_mem_rep_putb)
 	ALTENTRY(ddi_mem_rep_put8)
-	jmp	*ACC_REP_PUTB(%rdi)
+	movq	ACC_REP_PUTB(%rdi), %rax
+	INDIRECT_JMP_REG(rax)
 	SET_SIZE(ddi_rep_put8)
 	SET_SIZE(ddi_rep_putb)
 	SET_SIZE(ddi_mem_rep_putb)
@@ -836,7 +851,8 @@ ddi_mem_rep_put64(ddi_acc_handle_t handle, uint64_t *host_addr,
 	ALTENTRY(ddi_rep_putw)
 	ALTENTRY(ddi_mem_rep_putw)
 	ALTENTRY(ddi_mem_rep_put16)
-	jmp	*ACC_REP_PUTW(%rdi)
+	movq	ACC_REP_PUTW(%rdi), %rax
+	INDIRECT_JMP_REG(rax)
 	SET_SIZE(ddi_rep_put16)
 	SET_SIZE(ddi_rep_putw)
 	SET_SIZE(ddi_mem_rep_putw)
@@ -863,7 +879,8 @@ ddi_mem_rep_put64(ddi_acc_handle_t handle, uint64_t *host_addr,
 	ALTENTRY(ddi_rep_putl)
 	ALTENTRY(ddi_mem_rep_putl)
 	ALTENTRY(ddi_mem_rep_put32)
-	jmp	*ACC_REP_PUTL(%rdi)
+	movq	ACC_REP_PUTL(%rdi), %rax
+	INDIRECT_JMP_REG(rax)
 	SET_SIZE(ddi_rep_put32)
 	SET_SIZE(ddi_rep_putl)
 	SET_SIZE(ddi_mem_rep_putl)
@@ -890,7 +907,8 @@ ddi_mem_rep_put64(ddi_acc_handle_t handle, uint64_t *host_addr,
 	ALTENTRY(ddi_rep_putll)
 	ALTENTRY(ddi_mem_rep_putll)
 	ALTENTRY(ddi_mem_rep_put64)
-	jmp	*ACC_REP_PUTLL(%rdi)
+	movq	ACC_REP_PUTLL(%rdi), %rax
+	INDIRECT_JMP_REG(rax)
 	SET_SIZE(ddi_rep_put64)
 	SET_SIZE(ddi_rep_putll)
 	SET_SIZE(ddi_mem_rep_putll)
@@ -1336,7 +1354,7 @@ i_ddi_io_rep_get32(ddi_acc_impl_t *hdlp, uint32_t *host_addr,
 	ret
 
 gb_ioadv:
-	andq	%rcx, %rcx		
+	andq	%rcx, %rcx
 	jz	gb_ioadv_done
 gb_ioadv2:
 	inb	(%dx)
@@ -1363,7 +1381,7 @@ gb_ioadv_done:
 	cmpl	$DDI_DEV_AUTOINCR, 24(%esp)
 	je	gb_ioadv
 
-	rep	
+	rep
 	insb
 	popl	%edi
 	ret
@@ -1395,7 +1413,7 @@ gb_ioadv_done:
 	je	gw_ioadv
 
 	movq	%rsi, %rdi
-	rep	
+	rep
 	insw
 	ret
 
@@ -1426,7 +1444,7 @@ gw_ioadv_done:
 	cmpl	$DDI_DEV_AUTOINCR, 24(%esp)
 	je	gw_ioadv
 
-	rep	
+	rep
 	insw
 	popl	%edi
 	ret
@@ -1457,7 +1475,7 @@ gw_ioadv_done:
 	je	gl_ioadv
 
 	movq	%rsi, %rdi
-	rep	
+	rep
 	insl
 	ret
 
@@ -1490,7 +1508,7 @@ gl_ioadv_done:
 	cmpl	$DDI_DEV_AUTOINCR, 24(%esp)
 	je	gl_ioadv
 
-	rep	
+	rep
 	insl
 	popl	%edi
 	ret
@@ -1557,7 +1575,7 @@ i_ddi_io_rep_put32(ddi_acc_impl_t *hdlp, uint32_t *host_addr,
 	je	pb_ioadv
 
 	movq	%rsi, %rdi
-	rep	
+	rep
 	outsb
 	ret
 
@@ -1588,7 +1606,7 @@ pb_ioadv_done:
 	cmpl	$DDI_DEV_AUTOINCR, 24(%esp)
 	je	pb_ioadv
 
-	rep	
+	rep
 	outsb
 	popl	%esi
 	ret
@@ -1619,7 +1637,7 @@ pb_ioadv_done:
 	je	pw_ioadv
 
 	movq	%rsi, %rdi
-	rep	
+	rep
 	outsw
 	ret
 
@@ -1650,7 +1668,7 @@ pw_ioadv_done:
 	cmpl	$DDI_DEV_AUTOINCR, 24(%esp)
 	je	pw_ioadv
 
-	rep	
+	rep
 	outsw
 	popl	%esi
 	ret
@@ -1681,7 +1699,7 @@ pw_ioadv_done:
 	je	pl_ioadv
 
 	movq	%rsi, %rdi
-	rep	
+	rep
 	outsl
 	ret
 
@@ -1712,7 +1730,7 @@ pl_ioadv_done:
 	cmpl	$DDI_DEV_AUTOINCR, 24(%esp)
 	je	pl_ioadv
 
-	rep	
+	rep
 	outsl
 	popl	%esi
 	ret
diff --git a/usr/src/uts/intel/ia32/ml/exception.s b/usr/src/uts/intel/ia32/ml/exception.s
index e7fa6977f2..5806087ca1 100644
--- a/usr/src/uts/intel/ia32/ml/exception.s
+++ b/usr/src/uts/intel/ia32/ml/exception.s
@@ -153,7 +153,7 @@
 	/*
 	 * At this point the stack looks like this:
 	 *
-	 * (high address) 	r_ss
+	 * (high address)	r_ss
 	 *			r_rsp
 	 *			r_rfl
 	 *			r_cs
@@ -308,7 +308,7 @@
 	call	av_dispatch_nmivect
 
 	INTR_POP
-	call	*x86_md_clear
+	call	x86_md_clear
 	jmp	tr_iret_auto
 	/*NOTREACHED*/
 	SET_SIZE(nmiint)
@@ -1026,7 +1026,8 @@ check_for_user_address:
 	orl	%eax, %eax	/* (zero extend top 32-bits) */
 	leaq	fasttable(%rip), %r11
 	leaq	(%r11, %rax, CLONGSIZE), %r11
-	jmp	*(%r11)
+	movq	(%r11), %r11
+	INDIRECT_JMP_REG(r11)
 1:
 	/*
 	 * Fast syscall number was illegal.  Make it look
@@ -1086,7 +1087,7 @@ check_for_user_address:
 	ENTRY_NP(fast_null)
 	XPV_TRAP_POP
 	orq	$PS_C, 24(%rsp)	/* set carry bit in user flags */
-	call	*x86_md_clear
+	call	x86_md_clear
 	jmp	tr_iret_auto
 	/*NOTREACHED*/
 	SET_SIZE(fast_null)
diff --git a/usr/src/uts/intel/ia32/ml/hypersubr.s b/usr/src/uts/intel/ia32/ml/hypersubr.s
index f427aaff31..fb70bf1818 100644
--- a/usr/src/uts/intel/ia32/ml/hypersubr.s
+++ b/usr/src/uts/intel/ia32/ml/hypersubr.s
@@ -24,6 +24,10 @@
  * Use is subject to license terms.
  */
 
+/*
+ * Copyright 2019 Joyent, Inc.
+ */
+
 #include <sys/asm_linkage.h>
 #ifndef __xpv
 #include <sys/xpv_support.h>
@@ -34,12 +38,12 @@
  * Hypervisor "system calls"
  *
  * i386
- * 	%eax == call number
- * 	args in registers (%ebx, %ecx, %edx, %esi, %edi)
+ *	%eax == call number
+ *	args in registers (%ebx, %ecx, %edx, %esi, %edi)
  *
  * amd64
- * 	%rax == call number
- * 	args in registers (%rdi, %rsi, %rdx, %r10, %r8, %r9)
+ *	%rax == call number
+ *	args in registers (%rdi, %rsi, %rdx, %r10, %r8, %r9)
  *
  * Note that for amd64 we use %r10 instead of %rcx for passing 4th argument
  * as in C calling convention since the "syscall" instruction clobbers %rcx.
@@ -164,7 +168,7 @@ hypercall_page:
 #define	TRAP_INSTR			\
 	shll	$5, %eax;		\
 	addq	$hypercall_page, %rax;	\
-	jmp	*%rax
+	INDIRECT_JMP_REG(rax);
 #else
 #define	TRAP_INSTR			\
 	shll	$5, %eax;		\
@@ -182,7 +186,7 @@ hypercall_page:
 #endif /* !__xpv */
 
 
-#if defined(__amd64) 
+#if defined(__amd64)
 
 	ENTRY_NP(__hypercall0)
 	ALTENTRY(__hypercall0_int)
diff --git a/usr/src/uts/intel/ia32/ml/i86_subr.s b/usr/src/uts/intel/ia32/ml/i86_subr.s
index 072967fe07..746319e340 100644
--- a/usr/src/uts/intel/ia32/ml/i86_subr.s
+++ b/usr/src/uts/intel/ia32/ml/i86_subr.s
@@ -755,7 +755,7 @@ i86_mwait(uint32_t data, uint32_t extensions)
 
 	ENTRY_NP(i86_mwait)
 	pushq	%rbp
-	call	*x86_md_clear
+	call	x86_md_clear
 	movq	%rsp, %rbp
 	movq	%rdi, %rax		/* data */
 	movq	%rsi, %rcx		/* extensions */
@@ -1285,7 +1285,7 @@ efi_reset(void)
 #elif defined(__i386)
 	pop	%ebx
 #endif
-	ret	
+	ret
 	SET_SIZE(wait_500ms)
 
 #define	RESET_METHOD_KBC	1
@@ -1683,16 +1683,16 @@ repinsb(int port, uint8_t *addr, int count)
 #if defined(__amd64)
 
 	ENTRY(repinsb)
-	movl	%edx, %ecx	
+	movl	%edx, %ecx
 	movw	%di, %dx
 	movq	%rsi, %rdi
 	rep
 	  insb
-	ret		
+	ret
 	SET_SIZE(repinsb)
 
 #elif defined(__i386)
-	
+
 	/*
 	 * The arguments and saved registers are on the stack in the
 	 *  following order:
@@ -1737,7 +1737,7 @@ repinsd(int port, uint32_t *addr, int count)
 #else	/* __lint */
 
 #if defined(__amd64)
-	
+
 	ENTRY(repinsd)
 	movl	%edx, %ecx
 	movw	%di, %dx
@@ -1783,7 +1783,7 @@ repoutsb(int port, uint8_t *addr, int count)
 	movw	%di, %dx
 	rep
 	  outsb
-	ret	
+	ret
 	SET_SIZE(repoutsb)
 
 #elif defined(__i386)
@@ -1799,7 +1799,7 @@ repoutsb(int port, uint8_t *addr, int count)
 	ret
 	SET_SIZE(repoutsb)
 
-#endif	/* __i386 */	
+#endif	/* __i386 */
 #endif	/* __lint */
 
 /*
@@ -1822,7 +1822,7 @@ repoutsd(int port, uint32_t *addr, int count)
 	movw	%di, %dx
 	rep
 	  outsl
-	ret	
+	ret
 	SET_SIZE(repoutsd)
 
 #elif defined(__i386)
@@ -2283,7 +2283,7 @@ dtrace_interrupt_disable(void)
 	SET_SIZE(dtrace_interrupt_disable)
 
 #elif defined(__i386)
-		
+
 	ENTRY(dtrace_interrupt_disable)
 	pushfl
 	popl	%eax
@@ -2307,7 +2307,7 @@ dtrace_interrupt_disable(void)
 	ret
 	SET_SIZE(dtrace_interrupt_disable)
 
-#endif	/* __i386 */	
+#endif	/* __i386 */
 #endif	/* __lint */
 
 #if defined(__lint)
@@ -2341,7 +2341,7 @@ dtrace_interrupt_enable(dtrace_icookie_t cookie)
 	SET_SIZE(dtrace_interrupt_enable)
 
 #elif defined(__i386)
-		
+
 	ENTRY(dtrace_interrupt_enable)
 	movl	4(%esp), %eax
 	pushl	%eax
@@ -2362,7 +2362,7 @@ dtrace_interrupt_enable(dtrace_icookie_t cookie)
 	ret
 	SET_SIZE(dtrace_interrupt_enable)
 
-#endif	/* __i386 */	
+#endif	/* __i386 */
 #endif	/* __lint */
 
 
@@ -2399,7 +2399,7 @@ threadp(void)
 #else	/* __lint */
 
 #if defined(__amd64)
-	
+
 	ENTRY(threadp)
 	movq	%gs:CPU_THREAD, %rax
 	ret
@@ -2427,7 +2427,7 @@ ip_ocsum(
 	ushort_t *address,	/* ptr to 1st message buffer */
 	int halfword_count,	/* length of data */
 	unsigned int sum)	/* partial checksum */
-{ 
+{
 	int		i;
 	unsigned int	psum = 0;	/* partial sum */
 
@@ -2560,7 +2560,9 @@ ip_ocsum(
 	leaq	(%rdi, %rcx, 8), %rdi
 	xorl	%ecx, %ecx
 	clc
-	jmp 	*(%rdi)
+	/* XXX Probably should make this not a jump table */
+	movq	(%rdi), %rdi
+	INDIRECT_JMP_REG(rdi)
 
 	.align	8
 .ip_ocsum_jmptbl:
@@ -2671,7 +2673,7 @@ ip_ocsum(
 	lea	(%edi, %ecx, 4), %edi
 	xorl	%ecx, %ecx
 	clc
-	jmp 	*(%edi)
+	jmp	*(%edi)
 	SET_SIZE(ip_ocsum)
 
 	.data
@@ -2682,8 +2684,8 @@ ip_ocsum(
 	.long	.only24, .only28, .only32, .only36, .only40, .only44
 	.long	.only48, .only52, .only56, .only60
 
-	
-#endif	/* __i386 */		
+
+#endif	/* __i386 */
 #endif	/* __lint */
 
 /*
@@ -2707,7 +2709,7 @@ mul32(uint_t a, uint_t b)
 	xorl	%edx, %edx	/* XX64 joe, paranoia? */
 	movl	%edi, %eax
 	mull	%esi
-	shlq	$32, %rdx	
+	shlq	$32, %rdx
 	orq	%rdx, %rax
 	ret
 	SET_SIZE(mul32)
@@ -2854,7 +2856,7 @@ highbit64(uint64_t i)
 	ret
 0:
 	xorl	%eax, %eax
-	ret    
+	ret
 	SET_SIZE(highbit)
 
 	ENTRY(highbit64)
@@ -2908,7 +2910,7 @@ set_xcr(uint_t r, const uint64_t val)
 #define	XMSR_ACCESS_VAL		$0x9c5a203a
 
 #if defined(__amd64)
-	
+
 	ENTRY(rdmsr)
 	movl	%edi, %ecx
 	rdmsr
@@ -2981,7 +2983,7 @@ set_xcr(uint_t r, const uint64_t val)
 	ENTRY(wrmsr)
 	movl	4(%esp), %ecx
 	movl	8(%esp), %eax
-	movl	12(%esp), %edx 
+	movl	12(%esp), %edx
 	wrmsr
 	ret
 	SET_SIZE(wrmsr)
@@ -3003,7 +3005,7 @@ set_xcr(uint_t r, const uint64_t val)
 	movl	%esp, %ebp
 	movl	8(%esp), %ecx
 	movl	12(%esp), %eax
-	movl	16(%esp), %edx 
+	movl	16(%esp), %edx
 	pushl	%edi
 	movl	XMSR_ACCESS_VAL, %edi	/* this value is needed to access MSR */
 	wrmsr
@@ -3202,13 +3204,13 @@ dtrace_panic_trigger(int *tp)
 	lock
 	  xchgl	%edx, (%rdi)
 	cmpl	$0, %edx
-	je	0f 
+	je	0f
 	movl	$0, %eax
 	ret
 0:	movl	$1, %eax
 	ret
 	SET_SIZE(panic_trigger)
-	
+
 	ENTRY_NP(dtrace_panic_trigger)
 	xorl	%eax, %eax
 	movl	$0xdefacedd, %edx
@@ -3284,8 +3286,8 @@ dtrace_vpanic(const char *format, va_list alist)
 #if defined(__amd64)
 
 	ENTRY_NP(vpanic)			/* Initial stack layout: */
-	
-	pushq	%rbp				/* | %rip | 	0x60	*/
+
+	pushq	%rbp				/* | %rip |	0x60	*/
 	movq	%rsp, %rbp			/* | %rbp |	0x58	*/
 	pushfq					/* | rfl  |	0x50	*/
 	pushq	%r11				/* | %r11 |	0x48	*/
@@ -3385,8 +3387,8 @@ vpanic_common:
 	movq	%rcx, REGOFF_SS(%rsp)
 
 	/*
-	 * panicsys(format, alist, rp, on_panic_stack) 
-	 */	
+	 * panicsys(format, alist, rp, on_panic_stack)
+	 */
 	movq	REGOFF_RDI(%rsp), %rdi		/* format */
 	movq	REGOFF_RSI(%rsp), %rsi		/* alist */
 	movq	%rsp, %rdx			/* struct regs */
@@ -3410,7 +3412,7 @@ vpanic_common:
 
 	ENTRY_NP(dtrace_vpanic)			/* Initial stack layout: */
 
-	pushq	%rbp				/* | %rip | 	0x60	*/
+	pushq	%rbp				/* | %rip |	0x60	*/
 	movq	%rsp, %rbp			/* | %rbp |	0x58	*/
 	pushfq					/* | rfl  |	0x50	*/
 	pushq	%r11				/* | %r11 |	0x48	*/
@@ -3466,7 +3468,7 @@ vpanic_common:
 	/*
 	 * Now that we've got everything set up, store the register values as
 	 * they were when we entered vpanic() to the designated location in
-	 * the regs structure we allocated on the stack. 
+	 * the regs structure we allocated on the stack.
 	 */
 #if !defined(__GNUC_AS__)
 	movw	%gs, %edx
@@ -3610,7 +3612,8 @@ hrtime_t hrtime_base;
 	 * At worst, performing this now instead of under CLOCK_LOCK may
 	 * introduce some jitter in pc_gethrestime().
 	 */
-	call	*gethrtimef(%rip)
+	movq	gethrtimef(%rip), %rsi
+	INDIRECT_CALL_REG(rsi)
 	movq	%rax, %r8
 
 	leaq	hres_lock(%rip), %rax
@@ -3638,8 +3641,8 @@ hrtime_t hrtime_base;
 	addq	%r8, hrestime+8(%rip)	/* add interval to hrestime.tv_nsec */
 	/*
 	 * Now that we have CLOCK_LOCK, we can update hres_last_tick
-	 */ 	
-	movq	%r11, (%rax)	
+	 */
+	movq	%r11, (%rax)
 
 	call	__adj_hrestime
 
@@ -3650,7 +3653,7 @@ hrtime_t hrtime_base;
 	leave
 	ret
 	SET_SIZE(hres_tick)
-	
+
 #elif defined(__i386)
 
 	ENTRY_NP(hres_tick)
@@ -3693,13 +3696,13 @@ hrtime_t hrtime_base;
 	movl	%ebx, %edx
 	movl	%esi, %ecx
 
-	subl 	(%eax), %edx
-	sbbl 	4(%eax), %ecx
+	subl	(%eax), %edx
+	sbbl	4(%eax), %ecx
 
 	addl	%edx, hrtime_base	/ add interval to hrtime_base
 	adcl	%ecx, hrtime_base+4
 
-	addl 	%edx, hrestime+4	/ add interval to hrestime.tv_nsec
+	addl	%edx, hrestime+4	/ add interval to hrestime.tv_nsec
 
 	/
 	/ Now that we have CLOCK_LOCK, we can update hres_last_tick.
@@ -3933,9 +3936,9 @@ bcmp(const void *s1, const void *s2, size_t count)
 	movzbl	%dl, %eax
 	ret
 	SET_SIZE(bcmp)
-	
+
 #elif defined(__i386)
-	
+
 #define	ARG_S1		8
 #define	ARG_S2		12
 #define	ARG_LENGTH	16
@@ -4090,7 +4093,7 @@ switch_sp_and_call(void *newsp, void (*func)(uint_t, uint_t), uint_t arg1,
 	movq	%rdx, %rdi		/* pass func arg 1 */
 	movq	%rsi, %r11		/* save function to call */
 	movq	%rcx, %rsi		/* pass func arg 2 */
-	call	*%r11			/* call function */
+	INDIRECT_CALL_REG(r11)		/* call function */
 	leave				/* restore stack */
 	ret
 	SET_SIZE(switch_sp_and_call)
@@ -4140,7 +4143,7 @@ kmdb_enter(void)
 	call	intr_restore
 
 	leave
-	ret	
+	ret
 	SET_SIZE(kmdb_enter)
 
 #elif defined(__i386)
@@ -4164,7 +4167,7 @@ kmdb_enter(void)
 	addl	$4, %esp
 
 	leave
-	ret	
+	ret
 	SET_SIZE(kmdb_enter)
 
 #endif	/* __i386 */
@@ -4261,7 +4264,7 @@ ftrace_interrupt_disable(void)
 	SET_SIZE(ftrace_interrupt_disable)
 
 #elif defined(__i386)
-		
+
 	ENTRY(ftrace_interrupt_disable)
 	pushfl
 	popl	%eax
@@ -4269,7 +4272,7 @@ ftrace_interrupt_disable(void)
 	ret
 	SET_SIZE(ftrace_interrupt_disable)
 
-#endif	/* __i386 */	
+#endif	/* __i386 */
 #endif	/* __lint */
 
 #if defined(__lint)
@@ -4290,7 +4293,7 @@ ftrace_interrupt_enable(ftrace_icookie_t cookie)
 	SET_SIZE(ftrace_interrupt_enable)
 
 #elif defined(__i386)
-		
+
 	ENTRY(ftrace_interrupt_enable)
 	movl	4(%esp), %eax
 	pushl	%eax
@@ -4353,7 +4356,7 @@ mfence_insn(void)
  * depending on magic values in certain registers and modifies some registers
  * as a side effect.
  *
- * References: http://kb.vmware.com/kb/1009458 
+ * References: http://kb.vmware.com/kb/1009458
  */
 
 #if defined(__lint)
diff --git a/usr/src/uts/intel/ia32/ml/lock_prim.s b/usr/src/uts/intel/ia32/ml/lock_prim.s
index e67b1ef903..fa5aede007 100644
--- a/usr/src/uts/intel/ia32/ml/lock_prim.s
+++ b/usr/src/uts/intel/ia32/ml/lock_prim.s
@@ -587,7 +587,8 @@ mutex_exit(kmutex_t *lp)
 	pushq	%rbp				/* align stack properly */
 	movq	%rsp, %rbp
 	movl	%eax, %edi
-	call	*lockstat_probe
+	movq	lockstat_probe, %rax
+	INDIRECT_CALL_REG(rax)
 	leave					/* unwind stack */
 1:
 	movq	%gs:CPU_THREAD, %rdx		/* reload thread ptr */
@@ -609,7 +610,8 @@ mutex_exit(kmutex_t *lp)
 	pushq	%rbp				/* align stack properly */
 	movq	%rsp, %rbp
 	movl	%eax, %edi
-	call	*lockstat_probe
+	movq	lockstat_probe, %rax
+	INDIRECT_CALL_REG(rax)
 	leave					/* unwind stack */
 1:
 	movq	%gs:CPU_THREAD, %rdx		/* reload thread ptr */
diff --git a/usr/src/uts/intel/ia32/ml/retpoline.s b/usr/src/uts/intel/ia32/ml/retpoline.s
new file mode 100644
index 0000000000..cb6275a541
--- /dev/null
+++ b/usr/src/uts/intel/ia32/ml/retpoline.s
@@ -0,0 +1,322 @@
+/*
+ * This file and its contents are supplied under the terms of the
+ * Common Development and Distribution License ("CDDL"), version 1.0.
+ * You may only use this file in accordance with the terms of version
+ * 1.0 of the CDDL.
+ *
+ * A full copy of the text of the CDDL should have accompanied this
+ * source.  A copy of the CDDL is also available via the Internet at
+ * http://www.illumos.org/license/CDDL.
+ */
+
+/*
+ * Copyright 2019 Joyent, Inc.
+ */
+
+	.file	"retpoline.s"
+
+/*
+ * This file implements the various hooks that are needed for retpolines and
+ * return stack buffer (RSB) stuffing.
+ *
+ * ----------
+ * Background
+ * ----------
+ *
+ * There are various attacks that attempt to poison the CPU structures that are
+ * used when indirect function calls (such as using a function pointer /
+ * operations vector) are made or when we return from a function call. These
+ * issues were originally brought up with the Spectre v2 class of issues.
+ * However, it turned out that the RSB issues were further generalized to
+ * pre-Skylake Intel processors with the SpectreRSB paper.
+ *
+ * When thinking about all of these attacks, we need to consider what context
+ * can influence what context and how that will work. For example, some of the
+ * different cases that we need to consider include, but are not limited to:
+ *
+ *   - userland attacking userland
+ *   - userland attacking the kernel
+ *   - hardware virtual machines attacking the kernel
+ *   - hardware virtual machines attack another virtual machine
+ *
+ * There are two different sources of state that can be used to issue these
+ * attacks:
+ *   - The indirect branch predictors
+ *   - The return stack buffers (RSBs)
+ *
+ * To defend against these in the kernel, we use a combination of retpolines
+ * (indirect branch predictors), SMEP (RSBs), RSB stuffing (RSBs), and enhanced
+ * IBRS (indirect branch predictors, RSBs). To briefly summarize:
+ *
+ *     1. If enhanced IBRS is present, we must use that as retpolines may not
+ *        work and we've been told RSB stuffing will not be sufficient.
+ *
+ *     2. If enhanced IBRS is not present, then we enable the use of retpolines.
+ *
+ *     3. If enhanced IBRS is not present and SMEP is present, then we perform a
+ *        limited amount of RSB stuffing to protect ourselves from VMMs and to
+ *        help userland.
+ *
+ *     4. If enhanced IBRS is not present and SMEP is not present, then we have
+ *        to perform substantially more RSB stuffing to mitigate ourselves from
+ *        problems.
+ *
+ * For those that aren't running in the kernel, we provide an additional
+ * mechanism here that the kernel can use in various contexts, that of issuing
+ * an indirect branch prediction barrier. That takes care of both the indirect
+ * branch prediction and the return stack buffers. However, using it is not
+ * free. Therefore it is not used across the system by default and merely this
+ * is used to provide that infrastructure.
+ *
+ * ----------
+ * Retpolines
+ * ----------
+ *
+ * There are three different variants of retpolines that we might need to use.
+ *
+ *	Generic		This is the traditional retpoline and it is suitable for
+ *			use in all situations. This uses the macro
+ *			RETPOLINE_GENERIC.
+ *
+ *	AMD		This is a variant that exists for AMD that takes
+ *			advantage of the fact that lfence can be made a
+ *			'dispatch serializing' instruction. We have specific
+ *			conditions under which we'll use this. those conditions
+ *			are checked in uts/i86pc/os/cpuid.c. This uses the macro
+ *			RETPOLINE_MKLFENCE.
+ *
+ *	No-op		This is a variant that basically just is a simple jump
+ *			to the target register. This is used if retpolines are
+ *			disabled or if enhanced IBRS is enabled. This uses the
+ *			macro RETPOLINE_MKJUMP.
+ *
+ * The complier has arranged to call into a series of thunks
+ * (__x86_indirect_thunk_<reg>) whenever it needs to perform such a thing. This
+ * allows us to have a single place to modify all of the retpolines in the
+ * system, which we do need to do. In an ideal world, there would be a separate
+ * relocation that we'd use for this type of item and then krtld would be able
+ * to clean this up as appropriate without needing to pay the cost of jumping
+ * through the thunks. As part of cpuid_pass1 or after a microcode update, we'll
+ * go through and update all of the thunks based on the mitigation in question.
+ *
+ * By default, when we boot, we start with generic retpolines as the thunks.
+ * This is what the RETPOLINE_MKTHUNK macros do. These have the expected
+ * compiler names and these are what get coppied over by the actual thunk type
+ * we use at run-time if we need to change.
+ *
+ * Processors are being built with mitigations for Spectre v2. When that's the
+ * case, they'll end up supporting something called 'Enhanced IBRS (Indirect
+ * Branch Restricted Speculation)'. When this is the case, we actually need to
+ * disable retpolines as the processor claims that they'll interefer with them.
+ * Hence this is also why we use the No-op case.
+ *
+ * While the kernel is 64-bit only, dboot is still 32-bit, so there are a
+ * limited number of variants that are used for 32-bit. However as dboot is
+ * short lived and uses them sparingly, we only do the full variant and do not
+ * have an AMD specific version.
+ *
+ * ------------
+ * RSB Stuffing
+ * ------------
+ *
+ * RSB stuffing is a technique used to defend against the return stack buffer
+ * both overflowing and underflowing. The RSB can be poisoned just like the
+ * indirect branch predictor. One needs to clear the RSB when transitioning
+ * between two different privilege domains. Some examples include:
+ *
+ *   - Executing one process and then another
+ *   - Going between user land and the kernel
+ *   - Returning to the kernel from a hardware virtual machine
+ *
+ * Now, we end up having an ally here: SMEP (supervisor mode execution
+ * protection). When SMEP is enabled, which it always is by default on x86, then
+ * even when a user RSB entry is present in the kernel, it will not be used in a
+ * speculation as SMEP will stop it. This means, that if SMEP is present on the
+ * system, then we do not need to execute RSB stuffing upon entering the kernel
+ * from userland. At this time, we do not provide protection against RSB
+ * stuffing in cases that SMEP does not protect against.
+ *
+ * One final complication to the above is that when enhanced IBRS is present,
+ * then we know that RSB stuffing will not be sufficient. Therefore, the
+ * function that implements RSB stuffing __x86_rsb_stuff, has two nops at the
+ * start of it. The first will be turned into a ret instruction if we do not
+ * need to use this becaus either enhanced IBRS is present or the user has asked
+ * to disable mitigations.
+ *
+ * WARNING: the __x86_rsb_stuff functions are called from pretty arbitrary
+ * contexts. It's much easier for us to save and restore all the registers we
+ * touch rather than clobber them for callers. You must preserve this property
+ * or the system will panic at best.
+ *
+ * ----------------------------------
+ * Indirect Branch Prediction Barrier
+ * ----------------------------------
+ *
+ * An indirect branch prediction barrier (IBPB) is a means to cause all of the
+ * branch predictor and RSB state that could be used for speculation to not be
+ * after the barrier has completed. In this file we provide a means for using it
+ * if it is persent.
+ */
+
+#include <sys/asm_linkage.h>
+#include <sys/x86_archext.h>
+
+#if defined(__amd64)
+
+#define	RETPOLINE_MKTHUNK(reg) \
+	ENTRY(__x86_indirect_thunk_/**/reg)	\
+	call	2f;				\
+1:						\
+	pause;					\
+	lfence;					\
+	jmp	1b;				\
+2:						\
+	movq	%/**/reg, (%rsp);		\
+	ret;					\
+	SET_SIZE(__x86_indirect_thunk_/**/reg)
+
+#define	RETPOLINE_MKGENERIC(reg) \
+	ENTRY(__x86_indirect_thunk_gen_/**/reg)	\
+	call	2f;				\
+1:						\
+	pause;					\
+	lfence;					\
+	jmp	1b;				\
+2:						\
+	movq	%/**/reg, (%rsp);		\
+	ret;					\
+	SET_SIZE(__x86_indirect_thunk_gen_/**/reg)
+
+
+#define	RETPOLINE_MKLFENCE(reg)			\
+	ENTRY(__x86_indirect_thunk_amd_/**/reg)	\
+	lfence;					\
+	jmp	*%/**/reg;			\
+	SET_SIZE(__x86_indirect_thunk_amd_/**/reg)
+
+
+#define	RETPOLINE_MKJUMP(reg)			\
+	ENTRY(__x86_indirect_thunk_jmp_/**/reg)	\
+	jmp	*%/**/reg;			\
+	SET_SIZE(__x86_indirect_thunk_jmp_/**/reg)
+
+	RETPOLINE_MKTHUNK(rax)
+	RETPOLINE_MKTHUNK(rbx)
+	RETPOLINE_MKTHUNK(rcx)
+	RETPOLINE_MKTHUNK(rdx)
+	RETPOLINE_MKTHUNK(rdi)
+	RETPOLINE_MKTHUNK(rsi)
+	RETPOLINE_MKTHUNK(rbp)
+	RETPOLINE_MKTHUNK(r8)
+	RETPOLINE_MKTHUNK(r9)
+	RETPOLINE_MKTHUNK(r10)
+	RETPOLINE_MKTHUNK(r11)
+	RETPOLINE_MKTHUNK(r12)
+	RETPOLINE_MKTHUNK(r13)
+	RETPOLINE_MKTHUNK(r14)
+	RETPOLINE_MKTHUNK(r15)
+
+	RETPOLINE_MKGENERIC(rax)
+	RETPOLINE_MKGENERIC(rbx)
+	RETPOLINE_MKGENERIC(rcx)
+	RETPOLINE_MKGENERIC(rdx)
+	RETPOLINE_MKGENERIC(rdi)
+	RETPOLINE_MKGENERIC(rsi)
+	RETPOLINE_MKGENERIC(rbp)
+	RETPOLINE_MKGENERIC(r8)
+	RETPOLINE_MKGENERIC(r9)
+	RETPOLINE_MKGENERIC(r10)
+	RETPOLINE_MKGENERIC(r11)
+	RETPOLINE_MKGENERIC(r12)
+	RETPOLINE_MKGENERIC(r13)
+	RETPOLINE_MKGENERIC(r14)
+	RETPOLINE_MKGENERIC(r15)
+
+	RETPOLINE_MKLFENCE(rax)
+	RETPOLINE_MKLFENCE(rbx)
+	RETPOLINE_MKLFENCE(rcx)
+	RETPOLINE_MKLFENCE(rdx)
+	RETPOLINE_MKLFENCE(rdi)
+	RETPOLINE_MKLFENCE(rsi)
+	RETPOLINE_MKLFENCE(rbp)
+	RETPOLINE_MKLFENCE(r8)
+	RETPOLINE_MKLFENCE(r9)
+	RETPOLINE_MKLFENCE(r10)
+	RETPOLINE_MKLFENCE(r11)
+	RETPOLINE_MKLFENCE(r12)
+	RETPOLINE_MKLFENCE(r13)
+	RETPOLINE_MKLFENCE(r14)
+	RETPOLINE_MKLFENCE(r15)
+
+	RETPOLINE_MKJUMP(rax)
+	RETPOLINE_MKJUMP(rbx)
+	RETPOLINE_MKJUMP(rcx)
+	RETPOLINE_MKJUMP(rdx)
+	RETPOLINE_MKJUMP(rdi)
+	RETPOLINE_MKJUMP(rsi)
+	RETPOLINE_MKJUMP(rbp)
+	RETPOLINE_MKJUMP(r8)
+	RETPOLINE_MKJUMP(r9)
+	RETPOLINE_MKJUMP(r10)
+	RETPOLINE_MKJUMP(r11)
+	RETPOLINE_MKJUMP(r12)
+	RETPOLINE_MKJUMP(r13)
+	RETPOLINE_MKJUMP(r14)
+	RETPOLINE_MKJUMP(r15)
+
+	ENTRY(x86_rsb_stuff)
+	nop
+	nop
+	pushq	%rdi
+	pushq	%rax
+	movl	$16, %edi
+	movq	%rsp, %rax
+rsb_loop:
+	call	2f
+1:
+	pause
+	call	1b
+2:
+	call	2f
+1:
+	pause
+	call	1b
+2:
+	subl	$1, %edi
+	jnz	rsb_loop
+	movq	%rax, %rsp
+	popq	%rax
+	popq	%rdi
+	ret
+	SET_SIZE(x86_rsb_stuff)
+
+	ENTRY(x86_indirect_branch_barrier)
+	ret
+	nop
+	movl	$MSR_IA32_PRED_CMD, %ecx
+	xorq	%rdx, %rdx
+	movl	$IA32_PRED_CMD_IBPB, %eax
+	wrmsr
+	ret
+	SET_SIZE(x86_indirect_branch_barrier)
+
+#elif defined(__i386)
+
+#define	RETPOLINE_MKTHUNK(reg) \
+	ENTRY(__x86_indirect_thunk_/**/reg)	\
+	call	2f;				\
+1:						\
+	pause;					\
+	lfence;					\
+	jmp	1b;				\
+2:						\
+	movl	%/**/reg, (%esp);		\
+	ret;					\
+	SET_SIZE(__x86_indirect_thunk_/**/reg)
+
+	RETPOLINE_MKTHUNK(edi)
+	RETPOLINE_MKTHUNK(eax)
+
+#else
+#error	"Your architecture is in another castle."
+#endif
diff --git a/usr/src/uts/intel/ia32/ml/swtch.s b/usr/src/uts/intel/ia32/ml/swtch.s
index d0696652b7..22ff14eeef 100644
--- a/usr/src/uts/intel/ia32/ml/swtch.s
+++ b/usr/src/uts/intel/ia32/ml/swtch.s
@@ -170,6 +170,14 @@
 	movq	%rsi, T_USERACC(%rax)
 	call	smap_enable
 
+	/*
+	 * Take a moment to potentially clear the RSB buffer. This is done to
+	 * prevent various Spectre variant 2 and SpectreRSB attacks. This may
+	 * not be sufficient. Please see uts/intel/ia32/ml/retpoline.s for more
+	 * information about this.
+	 */
+	call	x86_rsb_stuff
+
 	/*
 	 * Save non-volatile registers, and set return address for current
 	 * thread to resume_return.
@@ -211,7 +219,7 @@
 	/*
 	 * Temporarily switch to the idle thread's stack
 	 */
-	movq	CPU_IDLE_THREAD(%r15), %rax 	/* idle thread pointer */
+	movq	CPU_IDLE_THREAD(%r15), %rax	/* idle thread pointer */
 
 	/*
 	 * Set the idle thread as the current thread
@@ -246,7 +254,7 @@
 	 */
 .lock_thread_mutex:
 	lock
-	btsl	$0, T_LOCK(%r12) 	/* attempt to lock new thread's mutex */
+	btsl	$0, T_LOCK(%r12)	/* attempt to lock new thread's mutex */
 	jnc	.thread_mutex_locked	/* got it */
 
 .spin_thread_mutex:
@@ -302,8 +310,8 @@
 	movq	%r12, CPU_THREAD(%r13)	/* set CPU's thread pointer */
 	mfence				/* synchronize with mutex_exit() */
 	xorl	%ebp, %ebp		/* make $<threadlist behave better */
-	movq	T_LWP(%r12), %rax 	/* set associated lwp to  */
-	movq	%rax, CPU_LWP(%r13) 	/* CPU's lwp ptr */
+	movq	T_LWP(%r12), %rax	/* set associated lwp to  */
+	movq	%rax, CPU_LWP(%r13)	/* CPU's lwp ptr */
 
 	movq	T_SP(%r12), %rsp	/* switch to outgoing thread's stack */
 	movq	T_PC(%r12), %r13	/* saved return addr */
@@ -481,7 +489,7 @@ resume_from_intr_return:
 	/*
 	 * Remove stack frame created in SAVE_REGS()
 	 */
-	addq 	$CLONGSIZE, %rsp
+	addq	$CLONGSIZE, %rsp
 	ret
 	SET_SIZE(resume_from_intr)
 
@@ -490,7 +498,7 @@ resume_from_intr_return:
 	popq	%rdi		/* arg */
 	popq	%rsi		/* len */
 	movq	%rsp, %rbp
-	call	*%rax
+	INDIRECT_CALL_REG(rax)
 	call	thread_exit	/* destroy thread if it returns. */
 	/*NOTREACHED*/
 	SET_SIZE(thread_start)
@@ -500,7 +508,7 @@ resume_from_intr_return:
 	movq	%rsp, %rbp		/* construct frame */
 	movq	%rdi, %rsp		/* set stack pinter */
 	movq	%rdx, %rdi		/* load arg */
-	call	*%rsi			/* call specified function */
+	INDIRECT_CALL_REG(rsi)		/* call specified function */
 	leave				/* pop base pointer */
 	ret
 	SET_SIZE(thread_splitstack_run)
diff --git a/usr/src/uts/intel/ia32/sys/asm_linkage.h b/usr/src/uts/intel/ia32/sys/asm_linkage.h
index ad6fbc6861..b78e519f34 100644
--- a/usr/src/uts/intel/ia32/sys/asm_linkage.h
+++ b/usr/src/uts/intel/ia32/sys/asm_linkage.h
@@ -24,11 +24,13 @@
  * Use is subject to license terms.
  */
 
+/*
+ * Copyright 2019 Joyent, Inc.
+ */
+
 #ifndef _IA32_SYS_ASM_LINKAGE_H
 #define	_IA32_SYS_ASM_LINKAGE_H
 
-#pragma ident	"%Z%%M%	%I%	%E% SMI"
-
 #include <sys/stack.h>
 #include <sys/trap.h>
 
@@ -300,6 +302,38 @@ name:
 
 #endif  /* __i386 */
 
+/*
+ * These macros should be used when making indirect calls in the kernel. They
+ * will perform a jump or call to the corresponding register in a way that knows
+ * about retpolines and handles whether such mitigations are enabled or not.
+ *
+ * INDIRECT_JMP_REG will jump to named register. INDIRECT_CALL_REG will instead
+ * do a call. These macros cannot be used to dereference a register. For
+ * example, if you need to do something that looks like the following:
+ *
+ *	call	*24(%rdi)
+ *	jmp	*(%r15)
+ *
+ * You must instead first do a movq into the corresponding location. If you are
+ * trying to call a global function, then use the following pattern
+ * (substituting the register in question)
+ *
+ *	leaq	my_favorte_function(%rip), %rax
+ *	INDIRECT_CALL_REG(rax)
+ *
+ * If you instead have a function pointer (say gethrtimef for example), then you
+ * instead need to do:
+ *
+ *	movq	my_favorte_function_pointer(%rip), %rax
+ *	INDIRECT_CALL_REG(rax)
+ */
+
+/* CSTYLED */
+#define	INDIRECT_JMP_REG(reg)	jmp	__x86_indirect_thunk_/**/reg;
+
+/* CSTYLED */
+#define	INDIRECT_CALL_REG(reg)	call	__x86_indirect_thunk_/**/reg;
+
 #endif /* _ASM */
 
 #ifdef	__cplusplus
diff --git a/usr/src/uts/intel/sys/x86_archext.h b/usr/src/uts/intel/sys/x86_archext.h
index ff1c3e4a1f..b590303074 100644
--- a/usr/src/uts/intel/sys/x86_archext.h
+++ b/usr/src/uts/intel/sys/x86_archext.h
@@ -584,6 +584,15 @@ extern "C" {
 #define	IA32_PKG_THERM_INTERRUPT_TR2_IE		0x00800000
 #define	IA32_PKG_THERM_INTERRUPT_PL_NE		0x01000000
 
+/*
+ * This MSR exists on families, 10h, 12h+ for AMD. This controls how instruction
+ * decoding is controlled. Most notably, for the AMD variant of retpolines, we
+ * must improve the serializability of lfence for the lfence based method to
+ * work.
+ */
+#define	MSR_AMD_DECODE_CONFIG			0xc0011029
+#define	AMD_DECODE_CONFIG_LFENCE_DISPATCH	0x02
+
 #define	MCI_CTL_VALUE		0xffffffff
 
 #define	MTRR_TYPE_UC		0
@@ -1015,7 +1024,7 @@ extern "C" {
 #define	INTC_MODEL_BROADWELL_XEON	0x4f
 #define	INTC_MODEL_BROADWELL_XEON_D	0x56
 
-#define	INCC_MODEL_SKYLAKE_MOBILE	0x4e
+#define	INTC_MODEL_SKYLAKE_MOBILE	0x4e
 #define	INTC_MODEL_SKYLAKE_XEON		0x55
 #define	INTC_MODEL_SKYLAKE_DESKTOP	0x5e
 
@@ -1092,21 +1101,19 @@ extern uint_t pentiumpro_bug4046376;
 
 extern const char CyrixInstead[];
 
+/*
+ * These functions are all used to perform various side-channel mitigations.
+ * Please see uts/i86pc/os/cpuid.c for more information.
+ */
 extern void (*spec_uarch_flush)(void);
+extern void x86_indirect_branch_barrier(void);
+extern void x86_rsb_stuff(void);
+extern void x86_md_clear(void);
 
 #endif
 
 #if defined(_KERNEL)
 
-/*
- * x86_md_clear is the main entry point that should be called to deal with
- * clearing u-arch buffers. Implementations are below because they're
- * implemented in ASM. They shouldn't be used.
- */
-extern void (*x86_md_clear)(void);
-extern void x86_md_clear_noop(void);
-extern void x86_md_clear_verw(void);
-
 /*
  * This structure is used to pass arguments and get return values back
  * from the CPUID instruction in __cpuid_insn() routine.
