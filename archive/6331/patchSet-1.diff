commit 14fffc7b151fef617081fa74443ffd8096202e07
Author: Patrick Mooney <pmooney@pfmooney.com>
Date:   2019-05-21T00:25:50+00:00 (5 months ago)
    
    OS-7804 viona VTNET_MAXSEGS is inadequate

diff --git a/usr/src/uts/i86pc/io/viona/viona.c b/usr/src/uts/i86pc/io/viona/viona.c
index 030c2dabad..d81550449f 100644
--- a/usr/src/uts/i86pc/io/viona/viona.c
+++ b/usr/src/uts/i86pc/io/viona/viona.c
@@ -267,8 +267,6 @@
 #define	VIONA_MAX_HDRS_LEN	(sizeof (struct ether_vlan_header) + \
 	IP_MAX_HDR_LENGTH + TCP_MAX_HDR_LENGTH)
 
-#define	VTNET_MAXSEGS		32
-
 #define	VRING_ALIGN		4096
 #define	VRING_MAX_LEN		32768
 
@@ -389,7 +387,8 @@ enum viona_ring_state {
 };
 enum viona_ring_state_flags {
 	VRSF_REQ_START	= 0x1,	/* start running from INIT state */
-	VRSF_REQ_STOP	= 0x2,	/* stop running, clean up, goto RESET state */
+	VRSF_REQ_STOP	= 0x2,	/* stop req'd, clean up and goto RESET state */
+	VRSF_STOPPING	= 0x4,	/* stop in progress */
 };
 
 #define	VRING_NEED_BAIL(ring, proc)					\
@@ -423,6 +422,10 @@ typedef struct viona_vring {
 	uint16_t	vr_mask;	/* cached from vr_size */
 	uint16_t	vr_cur_aidx;	/* trails behind 'avail_idx' */
 
+	/* Dynamic (read: ring-sized) data storage for RX/TX operations. */
+	struct iovec		*vr_iov;	/* vq_popchain results */
+	struct virtio_used	*vr_uelem;	/* mrgrx tracking */
+
 	/* Host-context pointers to the queue */
 	volatile struct virtio_desc	*vr_descr;
 
@@ -529,11 +532,6 @@ typedef struct viona_soft_state {
 	list_node_t		ss_node;
 } viona_soft_state_t;
 
-typedef struct used_elem {
-	uint16_t	id;
-	uint32_t	len;
-} used_elem_t;
-
 static void			*viona_state;
 static dev_info_t		*viona_dip;
 static id_space_t		*viona_minors;
@@ -1263,16 +1261,32 @@ viona_ring_alloc(viona_link_t *link, viona_vring_t *ring)
 }
 
 static void
-viona_ring_desb_free(viona_vring_t *ring)
+viona_ring_dynamic_free(viona_vring_t *ring)
 {
-	viona_desb_t *dp = ring->vr_desb;
+	const uint_t cnt = ring->vr_size;
 
-	for (uint_t i = 0; i < ring->vr_size; i++, dp++) {
-		kmem_free(dp->d_headers, VIONA_MAX_HDRS_LEN);
-	}
+	/*
+	 * Some resources are dynamically allocated at ring initialization time
+	 * based on type (RX/TX) and size.
+	 */
+	if (ring->vr_desb != NULL) {
+		viona_desb_t *dp = ring->vr_desb;
+
+		for (uint_t i = 0; i < cnt; i++, dp++) {
+			kmem_free(dp->d_headers, VIONA_MAX_HDRS_LEN);
+		}
 
-	kmem_free(ring->vr_desb, sizeof (viona_desb_t) * ring->vr_size);
-	ring->vr_desb = NULL;
+		kmem_free(ring->vr_desb, sizeof (viona_desb_t) * cnt);
+		ring->vr_desb = NULL;
+	}
+	if (ring->vr_iov != NULL) {
+		kmem_free(ring->vr_iov, sizeof (struct iovec) * cnt);
+		ring->vr_iov = NULL;
+	}
+	if (ring->vr_uelem != NULL) {
+		kmem_free(ring->vr_uelem, sizeof (struct virtio_used) * cnt);
+		ring->vr_uelem = NULL;
+	}
 }
 
 static void
@@ -1398,6 +1412,17 @@ viona_ioc_ring_init(viona_link_t *link, void *udata, int md)
 		}
 	}
 
+	/* Allocate iovecs for descriptor lookups */
+	ring->vr_iov = kmem_alloc(sizeof (struct iovec) * cnt, KM_SLEEP);
+	if (kri.ri_index == VIONA_VQ_RX) {
+		/*
+		 * Keep an ring-sized array of used descriptor entries for
+		 * merged-RX operations.
+		 */
+		ring->vr_uelem = kmem_alloc(sizeof (struct virtio_used) * cnt,
+		    KM_SLEEP);
+	}
+
 	/* Zero out MSI-X configuration */
 	ring->vr_msi_addr = 0;
 	ring->vr_msi_msg = 0;
@@ -1417,9 +1442,7 @@ viona_ioc_ring_init(viona_link_t *link, void *udata, int md)
 	return (0);
 
 fail:
-	if (ring->vr_desb != NULL) {
-		viona_ring_desb_free(ring);
-	}
+	viona_ring_dynamic_free(ring);
 	ring->vr_size = 0;
 	ring->vr_mask = 0;
 	ring->vr_descr = NULL;
@@ -1586,6 +1609,16 @@ viona_worker_rx(viona_vring_t *ring, viona_link_t *link)
 	} while (!VRING_NEED_BAIL(ring, p));
 
 	*ring->vr_used_flags &= ~VRING_USED_F_NO_NOTIFY;
+
+	/*
+	 * With VRSF_STOPPING asserted on the ring, initiate a mac RX barrier
+	 * to ensure that any threads in the midst of an RX callback on this
+	 * ring have completed.
+	 */
+	ring->vr_state_flags |= VRSF_STOPPING;
+	mutex_exit(&ring->vr_lock);
+	mac_rx_barrier(link->l_mch);
+	mutex_enter(&ring->vr_lock);
 }
 
 static void
@@ -1650,6 +1683,7 @@ viona_worker_tx(viona_vring_t *ring, viona_link_t *link)
 
 	ASSERT(MUTEX_HELD(&ring->vr_lock));
 
+	ring->vr_state_flags |= VRSF_STOPPING;
 	while (ring->vr_xfer_outstanding != 0) {
 		/*
 		 * Paying heed to signals is counterproductive here.  This is a
@@ -1658,11 +1692,6 @@ viona_worker_tx(viona_vring_t *ring, viona_link_t *link)
 		 */
 		cv_wait(&ring->vr_cv, &ring->vr_lock);
 	}
-
-	/* Free any desb resources before the ring is completely stopped */
-	if (ring->vr_desb != NULL) {
-		viona_ring_desb_free(ring);
-	}
 }
 
 static void
@@ -1706,17 +1735,26 @@ viona_worker(void *arg)
 	}
 
 cleanup:
-	/* Free any desb resources before the ring is completely stopped */
 	if (ring->vr_desb != NULL) {
+		/* All TX transfers must be completed before clean-up. */
 		VERIFY(ring->vr_xfer_outstanding == 0);
-		viona_ring_desb_free(ring);
 	}
+	viona_ring_dynamic_free(ring);
 
 	ring->vr_cur_aidx = 0;
-	ring->vr_state = VRS_RESET;
-	ring->vr_state_flags = 0;
 	ring->vr_worker_thread = NULL;
 	cv_broadcast(&ring->vr_cv);
+
+	/*
+	 * Since the RX callbacks access vr_state and vr_state_flags without
+	 * holding vr_lock, ensure that the state change is visible in a
+	 * predictable manner. (See the short-circuit drop logic in
+	 * viona_rx_classified and viona_rx_mcast)
+	 */
+	ring->vr_state = VRS_RESET;
+	membar_enter();
+	ring->vr_state_flags = 0;
+
 	mutex_exit(&ring->vr_lock);
 
 	mutex_enter(&ttoproc(curthread)->p_lock);
@@ -1784,19 +1822,22 @@ viona_ioc_intr_poll(viona_link_t *link, void *udata, int md, int *rv)
 }
 
 static int
-vq_popchain(viona_vring_t *ring, struct iovec *iov, int niov, uint16_t *cookie)
+vq_popchain(viona_vring_t *ring, struct iovec **iovpp, uint16_t *cookie)
 {
 	viona_link_t *link = ring->vr_link;
+	const uint_t niov = ring->vr_size;
+	struct iovec *iov = ring->vr_iov;
 	uint_t i, ndesc, idx, head, next;
 	struct virtio_desc vdir;
 	void *buf;
 
 	ASSERT(iov != NULL);
-	ASSERT(niov > 0);
+	ASSERT(niov > 0 && niov <= VRING_MAX_LEN);
 
 	mutex_enter(&ring->vr_a_mutex);
 	idx = ring->vr_cur_aidx;
 	ndesc = (uint16_t)((unsigned)*ring->vr_avail_idx - (unsigned)idx);
+	*iovpp = iov;
 
 	if (ndesc == 0) {
 		mutex_exit(&ring->vr_a_mutex);
@@ -1954,7 +1995,8 @@ vq_pushchain(viona_vring_t *ring, uint32_t len, uint16_t cookie)
 }
 
 static void
-vq_pushchain_mrgrx(viona_vring_t *ring, int num_bufs, used_elem_t *elem)
+vq_pushchain_mrgrx(viona_vring_t *ring, int num_bufs,
+    struct virtio_used *uelem)
 {
 	volatile struct virtio_used *vu;
 	uint_t uidx, i;
@@ -1964,13 +2006,11 @@ vq_pushchain_mrgrx(viona_vring_t *ring, int num_bufs, used_elem_t *elem)
 	uidx = *ring->vr_used_idx;
 	if (num_bufs == 1) {
 		vu = &ring->vr_used_ring[uidx++ & ring->vr_mask];
-		vu->vu_idx = elem[0].id;
-		vu->vu_tlen = elem[0].len;
+		*vu = uelem[0];
 	} else {
 		for (i = 0; i < num_bufs; i++) {
 			vu = &ring->vr_used_ring[(uidx + i) & ring->vr_mask];
-			vu->vu_idx = elem[i].id;
-			vu->vu_tlen = elem[i].len;
+			*vu = uelem[i];
 		}
 		uidx = uidx + num_bufs;
 	}
@@ -2062,7 +2102,7 @@ viona_copy_mblk(const mblk_t *mp, size_t seek, caddr_t buf, size_t len,
 static int
 viona_recv_plain(viona_vring_t *ring, const mblk_t *mp, size_t msz)
 {
-	struct iovec iov[VTNET_MAXSEGS];
+	struct iovec *iov;
 	uint16_t cookie;
 	int n;
 	const size_t hdr_sz = sizeof (struct virtio_net_hdr);
@@ -2074,7 +2114,7 @@ viona_recv_plain(viona_vring_t *ring, const mblk_t *mp, size_t msz)
 
 	ASSERT(msz >= MIN_BUF_SIZE);
 
-	n = vq_popchain(ring, iov, VTNET_MAXSEGS, &cookie);
+	n = vq_popchain(ring, &iov, &cookie);
 	if (n <= 0) {
 		/* Without available buffers, the frame must be dropped. */
 		return (ENOSPC);
@@ -2157,8 +2197,9 @@ bad_frame:
 static int
 viona_recv_merged(viona_vring_t *ring, const mblk_t *mp, size_t msz)
 {
-	struct iovec iov[VTNET_MAXSEGS];
-	used_elem_t uelem[VTNET_MAXSEGS];
+	struct iovec *iov;
+	struct virtio_used *uelem = ring->vr_uelem;
+	const uint_t max_idx = (ring->vr_size - 1);
 	int n, i = 0, buf_idx = 0, err = 0;
 	uint16_t cookie;
 	caddr_t buf;
@@ -2170,7 +2211,7 @@ viona_recv_merged(viona_vring_t *ring, const mblk_t *mp, size_t msz)
 
 	ASSERT(msz >= MIN_BUF_SIZE);
 
-	n = vq_popchain(ring, iov, VTNET_MAXSEGS, &cookie);
+	n = vq_popchain(ring, &iov, &cookie);
 	if (n <= 0) {
 		/* Without available buffers, the frame must be dropped. */
 		VIONA_PROBE2(no_space, viona_vring_t *, ring, mblk_t *, mp);
@@ -2184,8 +2225,8 @@ viona_recv_merged(viona_vring_t *ring, const mblk_t *mp, size_t msz)
 		 * act of desperation.
 		 */
 		bzero(iov[0].iov_base, iov[0].iov_len);
-		uelem[0].id = cookie;
-		uelem[0].len = iov[0].iov_len;
+		uelem[0].vu_idx = cookie;
+		uelem[0].vu_tlen = iov[0].iov_len;
 		err = EINVAL;
 		goto done;
 	}
@@ -2218,23 +2259,24 @@ viona_recv_merged(viona_vring_t *ring, const mblk_t *mp, size_t msz)
 			i++;
 		}
 
-		uelem[buf_idx].id = cookie;
-		uelem[buf_idx].len = chunk;
+		uelem[buf_idx].vu_idx = cookie;
+		uelem[buf_idx].vu_tlen = chunk;
 
 		/*
 		 * Try to grab another buffer from the ring if the mblk has not
 		 * yet been entirely copied out.
 		 */
 		if (!end) {
-			if (buf_idx == (VTNET_MAXSEGS - 1)) {
+			if (buf_idx == max_idx) {
 				/*
-				 * Our arbitrary limit on the number of buffers
-				 * to offer for merge has already been reached.
+				 * This merged RX operation has consumed every
+				 * descriptor in the ring.  It cannot be
+				 * allowed to go any further.
 				 */
 				err = EOVERFLOW;
 				break;
 			}
-			n = vq_popchain(ring, iov, VTNET_MAXSEGS, &cookie);
+			n = vq_popchain(ring, &iov, &cookie);
 			if (n <= 0) {
 				/*
 				 * Without more immediate space to perform the
@@ -2257,7 +2299,7 @@ viona_recv_merged(viona_vring_t *ring, const mblk_t *mp, size_t msz)
 	} while (!end && copied < msz);
 
 	/* Account for the header size in the first buffer */
-	uelem[0].len += hdr_sz;
+	uelem[0].vu_tlen += hdr_sz;
 
 	/*
 	 * If no other errors were encounted during the copy, was the expected
@@ -2515,8 +2557,9 @@ viona_rx_classified(void *arg, mac_resource_handle_t mrh, mblk_t *mp,
 {
 	viona_vring_t *ring = (viona_vring_t *)arg;
 
-	/* Immediately drop traffic if ring is inactive */
-	if (ring->vr_state != VRS_RUN) {
+	/* Immediately drop traffic if ring is inactive or stopping */
+	if (ring->vr_state != VRS_RUN ||
+	    (ring->vr_state_flags & VRSF_STOPPING) != 0) {
 		freemsgchain(mp);
 		return;
 	}
@@ -2533,8 +2576,9 @@ viona_rx_mcast(void *arg, mac_resource_handle_t mrh, mblk_t *mp,
 	mblk_t *mp_mcast_only = NULL;
 	mblk_t **mpp = &mp_mcast_only;
 
-	/* Immediately drop traffic if ring is inactive */
-	if (ring->vr_state != VRS_RUN) {
+	/* Immediately drop traffic if ring is inactive or stopping */
+	if (ring->vr_state != VRS_RUN ||
+	    (ring->vr_state_flags & VRSF_STOPPING) != 0) {
 		freemsgchain(mp);
 		return;
 	}
@@ -2814,7 +2858,7 @@ viona_tx_csum(viona_vring_t *ring, const struct virtio_net_hdr *hdr,
 static void
 viona_tx(viona_link_t *link, viona_vring_t *ring)
 {
-	struct iovec		iov[VTNET_MAXSEGS];
+	struct iovec		*iov;
 	uint16_t		cookie;
 	int			i, n;
 	uint32_t		len, base_off = 0;
@@ -2826,7 +2870,7 @@ viona_tx(viona_link_t *link, viona_vring_t *ring)
 
 	mp_head = mp_tail = NULL;
 
-	n = vq_popchain(ring, iov, VTNET_MAXSEGS, &cookie);
+	n = vq_popchain(ring, &iov, &cookie);
 	if (n <= 0) {
 		VIONA_PROBE1(tx_absent, viona_vring_t *, ring);
 		VIONA_RING_STAT_INCR(ring, tx_absent);
