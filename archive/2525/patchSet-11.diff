commit a5ab3e0f7db67e28043d945c0757acf91388afdc (refs/changes/25/2525/11)
Author: Julien Gilli <julien.gilli@joyent.com>
Date:   2017-11-03T14:57:15-07:00 (1 year, 11 months ago)
    
    DOCKER-863: Create shared volume when mounting nonexistent volume
    DOCKER-881: docker start should provision shared volume only if required
    VOLAPI-26: wf-runner crashes with: Uncaught AssertionError: "string" == "object"
    VOLAPI-9 Implement asynchronous NFS shared volume creation
    VOLAPI-6: Deleting a tritonnfs volume used by an active Docker container should fail
    VOLAPI-27 Divergence in `docker create -v` behavior; volume is not created until `docker start`
    VOLAPI-54 Custom errors' rest codes should follow engineering guide's guidelines
    VOLAPI-55 After enabling volapi, a sdcadm update without volapi fails
    DOCKER-1034 support docker volume ls -f dangling=true
    VOLAPI-65 rename required_nfs_volumes VM objects' property to volumes
    TOOLS-1830 replace sdcadm experimental nfs-volumes with cloudapi-nfs-volumes/docker-nfs-volumes subcommands
    ZAPI-796 NFS volumes provisioning tasks chain broken by switch from node-uuid to uuid module in sdc-workflow
    DOCKER-1088 sdc-docker should not modify VMs' internal docker:nfsvolumes metadata
    PUBAPI-1420 Add ability to mount NFS volumes with CreateMachine endpoint
    ZAPI-794 remove_volume_references task in destroy workflow should not error

diff --git a/lib/apis/moray.js b/lib/apis/moray.js
index c314a3d..a689f09 100644
--- a/lib/apis/moray.js
+++ b/lib/apis/moray.js
@@ -823,6 +823,8 @@ Moray.prototype.markAsDestroyed = function markAsDestroyed(vm, callback) {
 
     var self = this;
 
+    self.log.debug({vm: vm}, 'Marking VM as destroyed');
+
     var oldVm = jsprim.deepCopy(vm);
 
     if (!self.bucketsSetup()) {
diff --git a/lib/common/validation.js b/lib/common/validation.js
index 828e900..67f46c9 100644
--- a/lib/common/validation.js
+++ b/lib/common/validation.js
@@ -21,6 +21,7 @@ var libuuid = require('libuuid');
 var net = require('net');
 var restify = require('restify');
 var tritonTags = require('triton-tags');
+var util = require('util');
 var verror = require('verror');
 
 var common = require('./vm-common');
@@ -47,6 +48,8 @@ var PW_SUFFIX = /^(.*)_pw$/;
 
 var DOCKER_TAG_RE = DOCKER_TAG_DEFAULT_RE;
 
+var VOLUME_NAME_RE = /^[a-zA-Z0-9][a-zA-Z0-9_\.\-]+$/;
+
 var MAX_LIST_VMS_LIMIT = 1000;
 exports.MAX_LIST_VMS_LIMIT = MAX_LIST_VMS_LIMIT;
 
@@ -257,6 +260,10 @@ var VM_FIELDS = [
         name: 'vcpus',
         mutable: false
     },
+    {
+        name: 'volumes',
+        mutable: false
+    },
     {
         name: 'zfs_data_compression',
         mutable: true
@@ -440,6 +447,8 @@ var validators = {
 
     vcpus: createValidateNumberFn('vcpus', {min: 1}),
 
+    volumes: createValidateVolumes('volumes', false),
+
     zfs_data_compression: createValidateStringFn('zfs_data_compression'),
 
     zfs_io_priority: createValidateNumberFn('zfs_io_priority'),
@@ -495,8 +504,10 @@ function createValidateMetadataFn(field) {
 /*
  * Returns a validateArray function
  */
-function createValidateArrayFn(field) {
-    return function (params) {
+function createValidateArrayFn(field, options) {
+    assert.string(field, 'field');
+
+    return function validateArray(params) {
         var errs = [];
 
         if (params[field] !== undefined && !Array.isArray(params[field])) {
@@ -861,6 +872,104 @@ function createValidateMarkerFn(field, options) {
     };
 }
 
+function validVolumeName(volumeName) {
+    if (volumeName === undefined || volumeName === null ||
+        !VOLUME_NAME_RE.test(volumeName)) {
+        return false;
+    }
+
+    return true;
+}
+
+function validVolumeMountpoint(mountpoint) {
+    if (typeof (mountpoint) !== 'string') {
+        return false;
+    }
+
+    /*
+     * Must start with '/'.
+     */
+    if (mountpoint.length === 0 || mountpoint[0] !== '/') {
+        return false;
+    }
+
+    /*
+     * Must not contain the null character.
+     */
+    if (mountpoint.indexOf('\0') !== -1) {
+        return false;
+    }
+
+    /*
+     * Must contain at least one character that is not '/'.
+     */
+    if (mountpoint.search(/[^\/]/) === -1) {
+        return false;
+    }
+
+    return true;
+}
+
+function createValidateVolumes(fieldName, required) {
+    assert.string(fieldName, 'fieldName');
+    assert.optionalBool(required, 'required');
+
+    return function validateVolumesParam(params) {
+        var errs = [];
+        var idx = 0;
+        var VALID_VOLUME_MODES = ['ro', 'rw'];
+        var volume;
+        var volumes = params[fieldName];
+
+        if (required === false && volumes === undefined) {
+            return errs;
+        }
+
+        if (!Array.isArray(volumes)) {
+            errs.push(errors.invalidParamErr(fieldName,
+                fieldName + ' must be an array of objects'));
+            return errs;
+        }
+
+        if (volumes.length === 0) {
+            errs.push(errors.invalidParamErr(fieldName,
+                fieldName + ' must be a non-empty array of objects'));
+        }
+
+        for (idx = 0; idx < volumes.length; ++idx) {
+            volume = volumes[idx];
+            if (typeof (volume) !== 'object') {
+                errs.push(errors.invalidParamErr(fieldName,
+                    fieldName + ' must be a non-empty array of objects'));
+            }
+
+            if (volume === null) {
+                errs.push(errors.invalidParamErr(fieldName,
+                    fieldName + ' must be a non-empty array of objects'));
+            }
+
+            if (volume.mode !== undefined &&
+                VALID_VOLUME_MODES.indexOf(volume.mode) === -1) {
+                errs.push(errors.invalidParamErr(fieldName,
+                    'volume mode for volume ' + util.inspect(volume) +
+                        ' must be one of ' + VALID_VOLUME_MODES.join(', ')));
+            }
+
+            if (!validVolumeName(volume.name)) {
+                errs.push(errors.invalidParamErr(fieldName,
+                    'Invalid volume name for volume ' + util.inspect(volume)));
+            }
+
+            if (!validVolumeMountpoint(volume.mountpoint)) {
+                errs.push(errors.invalidParamErr(fieldName,
+                    'Invalid volume mount point for volume ' +
+                        util.inspect(volume)));
+            }
+        }
+
+        return errs;
+    };
+}
 /*
  * Validate an individual element of the firewall_rules array
  */
@@ -1433,7 +1542,11 @@ function validateListVmsParams(params, callback) {
                     {min: 1, max: MAX_LIST_VMS_LIMIT}),
                 offset: createValidateNumberFn('offset'),
                 marker: createValidateMarkerFn('marker',
-                    {sortParamName: 'sort'})
+                    {sortParamName: 'sort'}),
+                required_nfs_volume:
+                    createValidateStringFn('required_nfs_volume', {
+                        re: VOLUME_NAME_RE
+                    })
             };
 
             return validateParams(listVmValidators, params, {strict: true},
diff --git a/lib/common/vm-common.js b/lib/common/vm-common.js
index 427cd93..ec8e56f 100644
--- a/lib/common/vm-common.js
+++ b/lib/common/vm-common.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright 2016 Joyent, Inc.
+ * Copyright 2017 Joyent, Inc.
  */
 
 var assert = require('assert');
diff --git a/lib/workflows/destroy.js b/lib/workflows/destroy.js
index fec63d3..3b5d4f4 100644
--- a/lib/workflows/destroy.js
+++ b/lib/workflows/destroy.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright (c) 2015, Joyent, Inc.
+ * Copyright (c) 2017, Joyent, Inc.
  */
 
 /*
@@ -13,9 +13,13 @@
  */
 
 var async = require('async');
-var common = require('./job-common');
+
 var restify = require('restify');
-var VERSION = '7.1.0';
+
+var common = require('./job-common');
+var nfsVolumes = require('./nfs-volumes');
+
+var VERSION = '7.1.1';
 
 
 /*
@@ -206,7 +210,8 @@ var workflow = module.exports = {
         name: 'vmapi.refresh_vm',
         modules: { restify: 'restify' },
         body: common.refreshVm
-    }, {
+    },
+    {
         name: 'napi.remove_nics',
         timeout: 30,
         retry: 1,
@@ -218,6 +223,30 @@ var workflow = module.exports = {
         retry: 1,
         body: common.updateFwapi,
         modules: { sdcClients: 'sdc-clients' }
+    },
+    /*
+     * It is possible for this operation to either fail or never happen (due to
+     * the workflow job failing before getting to this task, etc.). It is not a
+     * critical problem though. Indeed, in this case, a background async process
+     * running in the VOLAPI zone will monitor VMs changing their state to
+     * 'failed' or 'deleted' to remove the corresponding references from the
+     * volumes they reference. We're still performing this operation here so
+     * that, *if possible*, the references from a VM to a volume are removed as
+     * soon as the corresponding user command (e.g "docker rm
+     * mounting-container") completes. This also explains the short timeout: if
+     * this task would slow down a destroy job by more than 10 seconds, then we
+     * timeout the whole job instead so that users can get a response back
+     * quicker. We also add this task at the end of the tasks chain (more
+     * precisely before the release ticket task, but that one runs on error) so
+     * that all other tasks have a chance to run, regardless of whether this one
+     * succeeds or not.
+     */
+    {
+        name: 'volapi.remove_volumes_references',
+        timeout: 10,
+        retry: 1,
+        body: nfsVolumes.removeVolumesReferences,
+        modules: { sdcClients: 'sdc-clients', vasync: 'vasync' }
     }, {
         name: 'cnapi.release_vm_ticket',
         timeout: 60,
diff --git a/lib/workflows/job-common.js b/lib/workflows/job-common.js
index 0a798b3..b5dd881 100644
--- a/lib/workflows/job-common.js
+++ b/lib/workflows/job-common.js
@@ -35,7 +35,15 @@ function validateForZoneAction(job, cb) {
     cb(null, 'All parameters OK!');
 }
 
+/*
+ * Clears the "skip_zone_action" flag on the job "job" so that subsequent tasks
+ * in a given workflow do not skip any action accidentally.
+ */
+function clearSkipZoneAction(job, cb) {
+    delete job.params.skip_zone_action;
 
+    cb();
+}
 
 /*
  * General purpose function to call a CNAPI endpoint. endpoint and requestMethod
@@ -1594,5 +1602,6 @@ module.exports = {
     releaseVMTicket: releaseVMTicket,
     acquireAllocationTicket: acquireAllocationTicket,
     waitOnAllocationTicket: waitOnAllocationTicket,
-    releaseAllocationTicket: releaseAllocationTicket
+    releaseAllocationTicket: releaseAllocationTicket,
+    clearSkipZoneAction: clearSkipZoneAction
 };
diff --git a/lib/workflows/nfs-volumes/index.js b/lib/workflows/nfs-volumes/index.js
new file mode 100644
index 0000000..0408f8d
--- /dev/null
+++ b/lib/workflows/nfs-volumes/index.js
@@ -0,0 +1,734 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright (c) 2017, Joyent, Inc.
+ */
+
+/*
+ * This module exports an object that has a "provisionChain" property which
+ * represents a list of tasks that allows to provision NFS shared volumes that
+ * are required/mounted by a given VM (e.g through the docker run -v
+ * some-nfs-volume:/some-mountpoint command).
+ * This list of task is currently used in the "provision" workflow.
+ */
+
+var vasync = require('vasync');
+var VError = require('verror');
+
+var common = require('../job-common');
+
+function provisionNfsVolumes(job, cb) {
+    var requiredVolumes;
+
+    if (typeof (volapiUrl) === 'undefined') {
+        cb(null, 'URL for volapi service not present, not provisioning NFS ' +
+            'volumes');
+        return;
+    }
+
+    requiredVolumes = job.params.volumes;
+    if (requiredVolumes === undefined || requiredVolumes.length === 0) {
+        /*
+         * No need to provision any volume, because this VM does not
+         * require any.
+         */
+        cb();
+        return;
+    }
+
+    vasync.forEachParallel({
+        func: provisionNfsVolume,
+        inputs: requiredVolumes
+    }, function onAllRequiredVolumesProvisioned(err, results) {
+        job.log.info({err: err, results: results},
+            'provisionNfsVolumes results');
+
+        if (!err) {
+            if (results === null || typeof (results) !== 'object') {
+                cb(new Error('results must be an object'));
+                return;
+            }
+
+            if (!Array.isArray(results.operations)) {
+                cb(new Error('results.operations must be an array'));
+                return;
+            }
+
+            var createdVolumes = {};
+            var operationResultIndex;
+
+            for (operationResultIndex in results.operations) {
+                var operationResult =
+                    results.operations[operationResultIndex].result;
+
+                if (operationResult === null ||
+                    typeof (operationResult) !== 'object') {
+                    cb(new Error('operationResult must be an object'));
+                    return;
+                }
+
+                createdVolumes[operationResult.uuid] = operationResult;
+            }
+
+            job.createdVolumes = createdVolumes;
+            cb(null, 'All volumes provisioned');
+        } else {
+            cb(new VError(err, 'Could not provision volumes'));
+        }
+    });
+
+    /*
+     * This function is responsible for:
+     *
+     * 1. Reserving the volume with name "volumeName" for the owner of the VM
+     * that is being provisioned.
+     *
+     * 2. Create that volume if it does not already exist.
+     *
+     * When this function call its "callback" function, it either:
+     *
+     * 1. succeeded to reserve the volume and create/load it
+     *
+     * 2. failed to reserve the volume
+     *
+     * 3. succeeded to reserve the volume, failed to create/load it, and
+     * attempted to cancel the reservation.
+     *
+     * In cases 2 and 3, "callback" will be called with an error object as its
+     * first argument.
+     */
+    function provisionNfsVolume(requiredVolume, callback) {
+        var vmNics;
+        var volumeName;
+
+        if (typeof (requiredVolume) !== 'object' || requiredVolume === null) {
+            callback(new Error('requiredVolume must be a non-null object'));
+            return;
+        }
+
+        volumeName = requiredVolume.name;
+        if (typeof (volumeName) !== 'string') {
+            callback(new Error('volumeName must be a string'));
+            return;
+        }
+
+        var volapi = new sdcClients.VOLAPI({
+            url: volapiUrl,
+            headers: { 'x-request-id': job.params['x-request-id'] },
+            userAgent: 'workflow/' + job.name
+        });
+
+        if (job.params !== undefined) {
+            vmNics = job.params.nics;
+        }
+
+        job.log.info({vmNics: vmNics}, 'VM nics');
+
+        var provisionContext = {};
+
+        vasync.pipeline({arg: provisionContext, funcs: [
+            function loadVolume(ctx, next) {
+                job.log.info({
+                    name: volumeName,
+                    owner_uuid: job.params.owner_uuid
+                }, 'Loading volume');
+
+                volapi.listVolumes({
+                    name: volumeName,
+                    owner_uuid: job.params.owner_uuid,
+                    predicate: JSON.stringify({
+                        eq: ['state', 'ready']
+                    })
+                }, function onListVolumes(listVolumesErr, volumes) {
+                    var errMsg;
+                    var err;
+
+                    if (!listVolumesErr) {
+                        if (volumes && volumes.length > 1) {
+                            errMsg = 'more than one volume with name '
+                                + volumeName + ' and owner_uuid: '
+                                + job.params.owner_uuid + ' when we '
+                                + 'expected exactly one';
+                            job.log.error({volumes: volumes},
+                                'Error: ' + errMsg);
+
+                            err = new Error(errMsg);
+                            next(err);
+                            return;
+                        }
+
+                        if (volumes && volumes.length === 1) {
+                            job.log.info({
+                                volume: volumes[0]
+                            }, 'Found volume');
+                            ctx.volume = volumes[0];
+                        } else {
+                            job.log.info({
+                                name: volumeName,
+                                owner_uuid: job.params.owner_uuid
+                            }, 'Did not find any volume');
+                        }
+
+                        next();
+                    } else {
+                        job.log.error({
+                            error: listVolumesErr
+                        }, 'Error when listing volumes');
+
+                        /*
+                         * Ignoring this error for now, since we'll try to
+                         * create the volume later, and retry to load it if
+                         * it already exists. If we get an error loading the
+                         * volume then, we'll make the task fail.
+                         */
+                        next();
+                    }
+                });
+            },
+            function reserve(ctx, next) {
+                if (job.params.vm_uuid === undefined) {
+                    job.params.vm_uuid = uuid.v4();
+                }
+
+                job.log.info({
+                    volume_uuid: ctx.volumeUuid,
+                    job_uuid: job.uuid,
+                    vm_uuid: job.params.vm_uuid,
+                    owner_uuid: job.params.owner_uuid
+                }, 'Reserving volume');
+
+                volapi.createVolumeReservation({
+                    owner_uuid: job.params.owner_uuid,
+                    job_uuid: job.uuid,
+                    vm_uuid: job.params.vm_uuid,
+                    volume_name: volumeName
+                }, function onVolRes(volResErr, volRes) {
+                    ctx.volumeReservation = volRes;
+                    next(volResErr);
+                });
+            },
+            function provision(ctx, next) {
+                var i;
+                var fabricNetworkUuids = [];
+                var invalidNics = [];
+                var vmNic;
+                var volumeCreationParams;
+
+                if (ctx.volume) {
+                    job.log.info({
+                        volume: ctx.volume
+                    }, 'Volume already exists, no need to create it');
+                    next();
+                    return;
+                }
+
+                job.log.info('Volume does not exist, creating it');
+
+                volumeCreationParams = {
+                    name: volumeName,
+                    owner_uuid: job.params.owner_uuid,
+                    type: 'tritonnfs'
+                };
+
+                /*
+                 * If the volume doesn't exist, then we created its uuid
+                 * beforehand to register a reservation for it, so we must
+                 * pass that uuid to VOLAPI so that it uses that uuid when
+                 * creating the volume to match the reservation.
+                 */
+                if (ctx.volumeUuid !== undefined) {
+                    volumeCreationParams.uuid = ctx.volumeUuid;
+                }
+
+                /*
+                 * If the VM being provisioned has nics attached to fabric
+                 * networks, we'll attach the volume to be provisioned to the
+                 * same networks. Otherwise, the default fabric network will be
+                 * picked as a default by VOLAPI.
+                 */
+                for (i = 0; i < vmNics.length; ++i) {
+                    vmNic = vmNics[i];
+
+                    if (typeof (vmNic) !== 'object') {
+                        invalidNics.push(vmNic);
+                        continue;
+                    }
+
+                    if (vmNic.nic_tag &&
+                        vmNic.nic_tag.indexOf('sdc_overlay/') === 0) {
+                        fabricNetworkUuids.push(vmNic.network_uuid);
+                    }
+                }
+
+                if (invalidNics.length > 0) {
+                    callback('invalid nics: ' + invalidNics);
+                    return;
+                }
+
+                volumeCreationParams.networks = fabricNetworkUuids;
+
+                volapi.createVolumeAndWait(volumeCreationParams,
+                    onVolumeCreated);
+
+                function onVolumeCreated(volumeCreationErr, createdVolume) {
+                    if (!volumeCreationErr) {
+                        job.log.info({createdVolume: createdVolume},
+                            'Created new volume');
+                        ctx.volume = createdVolume;
+                        next();
+                        return;
+                    }
+
+                    if (volumeCreationErr.restCode === 'VolumeAlreadyExists') {
+                        job.log.info('Volume with name: ' +
+                            volumeName + ' already exists, ' +
+                            'loading it...');
+
+                        volapi.listVolumes({
+                            name: volumeName,
+                            owner_uuid: job.params.owner_uuid,
+                            predicate: JSON.stringify({
+                                eq: ['state', 'ready']
+                            })
+                        }, function onListVolumes(listVolumesErr, volumes) {
+                            var loadedVolume;
+                            var errMsg;
+                            var existingVolNumberMismatchErr;
+
+                            if (listVolumesErr) {
+                                job.log.error({
+                                    err: listVolumesErr
+                                }, 'Error when loading existing volume');
+                                next(listVolumesErr);
+                                return;
+                            }
+
+                            if (!volumes || (volumes && volumes.length !== 1)) {
+                                errMsg =
+                                    'Zero or more than one volume with name ' +
+                                    volumeName + ' and ' + 'owner_uuid: ' +
+                                    job.params.owner_uuid + ' when we ' +
+                                    'expected exactly one';
+
+                                job.log.error({volumes: volumes}, errMsg);
+
+                                existingVolNumberMismatchErr =
+                                    new Error(errMsg);
+                                next(existingVolNumberMismatchErr);
+                                return;
+                            }
+
+                            job.log.info({loadedVolume: loadedVolume},
+                                'Loaded existing volume');
+                            ctx.volume = volumes[0];
+                            next();
+                            return;
+                        });
+                    } else {
+                        job.log.error({error: volumeCreationErr},
+                            'Failed to created volume');
+                        next(volumeCreationErr);
+                        return;
+                    }
+
+                    next(volumeCreationErr);
+                }
+            }]}, function onVolumeProvDone(volProvErr) {
+                if (provisionContext.volumeReservation === true &&
+                    volProvErr !== undefined) {
+                    job.log.info({
+                        volumeReservation: provisionContext.volumeReservation
+                    }, 'Cancelling volume reservation');
+
+                    volapi.deleteVolumeReservation({
+                        uuid: provisionContext.volumeReservation.uuid,
+                        owner_uuid: job.params.owner_uuid
+                    }, function onVolResRemoved(volResRemoveErr) {
+                        if (volResRemoveErr) {
+                            job.log.error({err: volResRemoveErr},
+                                'Error when removing volume reservation');
+                        } else {
+                            job.log.info('Successfully removed volume ' +
+                                'reservation');
+                        }
+
+                        callback(volProvErr, provisionContext.volume);
+                    });
+                } else {
+                    callback(volProvErr, provisionContext.volume);
+                }
+            });
+    }
+}
+
+/*
+ * Prepares a request to CNAPI that updates the internal_metadata property of a
+ * VM that depends on NFS volumes, in order for that VM to have the proper data
+ * in its 'docker:nfsvolumes' internal_metadata property. That data is used by
+ * dockerinit when the zone boots to mount exported filesystems from the
+ * corresponding NFS shared volumes.
+ *
+ * This function only sets the request up. The request is performed later by
+ * subsequent tasks in the same workflow. The next task with a body equal to
+ * 'common.zoneAction' sends the request, and the task after with a body of
+ * 'common.waitTask' waits for it to complete.
+ */
+function setupUpdateVmNfsVolumesMetadataRequest(job, callback) {
+    if (job.updatedVmInternalMetadata === undefined) {
+        // The VM's internal metadata doesn't need to be updated, so it's safe
+        // to skip the rest of the task, and the subsequent zoneAction and
+        // waitTask tasks.
+        job.params.skip_zone_action = true;
+        callback(null, 'VM internal metadata doesn\'t need to be updated');
+        return;
+    }
+
+    if (job.updatedVmInternalMetadata === null ||
+        typeof (job.updatedVmInternalMetadata) !== 'object') {
+        callback(new Error('job.updatedVmInternalMetadata must be an object'));
+        return;
+    }
+
+    job.endpoint = '/servers/' +
+                   job.params.server_uuid + '/vms/' +
+                   job.params.vm_uuid + '/update';
+    job.params.jobid = job.uuid;
+    job.requestMethod = 'post';
+    job.action = 'update';
+    job.server_uuid = job.params['server_uuid'];
+
+    job.params.payload = {
+        set_internal_metadata: job.updatedVmInternalMetadata
+    };
+
+    return callback(null, 'Request has been setup!');
+}
+
+
+function buildNfsVolumesMetadata(job, callback) {
+    job.log.info({createdVolumes: job.createdVolumes},
+        'Building docker:nfsvolumes internal metadata');
+
+    if (job.createdVolumes === undefined) {
+        // No NFS volume was created, so there's no need to update the
+        // docker:nfsvolumes metadata for dockerinit to mount any volume.
+        callback(null, 'No NFS volume with which to update VM\'s internal '
+            + 'metadata');
+        return;
+    }
+
+    if (job.createdVolumes === null ||
+        typeof (job.createdVolumes) !== 'object') {
+        callback(new Error('job.createdVolumes must be an object'));
+        return;
+    }
+
+    if (Object.keys(job.createdVolumes).length === 0) {
+        callback(null, 'No NFS volume with which to update VM\'s internal '
+            + 'metadata');
+        return;
+    }
+
+    var createdVolume;
+    var foundVolume;
+    var requiredVolumes = job.params.volumes;
+    var volume;
+    var volumeIndex;
+    var volumeUuid;
+
+    if (!Array.isArray(requiredVolumes)) {
+        callback(new Error('requiredVolumes must be an array'));
+        return;
+    }
+
+    for (volumeUuid in job.createdVolumes) {
+        createdVolume = job.createdVolumes[volumeUuid];
+
+        job.log.info('Updating docker:nfsvolumes metadata entry for volume: '
+            + createdVolume.name);
+
+        foundVolume = false;
+
+        for (volumeIndex in requiredVolumes) {
+            volume = requiredVolumes[volumeIndex];
+            if (volume && volume.name === createdVolume.name) {
+                foundVolume = true;
+                break;
+            }
+        }
+
+        if (foundVolume) {
+            job.log.info('Adding filesystem_path property ['
+                + createdVolume.filesystem_path + '] to '
+                + 'required volume: ' + createdVolume.name);
+            requiredVolumes[volumeIndex].filesystem_path =
+                createdVolume.filesystem_path;
+        }
+    }
+
+    job.nfsVolumesInternalMetadata =
+        JSON.stringify(requiredVolumes.map(volumeToNfsInternalMetadata));
+
+    callback(null, 'Built docker:nfsvolumes internal_metadata: '
+        + job.nfsVolumesInternalMetadata);
+
+    function volumeToNfsInternalMetadata(vol) {
+        return {
+            mode: (vol.mode === undefined) ? 'rw' : vol.mode,
+            mountpoint: vol.mountpoint,
+            name: vol.name,
+            nfsvolume: vol.filesystem_path,
+            type: (vol.type === undefined) ? 'tritonnfs' : vol.type
+        };
+    }
+}
+
+
+function waitForNfsVolumeProvisions(job, callback) {
+    if (job.requiredNfsVolumes !== undefined &&
+        !Array.isArray(job.requiredNfsVolumes)) {
+        callback(new Error('job.requiredNfsVolumes must be an array if '
+            + 'present'));
+        return;
+    }
+
+    if (!job.createdVolumes || Object.keys(job.createdVolumes).length === 0) {
+        callback(null, 'No required NFS volume to wait for');
+        return;
+    }
+
+    var volapi = new sdcClients.VOLAPI({
+        url: volapiUrl,
+        headers: {'x-request-id': job.params['x-request-id']},
+        userAgent: 'workflow/' + job.name
+    });
+
+    vasync.forEachParallel({
+        func: function checkVolumeCreated(nfsVolumeUuid, done) {
+            if (typeof (nfsVolumeUuid) !== 'string') {
+                done(new Error('nfsVolumeUuid must be a string'));
+                return;
+            }
+
+            function checkVolumeReady() {
+                volapi.getVolume({
+                    uuid: nfsVolumeUuid,
+                    owner_uuid: job.params.owner_uuid
+                }, function onGetVolume(getVolumeErr, volume) {
+                    if (getVolumeErr) {
+                        done(getVolumeErr);
+                        return;
+                    }
+
+                    if (volume && volume.state === 'ready') {
+                        job.createdVolumes[volume.uuid] = volume;
+                        done();
+                        return;
+                    }
+
+                    setTimeout(checkVolumeReady, 1000);
+                });
+            }
+
+            checkVolumeReady();
+        },
+        inputs: Object.keys(job.createdVolumes)
+    }, function allVolumesReady(err, results) {
+        if (err) {
+            callback(new Error('Could not determine if all required volumes '
+                + 'are ready'));
+        } else {
+            callback(null, 'All required volumes ready');
+        }
+    });
+}
+
+function addVolumesReferences(job, callback) {
+    var createdVolumeUuids = [];
+
+    if (job.createdVolumes !== undefined &&
+        typeof (job.createdVolumes) !== 'object') {
+        callback(new Error('job.createdVolumes must be an object if '
+            + 'present'));
+        return;
+    }
+
+    if (job.createdVolumes) {
+        createdVolumeUuids = Object.keys(job.createdVolumes);
+    }
+
+    if (createdVolumeUuids.length === 0) {
+        callback(null,
+            'No created NFS volume to which a reference needs to be added');
+        return;
+    }
+
+    var volapi = new sdcClients.VOLAPI({
+        url: volapiUrl,
+        headers: {'x-request-id': job.params['x-request-id']},
+        userAgent: 'workflow/' + job.name
+    });
+
+    var vmUuid = job.params.vm_uuid;
+
+    vasync.forEachParallel({
+        func: function addVolReference(volUuid, done) {
+            volapi.addVolumeReference({
+                owner_uuid: job.params.owner_uuid,
+                vm_uuid: vmUuid,
+                volume_uuid: volUuid
+            }, done);
+        },
+        inputs: createdVolumeUuids
+    }, function onRefsAdded(refsAddErr) {
+        if (refsAddErr) {
+            callback(new Error('Could not add references from VM ' + vmUuid +
+                ' to volumes ' + createdVolumeUuids));
+        } else {
+            callback(null, 'References from VM ' + vmUuid + ' to volumes ' +
+                createdVolumeUuids + ' added successfully');
+        }
+    });
+}
+
+/*
+ * Note that this workflow job task never fails. In case of error, it still
+ * never calls its "callback" with an error object, only with a message that
+ * describes the error encountered. Removing volume references is indeed a best
+ * effort task because:
+ *
+ * 1. we want to keep the destroy job as quick as possible so that users get a
+ *    response back as quickly as possible.
+ *
+ * 2. a VOLAPI service might not be available, in this case we don't want to
+ *    make VM destroy jobs fail just because we failed to remove volume
+ *    references.
+ *
+ * 3. In case we fail to remove volume references here, the "volapi-updater"
+ *    process will remove them asynchronously.
+ */
+function removeVolumesReferences(job, callback) {
+    if (typeof (volapiUrl) === 'undefined') {
+        callback(null,
+            'URL for volapi service not present, not provisioning NFS volume');
+        return;
+    }
+
+    if (!job.currentVm) {
+        callback(null, 'Skipping task -- VM missing from job');
+        return;
+    }
+
+    /*
+     * If the VM being destroyed doesn't use any volume, there's no point in
+     * removing volume references.
+     */
+    if (!job.currentVm.volumes) {
+        callback(null, 'Skipping task -- VM is missing .volumes');
+        return;
+    }
+
+    /*
+     * Even if the VM being destroyed depends on volumes, we want to minimize
+     * the time spent trying to remove volume references. Thus, we disable the
+     * retry mechanism provided by sdc-clients, and we set a connection timeout
+     * of 5 seconds (which is set to be < than the task's timeout in the destroy
+     * workflow), instead of using the defaults provided by the node runtime
+     * which is 2 minutes. This applies e.g if NFS volumes are enabled in a
+     * given DC, and then disabled with the VOLAPI core service's instance
+     * deleted: the client would retry indefinitely to connect until the task's
+     * timeout expire. With this change it doesn't retry and requests call the
+     * callback with a connection error after at most 5 seconds.
+     */
+    var volapi = new sdcClients.VOLAPI({
+        url: volapiUrl,
+        headers: {'x-request-id': job.params['x-request-id']},
+        userAgent: 'workflow/' + job.name,
+        connectTimeout: 5000,
+        retry: false
+    });
+
+    var vmUuid = job.params.vm_uuid;
+
+    volapi.listVolumes({
+        refs: vmUuid
+    }, function onRefedVolsListed(listErr, refedVolumes) {
+        if (listErr) {
+            callback(null, 'Failed to list volumes referenced by VM ' + vmUuid +
+                ', reason: ' + listErr);
+            return;
+        }
+
+        job.log.info({
+            vmUuid: vmUuid,
+            refedVolumes: refedVolumes,
+            err: listErr
+        }, 'listed volumes referenced by VM prior to removing references');
+
+        vasync.forEachParallel({
+            func: function removeVolReference(volume, done) {
+                job.log.info({
+                    owner_uuid: job.params.owner_uuid,
+                    vm_uuid: vmUuid,
+                    volume_uuid: volume.uuid
+                }, 'removing reference from VM to volume');
+
+                volapi.removeVolumeReference({
+                    owner_uuid: job.params.owner_uuid,
+                    vm_uuid: vmUuid,
+                    volume_uuid: volume.uuid
+                }, done);
+            },
+            inputs: refedVolumes
+        }, function onRefsRemoved(refsDelErr) {
+            if (refsDelErr) {
+                callback(null, 'Could not remove references from VM ' +
+                    vmUuid + ' to volumes ' + refedVolumes);
+            } else {
+                callback(null, 'References from VM ' + vmUuid + ' to volumes ' +
+                    refedVolumes + ' removed successfully');
+            }
+        });
+    });
+}
+
+module.exports = {
+    provisionChain: [
+        {
+            name: 'volapi.provision_nfs_volumes',
+            timeout: 120,
+            retry: 1,
+            body: provisionNfsVolumes,
+            modules: {
+                sdcClients: 'sdc-clients',
+                uuid: 'uuid',
+                vasync: 'vasync',
+                VError: 'verror'
+            }
+        }, {
+            name: 'cnapi.wait_for_nfs_volumes_provisions',
+            timeout: 120,
+            retry: 1,
+            body: waitForNfsVolumeProvisions,
+            modules: {
+                sdcClients: 'sdc-clients',
+                vasync: 'vasync'
+            }
+        }, {
+            name: 'cnapi.build_nfs_volumes_metadata',
+            timeout: 10,
+            retry: 1,
+            body: buildNfsVolumesMetadata,
+            modules: {
+                sdcClients: 'sdc-clients',
+                vasync: 'vasync'
+            }
+        }
+    ],
+    addVolumesReferences: addVolumesReferences,
+    removeVolumesReferences: removeVolumesReferences
+};
diff --git a/lib/workflows/provision.js b/lib/workflows/provision.js
index 8804bc7..0cd988e 100644
--- a/lib/workflows/provision.js
+++ b/lib/workflows/provision.js
@@ -13,13 +13,17 @@
  */
 
 var async = require('async');
-var fabricCommon = require('./fabric-common');
+var childProcess = require('child_process');
 var restify = require('restify');
+var VError = require('verror');
+
 var common = require('./job-common');
-var childProcess = require('child_process');
+var fabricCommon = require('./fabric-common');
+var nfsVolumes = require('./nfs-volumes');
+
 var wfapiUrl;
 
-var VERSION = '7.5.1';
+var VERSION = '8.0.0';
 
 
 /*
@@ -207,6 +211,7 @@ function preparePayload(job, cb) {
 
     var params = job.params;
     var i, j, nic;
+    var parsedNfsMetadata;
     var payload = { uuid: params['vm_uuid'], image: job.params.image };
     var wantResolvers = true;
 
@@ -309,6 +314,47 @@ function preparePayload(job, cb) {
         payload.imgapiPeers = params.imgapiPeers;
     }
 
+    if (job.nfsVolumesInternalMetadata !== undefined) {
+        job.log.info({
+            docker: Boolean(job.params.docker),
+            nfsVolumesInternalMetadata: job.nfsVolumesInternalMetadata
+        }, 'Setting nfsvolumes internal metadata');
+
+        if (!payload.hasOwnProperty('internal_metadata')) {
+            payload.internal_metadata = {};
+        }
+
+        if (job.params.docker === true) {
+            // We create a separate copy of the metadata for docker:nfsvolumes,
+            // because that needs a 'readonly' parameter instead of 'mode' for
+            // historical reasons.
+            try {
+                parsedNfsMetadata = JSON.parse(job.nfsVolumesInternalMetadata);
+            } catch (nfsMetadataParseErr) {
+                cb(new VError(nfsMetadataParseErr,
+                    'Could not parse NFS volumes metadata'));
+                return;
+            }
+
+            if (!Array.isArray(parsedNfsMetadata)) {
+                cb(new Error('parsed nfsvolumes is not an array'));
+                return;
+            }
+
+            // replace .mode = <string> with .readonly = true|false
+            parsedNfsMetadata.forEach(function _eachVol(volObj) {
+                volObj.readonly = (volObj.mode === 'ro');
+                delete volObj.mode;
+            });
+
+            payload.internal_metadata['docker:nfsvolumes']
+                = JSON.stringify(parsedNfsMetadata);
+        }
+
+        payload.internal_metadata['sdc:volumes'] =
+            job.nfsVolumesInternalMetadata;
+    }
+
     job.params.payload = payload;
     cb(null, 'Payload prepared successfully');
 }
@@ -519,59 +565,94 @@ var workflow = module.exports = {
      * Fabric NAT provisioning
      */
     ].concat(
-        fabricCommon.provisionChain).concat([
-
-    {
-        name: 'prepare_payload',
-        timeout: 10,
-        retry: 1,
-        body: preparePayload,
-        modules: { sdcClients: 'sdc-clients' }
-    }, {
+        fabricCommon.provisionChain).concat([ {
         name: 'cnapi.ensure_image',
         timeout: 300,
         retry: 1,
         body: ensureImage,
         modules: { sdcClients: 'sdc-clients' }
-    }, {
+    },
+    {
         name: 'cnapi.wait_task_ensure_image',
         timeout: 3600,
         retry: 1,
         body: common.waitTask,
         modules: { sdcClients: 'sdc-clients' }
-
     },
-
     /*
      * If we've provisioned fabric NAT zones for this VM, wait until
      * they've finished before sending off the provision.
      */
-    fabricCommon.provisionWaitTask,
+    fabricCommon.provisionWaitTask
+    ]).concat(
+        /*
+         * Now that all dependent tasks that can fail but provisioning required
+         * volumes and the VM itself have been done, it's time to provision
+         * required volumes. Doing it beforehand would potentially make the
+         * costly process of provisioning required volumes start before we even
+         * know if anything else that could fail has succeeded.
+         */
+        nfsVolumes.provisionChain).concat([
+    {
+        name: 'prepare_payload',
+        timeout: 10,
+        retry: 1,
+        body: preparePayload,
+        modules: { sdcClients: 'sdc-clients', VError: 'verror' }
+    },
     {
         name: 'cnapi.provision_vm',
         timeout: 10,
         retry: 1,
         body: provision,
         modules: { sdcClients: 'sdc-clients' }
-    }, {
+    },
+    {
         name: 'cnapi.wait_task',
         timeout: 3600,
         retry: 1,
         body: common.waitTask,
         modules: { sdcClients: 'sdc-clients' }
-    }, {
+    },
+    /*
+     * It is possible for this operation to either fail or never happen (due to
+     * the workflow job failing before getting to this task, etc.). It is not a
+     * critical problem though. Indeed, in this case, a volume reservation would
+     * have taken place beforehand, and a background async process running in
+     * the VOLAPI zone would monitor these reservations to add the corresponding
+     * references when the VM is provisioned.
+     * We're still performing this operation here so that:
+     *
+     * 1. The volume references are consistent with the provisioning request
+     *    when the provisioning workflow completes, not just at some point in
+     *    the future.
+     *
+     * 2. in case VOLAPI's background process is not functionning, the reference
+     *    is added (and the reservation cleaned up) before that process comes
+     *    back up.
+     */
+    {
+        name: 'volapi.add_volumes_references',
+        timeout: 120,
+        retry: 1,
+        body: nfsVolumes.addVolumesReferences,
+        modules: { sdcClients: 'sdc-clients', vasync: 'vasync' }
+    },
+    {
         name: 'vmapi.put_vm',
         timeout: 120,
         retry: 1,
         body: common.putVm,
         modules: { sdcClients: 'sdc-clients' }
-    }, {
+    },
+    {
         name: 'fwapi.update',
         timeout: 10,
         retry: 1,
         body: common.updateFwapi,
         modules: { sdcClients: 'sdc-clients' }
-    }, {
+    },
+    {
         name: 'cnapi.release_vm_ticket',
         timeout: 60,
         retry: 1,
@@ -584,7 +665,18 @@ var workflow = module.exports = {
 
     ]),
     timeout: 3810,
-    onerror: [ {
+    onerror: [
+    /*
+     * We don't cleanup volume references or reservations in case the provision
+     * failed. Instead, we rely on the 'common.post_back' task to update the VM
+     * in VMAPI to a state === 'failed', which would generate a changefeed event
+     * that VOLAPI would process to cleanup those references and reservations.
+     * That process itself could fail too, but we consider that adding
+     * additional task to the onerror tasks chain would be more disruptive, and
+     * would also fail sometimes. In other words, the net effect might not be
+     * positive, and we can revisit that decision later.
+     */
+    {
         name: 'napi.cleanup_nics',
         timeout: 10,
         retry: 1,
@@ -617,7 +709,6 @@ var workflow = module.exports = {
 
     // If this was a fabric nat provision, clean up the ticket
     fabricCommon.releaseTicketTask,
-
     {
         name: 'On error',
         body: function (job, cb) {
diff --git a/lib/workflows/start.js b/lib/workflows/start.js
index 2706420..83e19c1 100644
--- a/lib/workflows/start.js
+++ b/lib/workflows/start.js
@@ -5,7 +5,7 @@
  */
 
 /*
- * Copyright (c) 2014, Joyent, Inc.
+ * Copyright (c) 2017, Joyent, Inc.
  */
 
 /*
@@ -13,7 +13,8 @@
  */
 
 var common = require('./job-common');
-var VERSION = '7.0.6';
+
+var VERSION = '7.0.7';
 
 
 /*
@@ -24,7 +25,7 @@ var VERSION = '7.0.6';
  * - requestMethod
  * - expects (if you want to check a specific running status of the machine)
  */
-function setupRequest(job, cb) {
+function setupStartRequest(job, cb) {
     job.endpoint = '/servers/' +
                    job.params['server_uuid'] + '/vms/' +
                    job.params['vm_uuid'] + '/start';
@@ -34,7 +35,12 @@ function setupRequest(job, cb) {
     job.action = 'start';
     job.server_uuid = job.params['server_uuid'];
 
-    return cb(null, 'Request has been setup!');
+    // Clear any payload that may have been set by another request.
+    if (job.params && job.params.payload) {
+        delete job.params.payload;
+    }
+
+    return cb(null, 'Start request has been setup!');
 }
 
 var workflow = module.exports = {
@@ -46,12 +52,6 @@ var workflow = module.exports = {
         retry: 1,
         body: common.validateForZoneAction,
         modules: {}
-    }, {
-        name: 'common.setup_request',
-        timeout: 10,
-        retry: 1,
-        body: setupRequest,
-        modules: {}
     }, {
         name: 'cnapi.acquire_vm_ticket',
         timeout: 10,
@@ -70,6 +70,12 @@ var workflow = module.exports = {
         retry: 1,
         body: common.ensureVmState,
         modules: { sdcClients: 'sdc-clients' }
+    }, {
+        name: 'setup_start_request',
+        timeout: 10,
+        retry: 1,
+        body: setupStartRequest,
+        modules: {}
     }, {
         name: 'cnapi.start_vm',
         timeout: 10,
diff --git a/package.json b/package.json
index 565d77b..ce5ccf4 100644
--- a/package.json
+++ b/package.json
@@ -21,7 +21,7 @@
     "nodeunit": "0.9.1",
     "once": "^1.3.3",
     "restify": "4.3.0",
-    "sdc-clients": "10.0.3",
+    "sdc-clients": "git+ssh://git@github.com:joyent/node-sdc-clients.git#1457c7c8cd08e11aefe4ef7142207c827e8fe967",
     "sigyan": "0.2.0",
     "sprintf": "0.1.1",
     "strsplit": "1.0.0",
diff --git a/sapi_manifests/vmapi/template b/sapi_manifests/vmapi/template
index ffac729..e75210b 100644
--- a/sapi_manifests/vmapi/template
+++ b/sapi_manifests/vmapi/template
@@ -55,6 +55,9 @@
     "papi": {
         "url": "http://{{{PAPI_SERVICE}}}"
     },
+    "volapi": {
+        "url": "http://volapi.{{{datacenter_name}}}.{{{dns_domain}}}"
+    },
     "moray": {
         "srvDomain": "{{MORAY_SERVICE}}",
         "cueballOptions": {
diff --git a/test/common.js b/test/common.js
index b5be0e0..1c200db 100644
--- a/test/common.js
+++ b/test/common.js
@@ -16,7 +16,7 @@ var jsprim = require('jsprim');
 var moray = require('moray');
 var path = require('path');
 var restify = require('restify');
-var url = require('url');
+var mod_url = require('url');
 var util = require('util');
 var vasync = require('vasync');
 
@@ -35,9 +35,11 @@ try {
     config = JSON.parse(fs.readFileSync(DEFAULT_CFG, 'utf8'));
 } catch (e) {}
 
-var VMAPI_URL = process.env.VMAPI_URL || 'http://localhost';
-var NAPI_URL = config.napi.url || 'http://10.99.99.10';
 var CNAPI_URL = config.cnapi.url || 'http://10.99.99.22';
+var IMGAPI_URL = config.imgapi.url || 'http://10.99.99.21';
+var NAPI_URL = config.napi.url || 'http://10.99.99.10';
+var VMAPI_URL = process.env.VMAPI_URL || 'http://localhost';
+var VOLAPI_URL = config.volapi.url || 'http://10.99.99.42';
 
 var VMS_LIST_ENDPOINT = '/vms';
 
@@ -78,8 +80,29 @@ function setUp(callback) {
         agent: false
     });
 
-    client.napi = napi;
+    var volapi = restify.createJsonClient({
+        url: VOLAPI_URL,
+        /*
+         * Use a specific version and not the latest one (with "*"") to avoid
+         * breakage when VOLAPI's API changes in a way that is not backward
+         * compatible.
+         */
+        version: '^1',
+        log: logger,
+        agent: false
+    });
+
+    var imgapi = restify.createJsonClient({
+        url: IMGAPI_URL,
+        version: '*',
+        log: logger,
+        agent: false
+    });
+
     client.cnapi = cnapi;
+    client.imgapi = imgapi;
+    client.napi = napi;
+    client.volapi = volapi;
 
     return callback(null, client);
 }
@@ -89,7 +112,7 @@ function setUp(callback) {
  * the vms listing endpoint results in a request error.
  */
 function testListInvalidParams(client, params, expectedError, t, callback) {
-    var query = url.format({pathname: VMS_LIST_ENDPOINT, query: params});
+    var query = mod_url.format({pathname: VMS_LIST_ENDPOINT, query: params});
 
     return client.get(query, function (err, req, res, body) {
         t.equal(res.statusCode, 409,
@@ -107,7 +130,7 @@ function testListInvalidParams(client, params, expectedError, t, callback) {
  * the vms listing endpoint does not result in a request error.
  */
 function testListValidParams(client, params, t, callback) {
-    var query = url.format({pathname: VMS_LIST_ENDPOINT, query: params});
+    var query = mod_url.format({pathname: VMS_LIST_ENDPOINT, query: params});
 
     return client.get(query, function (err, req, res, body) {
         t.equal(res.statusCode, 200,
@@ -138,6 +161,75 @@ function ifError(t, err) {
     t.ok(!err, err ? ('error: ' + err.message) : 'no error');
 }
 
+
+function checkEqual(value, expected) {
+    if ((typeof (value) === 'object') && (typeof (expected) === 'object')) {
+        var exkeys = Object.keys(expected);
+        for (var i = 0; i < exkeys.length; i++) {
+            var key = exkeys[i];
+            if (value[key] !== expected[key])
+                return false;
+        }
+
+        return true;
+    } else {
+        return (value === expected);
+    }
+}
+
+function checkValue(client, url, key, value, callback) {
+    client.get(url, function (err, req, res, body) {
+        if (err) {
+            callback(err);
+            return;
+        }
+
+        callback(null, checkEqual(body[key], value));
+    });
+}
+
+function waitForValue(url, key, value, options, callback) {
+    assert.string(url, 'url');
+    assert.string(key, 'key');
+    assert.object(options, 'options');
+    assert.object(options.client, 'options.client');
+    assert.optionalNumber(options.timeout, 'options.timeout');
+    assert.func(callback, 'callback');
+
+    var client = options.client;
+    var timeout = 120;
+    var times = 0;
+
+    if (options.timeout !== undefined) {
+        timeout = options.timeout;
+    }
+
+    function performCheck() {
+        checkValue(client, url, key, value, function onChecked(err, ready) {
+            if (err) {
+                callback(err);
+                return;
+            }
+
+            if (!ready) {
+                times++;
+
+                if (times === timeout) {
+                    callback(new Error('Timeout waiting on ' + url));
+                } else {
+                    setTimeout(function () {
+                        performCheck();
+                    }, 1000);
+                }
+            } else {
+                callback(null);
+            }
+        });
+    }
+
+    performCheck();
+}
+
 module.exports = {
     setUp: setUp,
     checkHeaders: checkHeaders,
@@ -145,5 +237,6 @@ module.exports = {
     testListValidParams: testListValidParams,
     config: config,
     ifError: ifError,
-    VMS_LIST_ENDPOINT: VMS_LIST_ENDPOINT
+    VMS_LIST_ENDPOINT: VMS_LIST_ENDPOINT,
+    waitForValue: waitForValue
 };
diff --git a/test/lib/uuid.js b/test/lib/uuid.js
new file mode 100644
index 0000000..1e229eb
--- /dev/null
+++ b/test/lib/uuid.js
@@ -0,0 +1,13 @@
+var uuid = require('libuuid');
+
+/*
+ * Returns a string that represents the first few characters of a version 4
+ * UUID.
+ */
+function generateShortUuid() {
+    return uuid.create().split('-')[0];
+}
+
+module.exports = {
+    generateShortUuid: generateShortUuid
+};
\ No newline at end of file
diff --git a/test/vms.full.test.js b/test/vms.full.test.js
index 3ac7971..e925bad 100644
--- a/test/vms.full.test.js
+++ b/test/vms.full.test.js
@@ -16,6 +16,8 @@ var async = require('async');
 var util = require('util');
 
 var common = require('./common');
+var testUuid = require('./lib/uuid');
+var waitForValue = common.waitForValue;
 
 // --- Globals
 
@@ -37,10 +39,6 @@ var CALLER = {
     keyId: '/foo@joyent.com/keys/id_rsa'
 };
 
-// In seconds
-var TIMEOUT = 120;
-
-
 // --- Helpers
 
 function checkMachine(t, vm) {
@@ -71,63 +69,6 @@ function checkJob(t, job) {
 }
 
 
-function checkEqual(value, expected) {
-    if ((typeof (value) === 'object') && (typeof (expected) === 'object')) {
-        var exkeys = Object.keys(expected);
-        for (var i = 0; i < exkeys.length; i++) {
-            var key = exkeys[i];
-            if (value[key] !== expected[key])
-                return false;
-        }
-
-        return true;
-    } else {
-        return (value === expected);
-    }
-}
-
-
-function checkValue(url, key, value, callback) {
-    return client.get(url, function (err, req, res, body) {
-        if (err) {
-            return callback(err);
-        }
-
-        return callback(null, checkEqual(body[key], value));
-    });
-}
-
-
-var times = 0;
-
-function waitForValue(url, key, value, callback) {
-
-    function onReady(err, ready) {
-        if (err) {
-            callback(err);
-            return;
-        }
-
-        if (!ready) {
-            times++;
-
-            if (times === TIMEOUT) {
-                throw new Error('Timeout waiting on ' + url);
-            } else {
-                setTimeout(function () {
-                    waitForValue(url, key, value, callback);
-                }, 1000);
-            }
-        } else {
-            times = 0;
-            callback(null);
-        }
-    }
-
-    return checkValue(url, key, value, onReady);
-}
-
-
 function waitForNicState(t, query, state, waitCallback) {
     var stop = false;
     var count = 0;
@@ -644,7 +585,7 @@ exports.create_vm = function (t) {
     };
 
     var vm = {
-        alias: 'vmapitest-full-' + uuid.create().split('-')[0],
+        alias: 'vmapitest-full-' + testUuid.generateShortUuid(),
         owner_uuid: CUSTOMER,
         image_uuid: IMAGE,
         server_uuid: SERVER.uuid,
@@ -704,7 +645,9 @@ exports.get_job = function (t) {
 
 
 exports.wait_provisioned_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -746,7 +689,9 @@ exports.stop_vm = function (t) {
 
 
 exports.wait_stopped_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -787,7 +732,9 @@ exports.start_vm = function (t) {
 
 
 exports.wait_started_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -827,7 +774,9 @@ exports.reboot_vm = function (t) {
 
 
 exports.wait_rebooted_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -884,7 +833,9 @@ exports.add_nics_with_networks = function (t) {
 
 
 exports.wait_add_nics_with_networks = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -941,7 +892,9 @@ exports.add_nics_with_macs = function (t) {
 
 
 exports.wait_add_nics_with_macs = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1003,7 +956,9 @@ exports.remove_nics = function (t) {
 
 
 exports.wait_remove_nics = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1236,7 +1191,9 @@ exports.add_tags = function (t) {
 
 
 exports.wait_new_tag_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1249,7 +1206,9 @@ exports.wait_new_tag = function (t) {
         group: 'deployment'
     };
 
-    waitForValue(vmLocation, 'tags', tags, function (err) {
+    waitForValue(vmLocation, 'tags', tags, {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1288,7 +1247,9 @@ exports.delete_tag = function (t) {
 
 
 exports.wait_delete_tag_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1300,7 +1261,9 @@ exports.wait_delete_tag = function (t) {
         group: 'deployment'
     };
 
-    waitForValue(vmLocation, 'tags', tags, function (err) {
+    waitForValue(vmLocation, 'tags', tags, {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1325,7 +1288,9 @@ exports.delete_tags = function (t) {
 
 
 exports.wait_delete_tags_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1333,7 +1298,9 @@ exports.wait_delete_tags_job = function (t) {
 
 
 exports.wait_delete_tags = function (t) {
-    waitForValue(vmLocation, 'tags', {}, function (err) {
+    waitForValue(vmLocation, 'tags', {}, {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1366,7 +1333,9 @@ exports.set_tags = function (t) {
 
 
 exports.wait_set_tags_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1382,7 +1351,9 @@ exports.wait_set_tags = function (t) {
         withequals: 'foo=bar'
     };
 
-    waitForValue(vmLocation, 'tags', tags, function (err) {
+    waitForValue(vmLocation, 'tags', tags, {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1410,7 +1381,9 @@ exports.snapshot_vm = function (t) {
 
 
 exports.wait_snapshot_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1438,7 +1411,9 @@ exports.rollback_vm = function (t) {
 
 
 exports.wait_rollback_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1466,7 +1441,9 @@ exports.delete_snapshot = function (t) {
 
 
 exports.wait_delete_snapshot_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1494,7 +1471,9 @@ exports.reprovision_vm = function (t) {
 
 
 exports.wait_reprovision_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1517,7 +1496,9 @@ exports.destroy_vm = function (t) {
 
 
 exports.wait_destroyed_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1621,7 +1602,9 @@ exports.get_nonautoboot_job = function (t) {
 
 
 exports.wait_nonautoboot_provisioned_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1647,7 +1630,9 @@ exports.change_autoboot = function (t) {
 
 
 exports.wait_autoboot_update_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1685,7 +1670,9 @@ exports.destroy_nonautoboot_vm = function (t) {
 
 
 exports.wait_nonautoboot_destroyed_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1719,7 +1706,9 @@ exports.create_vm_with_package = function (t) {
 
 
 exports.wait_provisioned_with_package_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1808,7 +1797,9 @@ exports.resize_package = function (t) {
 
 
 exports.wait_resize_package_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1847,7 +1838,9 @@ exports.resize_package_down = function (t) {
 
 
 exports.wait_resize_package_job_2 = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1870,7 +1863,9 @@ exports.destroy_vm_with_package = function (t) {
 
 
 exports.wait_destroyed_with_package_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -1908,7 +1903,9 @@ exports.provision_network_names = function (t) {
 
 
 exports.wait_provision_network_names = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
@@ -2043,7 +2040,9 @@ exports.create_docker_vm = function (t) {
 
 
 exports.wait_provisioned_docker_job = function (t) {
-    waitForValue(jobLocation, 'execution', 'succeeded', function (err) {
+    waitForValue(jobLocation, 'execution', 'succeeded', {
+        client: client
+    }, function (err) {
         common.ifError(t, err);
         t.done();
     });
diff --git a/test/vms.volumes.test.js b/test/vms.volumes.test.js
new file mode 100644
index 0000000..e2f3cc9
--- /dev/null
+++ b/test/vms.volumes.test.js
@@ -0,0 +1,322 @@
+/*
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ */
+
+/*
+ * Copyright 2017 Joyent, Inc.
+ */
+
+var assert = require('assert-plus');
+var vasync = require('vasync');
+
+var common = require('./common');
+var testUuid = require('./lib/uuid');
+
+var waitForValue = common.waitForValue;
+
+var client;
+
+var ADMIN_USER_UUID = common.config.ufdsAdminUuid;
+var ADMIN_FABRIC_NETWORK;
+var VMAPI_ORIGIN_IMAGE_UUID;
+var SERVER;
+var TEST_VOLUMES_NAME_PREFIX = 'vmapitest-volumes-';
+var VOLAPI_SERVICE_PRESENT = false;
+
+function testIfVolapiPresent(testFunc) {
+    assert.func(testFunc, 'testFunc');
+
+    return function testWrapper(t) {
+        if (!VOLAPI_SERVICE_PRESENT) {
+            t.ok(true, 'VOLAPI core service not present, skipping tests');
+            t.done();
+            return;
+        }
+
+        testFunc(t);
+    };
+}
+
+function getVmPayloadTemplate() {
+    return {
+        alias: 'vmapitest-volumes-' + testUuid.generateShortUuid(),
+        owner_uuid: ADMIN_USER_UUID,
+        image_uuid: VMAPI_ORIGIN_IMAGE_UUID,
+        server_uuid: SERVER.uuid,
+        networks: [ { uuid: ADMIN_FABRIC_NETWORK.uuid } ],
+        brand: 'joyent-minimal',
+        billing_id: '00000000-0000-0000-0000-000000000000',
+        ram: 64,
+        quota: 10,
+        /*
+         * Not setting a cpu_cap here would break the ability for Triton to
+         * provision any VM with a non-null cpu_cap, since a mix of capped and
+         * cap-less VMs is not allowed by the allocation system.
+         */
+        cpu_cap: 10
+    };
+}
+
+exports.setUp = function (callback) {
+    common.setUp(function (err, _client) {
+        assert.ifError(err);
+        assert.ok(_client, 'restify client');
+        client = _client;
+        callback();
+    });
+};
+
+exports.check_volapi_instance_present = function (t) {
+    client.get('/vms?tag.smartdc_role=volapi&state=running',
+        function onListVolapiVms(vmsListErr, req, res, volapiVms) {
+            if (vmsListErr || !volapiVms || volapiVms.length === 0) {
+                VOLAPI_SERVICE_PRESENT = false;
+            } else {
+                VOLAPI_SERVICE_PRESENT = true;
+            }
+            t.done();
+        });
+};
+
+exports.get_vmapi_origin_image = testIfVolapiPresent(function (t) {
+    var vmapiVmImgUuid;
+
+    vasync.pipeline({funcs: [
+        function getVmapiImg(ctx, next) {
+            client.get('/vms?alias=vmapi&tag.smartdc_type=core',
+                function onListVms(listVmsErr, req, res, vms) {
+                    t.ifError(listVmsErr);
+                    t.ok(vms, 'listing VMAPI core VMs should result in a ' +
+                        'non-empty response');
+
+                    vmapiVmImgUuid = vms[0].image_uuid;
+
+                    next();
+                });
+        },
+
+        function getOrigImg(ctx, next) {
+            client.imgapi.get('/images/' + vmapiVmImgUuid,
+                function onGetImage(getImgErr, req, res, image) {
+                    t.ifError(getImgErr);
+                    t.ok(image, 'Listing VMAPI\'s VM\'s image should result ' +
+                        'in a non-empty response');
+
+                    VMAPI_ORIGIN_IMAGE_UUID = image.origin;
+
+                    next();
+                });
+        }
+    ]}, function onVmapiOriginImgRetrieved(err) {
+        t.ifError(err);
+        t.done();
+    });
+});
+
+exports.get_admin_fabric_network = testIfVolapiPresent(function (t) {
+    client.napi.get('/networks?owner_uuid=' + ADMIN_USER_UUID + '&fabric=true',
+        function (err, req, res, networks) {
+        common.ifError(t, err);
+        t.equal(res.statusCode, 200, '200 OK');
+        t.ok(networks, 'networks is set');
+        t.ok(Array.isArray(networks), 'networks is Array');
+        t.ok(networks.length === 1, '1 network found');
+
+        ADMIN_FABRIC_NETWORK = networks[0];
+        t.ok(ADMIN_FABRIC_NETWORK,
+            'Admin fabric network should have been found');
+
+        t.done();
+    });
+});
+
+exports.find_headnode = testIfVolapiPresent(function (t) {
+    client.cnapi.get('/servers', function (err, req, res, servers) {
+        common.ifError(t, err);
+        t.equal(res.statusCode, 200, '200 OK');
+        t.ok(servers, 'servers is set');
+        t.ok(Array.isArray(servers), 'servers is Array');
+        for (var i = 0; i < servers.length; i++) {
+            if (servers[i].headnode === true) {
+                SERVER = servers[i];
+                break;
+            }
+        }
+        t.ok(SERVER, 'server found');
+        t.done();
+    });
+});
+
+exports.create_vm_invalid_volumes_params = testIfVolapiPresent(function (t) {
+    var INVALID_VOLUMES_PARAMS = [
+        null,
+        [],
+        [ {} ],
+        [ {foo: 'bar'} ],
+        [ {name: '---,*;'} ],
+        [ {name: 'foo', mountpoint: 'invalidmountpoint'} ],
+        [ {name: 'foo', mountpoint: '/bar', mode: 'invalidmode'} ]
+    ];
+
+    vasync.forEachPipeline({
+        func: function provisionVmWithInvalidVol(volumesParam, next) {
+            var vmPayload = getVmPayloadTemplate();
+
+            vmPayload.volumes = volumesParam;
+
+            client.post({
+                path: '/vms'
+            }, vmPayload, function onVmCreated(createVmErr, req, res, body) {
+                var expectedResStatusCode = 409;
+
+                t.ok(createVmErr, 'VM creation should error');
+                t.equal(res.statusCode, expectedResStatusCode,
+                    'HTTP status code should be ' + expectedResStatusCode);
+                next();
+            });
+        },
+        inputs: INVALID_VOLUMES_PARAMS
+    }, function onAllInvalidVolParamsTestsDone(err) {
+        t.done();
+    });
+});
+
+exports.create_vm_with_valid_volumes_params = testIfVolapiPresent(function (t) {
+    var INVALID_VOLUMES_PARAMS = [
+        [
+            {
+                name: TEST_VOLUMES_NAME_PREFIX + testUuid.generateShortUuid(),
+                mountpoint: '/bar'
+            }
+        ],
+        [
+            {
+                name: TEST_VOLUMES_NAME_PREFIX + testUuid.generateShortUuid(),
+                mountpoint: '/bar',
+                mode: 'ro'
+            }
+        ],
+        [
+            {
+                name: TEST_VOLUMES_NAME_PREFIX + testUuid.generateShortUuid(),
+                mountpoint: '/bar',
+                mode: 'rw'
+            }
+        ]
+    ];
+
+    vasync.forEachPipeline({
+        func: function provisionVmWithValidVol(volumesParam, nextVolParam) {
+            var vmPayload = getVmPayloadTemplate();
+            var vmProvisioningJobUuid;
+            var vmUuid;
+            var volumeName = volumesParam[0].name;
+            var volumeUuid;
+
+            vmPayload.volumes = volumesParam;
+
+            vasync.pipeline({funcs: [
+                function createVm(ctx, next) {
+                    client.post({
+                        path: '/vms'
+                    }, vmPayload,
+                        function onVmCreated(createVmErr, req, res, body) {
+                            var expectedResStatusCode = 202;
+
+                            if (body) {
+                                vmProvisioningJobUuid = body.job_uuid;
+                                vmUuid = body.vm_uuid;
+                            }
+
+                            t.ifError(createVmErr,
+                                'VM creation should not error');
+                            t.equal(res.statusCode, expectedResStatusCode,
+                                'HTTP status code should be ' +
+                                    expectedResStatusCode);
+
+                            next();
+                        });
+                },
+
+                function waitForVmProvisioned(ctx, next) {
+                    if (!vmProvisioningJobUuid) {
+                        next();
+                        return;
+                    }
+
+                    waitForValue('/jobs/' + vmProvisioningJobUuid, 'execution',
+                        'succeeded',
+                        { client: client, timeout: 4 * 60 * 1000 },
+                        function onVmProvisioned(provisionErr) {
+                            t.ifError(provisionErr,
+                                'VM should be provisioned successfully');
+
+                            next();
+                        });
+                },
+
+                function getVolumeUuid(ctx, next) {
+                    client.volapi.get('/volumes?name=' + volumeName,
+                        function onListVolumes(listVolErr, req, res, body) {
+                            t.ifError(listVolErr, 'Listing volumes with name ' +
+                                volumeName + ' should succeed');
+                            if (body) {
+                                volumeUuid = body[0].uuid;
+                            }
+
+                            next();
+                        });
+                },
+
+                function checkVolumeProvisioned(ctx, next) {
+                    waitForValue('/volumes/' + volumeUuid, 'state', 'ready',
+                        { client: client.volapi },
+                        function onVolCreated(volCreatErr) {
+                            t.ifError(volCreatErr,
+                                'VM should be provisioned successfully');
+
+                            next();
+                        });
+                },
+
+                function deleteVm(ctx, next) {
+                    client.del({
+                        path: '/vms/' + vmUuid
+                    }, function onVmDeleted(vmDelErr) {
+                        t.ifError(vmDelErr, 'Deleting VM with UUID ' + vmUuid +
+                            'should succeed');
+
+                        next();
+                    });
+                },
+
+                function waitForVmDeleted(ctx, next) {
+                    waitForValue('/vms/' + vmUuid, 'state', 'destroyed', {
+                        client: client
+                    }, function onVmDeleted(vmDelErr) {
+                        t.ifError(vmDelErr,
+                            'VM should have been deleted successfully');
+
+                        next();
+                    });
+                },
+
+                function deleteVolume(ctx, next) {
+                    client.volapi.del('/volumes/' + volumeUuid + '?force=true',
+                        function onVolDeleted(volDelErr) {
+                            t.ifError(volDelErr);
+
+                            next();
+                        });
+                }
+            ]}, function onTestValidVolParamDone(err) {
+                nextVolParam();
+            });
+        },
+        inputs: INVALID_VOLUMES_PARAMS
+    }, function onAllInvalidVolParamsTestsDone(err) {
+        t.done();
+    });
+});
\ No newline at end of file
diff --git a/tools/jsl.node.conf b/tools/jsl.node.conf
index 8bf9239..b0e4d12 100644
--- a/tools/jsl.node.conf
+++ b/tools/jsl.node.conf
@@ -137,6 +137,8 @@
 +define sapiUrl
 +define vmapiUrl
 +define urlModule
++define volapiUrl
++define uuid
 
 ### JavaScript Version
 # To change the default JavaScript version:
